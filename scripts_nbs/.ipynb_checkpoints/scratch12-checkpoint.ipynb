{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn import metrics\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import shap\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/sa/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning:\n",
      "\n",
      "The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv.zip')\n",
    "y = train['target']\n",
    "test = pd.read_csv('../data/test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = pd.read_csv('../cache/X1_qt.csv')\n",
    "X2 = pd.read_csv('../cache/X2_qt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.83072\tvalid_1's auc: 0.819951\n",
      "[2000]\ttraining's auc: 0.860805\tvalid_1's auc: 0.850025\n",
      "[3000]\ttraining's auc: 0.875368\tvalid_1's auc: 0.863899\n",
      "[4000]\ttraining's auc: 0.884027\tvalid_1's auc: 0.871963\n",
      "[5000]\ttraining's auc: 0.889954\tvalid_1's auc: 0.877426\n",
      "[6000]\ttraining's auc: 0.894411\tvalid_1's auc: 0.881449\n",
      "[7000]\ttraining's auc: 0.897707\tvalid_1's auc: 0.88442\n",
      "[8000]\ttraining's auc: 0.900292\tvalid_1's auc: 0.886616\n",
      "[9000]\ttraining's auc: 0.902348\tvalid_1's auc: 0.888456\n",
      "[10000]\ttraining's auc: 0.903978\tvalid_1's auc: 0.889909\n",
      "[11000]\ttraining's auc: 0.905334\tvalid_1's auc: 0.891054\n",
      "[12000]\ttraining's auc: 0.906463\tvalid_1's auc: 0.892073\n",
      "[13000]\ttraining's auc: 0.907408\tvalid_1's auc: 0.892858\n",
      "[14000]\ttraining's auc: 0.90823\tvalid_1's auc: 0.893513\n",
      "[15000]\ttraining's auc: 0.908936\tvalid_1's auc: 0.894094\n",
      "[16000]\ttraining's auc: 0.909578\tvalid_1's auc: 0.894599\n",
      "[17000]\ttraining's auc: 0.910138\tvalid_1's auc: 0.895008\n",
      "[18000]\ttraining's auc: 0.910633\tvalid_1's auc: 0.895359\n",
      "[19000]\ttraining's auc: 0.911074\tvalid_1's auc: 0.895645\n",
      "[20000]\ttraining's auc: 0.911465\tvalid_1's auc: 0.895892\n",
      "[21000]\ttraining's auc: 0.911815\tvalid_1's auc: 0.896092\n",
      "[22000]\ttraining's auc: 0.91214\tvalid_1's auc: 0.896265\n",
      "[23000]\ttraining's auc: 0.912433\tvalid_1's auc: 0.896425\n",
      "[24000]\ttraining's auc: 0.912711\tvalid_1's auc: 0.896559\n",
      "[25000]\ttraining's auc: 0.912973\tvalid_1's auc: 0.896705\n",
      "[26000]\ttraining's auc: 0.913215\tvalid_1's auc: 0.896813\n",
      "[27000]\ttraining's auc: 0.913439\tvalid_1's auc: 0.896913\n",
      "[28000]\ttraining's auc: 0.913659\tvalid_1's auc: 0.896995\n",
      "[29000]\ttraining's auc: 0.91386\tvalid_1's auc: 0.897066\n",
      "[30000]\ttraining's auc: 0.914071\tvalid_1's auc: 0.897149\n",
      "[31000]\ttraining's auc: 0.914265\tvalid_1's auc: 0.897208\n",
      "[32000]\ttraining's auc: 0.914445\tvalid_1's auc: 0.897262\n",
      "[33000]\ttraining's auc: 0.914622\tvalid_1's auc: 0.897304\n",
      "[34000]\ttraining's auc: 0.914789\tvalid_1's auc: 0.897345\n",
      "[35000]\ttraining's auc: 0.914947\tvalid_1's auc: 0.897389\n",
      "[36000]\ttraining's auc: 0.915095\tvalid_1's auc: 0.89741\n",
      "[37000]\ttraining's auc: 0.915243\tvalid_1's auc: 0.897439\n",
      "[38000]\ttraining's auc: 0.915387\tvalid_1's auc: 0.89746\n",
      "[39000]\ttraining's auc: 0.915529\tvalid_1's auc: 0.897483\n",
      "[40000]\ttraining's auc: 0.915671\tvalid_1's auc: 0.897497\n",
      "[41000]\ttraining's auc: 0.915806\tvalid_1's auc: 0.8975\n",
      "[42000]\ttraining's auc: 0.915943\tvalid_1's auc: 0.897523\n",
      "[43000]\ttraining's auc: 0.916073\tvalid_1's auc: 0.89753\n",
      "[44000]\ttraining's auc: 0.916203\tvalid_1's auc: 0.897537\n",
      "[45000]\ttraining's auc: 0.916326\tvalid_1's auc: 0.897549\n",
      "[46000]\ttraining's auc: 0.916452\tvalid_1's auc: 0.897556\n",
      "[47000]\ttraining's auc: 0.916568\tvalid_1's auc: 0.897575\n",
      "[48000]\ttraining's auc: 0.916683\tvalid_1's auc: 0.897571\n",
      "Early stopping, best iteration is:\n",
      "[47946]\ttraining's auc: 0.916677\tvalid_1's auc: 0.897577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.3,\n",
       "        importance_type='split', learning_rate=0.02, max_depth=-1,\n",
       "        metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
       "        min_split_gain=0.0, n_estimators=999999, n_jobs=-1, num_leaves=2,\n",
       "        objective='binary', random_state=None, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier(\n",
    "                 max_depth=-1,\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate=0.02,\n",
    "                 colsample_bytree=0.3,\n",
    "                 num_leaves=2,\n",
    "                 metric='auc',\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1, y, test_size=0.2, stratify=y)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0321\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_81\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 82.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0262\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_139\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 83.96%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0235\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_12\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 85.78%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0197\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_26\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.19%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0189\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_110\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.22%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0189\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_53\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.57%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0182\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_6\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.63%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0181\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_174\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.50%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0164\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_22\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.81%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0158\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_146\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.12%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0153\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_80\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.16%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0152\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_76\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.27%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0150\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_99\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.37%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0148\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_21\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.52%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0145\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_166\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.01%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0137\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_165\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.07%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0136\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_109\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.19%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0134\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_2\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.26%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0132\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_78\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.26%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0132\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_133\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.35%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0131\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_0\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.42%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0129\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_44\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0125\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_13\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.67%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0125\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_198\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.76%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0123\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_148\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.87%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0122\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_190\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.02%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0119\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_40\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.02%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0119\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_179\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.53%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0111\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_108\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.72%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0107\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_34\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.78%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0106\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_1\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.82%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0106\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_170\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.83%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0106\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_154\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.84%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0105\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_94\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.02%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0102\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_164\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0102\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_33\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.05%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0102\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_92\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.30%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0098\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_177\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.32%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0098\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_9\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.44%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0096\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_184\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 91.44%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 160 more &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_weights(model, targets=[0, 1], feature_names=list(X1.columns), top=40, feature_filter=lambda x: x != '<BIAS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:46<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "cols = [i for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i][:30]\n",
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "\n",
    "len_cols = len(cols)\n",
    "for i in tqdm(range(len_cols)):\n",
    "    for j in range(i+1, len_cols):\n",
    "        col1 = cols[i]\n",
    "        col2 = cols[j]\n",
    "        \n",
    "        col3 = col1 + '_plus_' + col2\n",
    "        X3[col3] = X3[col1] + X3[col2]\n",
    "        X4[col3] = X4[col1] + X4[col2]\n",
    "        \n",
    "        col4 = col1 + '_minus_' + col2\n",
    "        X3[col4] = X3[col1] - X3[col2]\n",
    "        X4[col4] = X4[col1] - X4[col2]\n",
    "        \n",
    "        col5 = col1 + '_mult_' + col2\n",
    "        X3[col5] = X3[col1] * X3[col2]\n",
    "        X4[col5] = X4[col1] * X4[col2]\n",
    "        \n",
    "        col6 = col1 + '_div_' + col2\n",
    "        X3[col6] = X3[col1] / X3[col2]\n",
    "        X4[col6] = X4[col1] / X4[col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 1940)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params, folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X.loc[train_index], X.loc[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "            \n",
    "#             model = lgb.train(params,\n",
    "#                     train_data,\n",
    "#                     num_boost_round=20000,\n",
    "#                     valid_sets = [train_data, valid_data],\n",
    "#                     verbose_eval=1000,\n",
    "#                     early_stopping_rounds = 1000)\n",
    "#             params <- list(objective = \"binary\", \n",
    "#                boost=\"gbdt\",\n",
    "#                metric=\"auc\",\n",
    "#                boost_from_average=\"false\",\n",
    "#                num_threads=28,\n",
    "#                learning_rate = 0.01,\n",
    "#                num_leaves = 13,\n",
    "#                max_depth=-1,\n",
    "#                tree_learner = \"serial\",\n",
    "#                feature_fraction = 0.05,\n",
    "#                bagging_freq = 5,\n",
    "#                bagging_fraction = 0.4,\n",
    "#                min_data_in_leaf = 80,\n",
    "#                min_sum_hessian_in_leaf = 10.0,\n",
    "#                verbosity = 1)\n",
    "# param = {\n",
    "#     'bagging_freq': 5,          'bagging_fraction': 0.335,   'boost_from_average':'false',   'boost': 'gbdt',\n",
    "#     'feature_fraction': 0.041,   'learning_rate': 0.0083,     'max_depth': -1,                'metric':'auc',\n",
    "#     'min_data_in_leaf': 80,     'min_sum_hessian_in_leaf': 10.0,'num_leaves': 13,           'num_threads': 8,\n",
    "#     'tree_learner': 'serial',   'objective': 'binary',      'verbosity': 1\n",
    "# }\n",
    "\n",
    "# param = {\n",
    "#     'bagging_freq': 5,          'bagging_fraction': 0.38,   'boost_from_average':'false',   'boost': 'gbdt',\n",
    "#     'feature_fraction': 0.045,   'learning_rate': 0.0105,     'max_depth': -1,                'metric':'auc',\n",
    "#     'min_data_in_leaf': 80,     'min_sum_hessian_in_leaf': 10.0,'num_leaves': 13,           'num_threads': 8,\n",
    "#     'tree_learner': 'serial',   'objective': 'binary',      'verbosity': 1\n",
    "# }\n",
    "            model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#             model = lgb.LGBMClassifier(\n",
    "#                  boost=\"gbdt\",\n",
    "#                  metric=\"auc\",\n",
    "#                  boost_from_average=\"false\",\n",
    "#                  objective='binary', \n",
    "#                  n_jobs=-1)\n",
    "#             model = lgb.LGBMClassifier(\n",
    "#                  max_depth=-1,\n",
    "#                  n_estimators=999999,\n",
    "#                  learning_rate=0.02,\n",
    "#                  colsample_bytree=0.3,\n",
    "#                  num_leaves=2,\n",
    "#                  metric='auc',\n",
    "# #                  random_seed = 42 + params,\n",
    "#                  objective='binary', \n",
    "#                  n_jobs=-1)\n",
    "            \n",
    "            model.fit(X_train, y_train, \n",
    "                eval_set=[(X_valid, y_valid)],\n",
    "                verbose=1000, \n",
    "                early_stopping_rounds=3000)\n",
    "#             y_pred_valid = model.predict(X_valid)\n",
    "#             y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "            \n",
    "            y_pred_valid = model.predict_proba(X_valid)[:,1]       \n",
    "            y_pred = model.predict_proba(X_test)[:,1]\n",
    "        if model_type == 'xgb':\n",
    "#             print('1')\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_train.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_train.columns)\n",
    "#             print('2')\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.XGBClassifier(max_depth=6,\n",
    "                                      max_leaves=13,\n",
    "                                      booster='gbtree',\n",
    "#                                       feature_fraction = 0.05,\n",
    "#                                       bagging_freq = 5,\n",
    "                                      subsample = 0.335,\n",
    "                                      min_data_in_leaf = 80,\n",
    "                                      min_child_weight = 10.0,\n",
    "                                      n_estimators=999999,\n",
    "                                      colsample_bytree=0.041,\n",
    "                                      learning_rate=0.0083,\n",
    "                                      objective='binary:logistic', \n",
    "                                      eval_metric='auc',\n",
    "#                                       seed = 42 + params,\n",
    "                                      n_jobs=-1)\n",
    "#             model = xgb.XGBClassifier(max_depth=2,\n",
    "#                                       n_estimators=999999,\n",
    "#                                       colsample_bytree=0.3,\n",
    "#                                       learning_rate=0.02,\n",
    "#                                       objective='binary:logistic', \n",
    "#                                       eval_metric='auc',\n",
    "# #                                       seed = 42 + params,\n",
    "#                                       n_jobs=-1)\n",
    "     \n",
    "            model.fit(X_train, y_train, \n",
    "                eval_set=[(X_valid, y_valid)],\n",
    "                verbose=1000, \n",
    "                early_stopping_rounds=1000)\n",
    "              \n",
    "    \n",
    "#             model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n",
    "#             print('2a')\n",
    "#             y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n",
    "#             y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n",
    "#             print('3')\n",
    "            y_pred_valid = model.predict_proba(X_valid)[:,1]\n",
    "            y_pred = model.predict_proba(X_test)[:,1]\n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_valid = model.predict_proba(X_valid)[:,1].reshape(-1,)\n",
    "            score = roc_auc_score(y_valid, y_pred_valid)\n",
    "            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n",
    "            # print('')\n",
    "            \n",
    "            y_pred = model.predict_proba(X_test)[:, 1]\n",
    "        if model_type == 'glm':\n",
    "            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "            model_results = model.fit()\n",
    "            model_results.predict(X_test)\n",
    "            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n",
    "            score = roc_auc_score(y_valid, y_pred_valid)\n",
    "            \n",
    "            y_pred = model_results.predict(X_test)\n",
    "            \n",
    "        if model_type == 'cat':\n",
    "#             model = CatBoostClassifier(iterations=20000, learning_rate=0.05, loss_function='Logloss',  eval_metric='AUC', **params)\n",
    "#             model = CatBoostClassifier(iterations=999999,\n",
    "#                                   max_depth=2,\n",
    "#                                   learning_rate=0.02,\n",
    "#                                   colsample_bylevel=0.03,\n",
    "#                                   eval_metric='AUC',\n",
    "#                                   objective=\"Logloss\")\n",
    "# max_depth=6,\n",
    "#                                       max_leaves=13,\n",
    "#                                       booster='gbtree',\n",
    "# #                                       feature_fraction = 0.05,\n",
    "# #                                       bagging_freq = 5,\n",
    "#                                       subsample = 0.4,\n",
    "#                                       min_data_in_leaf = 80,\n",
    "#                                       min_child_weight = 10.0,\n",
    "#                                       n_estimators=999999,\n",
    "#                                       colsample_bytree=0.05,\n",
    "#                                       learning_rate=0.01,\n",
    "#                                       objective='binary:logistic', \n",
    "#                                       eval_metric='auc',\n",
    "# #                                       seed = 42 + params,\n",
    "#                                       n_jobs=-1)\n",
    "            model = CatBoostClassifier(iterations=999999,\n",
    "                                  max_depth=4,\n",
    "                                  learning_rate=0.01,\n",
    "#                                   subsample=0.4,\n",
    "                                  l2_leaf_reg = 10.0,     \n",
    "                                  colsample_bylevel=0.05,\n",
    "#                                   thread_count=np.nan,\n",
    "                                  eval_metric='AUC',\n",
    "                                  objective=\"Logloss\")\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[12, 91], \n",
    "                      use_best_model=True, verbose=1000,early_stopping_rounds=1000)\n",
    "\n",
    "            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n",
    "            y_pred = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(roc_auc_score(y_valid, y_pred_valid))\n",
    "        if averaging == 'usual':\n",
    "            prediction += y_pred\n",
    "        elif averaging == 'rank':\n",
    "            prediction += pd.Series(y_pred).rank().values  \n",
    "        \n",
    "        if (model_type == 'lgb') & (plot_feature_importance):\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importance()\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if (model_type == 'lgb') & (plot_feature_importance):\n",
    "        feature_importance[\"importance\"] /= n_fold\n",
    "        if plot_feature_importance:\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance\n",
    "        return oof, prediction, scores\n",
    "    \n",
    "    else:\n",
    "        return oof, prediction, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 10\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=123)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Mar  4 18:09:23 2019\n",
      "0:\ttest: 0.5303807\tbest: 0.5303807 (0)\ttotal: 115ms\tremaining: 1d 7h 48m 30s\n",
      "1000:\ttest: 0.8402979\tbest: 0.8402979 (1000)\ttotal: 35.5s\tremaining: 9h 50m 40s\n",
      "2000:\ttest: 0.8669733\tbest: 0.8669736 (1999)\ttotal: 1m 7s\tremaining: 9h 18m 7s\n",
      "3000:\ttest: 0.8790965\tbest: 0.8790965 (3000)\ttotal: 1m 37s\tremaining: 9h 37s\n",
      "4000:\ttest: 0.8854599\tbest: 0.8854599 (4000)\ttotal: 2m 8s\tremaining: 8h 51m 12s\n",
      "5000:\ttest: 0.8892917\tbest: 0.8892932 (4999)\ttotal: 2m 37s\tremaining: 8h 43m 39s\n",
      "6000:\ttest: 0.8918151\tbest: 0.8918151 (6000)\ttotal: 3m 8s\tremaining: 8h 40m 28s\n",
      "7000:\ttest: 0.8934737\tbest: 0.8934849 (6992)\ttotal: 3m 41s\tremaining: 8h 43m 50s\n",
      "8000:\ttest: 0.8946333\tbest: 0.8946376 (7994)\ttotal: 4m 16s\tremaining: 8h 49m 56s\n",
      "9000:\ttest: 0.8957269\tbest: 0.8957364 (8997)\ttotal: 4m 52s\tremaining: 8h 56m 54s\n",
      "10000:\ttest: 0.8963235\tbest: 0.8963288 (9982)\ttotal: 5m 30s\tremaining: 9h 5m 44s\n",
      "11000:\ttest: 0.8968384\tbest: 0.8968407 (10999)\ttotal: 6m 7s\tremaining: 9h 10m 22s\n",
      "12000:\ttest: 0.8971994\tbest: 0.8972020 (11996)\ttotal: 6m 45s\tremaining: 9h 17m 1s\n",
      "13000:\ttest: 0.8975209\tbest: 0.8975236 (12981)\ttotal: 7m 26s\tremaining: 9h 24m 49s\n",
      "14000:\ttest: 0.8978415\tbest: 0.8978457 (13990)\ttotal: 8m 5s\tremaining: 9h 30m 24s\n",
      "15000:\ttest: 0.8980137\tbest: 0.8980206 (14996)\ttotal: 8m 45s\tremaining: 9h 34m 53s\n",
      "16000:\ttest: 0.8981823\tbest: 0.8981914 (15942)\ttotal: 9m 26s\tremaining: 9h 40m 12s\n",
      "17000:\ttest: 0.8983367\tbest: 0.8983367 (17000)\ttotal: 10m 7s\tremaining: 9h 45m 15s\n",
      "18000:\ttest: 0.8984898\tbest: 0.8985039 (17944)\ttotal: 10m 46s\tremaining: 9h 47m 59s\n",
      "19000:\ttest: 0.8986254\tbest: 0.8986333 (18981)\ttotal: 11m 26s\tremaining: 9h 50m 47s\n",
      "20000:\ttest: 0.8987137\tbest: 0.8987189 (19872)\ttotal: 12m 8s\tremaining: 9h 54m 48s\n",
      "21000:\ttest: 0.8987635\tbest: 0.8987694 (20962)\ttotal: 12m 51s\tremaining: 9h 59m 2s\n",
      "22000:\ttest: 0.8988153\tbest: 0.8988236 (21933)\ttotal: 13m 32s\tremaining: 10h 1m 42s\n",
      "23000:\ttest: 0.8988238\tbest: 0.8988531 (22487)\ttotal: 14m 13s\tremaining: 10h 4m 10s\n",
      "24000:\ttest: 0.8988520\tbest: 0.8988687 (23712)\ttotal: 14m 55s\tremaining: 10h 6m 45s\n",
      "25000:\ttest: 0.8988908\tbest: 0.8989129 (24839)\ttotal: 15m 33s\tremaining: 10h 7m 3s\n",
      "26000:\ttest: 0.8989876\tbest: 0.8989937 (25831)\ttotal: 16m 14s\tremaining: 10h 8m 35s\n",
      "27000:\ttest: 0.8990396\tbest: 0.8990486 (26825)\ttotal: 16m 54s\tremaining: 10h 9m 1s\n",
      "28000:\ttest: 0.8990489\tbest: 0.8990850 (27681)\ttotal: 17m 34s\tremaining: 10h 9m 52s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8990850317\n",
      "bestIteration = 27681\n",
      "\n",
      "Shrink model to first 27682 iterations.\n",
      "Fold 1 started at Mon Mar  4 18:30:39 2019\n",
      "0:\ttest: 0.5326659\tbest: 0.5326659 (0)\ttotal: 60.8ms\tremaining: 16h 53m 18s\n",
      "1000:\ttest: 0.8338390\tbest: 0.8338390 (1000)\ttotal: 33.6s\tremaining: 9h 19m 6s\n",
      "2000:\ttest: 0.8614484\tbest: 0.8614484 (2000)\ttotal: 1m 6s\tremaining: 9h 14m 28s\n",
      "3000:\ttest: 0.8734932\tbest: 0.8734932 (3000)\ttotal: 1m 41s\tremaining: 9h 21m 3s\n",
      "4000:\ttest: 0.8803446\tbest: 0.8803515 (3999)\ttotal: 2m 17s\tremaining: 9h 29m 43s\n",
      "5000:\ttest: 0.8842283\tbest: 0.8842283 (5000)\ttotal: 2m 53s\tremaining: 9h 35m 2s\n",
      "6000:\ttest: 0.8871041\tbest: 0.8871041 (6000)\ttotal: 3m 30s\tremaining: 9h 39m 49s\n",
      "7000:\ttest: 0.8890539\tbest: 0.8890575 (6998)\ttotal: 4m 7s\tremaining: 9h 43m 56s\n",
      "8000:\ttest: 0.8904426\tbest: 0.8904441 (7999)\ttotal: 4m 43s\tremaining: 9h 46m 6s\n",
      "9000:\ttest: 0.8914827\tbest: 0.8914827 (9000)\ttotal: 5m 20s\tremaining: 9h 47m 42s\n",
      "10000:\ttest: 0.8922378\tbest: 0.8922415 (9993)\ttotal: 5m 57s\tremaining: 9h 49m 31s\n",
      "11000:\ttest: 0.8927032\tbest: 0.8927032 (11000)\ttotal: 6m 34s\tremaining: 9h 50m 52s\n",
      "12000:\ttest: 0.8932438\tbest: 0.8932438 (12000)\ttotal: 7m 12s\tremaining: 9h 53m 22s\n",
      "13000:\ttest: 0.8936321\tbest: 0.8936392 (12968)\ttotal: 7m 49s\tremaining: 9h 54m 22s\n",
      "14000:\ttest: 0.8939249\tbest: 0.8939273 (13997)\ttotal: 8m 27s\tremaining: 9h 55m 22s\n",
      "15000:\ttest: 0.8942155\tbest: 0.8942155 (15000)\ttotal: 9m 4s\tremaining: 9h 55m 38s\n",
      "16000:\ttest: 0.8944497\tbest: 0.8944534 (15972)\ttotal: 9m 41s\tremaining: 9h 56m 30s\n",
      "17000:\ttest: 0.8946388\tbest: 0.8946485 (16987)\ttotal: 10m 20s\tremaining: 9h 57m 44s\n",
      "18000:\ttest: 0.8948186\tbest: 0.8948219 (17975)\ttotal: 10m 57s\tremaining: 9h 57m 54s\n",
      "19000:\ttest: 0.8950121\tbest: 0.8950130 (18923)\ttotal: 11m 35s\tremaining: 9h 58m 36s\n",
      "20000:\ttest: 0.8951200\tbest: 0.8951212 (19971)\ttotal: 12m 12s\tremaining: 9h 58m 24s\n",
      "21000:\ttest: 0.8951928\tbest: 0.8951977 (20987)\ttotal: 12m 51s\tremaining: 9h 59m 5s\n",
      "22000:\ttest: 0.8953473\tbest: 0.8953522 (21987)\ttotal: 13m 30s\tremaining: 10h 23s\n",
      "23000:\ttest: 0.8954170\tbest: 0.8954203 (22958)\ttotal: 14m 6s\tremaining: 9h 59m 25s\n",
      "24000:\ttest: 0.8955645\tbest: 0.8955732 (23933)\ttotal: 14m 44s\tremaining: 9h 59m 32s\n",
      "25000:\ttest: 0.8956516\tbest: 0.8956550 (24998)\ttotal: 15m 21s\tremaining: 9h 59m 6s\n",
      "26000:\ttest: 0.8956889\tbest: 0.8956944 (25850)\ttotal: 15m 58s\tremaining: 9h 58m 38s\n",
      "27000:\ttest: 0.8956343\tbest: 0.8957009 (26034)\ttotal: 16m 36s\tremaining: 9h 58m 11s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8957009185\n",
      "bestIteration = 26034\n",
      "\n",
      "Shrink model to first 26035 iterations.\n",
      "Fold 2 started at Mon Mar  4 18:50:12 2019\n",
      "0:\ttest: 0.5108648\tbest: 0.5108648 (0)\ttotal: 77.6ms\tremaining: 21h 33m 35s\n",
      "1000:\ttest: 0.8372657\tbest: 0.8372657 (1000)\ttotal: 33.7s\tremaining: 9h 21m 19s\n",
      "2000:\ttest: 0.8654698\tbest: 0.8654698 (2000)\ttotal: 1m 11s\tremaining: 9h 53m 32s\n",
      "3000:\ttest: 0.8770907\tbest: 0.8770907 (3000)\ttotal: 1m 50s\tremaining: 10h 13m 5s\n",
      "4000:\ttest: 0.8833507\tbest: 0.8833595 (3997)\ttotal: 2m 29s\tremaining: 10h 19m 13s\n",
      "5000:\ttest: 0.8872862\tbest: 0.8872862 (5000)\ttotal: 3m 7s\tremaining: 10h 20m 32s\n",
      "6000:\ttest: 0.8897203\tbest: 0.8897203 (6000)\ttotal: 3m 44s\tremaining: 10h 18m 29s\n",
      "7000:\ttest: 0.8913588\tbest: 0.8913716 (6990)\ttotal: 4m 22s\tremaining: 10h 19m 33s\n",
      "8000:\ttest: 0.8925984\tbest: 0.8926036 (7997)\ttotal: 5m 1s\tremaining: 10h 22m 5s\n",
      "9000:\ttest: 0.8932464\tbest: 0.8932494 (8999)\ttotal: 5m 40s\tremaining: 10h 24m 8s\n",
      "10000:\ttest: 0.8938879\tbest: 0.8938879 (10000)\ttotal: 6m 18s\tremaining: 10h 25m 4s\n",
      "11000:\ttest: 0.8943877\tbest: 0.8944069 (10965)\ttotal: 6m 58s\tremaining: 10h 27m\n",
      "12000:\ttest: 0.8947911\tbest: 0.8947911 (12000)\ttotal: 7m 37s\tremaining: 10h 28m 20s\n",
      "13000:\ttest: 0.8951570\tbest: 0.8951583 (12996)\ttotal: 8m 17s\tremaining: 10h 29m 42s\n",
      "14000:\ttest: 0.8952692\tbest: 0.8952756 (13866)\ttotal: 8m 55s\tremaining: 10h 29m 2s\n",
      "15000:\ttest: 0.8954682\tbest: 0.8954763 (14918)\ttotal: 9m 33s\tremaining: 10h 27m 21s\n",
      "16000:\ttest: 0.8955705\tbest: 0.8955715 (15992)\ttotal: 10m 12s\tremaining: 10h 27m 40s\n",
      "17000:\ttest: 0.8956826\tbest: 0.8956826 (17000)\ttotal: 10m 51s\tremaining: 10h 27m 57s\n",
      "18000:\ttest: 0.8956550\tbest: 0.8957299 (17219)\ttotal: 11m 31s\tremaining: 10h 28m 22s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8957299384\n",
      "bestIteration = 17219\n",
      "\n",
      "Shrink model to first 17220 iterations.\n",
      "Fold 3 started at Mon Mar  4 19:04:20 2019\n",
      "0:\ttest: 0.5129318\tbest: 0.5129318 (0)\ttotal: 49.2ms\tremaining: 13h 39m 32s\n",
      "1000:\ttest: 0.8347783\tbest: 0.8347901 (999)\ttotal: 32.3s\tremaining: 8h 57m 53s\n",
      "2000:\ttest: 0.8647237\tbest: 0.8647237 (2000)\ttotal: 1m 6s\tremaining: 9h 12m 20s\n",
      "3000:\ttest: 0.8769643\tbest: 0.8769643 (3000)\ttotal: 1m 42s\tremaining: 9h 25m 57s\n",
      "4000:\ttest: 0.8834870\tbest: 0.8834870 (4000)\ttotal: 2m 18s\tremaining: 9h 36m 25s\n",
      "5000:\ttest: 0.8875991\tbest: 0.8876003 (4999)\ttotal: 2m 55s\tremaining: 9h 43m 18s\n",
      "6000:\ttest: 0.8905175\tbest: 0.8905175 (6000)\ttotal: 3m 33s\tremaining: 9h 49m 30s\n",
      "7000:\ttest: 0.8924433\tbest: 0.8924470 (6999)\ttotal: 4m 12s\tremaining: 9h 55m 55s\n",
      "8000:\ttest: 0.8938175\tbest: 0.8938259 (7996)\ttotal: 4m 49s\tremaining: 9h 57m 44s\n",
      "9000:\ttest: 0.8947544\tbest: 0.8947570 (8993)\ttotal: 5m 26s\tremaining: 9h 59m 49s\n",
      "10000:\ttest: 0.8954670\tbest: 0.8954697 (9996)\ttotal: 6m 6s\tremaining: 10h 3m 53s\n",
      "11000:\ttest: 0.8961837\tbest: 0.8961837 (11000)\ttotal: 6m 44s\tremaining: 10h 5m 42s\n",
      "12000:\ttest: 0.8966534\tbest: 0.8966555 (11998)\ttotal: 7m 22s\tremaining: 10h 7m 14s\n",
      "13000:\ttest: 0.8970872\tbest: 0.8970914 (12993)\ttotal: 8m\tremaining: 10h 7m 39s\n",
      "14000:\ttest: 0.8973380\tbest: 0.8973404 (13922)\ttotal: 8m 41s\tremaining: 10h 12m 6s\n",
      "15000:\ttest: 0.8975477\tbest: 0.8975585 (14894)\ttotal: 9m 23s\tremaining: 10h 16m 29s\n",
      "16000:\ttest: 0.8976460\tbest: 0.8976469 (15998)\ttotal: 10m 2s\tremaining: 10h 17m 17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000:\ttest: 0.8977820\tbest: 0.8977915 (16985)\ttotal: 10m 40s\tremaining: 10h 17m\n",
      "18000:\ttest: 0.8979850\tbest: 0.8979850 (18000)\ttotal: 11m 17s\tremaining: 10h 15m 56s\n",
      "19000:\ttest: 0.8981048\tbest: 0.8981087 (18992)\ttotal: 11m 54s\tremaining: 10h 14m 56s\n",
      "20000:\ttest: 0.8982193\tbest: 0.8982324 (19864)\ttotal: 12m 32s\tremaining: 10h 14m 26s\n",
      "21000:\ttest: 0.8982517\tbest: 0.8982923 (20813)\ttotal: 13m 10s\tremaining: 10h 14m 10s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8982923349\n",
      "bestIteration = 20813\n",
      "\n",
      "Shrink model to first 20814 iterations.\n",
      "Fold 4 started at Mon Mar  4 19:20:39 2019\n",
      "0:\ttest: 0.5220267\tbest: 0.5220267 (0)\ttotal: 45.1ms\tremaining: 12h 32m 19s\n",
      "1000:\ttest: 0.8380915\tbest: 0.8380915 (1000)\ttotal: 36.2s\tremaining: 10h 1m 58s\n",
      "2000:\ttest: 0.8681178\tbest: 0.8681178 (2000)\ttotal: 1m 16s\tremaining: 10h 31m 50s\n",
      "3000:\ttest: 0.8811279\tbest: 0.8811279 (3000)\ttotal: 1m 59s\tremaining: 11h 2m 57s\n",
      "4000:\ttest: 0.8884185\tbest: 0.8884185 (4000)\ttotal: 2m 45s\tremaining: 11h 25m 28s\n",
      "5000:\ttest: 0.8927685\tbest: 0.8927685 (5000)\ttotal: 3m 25s\tremaining: 11h 22m 50s\n",
      "6000:\ttest: 0.8957781\tbest: 0.8957781 (6000)\ttotal: 4m 5s\tremaining: 11h 18m 33s\n",
      "7000:\ttest: 0.8979127\tbest: 0.8979148 (6998)\ttotal: 4m 46s\tremaining: 11h 17m 12s\n",
      "8000:\ttest: 0.8994275\tbest: 0.8994337 (7995)\ttotal: 5m 26s\tremaining: 11h 15m 2s\n",
      "9000:\ttest: 0.9005908\tbest: 0.9005908 (9000)\ttotal: 6m 7s\tremaining: 11h 15m 14s\n",
      "10000:\ttest: 0.9014270\tbest: 0.9014385 (9975)\ttotal: 6m 47s\tremaining: 11h 12m 23s\n",
      "11000:\ttest: 0.9020086\tbest: 0.9020086 (11000)\ttotal: 7m 29s\tremaining: 11h 13m 48s\n",
      "12000:\ttest: 0.9024568\tbest: 0.9024568 (12000)\ttotal: 8m 11s\tremaining: 11h 13m 53s\n",
      "13000:\ttest: 0.9028950\tbest: 0.9029026 (12979)\ttotal: 8m 53s\tremaining: 11h 14m 32s\n",
      "14000:\ttest: 0.9032004\tbest: 0.9032110 (13961)\ttotal: 9m 32s\tremaining: 11h 11m 26s\n",
      "15000:\ttest: 0.9034217\tbest: 0.9034217 (15000)\ttotal: 10m 12s\tremaining: 11h 10m 28s\n",
      "16000:\ttest: 0.9035800\tbest: 0.9035860 (15912)\ttotal: 10m 52s\tremaining: 11h 9m 5s\n",
      "17000:\ttest: 0.9037949\tbest: 0.9037974 (16996)\ttotal: 11m 31s\tremaining: 11h 6m 36s\n",
      "18000:\ttest: 0.9039773\tbest: 0.9039812 (17997)\ttotal: 12m 12s\tremaining: 11h 6m 10s\n",
      "19000:\ttest: 0.9040543\tbest: 0.9040625 (18977)\ttotal: 12m 53s\tremaining: 11h 5m 31s\n",
      "20000:\ttest: 0.9040753\tbest: 0.9040754 (19999)\ttotal: 13m 33s\tremaining: 11h 4m 2s\n",
      "21000:\ttest: 0.9040933\tbest: 0.9040974 (20991)\ttotal: 14m 11s\tremaining: 11h 1m 47s\n",
      "22000:\ttest: 0.9041688\tbest: 0.9041938 (21914)\ttotal: 14m 55s\tremaining: 11h 3m 35s\n",
      "23000:\ttest: 0.9041411\tbest: 0.9041995 (22124)\ttotal: 15m 38s\tremaining: 11h 4m 5s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9041994862\n",
      "bestIteration = 22124\n",
      "\n",
      "Shrink model to first 22125 iterations.\n",
      "Fold 5 started at Mon Mar  4 19:39:17 2019\n",
      "0:\ttest: 0.5073563\tbest: 0.5073563 (0)\ttotal: 69.1ms\tremaining: 19h 11m 55s\n",
      "1000:\ttest: 0.8352752\tbest: 0.8352953 (999)\ttotal: 35.9s\tremaining: 9h 57m 23s\n",
      "2000:\ttest: 0.8636349\tbest: 0.8636349 (2000)\ttotal: 1m 7s\tremaining: 9h 24m 50s\n",
      "3000:\ttest: 0.8764072\tbest: 0.8764093 (2999)\ttotal: 1m 42s\tremaining: 9h 27m\n",
      "4000:\ttest: 0.8834993\tbest: 0.8834993 (4000)\ttotal: 2m 21s\tremaining: 9h 45m 46s\n",
      "5000:\ttest: 0.8879084\tbest: 0.8879084 (5000)\ttotal: 3m 1s\tremaining: 10h 1m 17s\n",
      "6000:\ttest: 0.8908424\tbest: 0.8908443 (5999)\ttotal: 3m 40s\tremaining: 10h 9m 25s\n",
      "7000:\ttest: 0.8929760\tbest: 0.8929760 (7000)\ttotal: 4m 21s\tremaining: 10h 18m 51s\n",
      "8000:\ttest: 0.8942168\tbest: 0.8942168 (8000)\ttotal: 5m 1s\tremaining: 10h 23m 24s\n",
      "9000:\ttest: 0.8953765\tbest: 0.8953765 (9000)\ttotal: 5m 43s\tremaining: 10h 31m\n",
      "10000:\ttest: 0.8961315\tbest: 0.8961315 (10000)\ttotal: 6m 24s\tremaining: 10h 34m 58s\n",
      "11000:\ttest: 0.8967761\tbest: 0.8967807 (10992)\ttotal: 7m 6s\tremaining: 10h 39m 45s\n",
      "12000:\ttest: 0.8972896\tbest: 0.8972900 (11989)\ttotal: 7m 47s\tremaining: 10h 40m 50s\n",
      "13000:\ttest: 0.8976683\tbest: 0.8976767 (12985)\ttotal: 8m 27s\tremaining: 10h 42m 4s\n",
      "14000:\ttest: 0.8980286\tbest: 0.8980323 (13989)\ttotal: 9m 8s\tremaining: 10h 43m 28s\n",
      "15000:\ttest: 0.8982873\tbest: 0.8982939 (14990)\ttotal: 9m 49s\tremaining: 10h 45m 39s\n",
      "16000:\ttest: 0.8986001\tbest: 0.8986023 (15991)\ttotal: 10m 32s\tremaining: 10h 48m 16s\n",
      "17000:\ttest: 0.8988312\tbest: 0.8988354 (16997)\ttotal: 11m 13s\tremaining: 10h 49m 5s\n",
      "18000:\ttest: 0.8989646\tbest: 0.8989646 (17965)\ttotal: 11m 55s\tremaining: 10h 50m 37s\n",
      "19000:\ttest: 0.8990674\tbest: 0.8990783 (18953)\ttotal: 12m 36s\tremaining: 10h 50m 51s\n",
      "20000:\ttest: 0.8991263\tbest: 0.8991330 (19996)\ttotal: 13m 17s\tremaining: 10h 51m 21s\n",
      "21000:\ttest: 0.8991583\tbest: 0.8992050 (20587)\ttotal: 13m 58s\tremaining: 10h 51m 49s\n",
      "22000:\ttest: 0.8992649\tbest: 0.8992728 (21982)\ttotal: 14m 38s\tremaining: 10h 50m 48s\n",
      "23000:\ttest: 0.8993629\tbest: 0.8993712 (22967)\ttotal: 15m 18s\tremaining: 10h 50m 19s\n",
      "24000:\ttest: 0.8994037\tbest: 0.8994093 (23991)\ttotal: 15m 58s\tremaining: 10h 49m 26s\n",
      "25000:\ttest: 0.8994720\tbest: 0.8994790 (24990)\ttotal: 16m 37s\tremaining: 10h 48m 14s\n",
      "26000:\ttest: 0.8995613\tbest: 0.8995640 (25959)\ttotal: 17m 18s\tremaining: 10h 48m 16s\n",
      "27000:\ttest: 0.8995761\tbest: 0.8995955 (26445)\ttotal: 17m 57s\tremaining: 10h 47m 11s\n",
      "28000:\ttest: 0.8996341\tbest: 0.8996423 (27976)\ttotal: 18m 40s\tremaining: 10h 47m 59s\n",
      "29000:\ttest: 0.8996619\tbest: 0.8996691 (28883)\ttotal: 19m 20s\tremaining: 10h 47m 46s\n",
      "30000:\ttest: 0.8996917\tbest: 0.8997259 (29549)\ttotal: 20m 2s\tremaining: 10h 48m\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8997259119\n",
      "bestIteration = 29549\n",
      "\n",
      "Shrink model to first 29550 iterations.\n",
      "Fold 6 started at Mon Mar  4 20:03:06 2019\n",
      "0:\ttest: 0.5105300\tbest: 0.5105300 (0)\ttotal: 47.5ms\tremaining: 13h 12m 20s\n",
      "1000:\ttest: 0.8328355\tbest: 0.8328603 (999)\ttotal: 32.9s\tremaining: 9h 7m 50s\n",
      "2000:\ttest: 0.8634165\tbest: 0.8634165 (2000)\ttotal: 1m 8s\tremaining: 9h 30m 17s\n",
      "3000:\ttest: 0.8772828\tbest: 0.8772828 (3000)\ttotal: 1m 47s\tremaining: 9h 54m 14s\n",
      "4000:\ttest: 0.8846428\tbest: 0.8846461 (3997)\ttotal: 2m 25s\tremaining: 10h 5m 1s\n",
      "5000:\ttest: 0.8892103\tbest: 0.8892103 (5000)\ttotal: 3m 3s\tremaining: 10h 7m 5s\n",
      "6000:\ttest: 0.8921641\tbest: 0.8921686 (5996)\ttotal: 3m 40s\tremaining: 10h 7m 46s\n",
      "7000:\ttest: 0.8940831\tbest: 0.8940835 (6991)\ttotal: 4m 16s\tremaining: 10h 7m 18s\n",
      "8000:\ttest: 0.8955345\tbest: 0.8955395 (7993)\ttotal: 4m 53s\tremaining: 10h 6m 24s\n",
      "9000:\ttest: 0.8966849\tbest: 0.8966866 (8999)\ttotal: 5m 29s\tremaining: 10h 4m 49s\n",
      "10000:\ttest: 0.8976524\tbest: 0.8976587 (9994)\ttotal: 6m 5s\tremaining: 10h 3m 50s\n",
      "11000:\ttest: 0.8983937\tbest: 0.8983989 (10997)\ttotal: 6m 42s\tremaining: 10h 3m 29s\n",
      "12000:\ttest: 0.8989342\tbest: 0.8989342 (12000)\ttotal: 7m 18s\tremaining: 10h 2m 9s\n",
      "13000:\ttest: 0.8993065\tbest: 0.8993172 (12980)\ttotal: 7m 56s\tremaining: 10h 2m 22s\n",
      "14000:\ttest: 0.8997207\tbest: 0.8997238 (13981)\ttotal: 8m 33s\tremaining: 10h 3m 13s\n",
      "15000:\ttest: 0.9001014\tbest: 0.9001076 (14988)\ttotal: 9m 9s\tremaining: 10h 1m 44s\n",
      "16000:\ttest: 0.9004013\tbest: 0.9004105 (15977)\ttotal: 9m 46s\tremaining: 10h 1m 12s\n",
      "17000:\ttest: 0.9006320\tbest: 0.9006364 (16994)\ttotal: 10m 23s\tremaining: 10h 1m 5s\n",
      "18000:\ttest: 0.9007848\tbest: 0.9007958 (17971)\ttotal: 11m\tremaining: 10h 36s\n",
      "19000:\ttest: 0.9008892\tbest: 0.9008918 (18997)\ttotal: 11m 37s\tremaining: 10h 5s\n",
      "20000:\ttest: 0.9010935\tbest: 0.9010969 (19994)\ttotal: 12m 14s\tremaining: 9h 59m 32s\n",
      "21000:\ttest: 0.9012469\tbest: 0.9012530 (20990)\ttotal: 12m 50s\tremaining: 9h 58m 25s\n",
      "22000:\ttest: 0.9013796\tbest: 0.9013854 (21973)\ttotal: 13m 26s\tremaining: 9h 57m 18s\n",
      "23000:\ttest: 0.9014693\tbest: 0.9014835 (22954)\ttotal: 14m 2s\tremaining: 9h 56m 43s\n",
      "24000:\ttest: 0.9015362\tbest: 0.9015395 (23980)\ttotal: 14m 39s\tremaining: 9h 56m 24s\n",
      "25000:\ttest: 0.9016286\tbest: 0.9016321 (24999)\ttotal: 15m 16s\tremaining: 9h 55m 39s\n",
      "26000:\ttest: 0.9017171\tbest: 0.9017188 (25993)\ttotal: 15m 53s\tremaining: 9h 55m 4s\n",
      "27000:\ttest: 0.9017683\tbest: 0.9018194 (26563)\ttotal: 16m 29s\tremaining: 9h 54m 5s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9018193634\n",
      "bestIteration = 26563\n",
      "\n",
      "Shrink model to first 26564 iterations.\n",
      "Fold 7 started at Mon Mar  4 20:22:49 2019\n",
      "0:\ttest: 0.5070829\tbest: 0.5070829 (0)\ttotal: 67.7ms\tremaining: 18h 48m 48s\n",
      "1000:\ttest: 0.8265478\tbest: 0.8265512 (999)\ttotal: 31.1s\tremaining: 8h 37m 43s\n",
      "2000:\ttest: 0.8574067\tbest: 0.8574067 (2000)\ttotal: 1m 3s\tremaining: 8h 44m 45s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000:\ttest: 0.8699084\tbest: 0.8699084 (3000)\ttotal: 1m 39s\tremaining: 9h 12m 1s\n",
      "4000:\ttest: 0.8763896\tbest: 0.8763896 (4000)\ttotal: 2m 20s\tremaining: 9h 41m 21s\n",
      "5000:\ttest: 0.8804046\tbest: 0.8804046 (5000)\ttotal: 3m 1s\tremaining: 10h 2m 3s\n",
      "6000:\ttest: 0.8830299\tbest: 0.8830341 (5999)\ttotal: 3m 49s\tremaining: 10h 33m 11s\n",
      "7000:\ttest: 0.8846765\tbest: 0.8846780 (6998)\ttotal: 4m 31s\tremaining: 10h 40m 47s\n",
      "8000:\ttest: 0.8859732\tbest: 0.8859732 (8000)\ttotal: 5m 10s\tremaining: 10h 41m 28s\n",
      "9000:\ttest: 0.8869708\tbest: 0.8869749 (8996)\ttotal: 5m 48s\tremaining: 10h 38m 42s\n",
      "10000:\ttest: 0.8876470\tbest: 0.8876476 (9997)\ttotal: 6m 24s\tremaining: 10h 34m 44s\n",
      "11000:\ttest: 0.8884562\tbest: 0.8884586 (10996)\ttotal: 7m 3s\tremaining: 10h 34m 16s\n",
      "12000:\ttest: 0.8889885\tbest: 0.8889923 (11997)\ttotal: 7m 42s\tremaining: 10h 34m 13s\n",
      "13000:\ttest: 0.8894429\tbest: 0.8894495 (12994)\ttotal: 8m 21s\tremaining: 10h 33m 58s\n",
      "14000:\ttest: 0.8897405\tbest: 0.8897413 (13997)\ttotal: 9m 1s\tremaining: 10h 35m 19s\n",
      "15000:\ttest: 0.8900418\tbest: 0.8900419 (14995)\ttotal: 9m 37s\tremaining: 10h 32m 8s\n",
      "16000:\ttest: 0.8904111\tbest: 0.8904111 (16000)\ttotal: 10m 15s\tremaining: 10h 31m 9s\n",
      "17000:\ttest: 0.8906824\tbest: 0.8906850 (16999)\ttotal: 10m 53s\tremaining: 10h 29m 54s\n",
      "18000:\ttest: 0.8908790\tbest: 0.8908833 (17992)\ttotal: 11m 32s\tremaining: 10h 29m 21s\n",
      "19000:\ttest: 0.8910786\tbest: 0.8910786 (19000)\ttotal: 12m 9s\tremaining: 10h 27m 23s\n",
      "20000:\ttest: 0.8911701\tbest: 0.8911891 (19948)\ttotal: 12m 46s\tremaining: 10h 25m 46s\n",
      "21000:\ttest: 0.8912940\tbest: 0.8912940 (21000)\ttotal: 13m 24s\tremaining: 10h 25m 2s\n",
      "22000:\ttest: 0.8913749\tbest: 0.8913817 (21991)\ttotal: 14m 2s\tremaining: 10h 24m 32s\n",
      "23000:\ttest: 0.8914737\tbest: 0.8915090 (22732)\ttotal: 14m 40s\tremaining: 10h 23m 18s\n",
      "24000:\ttest: 0.8915312\tbest: 0.8915494 (23731)\ttotal: 15m 19s\tremaining: 10h 22m 55s\n",
      "25000:\ttest: 0.8916774\tbest: 0.8916774 (25000)\ttotal: 15m 56s\tremaining: 10h 21m 29s\n",
      "26000:\ttest: 0.8917297\tbest: 0.8917405 (25732)\ttotal: 16m 32s\tremaining: 10h 19m 41s\n",
      "27000:\ttest: 0.8917973\tbest: 0.8918099 (26839)\ttotal: 17m 9s\tremaining: 10h 18m 16s\n",
      "28000:\ttest: 0.8918751\tbest: 0.8918754 (27999)\ttotal: 17m 46s\tremaining: 10h 16m 56s\n",
      "29000:\ttest: 0.8919883\tbest: 0.8919905 (28998)\ttotal: 18m 23s\tremaining: 10h 15m 39s\n",
      "30000:\ttest: 0.8920892\tbest: 0.8921078 (29879)\ttotal: 19m\tremaining: 10h 14m 23s\n",
      "31000:\ttest: 0.8921232\tbest: 0.8921358 (30752)\ttotal: 19m 36s\tremaining: 10h 12m 54s\n",
      "32000:\ttest: 0.8921691\tbest: 0.8921740 (31852)\ttotal: 20m 13s\tremaining: 10h 11m 52s\n",
      "33000:\ttest: 0.8921861\tbest: 0.8922068 (32613)\ttotal: 20m 51s\tremaining: 10h 10m 58s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8922068092\n",
      "bestIteration = 32613\n",
      "\n",
      "Shrink model to first 32614 iterations.\n",
      "Fold 8 started at Mon Mar  4 20:47:18 2019\n",
      "0:\ttest: 0.5140213\tbest: 0.5140213 (0)\ttotal: 73.6ms\tremaining: 20h 27m 3s\n",
      "1000:\ttest: 0.8257264\tbest: 0.8257264 (1000)\ttotal: 31.6s\tremaining: 8h 44m 50s\n",
      "2000:\ttest: 0.8537539\tbest: 0.8537539 (2000)\ttotal: 1m 5s\tremaining: 9h 57s\n",
      "3000:\ttest: 0.8669729\tbest: 0.8669729 (3000)\ttotal: 1m 40s\tremaining: 9h 16m 20s\n",
      "4000:\ttest: 0.8747905\tbest: 0.8747917 (3999)\ttotal: 2m 16s\tremaining: 9h 26m 12s\n",
      "5000:\ttest: 0.8795435\tbest: 0.8795435 (5000)\ttotal: 2m 53s\tremaining: 9h 34m 20s\n",
      "6000:\ttest: 0.8828170\tbest: 0.8828197 (5999)\ttotal: 3m 29s\tremaining: 9h 39m 43s\n",
      "7000:\ttest: 0.8849310\tbest: 0.8849310 (7000)\ttotal: 4m 9s\tremaining: 9h 50m 26s\n",
      "8000:\ttest: 0.8864577\tbest: 0.8864655 (7992)\ttotal: 4m 49s\tremaining: 9h 57m 24s\n",
      "9000:\ttest: 0.8875357\tbest: 0.8875357 (9000)\ttotal: 5m 27s\tremaining: 10h 1m 6s\n",
      "10000:\ttest: 0.8883854\tbest: 0.8883854 (10000)\ttotal: 6m 5s\tremaining: 10h 3m 38s\n",
      "11000:\ttest: 0.8890423\tbest: 0.8890485 (10984)\ttotal: 6m 43s\tremaining: 10h 4m 52s\n",
      "12000:\ttest: 0.8895900\tbest: 0.8895907 (11997)\ttotal: 7m 22s\tremaining: 10h 7m 10s\n",
      "13000:\ttest: 0.8899859\tbest: 0.8899866 (12994)\ttotal: 8m 1s\tremaining: 10h 9m 5s\n",
      "14000:\ttest: 0.8903430\tbest: 0.8903635 (13965)\ttotal: 8m 39s\tremaining: 10h 9m 58s\n",
      "15000:\ttest: 0.8907779\tbest: 0.8907784 (14996)\ttotal: 9m 18s\tremaining: 10h 11m 1s\n",
      "16000:\ttest: 0.8910194\tbest: 0.8910206 (15997)\ttotal: 9m 56s\tremaining: 10h 11m 17s\n",
      "17000:\ttest: 0.8911723\tbest: 0.8911778 (16994)\ttotal: 10m 35s\tremaining: 10h 12m 23s\n",
      "18000:\ttest: 0.8912990\tbest: 0.8913150 (17705)\ttotal: 11m 14s\tremaining: 10h 13m 2s\n",
      "19000:\ttest: 0.8914832\tbest: 0.8914870 (18987)\ttotal: 11m 52s\tremaining: 10h 12m 53s\n",
      "20000:\ttest: 0.8915813\tbest: 0.8915836 (19996)\ttotal: 12m 29s\tremaining: 10h 12m 14s\n",
      "21000:\ttest: 0.8917017\tbest: 0.8917047 (20989)\ttotal: 13m 7s\tremaining: 10h 12m 1s\n",
      "22000:\ttest: 0.8917707\tbest: 0.8917936 (21811)\ttotal: 13m 46s\tremaining: 10h 12m 2s\n",
      "23000:\ttest: 0.8918010\tbest: 0.8918129 (22849)\ttotal: 14m 23s\tremaining: 10h 11m 32s\n",
      "24000:\ttest: 0.8918100\tbest: 0.8918182 (23136)\ttotal: 15m 2s\tremaining: 10h 11m 37s\n",
      "25000:\ttest: 0.8918978\tbest: 0.8919073 (24982)\ttotal: 15m 40s\tremaining: 10h 11m 36s\n",
      "26000:\ttest: 0.8919419\tbest: 0.8919581 (25615)\ttotal: 16m 18s\tremaining: 10h 10m 40s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8919580897\n",
      "bestIteration = 25615\n",
      "\n",
      "Shrink model to first 25616 iterations.\n",
      "Fold 9 started at Mon Mar  4 21:06:51 2019\n",
      "0:\ttest: 0.5124526\tbest: 0.5124526 (0)\ttotal: 61.1ms\tremaining: 16h 58m\n",
      "1000:\ttest: 0.8367543\tbest: 0.8367543 (1000)\ttotal: 31.6s\tremaining: 8h 45m 14s\n",
      "2000:\ttest: 0.8614731\tbest: 0.8614757 (1998)\ttotal: 1m 4s\tremaining: 8h 58m 53s\n",
      "3000:\ttest: 0.8734834\tbest: 0.8734834 (3000)\ttotal: 1m 39s\tremaining: 9h 12m 27s\n",
      "4000:\ttest: 0.8802230\tbest: 0.8802270 (3999)\ttotal: 2m 17s\tremaining: 9h 30m 35s\n",
      "5000:\ttest: 0.8842225\tbest: 0.8842265 (4999)\ttotal: 2m 54s\tremaining: 9h 39m 23s\n",
      "6000:\ttest: 0.8867816\tbest: 0.8867837 (5999)\ttotal: 3m 32s\tremaining: 9h 47m 18s\n",
      "7000:\ttest: 0.8887514\tbest: 0.8887532 (6999)\ttotal: 4m 11s\tremaining: 9h 54m 59s\n",
      "8000:\ttest: 0.8901073\tbest: 0.8901073 (8000)\ttotal: 4m 48s\tremaining: 9h 56m 59s\n",
      "9000:\ttest: 0.8910816\tbest: 0.8910966 (8969)\ttotal: 5m 27s\tremaining: 10h 19s\n",
      "10000:\ttest: 0.8917126\tbest: 0.8917126 (10000)\ttotal: 6m 5s\tremaining: 10h 2m 24s\n",
      "11000:\ttest: 0.8923983\tbest: 0.8923983 (11000)\ttotal: 6m 44s\tremaining: 10h 5m 42s\n",
      "12000:\ttest: 0.8928096\tbest: 0.8928151 (11998)\ttotal: 7m 22s\tremaining: 10h 7m 9s\n",
      "13000:\ttest: 0.8931826\tbest: 0.8931963 (12992)\ttotal: 8m 1s\tremaining: 10h 8m 51s\n",
      "14000:\ttest: 0.8934321\tbest: 0.8934356 (13996)\ttotal: 8m 39s\tremaining: 10h 9m 41s\n",
      "15000:\ttest: 0.8936461\tbest: 0.8936472 (14999)\ttotal: 9m 18s\tremaining: 10h 11m 29s\n",
      "16000:\ttest: 0.8938963\tbest: 0.8939041 (15992)\ttotal: 9m 56s\tremaining: 10h 11m 35s\n",
      "17000:\ttest: 0.8941089\tbest: 0.8941089 (17000)\ttotal: 10m 35s\tremaining: 10h 12m 50s\n",
      "18000:\ttest: 0.8941818\tbest: 0.8941870 (17975)\ttotal: 11m 14s\tremaining: 10h 12m 57s\n",
      "19000:\ttest: 0.8942842\tbest: 0.8942956 (18924)\ttotal: 11m 53s\tremaining: 10h 13m 38s\n",
      "20000:\ttest: 0.8944210\tbest: 0.8944210 (20000)\ttotal: 12m 29s\tremaining: 10h 12m 27s\n",
      "21000:\ttest: 0.8944656\tbest: 0.8944736 (20472)\ttotal: 13m 9s\tremaining: 10h 13m 8s\n",
      "22000:\ttest: 0.8945131\tbest: 0.8945334 (21250)\ttotal: 13m 47s\tremaining: 10h 13m 10s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8945333824\n",
      "bestIteration = 21250\n",
      "\n",
      "Shrink model to first 21251 iterations.\n",
      "CV mean score: 0.8973, std: 0.0038.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_cat, prediction_cat, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, model_type='cat', plot_feature_importance=False)\n",
    "oof.append(oof_cat)\n",
    "preds.append(prediction_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_cat_top30_inter_1_10_1', oof)\n",
    "np.save('../cache/preds_cat_top30_inter_1_10_1', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_0 94667\n",
      "var_1 108929\n",
      "var_2 86554\n",
      "var_3 74594\n",
      "var_4 63514\n",
      "var_5 141028\n",
      "var_6 38595\n",
      "var_7 103057\n",
      "var_8 98616\n",
      "var_9 49416\n",
      "var_10 128762\n",
      "var_11 130188\n",
      "var_12 9559\n",
      "var_13 115181\n",
      "var_14 79122\n",
      "var_15 19809\n",
      "var_16 86915\n",
      "var_17 137823\n",
      "var_18 139513\n",
      "var_19 144179\n",
      "var_20 127763\n",
      "var_21 140062\n",
      "var_22 90653\n",
      "var_23 24911\n",
      "var_24 105101\n",
      "var_25 14850\n",
      "var_26 127086\n",
      "var_27 60183\n",
      "var_28 35857\n",
      "var_29 88338\n",
      "var_30 145973\n",
      "var_31 77387\n",
      "var_32 85961\n",
      "var_33 112235\n",
      "var_34 25163\n",
      "var_35 122384\n",
      "var_36 96403\n",
      "var_37 79039\n",
      "var_38 115364\n",
      "var_39 112669\n",
      "var_40 141878\n",
      "var_41 131896\n",
      "var_42 31590\n",
      "var_43 15183\n",
      "var_44 127702\n",
      "var_45 169965\n",
      "var_46 93446\n",
      "var_47 154779\n",
      "var_48 152039\n",
      "var_49 140640\n",
      "var_50 32307\n",
      "var_51 143455\n",
      "var_52 121311\n",
      "var_53 33460\n",
      "var_54 144775\n",
      "var_55 128076\n",
      "var_56 103044\n",
      "var_57 35542\n",
      "var_58 113904\n",
      "var_59 37741\n",
      "var_60 113762\n",
      "var_61 159368\n",
      "var_62 74774\n",
      "var_63 97098\n",
      "var_64 59376\n",
      "var_65 108347\n",
      "var_66 47716\n",
      "var_67 137252\n",
      "var_68 459\n",
      "var_69 110346\n",
      "var_70 153193\n",
      "var_71 13526\n",
      "var_72 110112\n",
      "var_73 142582\n",
      "var_74 161057\n",
      "var_75 129382\n",
      "var_76 139316\n",
      "var_77 106808\n",
      "var_78 72253\n",
      "var_79 53212\n",
      "var_80 136426\n",
      "var_81 79060\n",
      "var_82 144827\n",
      "var_83 144277\n",
      "var_84 133766\n",
      "var_85 108435\n",
      "var_86 140593\n",
      "var_87 125295\n",
      "var_88 84914\n",
      "var_89 103520\n",
      "var_90 157207\n",
      "var_91 7959\n",
      "var_92 110740\n",
      "var_93 26707\n",
      "var_94 89145\n",
      "var_95 29387\n",
      "var_96 148098\n",
      "var_97 158735\n",
      "var_98 33266\n",
      "var_99 69295\n",
      "var_100 150725\n",
      "var_101 122293\n",
      "var_102 146237\n",
      "var_103 9376\n",
      "var_104 72624\n",
      "var_105 39111\n",
      "var_106 71061\n",
      "var_107 137826\n",
      "var_108 8524\n",
      "var_109 112171\n",
      "var_110 106116\n",
      "var_111 46462\n",
      "var_112 60482\n",
      "var_113 116491\n",
      "var_114 43081\n",
      "var_115 86725\n",
      "var_116 63467\n",
      "var_117 164465\n",
      "var_118 143667\n",
      "var_119 112399\n",
      "var_120 158266\n",
      "var_121 64695\n",
      "var_122 121766\n",
      "var_123 129886\n",
      "var_124 91022\n",
      "var_125 16056\n",
      "var_126 32410\n",
      "var_127 95709\n",
      "var_128 98200\n",
      "var_129 113425\n",
      "var_130 36636\n",
      "var_131 21463\n",
      "var_132 57922\n",
      "var_133 19233\n",
      "var_134 131618\n",
      "var_135 140774\n",
      "var_136 156614\n",
      "var_137 144394\n",
      "var_138 117425\n",
      "var_139 137294\n",
      "var_140 121383\n",
      "var_141 134441\n",
      "var_142 128612\n",
      "var_143 94372\n",
      "var_144 40595\n",
      "var_145 108525\n",
      "var_146 84314\n",
      "var_147 137557\n",
      "var_148 10605\n",
      "var_149 148501\n",
      "var_150 83659\n",
      "var_151 109665\n",
      "var_152 95822\n",
      "var_153 73724\n",
      "var_154 119339\n",
      "var_155 127454\n",
      "var_156 40634\n",
      "var_157 126534\n",
      "var_158 144554\n",
      "var_159 112828\n",
      "var_160 156274\n",
      "var_161 11067\n",
      "var_162 57395\n",
      "var_163 123166\n",
      "var_164 122735\n",
      "var_165 119401\n",
      "var_166 17900\n",
      "var_167 140954\n",
      "var_168 97224\n",
      "var_169 18242\n",
      "var_170 113720\n",
      "var_171 125912\n",
      "var_172 143366\n",
      "var_173 128119\n",
      "var_174 134944\n",
      "var_175 92658\n",
      "var_176 142521\n",
      "var_177 85716\n",
      "var_178 145234\n",
      "var_179 90089\n",
      "var_180 123473\n",
      "var_181 56164\n",
      "var_182 149195\n",
      "var_183 117527\n",
      "var_184 145174\n",
      "var_185 120745\n",
      "var_186 98058\n",
      "var_187 157031\n",
      "var_188 108813\n",
      "var_189 41764\n",
      "var_190 114958\n",
      "var_191 94265\n",
      "var_192 59062\n",
      "var_193 110556\n",
      "var_194 97067\n",
      "var_195 57868\n",
      "var_196 125558\n",
      "var_197 40533\n",
      "var_198 94148\n",
      "var_199 149430\n"
     ]
    }
   ],
   "source": [
    "for col in list(X1.columns):\n",
    "    print(col, X1[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_68\n"
     ]
    }
   ],
   "source": [
    "for col in list(X1.columns):\n",
    "    if X1[col].nunique() < 1000:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_12\n",
      "var_68\n",
      "var_91\n",
      "var_103\n",
      "var_108\n"
     ]
    }
   ],
   "source": [
    "for col in list(X1.columns):\n",
    "    if X1[col].nunique() < 10000:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X1, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "class roc_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         y_pred = self.model.predict(self.x)\n",
    "#         roc = roc_auc_score(self.y, y_pred)\n",
    "#         y_pred_val = self.model.predict(self.x_val)\n",
    "#         roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "#         print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "#         return\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "#         y_pred = self.model.predict(self.x)\n",
    "#         roc = roc_auc_score(self.y, y_pred)\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        print('\\rroc-auc_val: %s' % (str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "        return\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "gamma = 2.0\n",
    "alpha=.25\n",
    "epsilon = K.epsilon()\n",
    "\n",
    "def focal_loss(y_true, y_pred):\n",
    "    pt_1 = y_pred * y_true\n",
    "    pt_1 = K.clip(pt_1, epsilon, 1-epsilon)\n",
    "    CE_1 = -K.log(pt_1)\n",
    "    FL_1 = alpha* K.pow(1-pt_1, gamma) * CE_1\n",
    "    \n",
    "    pt_0 = (1-y_pred) * (1-y_true)\n",
    "    pt_0 = K.clip(pt_0, epsilon, 1-epsilon)\n",
    "    CE_0 = -K.log(pt_0)\n",
    "    FL_0 = (1-alpha)* K.pow(1-pt_0, gamma) * CE_0\n",
    "    \n",
    "    loss = K.sum(FL_1, axis=1) + K.sum(FL_0, axis=1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Flatten, LeakyReLU, ReLU, PReLU, LSTM\n",
    "\n",
    "a = Input(shape=(200,1))\n",
    "x = Dense(200)(a)\n",
    "x = PReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Dense(64)(x)\n",
    "x = PReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=a, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Flatten, LeakyReLU, ReLU, PReLU, LSTM\n",
    "\n",
    "a = Input(shape=(200,1))\n",
    "x = LSTM(10, return_sequences=True)(a)\n",
    "x = Dense(200)(x)\n",
    "x = PReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Dense(64)(x)\n",
    "x = PReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=a, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/200\n",
      " - 14s - loss: 4.8821 - acc: 0.8842 - val_loss: 4.8782 - val_acc: 0.9173\n",
      "roc-auc_val: 0.8781                                                                                                    \n",
      "Epoch 2/200\n",
      " - 11s - loss: 4.8626 - acc: 0.9117 - val_loss: 4.8776 - val_acc: 0.9171\n",
      "roc-auc_val: 0.8789                                                                                                    \n",
      "Epoch 3/200\n",
      " - 11s - loss: 4.8611 - acc: 0.9128 - val_loss: 4.8774 - val_acc: 0.9166\n",
      "roc-auc_val: 0.8808                                                                                                    \n",
      "Epoch 4/200\n",
      " - 11s - loss: 4.8603 - acc: 0.9134 - val_loss: 4.8770 - val_acc: 0.9158\n",
      "roc-auc_val: 0.8811                                                                                                    \n",
      "Epoch 5/200\n",
      " - 11s - loss: 4.8594 - acc: 0.9148 - val_loss: 4.8772 - val_acc: 0.9137\n",
      "roc-auc_val: 0.8803                                                                                                    \n",
      "Epoch 6/200\n",
      " - 11s - loss: 4.8589 - acc: 0.9149 - val_loss: 4.8774 - val_acc: 0.9136\n",
      "roc-auc_val: 0.8796                                                                                                    \n",
      "Epoch 7/200\n",
      " - 12s - loss: 4.8587 - acc: 0.9151 - val_loss: 4.8770 - val_acc: 0.9141\n",
      "roc-auc_val: 0.8814                                                                                                    \n",
      "Epoch 8/200\n",
      " - 11s - loss: 4.8583 - acc: 0.9153 - val_loss: 4.8773 - val_acc: 0.9130\n",
      "roc-auc_val: 0.8819                                                                                                    \n",
      "Epoch 9/200\n",
      " - 11s - loss: 4.8580 - acc: 0.9158 - val_loss: 4.8770 - val_acc: 0.9142\n",
      "roc-auc_val: 0.8819                                                                                                    \n",
      "Epoch 10/200\n",
      " - 11s - loss: 4.8578 - acc: 0.9160 - val_loss: 4.8767 - val_acc: 0.9125\n",
      "roc-auc_val: 0.8854                                                                                                    \n",
      "Epoch 11/200\n",
      " - 12s - loss: 4.8578 - acc: 0.9158 - val_loss: 4.8769 - val_acc: 0.9133\n",
      "roc-auc_val: 0.884                                                                                                    \n",
      "Epoch 12/200\n",
      " - 12s - loss: 4.8576 - acc: 0.9159 - val_loss: 4.8765 - val_acc: 0.9157\n",
      "roc-auc_val: 0.8854                                                                                                    \n",
      "Epoch 13/200\n",
      " - 12s - loss: 4.8575 - acc: 0.9159 - val_loss: 4.8768 - val_acc: 0.9127\n",
      "roc-auc_val: 0.8868                                                                                                    \n",
      "Epoch 14/200\n",
      " - 12s - loss: 4.8574 - acc: 0.9159 - val_loss: 4.8766 - val_acc: 0.9143\n",
      "roc-auc_val: 0.8846                                                                                                    \n",
      "Epoch 15/200\n",
      " - 11s - loss: 4.8572 - acc: 0.9164 - val_loss: 4.8766 - val_acc: 0.9161\n",
      "roc-auc_val: 0.885                                                                                                    \n",
      "Epoch 16/200\n",
      " - 12s - loss: 4.8572 - acc: 0.9166 - val_loss: 4.8767 - val_acc: 0.9138\n",
      "roc-auc_val: 0.8847                                                                                                    \n",
      "Epoch 17/200\n",
      " - 11s - loss: 4.8572 - acc: 0.9164 - val_loss: 4.8770 - val_acc: 0.9118\n",
      "roc-auc_val: 0.8842                                                                                                    \n",
      "Epoch 18/200\n",
      " - 11s - loss: 4.8571 - acc: 0.9166 - val_loss: 4.8767 - val_acc: 0.9138\n",
      "roc-auc_val: 0.8841                                                                                                    \n",
      "Epoch 19/200\n",
      " - 11s - loss: 4.8571 - acc: 0.9167 - val_loss: 4.8771 - val_acc: 0.9105\n",
      "roc-auc_val: 0.8831                                                                                                    \n",
      "Epoch 20/200\n",
      " - 12s - loss: 4.8570 - acc: 0.9161 - val_loss: 4.8767 - val_acc: 0.9121\n",
      "roc-auc_val: 0.884                                                                                                    \n",
      "Epoch 21/200\n",
      " - 11s - loss: 4.8570 - acc: 0.9167 - val_loss: 4.8766 - val_acc: 0.9142\n",
      "roc-auc_val: 0.8858                                                                                                    \n",
      "Epoch 22/200\n",
      " - 12s - loss: 4.8570 - acc: 0.9167 - val_loss: 4.8768 - val_acc: 0.9120\n",
      "roc-auc_val: 0.8865                                                                                                    \n",
      "Epoch 23/200\n",
      " - 11s - loss: 4.8569 - acc: 0.9169 - val_loss: 4.8767 - val_acc: 0.9140\n",
      "roc-auc_val: 0.8857                                                                                                    \n",
      "Epoch 24/200\n",
      " - 12s - loss: 4.8569 - acc: 0.9166 - val_loss: 4.8768 - val_acc: 0.9120\n",
      "roc-auc_val: 0.8858                                                                                                    \n",
      "Epoch 25/200\n",
      " - 11s - loss: 4.8569 - acc: 0.9165 - val_loss: 4.8767 - val_acc: 0.9138\n",
      "roc-auc_val: 0.8841                                                                                                    \n",
      "Epoch 26/200\n",
      " - 11s - loss: 4.8569 - acc: 0.9169 - val_loss: 4.8767 - val_acc: 0.9134\n",
      "roc-auc_val: 0.8858                                                                                                    \n",
      "Epoch 27/200\n",
      " - 11s - loss: 4.8568 - acc: 0.9170 - val_loss: 4.8770 - val_acc: 0.9125\n",
      "roc-auc_val: 0.8827                                                                                                    \n",
      "Epoch 28/200\n",
      " - 12s - loss: 4.8569 - acc: 0.9163 - val_loss: 4.8766 - val_acc: 0.9140\n",
      "roc-auc_val: 0.8861                                                                                                    \n",
      "Epoch 29/200\n",
      " - 12s - loss: 4.8568 - acc: 0.9167 - val_loss: 4.8769 - val_acc: 0.9134\n",
      "roc-auc_val: 0.8839                                                                                                    \n",
      "Epoch 30/200\n",
      " - 11s - loss: 4.8568 - acc: 0.9169 - val_loss: 4.8766 - val_acc: 0.9149\n",
      "roc-auc_val: 0.8861                                                                                                    \n",
      "Epoch 31/200\n",
      " - 11s - loss: 4.8568 - acc: 0.9170 - val_loss: 4.8766 - val_acc: 0.9148\n",
      "roc-auc_val: 0.8849                                                                                                    \n",
      "Epoch 32/200\n",
      " - 11s - loss: 4.8568 - acc: 0.9166 - val_loss: 4.8766 - val_acc: 0.9164\n",
      "roc-auc_val: 0.8852                                                                                                    \n",
      "Epoch 33/200\n",
      " - 11s - loss: 4.8568 - acc: 0.9172 - val_loss: 4.8766 - val_acc: 0.9153\n",
      "roc-auc_val: 0.8859                                                                                                    \n",
      "Epoch 34/200\n",
      " - 11s - loss: 4.8568 - acc: 0.9168 - val_loss: 4.8768 - val_acc: 0.9124\n",
      "roc-auc_val: 0.886                                                                                                    \n",
      "Epoch 35/200\n",
      " - 11s - loss: 4.8568 - acc: 0.9167 - val_loss: 4.8768 - val_acc: 0.9128\n",
      "roc-auc_val: 0.8838                                                                                                    \n",
      "Epoch 36/200\n",
      " - 12s - loss: 4.8568 - acc: 0.9168 - val_loss: 4.8769 - val_acc: 0.9127\n",
      "roc-auc_val: 0.8832                                                                                                    \n",
      "Epoch 37/200\n",
      " - 12s - loss: 4.8568 - acc: 0.9170 - val_loss: 4.8765 - val_acc: 0.9160\n",
      "roc-auc_val: 0.8857                                                                                                    \n",
      "Epoch 38/200\n",
      " - 12s - loss: 4.8568 - acc: 0.9171 - val_loss: 4.8765 - val_acc: 0.9131\n",
      "roc-auc_val: 0.888                                                                                                    \n",
      "Epoch 39/200\n",
      " - 12s - loss: 4.8567 - acc: 0.9170 - val_loss: 4.8768 - val_acc: 0.9136\n",
      "roc-auc_val: 0.8842                                                                                                    \n",
      "Epoch 40/200\n",
      " - 12s - loss: 4.8567 - acc: 0.9170 - val_loss: 4.8766 - val_acc: 0.9134\n",
      "roc-auc_val: 0.8868                                                                                                    \n",
      "Epoch 41/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-2c69976871fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     callbacks=[roc_callback(training_data=(X_train, y_train),validation_data=(X_val, y_val))])\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# LSTM, stuck around 0.72 auc, adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(123)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X1, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = optimizers.Adam(lr=0.003)\n",
    "rmsprop = optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss=focal_loss,\n",
    "              optimizer=sgd,\n",
    "              metrics=['acc'])\n",
    "batch_size=1024\n",
    "epochs=200\n",
    "X_train = np.reshape(X_train.values, (-1, 200, 1))\n",
    "\n",
    "X_val = np.reshape(X_val.values, (-1, 200, 1))\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[roc_callback(training_data=(X_train, y_train),validation_data=(X_val, y_val))])\n",
    "\n",
    "# LSTM, stuck around 0.72 auc, adam\n",
    "# LSTM with return seq, around 0.895 auc, adam\n",
    "# SGD with relu\n",
    "# Epoch 100/100 + plus 40 epochs earlier\n",
    "#  - 9s - loss: 0.1946 - acc: 0.9267 - val_loss: 0.2053 - val_acc: 0.9236\n",
    "# roc-auc: 0.9131 - roc-auc_val: 0.898    \n",
    "# Adam with relu (more or less same  with prelu)\n",
    "# Epoch 100/100\n",
    "#  - 9s - loss: 0.1949 - acc: 0.9271 - val_loss: 0.2183 - val_acc: 0.9161\n",
    "# roc-auc: 0.9116 - roc-auc_val: 0.8954 (goes upto 0.8974)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "p = PolynomialFeatures()\n",
    "X3 = pd.concat([X1,X2], axis=0)\n",
    "print(X3.shape)\n",
    "X3 = p.fit_transform(X3)\n",
    "X = X3[:len(X1)]\n",
    "X_test = X3[len(X1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X)\n",
    "X.columns = ['col_'+str(i) for i in range(X.shape[1])]\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_test.columns = ['col_'+str(i) for i in range(X.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Mar  4 21:50:34 2019\n",
      "0:\ttest: 0.5086073\tbest: 0.5086073 (0)\ttotal: 115ms\tremaining: 1d 7h 53m 56s\n",
      "1000:\ttest: 0.8433571\tbest: 0.8433571 (1000)\ttotal: 1m 7s\tremaining: 18h 42m 24s\n",
      "2000:\ttest: 0.8685388\tbest: 0.8685565 (1999)\ttotal: 2m 24s\tremaining: 20h 4m 19s\n",
      "3000:\ttest: 0.8795600\tbest: 0.8795600 (3000)\ttotal: 3m 53s\tremaining: 21h 35m 6s\n",
      "4000:\ttest: 0.8853149\tbest: 0.8853149 (4000)\ttotal: 5m 25s\tremaining: 22h 28m 42s\n",
      "5000:\ttest: 0.8886569\tbest: 0.8886658 (4996)\ttotal: 6m 56s\tremaining: 23h 2s\n",
      "6000:\ttest: 0.8909283\tbest: 0.8909283 (6000)\ttotal: 8m 28s\tremaining: 23h 23m 33s\n",
      "7000:\ttest: 0.8921674\tbest: 0.8921720 (6999)\ttotal: 10m 1s\tremaining: 23h 42m 7s\n",
      "8000:\ttest: 0.8932622\tbest: 0.8932622 (8000)\ttotal: 11m 30s\tremaining: 23h 47m 42s\n",
      "9000:\ttest: 0.8940154\tbest: 0.8940157 (8996)\ttotal: 13m 2s\tremaining: 23h 55m 11s\n",
      "10000:\ttest: 0.8944373\tbest: 0.8944562 (9977)\ttotal: 14m 35s\tremaining: 1d 4m 34s\n",
      "11000:\ttest: 0.8948246\tbest: 0.8948282 (10998)\ttotal: 16m 9s\tremaining: 1d 12m 14s\n",
      "12000:\ttest: 0.8952860\tbest: 0.8952870 (11992)\ttotal: 17m 42s\tremaining: 1d 17m 24s\n",
      "13000:\ttest: 0.8953605\tbest: 0.8953679 (12919)\ttotal: 19m 19s\tremaining: 1d 26m 43s\n",
      "14000:\ttest: 0.8953790\tbest: 0.8954466 (13894)\ttotal: 20m 56s\tremaining: 1d 35m 9s\n",
      "15000:\ttest: 0.8955051\tbest: 0.8955204 (14784)\ttotal: 22m 31s\tremaining: 1d 38m 56s\n",
      "16000:\ttest: 0.8956669\tbest: 0.8956669 (16000)\ttotal: 24m 7s\tremaining: 1d 43m 17s\n",
      "17000:\ttest: 0.8956705\tbest: 0.8956974 (16097)\ttotal: 25m 41s\tremaining: 1d 45m 34s\n",
      "18000:\ttest: 0.8957299\tbest: 0.8957727 (17451)\ttotal: 27m 15s\tremaining: 1d 47m 24s\n",
      "19000:\ttest: 0.8958524\tbest: 0.8958749 (18785)\ttotal: 28m 49s\tremaining: 1d 47m 57s\n",
      "20000:\ttest: 0.8959031\tbest: 0.8959281 (19894)\ttotal: 30m 19s\tremaining: 1d 46m 7s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8959280635\n",
      "bestIteration = 19894\n",
      "\n",
      "Shrink model to first 19895 iterations.\n",
      "Fold 1 started at Mon Mar  4 22:57:12 2019\n",
      "0:\ttest: 0.5110938\tbest: 0.5110938 (0)\ttotal: 166ms\tremaining: 1d 22h 13m 15s\n",
      "1000:\ttest: 0.8388015\tbest: 0.8388015 (1000)\ttotal: 1m 6s\tremaining: 18h 23m 1s\n",
      "2000:\ttest: 0.8626625\tbest: 0.8626627 (1999)\ttotal: 2m 17s\tremaining: 18h 59m 55s\n",
      "3000:\ttest: 0.8752098\tbest: 0.8752098 (3000)\ttotal: 3m 39s\tremaining: 20h 13m 52s\n",
      "4000:\ttest: 0.8816649\tbest: 0.8816649 (4000)\ttotal: 5m 10s\tremaining: 21h 27m 27s\n",
      "5000:\ttest: 0.8855607\tbest: 0.8855693 (4999)\ttotal: 6m 42s\tremaining: 22h 14m 56s\n",
      "6000:\ttest: 0.8876354\tbest: 0.8876368 (5999)\ttotal: 8m 13s\tremaining: 22h 41m 9s\n",
      "7000:\ttest: 0.8893332\tbest: 0.8893332 (7000)\ttotal: 9m 46s\tremaining: 23h 5m 45s\n",
      "8000:\ttest: 0.8903341\tbest: 0.8903341 (8000)\ttotal: 11m 18s\tremaining: 23h 22m 38s\n",
      "9000:\ttest: 0.8910822\tbest: 0.8910838 (8993)\ttotal: 12m 54s\tremaining: 23h 42m 3s\n",
      "10000:\ttest: 0.8916641\tbest: 0.8916641 (10000)\ttotal: 14m 31s\tremaining: 23h 57m 4s\n",
      "11000:\ttest: 0.8919268\tbest: 0.8919315 (10997)\ttotal: 16m 7s\tremaining: 1d 9m 13s\n",
      "12000:\ttest: 0.8922787\tbest: 0.8922874 (11993)\ttotal: 17m 43s\tremaining: 1d 18m 58s\n",
      "13000:\ttest: 0.8925586\tbest: 0.8925644 (12940)\ttotal: 19m 21s\tremaining: 1d 29m 32s\n",
      "14000:\ttest: 0.8928167\tbest: 0.8928358 (13952)\ttotal: 20m 59s\tremaining: 1d 38m 26s\n",
      "15000:\ttest: 0.8929388\tbest: 0.8929952 (14831)\ttotal: 22m 32s\tremaining: 1d 40m 38s\n",
      "16000:\ttest: 0.8930443\tbest: 0.8930652 (15576)\ttotal: 24m 12s\tremaining: 1d 48m 13s\n",
      "17000:\ttest: 0.8931584\tbest: 0.8931640 (16946)\ttotal: 25m 46s\tremaining: 1d 50m 41s\n",
      "18000:\ttest: 0.8931111\tbest: 0.8932127 (17099)\ttotal: 27m 26s\tremaining: 1d 56m 55s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8932126649\n",
      "bestIteration = 17099\n",
      "\n",
      "Shrink model to first 17100 iterations.\n",
      "Fold 2 started at Tue Mar  5 00:08:53 2019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ce4bef34ec7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moof_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_feature_importance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-cd7d58cf1759>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, averaging, model)\u001b[0m\n\u001b[1;32m     94\u001b[0m                                   objective=\"Logloss\")\n\u001b[1;32m     95\u001b[0m             model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], \n\u001b[0;32m---> 96\u001b[0;31m                       use_best_model=True, verbose=1000,early_stopping_rounds=1000)\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0my_pred_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval)\u001b[0m\n\u001b[1;32m   2181\u001b[0m         self._fit(X, y, cat_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[1;32m   2182\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2183\u001b[0;31m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval)\n\u001b[0m\u001b[1;32m   2184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0m_check_train_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m         \u001b[0mtrain_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_train_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_empty_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCatboostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X is empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_build_train_pool\u001b[0;34m(X, y, cat_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, column_description)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCatboostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y has not initialized in fit(): X is not catboost.Pool object, y must be not None in fit().\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         train_pool = Pool(X, y, cat_features=cat_features, pairs=pairs, weight=sample_weight, group_id=group_id,\n\u001b[0;32m--> 655\u001b[0;31m                           group_weight=group_weight, subgroup_id=subgroup_id, pairs_weight=pairs_weight, baseline=baseline)\n\u001b[0m\u001b[1;32m    656\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, cat_features, column_description, pairs, delimiter, has_header, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, feature_names, thread_count)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         )\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, data, label, cat_features, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, feature_names)\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_features_order_layout_pool\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._set_features_order_data_pd_data_frame\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miteritems\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_item_cache'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_cat, prediction_cat, _ = train_model(X, X_test, y, params=np.random.randint(1,101), folds=folds, model_type='cat', plot_feature_importance=False)\n",
    "oof.append(oof_cat)\n",
    "preds.append(prediction_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 57.8ms\tremaining: 9m 37s\n",
      "1:\ttotal: 96.4ms\tremaining: 8m 1s\n",
      "2:\ttotal: 139ms\tremaining: 7m 43s\n",
      "3:\ttotal: 157ms\tremaining: 6m 31s\n",
      "4:\ttotal: 174ms\tremaining: 5m 47s\n",
      "5:\ttotal: 190ms\tremaining: 5m 17s\n",
      "6:\ttotal: 218ms\tremaining: 5m 10s\n",
      "7:\ttotal: 253ms\tremaining: 5m 15s\n",
      "8:\ttotal: 292ms\tremaining: 5m 24s\n",
      "9:\ttotal: 323ms\tremaining: 5m 22s\n",
      "10:\ttotal: 364ms\tremaining: 5m 30s\n",
      "11:\ttotal: 391ms\tremaining: 5m 25s\n",
      "12:\ttotal: 421ms\tremaining: 5m 23s\n",
      "13:\ttotal: 450ms\tremaining: 5m 20s\n",
      "14:\ttotal: 478ms\tremaining: 5m 18s\n",
      "15:\ttotal: 509ms\tremaining: 5m 17s\n",
      "16:\ttotal: 547ms\tremaining: 5m 21s\n",
      "17:\ttotal: 578ms\tremaining: 5m 20s\n",
      "18:\ttotal: 602ms\tremaining: 5m 16s\n",
      "19:\ttotal: 622ms\tremaining: 5m 10s\n",
      "20:\ttotal: 638ms\tremaining: 5m 3s\n",
      "21:\ttotal: 668ms\tremaining: 5m 2s\n",
      "22:\ttotal: 695ms\tremaining: 5m 1s\n",
      "23:\ttotal: 719ms\tremaining: 4m 58s\n",
      "24:\ttotal: 748ms\tremaining: 4m 58s\n",
      "25:\ttotal: 780ms\tremaining: 4m 59s\n",
      "26:\ttotal: 804ms\tremaining: 4m 56s\n",
      "27:\ttotal: 820ms\tremaining: 4m 51s\n",
      "28:\ttotal: 836ms\tremaining: 4m 47s\n",
      "29:\ttotal: 853ms\tremaining: 4m 43s\n",
      "30:\ttotal: 871ms\tremaining: 4m 39s\n",
      "31:\ttotal: 887ms\tremaining: 4m 36s\n",
      "32:\ttotal: 932ms\tremaining: 4m 41s\n",
      "33:\ttotal: 961ms\tremaining: 4m 41s\n",
      "34:\ttotal: 984ms\tremaining: 4m 40s\n",
      "35:\ttotal: 1s\tremaining: 4m 38s\n",
      "36:\ttotal: 1.02s\tremaining: 4m 35s\n",
      "37:\ttotal: 1.04s\tremaining: 4m 31s\n",
      "38:\ttotal: 1.06s\tremaining: 4m 30s\n",
      "39:\ttotal: 1.09s\tremaining: 4m 31s\n",
      "40:\ttotal: 1.11s\tremaining: 4m 30s\n",
      "41:\ttotal: 1.14s\tremaining: 4m 29s\n",
      "42:\ttotal: 1.17s\tremaining: 4m 31s\n",
      "43:\ttotal: 1.21s\tremaining: 4m 33s\n",
      "44:\ttotal: 1.23s\tremaining: 4m 31s\n",
      "45:\ttotal: 1.24s\tremaining: 4m 28s\n",
      "46:\ttotal: 1.26s\tremaining: 4m 27s\n",
      "47:\ttotal: 1.28s\tremaining: 4m 25s\n",
      "48:\ttotal: 1.3s\tremaining: 4m 24s\n",
      "49:\ttotal: 1.32s\tremaining: 4m 22s\n",
      "50:\ttotal: 1.33s\tremaining: 4m 19s\n",
      "51:\ttotal: 1.36s\tremaining: 4m 19s\n",
      "52:\ttotal: 1.38s\tremaining: 4m 19s\n",
      "53:\ttotal: 1.41s\tremaining: 4m 20s\n",
      "54:\ttotal: 1.44s\tremaining: 4m 20s\n",
      "55:\ttotal: 1.47s\tremaining: 4m 20s\n",
      "56:\ttotal: 1.48s\tremaining: 4m 18s\n",
      "57:\ttotal: 1.5s\tremaining: 4m 16s\n",
      "58:\ttotal: 1.51s\tremaining: 4m 15s\n",
      "59:\ttotal: 1.54s\tremaining: 4m 15s\n",
      "60:\ttotal: 1.57s\tremaining: 4m 16s\n",
      "61:\ttotal: 1.6s\tremaining: 4m 16s\n",
      "62:\ttotal: 1.62s\tremaining: 4m 15s\n",
      "63:\ttotal: 1.64s\tremaining: 4m 14s\n",
      "64:\ttotal: 1.65s\tremaining: 4m 12s\n",
      "65:\ttotal: 1.67s\tremaining: 4m 11s\n",
      "66:\ttotal: 1.69s\tremaining: 4m 9s\n",
      "67:\ttotal: 1.71s\tremaining: 4m 9s\n",
      "68:\ttotal: 1.73s\tremaining: 4m 9s\n",
      "69:\ttotal: 1.75s\tremaining: 4m 7s\n",
      "70:\ttotal: 1.76s\tremaining: 4m 6s\n",
      "71:\ttotal: 1.78s\tremaining: 4m 5s\n",
      "72:\ttotal: 1.81s\tremaining: 4m 6s\n",
      "73:\ttotal: 1.83s\tremaining: 4m 6s\n",
      "74:\ttotal: 1.85s\tremaining: 4m 5s\n",
      "75:\ttotal: 1.87s\tremaining: 4m 4s\n",
      "76:\ttotal: 1.88s\tremaining: 4m 2s\n",
      "77:\ttotal: 1.9s\tremaining: 4m 1s\n",
      "78:\ttotal: 1.92s\tremaining: 4m 1s\n",
      "79:\ttotal: 1.94s\tremaining: 4m\n",
      "80:\ttotal: 1.96s\tremaining: 4m\n",
      "81:\ttotal: 1.98s\tremaining: 3m 59s\n",
      "82:\ttotal: 2s\tremaining: 3m 59s\n",
      "83:\ttotal: 2.02s\tremaining: 3m 58s\n",
      "84:\ttotal: 2.05s\tremaining: 3m 59s\n",
      "85:\ttotal: 2.07s\tremaining: 3m 58s\n",
      "86:\ttotal: 2.08s\tremaining: 3m 57s\n",
      "87:\ttotal: 2.1s\tremaining: 3m 56s\n",
      "88:\ttotal: 2.12s\tremaining: 3m 55s\n",
      "89:\ttotal: 2.14s\tremaining: 3m 55s\n",
      "90:\ttotal: 2.17s\tremaining: 3m 56s\n",
      "91:\ttotal: 2.21s\tremaining: 3m 57s\n",
      "92:\ttotal: 2.25s\tremaining: 3m 59s\n",
      "93:\ttotal: 2.28s\tremaining: 4m\n",
      "94:\ttotal: 2.32s\tremaining: 4m 2s\n",
      "95:\ttotal: 2.35s\tremaining: 4m 2s\n",
      "96:\ttotal: 2.39s\tremaining: 4m 3s\n",
      "97:\ttotal: 2.42s\tremaining: 4m 4s\n",
      "98:\ttotal: 2.45s\tremaining: 4m 5s\n",
      "99:\ttotal: 2.48s\tremaining: 4m 5s\n",
      "100:\ttotal: 2.5s\tremaining: 4m 5s\n",
      "101:\ttotal: 2.53s\tremaining: 4m 5s\n",
      "102:\ttotal: 2.55s\tremaining: 4m 5s\n",
      "103:\ttotal: 2.59s\tremaining: 4m 6s\n",
      "104:\ttotal: 2.61s\tremaining: 4m 6s\n",
      "105:\ttotal: 2.65s\tremaining: 4m 7s\n",
      "106:\ttotal: 2.68s\tremaining: 4m 7s\n",
      "107:\ttotal: 2.71s\tremaining: 4m 8s\n",
      "108:\ttotal: 2.75s\tremaining: 4m 9s\n",
      "109:\ttotal: 2.77s\tremaining: 4m 9s\n",
      "110:\ttotal: 2.79s\tremaining: 4m 8s\n",
      "111:\ttotal: 2.81s\tremaining: 4m 7s\n",
      "112:\ttotal: 2.82s\tremaining: 4m 6s\n",
      "113:\ttotal: 2.85s\tremaining: 4m 7s\n",
      "114:\ttotal: 2.89s\tremaining: 4m 8s\n",
      "115:\ttotal: 2.92s\tremaining: 4m 8s\n",
      "116:\ttotal: 2.94s\tremaining: 4m 8s\n",
      "117:\ttotal: 2.97s\tremaining: 4m 9s\n",
      "118:\ttotal: 3s\tremaining: 4m 9s\n",
      "119:\ttotal: 3.03s\tremaining: 4m 9s\n",
      "120:\ttotal: 3.06s\tremaining: 4m 10s\n",
      "121:\ttotal: 3.1s\tremaining: 4m 10s\n",
      "122:\ttotal: 3.13s\tremaining: 4m 11s\n",
      "123:\ttotal: 3.16s\tremaining: 4m 11s\n",
      "124:\ttotal: 3.19s\tremaining: 4m 11s\n",
      "125:\ttotal: 3.2s\tremaining: 4m 10s\n",
      "126:\ttotal: 3.22s\tremaining: 4m 10s\n",
      "127:\ttotal: 3.24s\tremaining: 4m 9s\n",
      "128:\ttotal: 3.25s\tremaining: 4m 8s\n",
      "129:\ttotal: 3.27s\tremaining: 4m 8s\n",
      "130:\ttotal: 3.3s\tremaining: 4m 8s\n",
      "131:\ttotal: 3.33s\tremaining: 4m 9s\n",
      "132:\ttotal: 3.36s\tremaining: 4m 9s\n",
      "133:\ttotal: 3.39s\tremaining: 4m 9s\n",
      "134:\ttotal: 3.42s\tremaining: 4m 9s\n",
      "135:\ttotal: 3.45s\tremaining: 4m 10s\n",
      "136:\ttotal: 3.48s\tremaining: 4m 10s\n",
      "137:\ttotal: 3.52s\tremaining: 4m 11s\n",
      "138:\ttotal: 3.55s\tremaining: 4m 11s\n",
      "139:\ttotal: 3.57s\tremaining: 4m 11s\n",
      "140:\ttotal: 3.59s\tremaining: 4m 10s\n",
      "141:\ttotal: 3.6s\tremaining: 4m 10s\n",
      "142:\ttotal: 3.62s\tremaining: 4m 9s\n",
      "143:\ttotal: 3.63s\tremaining: 4m 8s\n",
      "144:\ttotal: 3.65s\tremaining: 4m 8s\n",
      "145:\ttotal: 3.67s\tremaining: 4m 7s\n",
      "146:\ttotal: 3.68s\tremaining: 4m 6s\n",
      "147:\ttotal: 3.71s\tremaining: 4m 6s\n",
      "148:\ttotal: 3.75s\tremaining: 4m 7s\n",
      "149:\ttotal: 3.78s\tremaining: 4m 8s\n",
      "150:\ttotal: 3.79s\tremaining: 4m 7s\n",
      "151:\ttotal: 3.81s\tremaining: 4m 6s\n",
      "152:\ttotal: 3.83s\tremaining: 4m 6s\n",
      "153:\ttotal: 3.84s\tremaining: 4m 5s\n",
      "154:\ttotal: 3.86s\tremaining: 4m 5s\n",
      "155:\ttotal: 3.87s\tremaining: 4m 4s\n",
      "156:\ttotal: 3.9s\tremaining: 4m 4s\n",
      "157:\ttotal: 3.93s\tremaining: 4m 4s\n",
      "158:\ttotal: 3.95s\tremaining: 4m 4s\n",
      "159:\ttotal: 3.98s\tremaining: 4m 5s\n",
      "160:\ttotal: 4.02s\tremaining: 4m 5s\n",
      "161:\ttotal: 4.05s\tremaining: 4m 6s\n",
      "162:\ttotal: 4.09s\tremaining: 4m 6s\n",
      "163:\ttotal: 4.14s\tremaining: 4m 8s\n",
      "164:\ttotal: 4.18s\tremaining: 4m 9s\n",
      "165:\ttotal: 4.21s\tremaining: 4m 9s\n",
      "166:\ttotal: 4.26s\tremaining: 4m 11s\n",
      "167:\ttotal: 4.29s\tremaining: 4m 10s\n",
      "168:\ttotal: 4.32s\tremaining: 4m 11s\n",
      "169:\ttotal: 4.35s\tremaining: 4m 11s\n",
      "170:\ttotal: 4.38s\tremaining: 4m 11s\n",
      "171:\ttotal: 4.42s\tremaining: 4m 12s\n",
      "172:\ttotal: 4.46s\tremaining: 4m 13s\n",
      "173:\ttotal: 4.49s\tremaining: 4m 13s\n",
      "174:\ttotal: 4.52s\tremaining: 4m 13s\n",
      "175:\ttotal: 4.55s\tremaining: 4m 14s\n",
      "176:\ttotal: 4.58s\tremaining: 4m 14s\n",
      "177:\ttotal: 4.62s\tremaining: 4m 14s\n",
      "178:\ttotal: 4.64s\tremaining: 4m 14s\n",
      "179:\ttotal: 4.67s\tremaining: 4m 14s\n",
      "180:\ttotal: 4.7s\tremaining: 4m 14s\n",
      "181:\ttotal: 4.72s\tremaining: 4m 14s\n",
      "182:\ttotal: 4.74s\tremaining: 4m 14s\n",
      "183:\ttotal: 4.76s\tremaining: 4m 14s\n",
      "184:\ttotal: 4.79s\tremaining: 4m 14s\n",
      "185:\ttotal: 4.82s\tremaining: 4m 14s\n",
      "186:\ttotal: 4.85s\tremaining: 4m 14s\n",
      "187:\ttotal: 4.87s\tremaining: 4m 14s\n",
      "188:\ttotal: 4.9s\tremaining: 4m 14s\n",
      "189:\ttotal: 4.93s\tremaining: 4m 14s\n",
      "190:\ttotal: 4.96s\tremaining: 4m 14s\n",
      "191:\ttotal: 4.99s\tremaining: 4m 14s\n",
      "192:\ttotal: 5.02s\tremaining: 4m 15s\n",
      "193:\ttotal: 5.05s\tremaining: 4m 15s\n",
      "194:\ttotal: 5.08s\tremaining: 4m 15s\n",
      "195:\ttotal: 5.11s\tremaining: 4m 15s\n",
      "196:\ttotal: 5.14s\tremaining: 4m 15s\n",
      "197:\ttotal: 5.17s\tremaining: 4m 15s\n",
      "198:\ttotal: 5.21s\tremaining: 4m 16s\n",
      "199:\ttotal: 5.24s\tremaining: 4m 16s\n",
      "200:\ttotal: 5.27s\tremaining: 4m 17s\n",
      "201:\ttotal: 5.31s\tremaining: 4m 17s\n",
      "202:\ttotal: 5.35s\tremaining: 4m 18s\n",
      "203:\ttotal: 5.38s\tremaining: 4m 18s\n",
      "204:\ttotal: 5.4s\tremaining: 4m 18s\n",
      "205:\ttotal: 5.43s\tremaining: 4m 18s\n",
      "206:\ttotal: 5.46s\tremaining: 4m 18s\n",
      "207:\ttotal: 5.49s\tremaining: 4m 18s\n",
      "208:\ttotal: 5.53s\tremaining: 4m 18s\n",
      "209:\ttotal: 5.56s\tremaining: 4m 19s\n",
      "210:\ttotal: 5.59s\tremaining: 4m 19s\n",
      "211:\ttotal: 5.63s\tremaining: 4m 20s\n",
      "212:\ttotal: 5.67s\tremaining: 4m 20s\n",
      "213:\ttotal: 5.7s\tremaining: 4m 20s\n",
      "214:\ttotal: 5.75s\tremaining: 4m 21s\n",
      "215:\ttotal: 5.77s\tremaining: 4m 21s\n",
      "216:\ttotal: 5.8s\tremaining: 4m 21s\n",
      "217:\ttotal: 5.83s\tremaining: 4m 21s\n",
      "218:\ttotal: 5.85s\tremaining: 4m 21s\n",
      "219:\ttotal: 5.88s\tremaining: 4m 21s\n",
      "220:\ttotal: 5.91s\tremaining: 4m 21s\n",
      "221:\ttotal: 5.94s\tremaining: 4m 21s\n",
      "222:\ttotal: 5.98s\tremaining: 4m 22s\n",
      "223:\ttotal: 6.01s\tremaining: 4m 22s\n",
      "224:\ttotal: 6.05s\tremaining: 4m 22s\n",
      "225:\ttotal: 6.08s\tremaining: 4m 23s\n",
      "226:\ttotal: 6.11s\tremaining: 4m 22s\n",
      "227:\ttotal: 6.13s\tremaining: 4m 22s\n",
      "228:\ttotal: 6.16s\tremaining: 4m 22s\n",
      "229:\ttotal: 6.18s\tremaining: 4m 22s\n",
      "230:\ttotal: 6.21s\tremaining: 4m 22s\n",
      "231:\ttotal: 6.23s\tremaining: 4m 22s\n",
      "232:\ttotal: 6.25s\tremaining: 4m 21s\n",
      "233:\ttotal: 6.26s\tremaining: 4m 21s\n",
      "234:\ttotal: 6.29s\tremaining: 4m 21s\n",
      "235:\ttotal: 6.32s\tremaining: 4m 21s\n",
      "236:\ttotal: 6.35s\tremaining: 4m 21s\n",
      "237:\ttotal: 6.39s\tremaining: 4m 22s\n",
      "238:\ttotal: 6.43s\tremaining: 4m 22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239:\ttotal: 6.47s\tremaining: 4m 22s\n",
      "240:\ttotal: 6.5s\tremaining: 4m 23s\n",
      "241:\ttotal: 6.53s\tremaining: 4m 23s\n",
      "242:\ttotal: 6.55s\tremaining: 4m 22s\n",
      "243:\ttotal: 6.57s\tremaining: 4m 22s\n",
      "244:\ttotal: 6.59s\tremaining: 4m 22s\n",
      "245:\ttotal: 6.6s\tremaining: 4m 21s\n",
      "246:\ttotal: 6.62s\tremaining: 4m 21s\n",
      "247:\ttotal: 6.63s\tremaining: 4m 20s\n",
      "248:\ttotal: 6.65s\tremaining: 4m 20s\n",
      "249:\ttotal: 6.67s\tremaining: 4m 19s\n",
      "250:\ttotal: 6.7s\tremaining: 4m 20s\n",
      "251:\ttotal: 6.74s\tremaining: 4m 20s\n",
      "252:\ttotal: 6.75s\tremaining: 4m 20s\n",
      "253:\ttotal: 6.77s\tremaining: 4m 19s\n",
      "254:\ttotal: 6.78s\tremaining: 4m 19s\n",
      "255:\ttotal: 6.8s\tremaining: 4m 18s\n",
      "256:\ttotal: 6.82s\tremaining: 4m 18s\n",
      "257:\ttotal: 6.83s\tremaining: 4m 17s\n",
      "258:\ttotal: 6.85s\tremaining: 4m 17s\n",
      "259:\ttotal: 6.86s\tremaining: 4m 17s\n",
      "260:\ttotal: 6.88s\tremaining: 4m 16s\n",
      "261:\ttotal: 6.89s\tremaining: 4m 16s\n",
      "262:\ttotal: 6.92s\tremaining: 4m 16s\n",
      "263:\ttotal: 6.94s\tremaining: 4m 15s\n",
      "264:\ttotal: 6.96s\tremaining: 4m 15s\n",
      "265:\ttotal: 6.97s\tremaining: 4m 15s\n",
      "266:\ttotal: 6.99s\tremaining: 4m 14s\n",
      "267:\ttotal: 7.01s\tremaining: 4m 14s\n",
      "268:\ttotal: 7.03s\tremaining: 4m 14s\n",
      "269:\ttotal: 7.04s\tremaining: 4m 13s\n",
      "270:\ttotal: 7.06s\tremaining: 4m 13s\n",
      "271:\ttotal: 7.07s\tremaining: 4m 12s\n",
      "272:\ttotal: 7.11s\tremaining: 4m 13s\n",
      "273:\ttotal: 7.14s\tremaining: 4m 13s\n",
      "274:\ttotal: 7.18s\tremaining: 4m 13s\n",
      "275:\ttotal: 7.22s\tremaining: 4m 14s\n",
      "276:\ttotal: 7.25s\tremaining: 4m 14s\n",
      "277:\ttotal: 7.27s\tremaining: 4m 14s\n",
      "278:\ttotal: 7.31s\tremaining: 4m 14s\n",
      "279:\ttotal: 7.33s\tremaining: 4m 14s\n",
      "280:\ttotal: 7.36s\tremaining: 4m 14s\n",
      "281:\ttotal: 7.39s\tremaining: 4m 14s\n",
      "282:\ttotal: 7.41s\tremaining: 4m 14s\n",
      "283:\ttotal: 7.42s\tremaining: 4m 13s\n",
      "284:\ttotal: 7.44s\tremaining: 4m 13s\n",
      "285:\ttotal: 7.45s\tremaining: 4m 13s\n",
      "286:\ttotal: 7.47s\tremaining: 4m 12s\n",
      "287:\ttotal: 7.49s\tremaining: 4m 12s\n",
      "288:\ttotal: 7.52s\tremaining: 4m 12s\n",
      "289:\ttotal: 7.55s\tremaining: 4m 12s\n",
      "290:\ttotal: 7.58s\tremaining: 4m 13s\n",
      "291:\ttotal: 7.61s\tremaining: 4m 13s\n",
      "292:\ttotal: 7.65s\tremaining: 4m 13s\n",
      "293:\ttotal: 7.67s\tremaining: 4m 13s\n",
      "294:\ttotal: 7.71s\tremaining: 4m 13s\n",
      "295:\ttotal: 7.76s\tremaining: 4m 14s\n",
      "296:\ttotal: 7.79s\tremaining: 4m 14s\n",
      "297:\ttotal: 7.82s\tremaining: 4m 14s\n",
      "298:\ttotal: 7.85s\tremaining: 4m 14s\n",
      "299:\ttotal: 7.88s\tremaining: 4m 14s\n",
      "300:\ttotal: 7.9s\tremaining: 4m 14s\n",
      "301:\ttotal: 7.92s\tremaining: 4m 14s\n",
      "302:\ttotal: 7.95s\tremaining: 4m 14s\n",
      "303:\ttotal: 7.98s\tremaining: 4m 14s\n",
      "304:\ttotal: 8.01s\tremaining: 4m 14s\n",
      "305:\ttotal: 8.05s\tremaining: 4m 15s\n",
      "306:\ttotal: 8.08s\tremaining: 4m 15s\n",
      "307:\ttotal: 8.11s\tremaining: 4m 15s\n",
      "308:\ttotal: 8.14s\tremaining: 4m 15s\n",
      "309:\ttotal: 8.16s\tremaining: 4m 15s\n",
      "310:\ttotal: 8.2s\tremaining: 4m 15s\n",
      "311:\ttotal: 8.23s\tremaining: 4m 15s\n",
      "312:\ttotal: 8.25s\tremaining: 4m 15s\n",
      "313:\ttotal: 8.28s\tremaining: 4m 15s\n",
      "314:\ttotal: 8.31s\tremaining: 4m 15s\n",
      "315:\ttotal: 8.34s\tremaining: 4m 15s\n",
      "316:\ttotal: 8.39s\tremaining: 4m 16s\n",
      "317:\ttotal: 8.42s\tremaining: 4m 16s\n",
      "318:\ttotal: 8.45s\tremaining: 4m 16s\n",
      "319:\ttotal: 8.49s\tremaining: 4m 16s\n",
      "320:\ttotal: 8.54s\tremaining: 4m 17s\n",
      "321:\ttotal: 8.56s\tremaining: 4m 17s\n",
      "322:\ttotal: 8.59s\tremaining: 4m 17s\n",
      "323:\ttotal: 8.62s\tremaining: 4m 17s\n",
      "324:\ttotal: 8.65s\tremaining: 4m 17s\n",
      "325:\ttotal: 8.68s\tremaining: 4m 17s\n",
      "326:\ttotal: 8.72s\tremaining: 4m 17s\n",
      "327:\ttotal: 8.76s\tremaining: 4m 18s\n",
      "328:\ttotal: 8.8s\tremaining: 4m 18s\n",
      "329:\ttotal: 8.84s\tremaining: 4m 19s\n",
      "330:\ttotal: 8.87s\tremaining: 4m 19s\n",
      "331:\ttotal: 8.91s\tremaining: 4m 19s\n",
      "332:\ttotal: 8.95s\tremaining: 4m 19s\n",
      "333:\ttotal: 8.97s\tremaining: 4m 19s\n",
      "334:\ttotal: 8.99s\tremaining: 4m 19s\n",
      "335:\ttotal: 9.01s\tremaining: 4m 19s\n",
      "336:\ttotal: 9.03s\tremaining: 4m 18s\n",
      "337:\ttotal: 9.04s\tremaining: 4m 18s\n",
      "338:\ttotal: 9.08s\tremaining: 4m 18s\n",
      "339:\ttotal: 9.1s\tremaining: 4m 18s\n",
      "340:\ttotal: 9.13s\tremaining: 4m 18s\n",
      "341:\ttotal: 9.16s\tremaining: 4m 18s\n",
      "342:\ttotal: 9.19s\tremaining: 4m 18s\n",
      "343:\ttotal: 9.2s\tremaining: 4m 18s\n",
      "344:\ttotal: 9.22s\tremaining: 4m 18s\n",
      "345:\ttotal: 9.24s\tremaining: 4m 17s\n",
      "346:\ttotal: 9.26s\tremaining: 4m 17s\n",
      "347:\ttotal: 9.3s\tremaining: 4m 18s\n",
      "348:\ttotal: 9.34s\tremaining: 4m 18s\n",
      "349:\ttotal: 9.37s\tremaining: 4m 18s\n",
      "350:\ttotal: 9.4s\tremaining: 4m 18s\n",
      "351:\ttotal: 9.44s\tremaining: 4m 18s\n",
      "352:\ttotal: 9.47s\tremaining: 4m 18s\n",
      "353:\ttotal: 9.51s\tremaining: 4m 19s\n",
      "354:\ttotal: 9.54s\tremaining: 4m 19s\n",
      "355:\ttotal: 9.57s\tremaining: 4m 19s\n",
      "356:\ttotal: 9.61s\tremaining: 4m 19s\n",
      "357:\ttotal: 9.63s\tremaining: 4m 19s\n",
      "358:\ttotal: 9.66s\tremaining: 4m 19s\n",
      "359:\ttotal: 9.68s\tremaining: 4m 19s\n",
      "360:\ttotal: 9.71s\tremaining: 4m 19s\n",
      "361:\ttotal: 9.73s\tremaining: 4m 18s\n",
      "362:\ttotal: 9.74s\tremaining: 4m 18s\n",
      "363:\ttotal: 9.76s\tremaining: 4m 18s\n",
      "364:\ttotal: 9.77s\tremaining: 4m 17s\n",
      "365:\ttotal: 9.79s\tremaining: 4m 17s\n",
      "366:\ttotal: 9.81s\tremaining: 4m 17s\n",
      "367:\ttotal: 9.84s\tremaining: 4m 17s\n",
      "368:\ttotal: 9.85s\tremaining: 4m 17s\n",
      "369:\ttotal: 9.87s\tremaining: 4m 16s\n",
      "370:\ttotal: 9.88s\tremaining: 4m 16s\n",
      "371:\ttotal: 9.9s\tremaining: 4m 16s\n",
      "372:\ttotal: 9.93s\tremaining: 4m 16s\n",
      "373:\ttotal: 9.96s\tremaining: 4m 16s\n",
      "374:\ttotal: 9.98s\tremaining: 4m 16s\n",
      "375:\ttotal: 10s\tremaining: 4m 15s\n",
      "376:\ttotal: 10s\tremaining: 4m 15s\n",
      "377:\ttotal: 10s\tremaining: 4m 15s\n",
      "378:\ttotal: 10s\tremaining: 4m 14s\n",
      "379:\ttotal: 10.1s\tremaining: 4m 14s\n",
      "380:\ttotal: 10.1s\tremaining: 4m 14s\n",
      "381:\ttotal: 10.1s\tremaining: 4m 14s\n",
      "382:\ttotal: 10.1s\tremaining: 4m 14s\n",
      "383:\ttotal: 10.2s\tremaining: 4m 14s\n",
      "384:\ttotal: 10.2s\tremaining: 4m 14s\n",
      "385:\ttotal: 10.2s\tremaining: 4m 13s\n",
      "386:\ttotal: 10.2s\tremaining: 4m 13s\n",
      "387:\ttotal: 10.2s\tremaining: 4m 13s\n",
      "388:\ttotal: 10.2s\tremaining: 4m 13s\n",
      "389:\ttotal: 10.3s\tremaining: 4m 12s\n",
      "390:\ttotal: 10.3s\tremaining: 4m 12s\n",
      "391:\ttotal: 10.3s\tremaining: 4m 12s\n",
      "392:\ttotal: 10.3s\tremaining: 4m 12s\n",
      "393:\ttotal: 10.4s\tremaining: 4m 12s\n",
      "394:\ttotal: 10.4s\tremaining: 4m 12s\n",
      "395:\ttotal: 10.4s\tremaining: 4m 12s\n",
      "396:\ttotal: 10.4s\tremaining: 4m 12s\n",
      "397:\ttotal: 10.4s\tremaining: 4m 11s\n",
      "398:\ttotal: 10.5s\tremaining: 4m 11s\n",
      "399:\ttotal: 10.5s\tremaining: 4m 11s\n",
      "400:\ttotal: 10.5s\tremaining: 4m 11s\n",
      "401:\ttotal: 10.6s\tremaining: 4m 12s\n",
      "402:\ttotal: 10.6s\tremaining: 4m 12s\n",
      "403:\ttotal: 10.6s\tremaining: 4m 12s\n",
      "404:\ttotal: 10.7s\tremaining: 4m 12s\n",
      "405:\ttotal: 10.7s\tremaining: 4m 12s\n",
      "406:\ttotal: 10.7s\tremaining: 4m 12s\n",
      "407:\ttotal: 10.7s\tremaining: 4m 12s\n",
      "408:\ttotal: 10.8s\tremaining: 4m 12s\n",
      "409:\ttotal: 10.8s\tremaining: 4m 13s\n",
      "410:\ttotal: 10.8s\tremaining: 4m 12s\n",
      "411:\ttotal: 10.9s\tremaining: 4m 12s\n",
      "412:\ttotal: 10.9s\tremaining: 4m 12s\n",
      "413:\ttotal: 10.9s\tremaining: 4m 12s\n",
      "414:\ttotal: 10.9s\tremaining: 4m 12s\n",
      "415:\ttotal: 11s\tremaining: 4m 12s\n",
      "416:\ttotal: 11s\tremaining: 4m 12s\n",
      "417:\ttotal: 11s\tremaining: 4m 13s\n",
      "418:\ttotal: 11.1s\tremaining: 4m 13s\n",
      "419:\ttotal: 11.1s\tremaining: 4m 13s\n",
      "420:\ttotal: 11.1s\tremaining: 4m 13s\n",
      "421:\ttotal: 11.1s\tremaining: 4m 12s\n",
      "422:\ttotal: 11.2s\tremaining: 4m 13s\n",
      "423:\ttotal: 11.2s\tremaining: 4m 13s\n",
      "424:\ttotal: 11.2s\tremaining: 4m 13s\n",
      "425:\ttotal: 11.3s\tremaining: 4m 13s\n",
      "426:\ttotal: 11.3s\tremaining: 4m 13s\n",
      "427:\ttotal: 11.3s\tremaining: 4m 12s\n",
      "428:\ttotal: 11.3s\tremaining: 4m 12s\n",
      "429:\ttotal: 11.3s\tremaining: 4m 12s\n",
      "430:\ttotal: 11.4s\tremaining: 4m 11s\n",
      "431:\ttotal: 11.4s\tremaining: 4m 11s\n",
      "432:\ttotal: 11.4s\tremaining: 4m 11s\n",
      "433:\ttotal: 11.4s\tremaining: 4m 11s\n",
      "434:\ttotal: 11.4s\tremaining: 4m 11s\n",
      "435:\ttotal: 11.5s\tremaining: 4m 11s\n",
      "436:\ttotal: 11.5s\tremaining: 4m 11s\n",
      "437:\ttotal: 11.5s\tremaining: 4m 11s\n",
      "438:\ttotal: 11.5s\tremaining: 4m 11s\n",
      "439:\ttotal: 11.6s\tremaining: 4m 10s\n",
      "440:\ttotal: 11.6s\tremaining: 4m 10s\n",
      "441:\ttotal: 11.6s\tremaining: 4m 11s\n",
      "442:\ttotal: 11.6s\tremaining: 4m 10s\n",
      "443:\ttotal: 11.7s\tremaining: 4m 10s\n",
      "444:\ttotal: 11.7s\tremaining: 4m 10s\n",
      "445:\ttotal: 11.7s\tremaining: 4m 10s\n",
      "446:\ttotal: 11.7s\tremaining: 4m 10s\n",
      "447:\ttotal: 11.8s\tremaining: 4m 10s\n",
      "448:\ttotal: 11.8s\tremaining: 4m 10s\n",
      "449:\ttotal: 11.8s\tremaining: 4m 11s\n",
      "450:\ttotal: 11.9s\tremaining: 4m 10s\n",
      "451:\ttotal: 11.9s\tremaining: 4m 11s\n",
      "452:\ttotal: 11.9s\tremaining: 4m 11s\n",
      "453:\ttotal: 11.9s\tremaining: 4m 11s\n",
      "454:\ttotal: 12s\tremaining: 4m 11s\n",
      "455:\ttotal: 12s\tremaining: 4m 10s\n",
      "456:\ttotal: 12s\tremaining: 4m 10s\n",
      "457:\ttotal: 12s\tremaining: 4m 10s\n",
      "458:\ttotal: 12s\tremaining: 4m 10s\n",
      "459:\ttotal: 12.1s\tremaining: 4m 10s\n",
      "460:\ttotal: 12.1s\tremaining: 4m 9s\n",
      "461:\ttotal: 12.1s\tremaining: 4m 9s\n",
      "462:\ttotal: 12.1s\tremaining: 4m 9s\n",
      "463:\ttotal: 12.2s\tremaining: 4m 9s\n",
      "464:\ttotal: 12.2s\tremaining: 4m 9s\n",
      "465:\ttotal: 12.2s\tremaining: 4m 9s\n",
      "466:\ttotal: 12.2s\tremaining: 4m 9s\n",
      "467:\ttotal: 12.3s\tremaining: 4m 9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468:\ttotal: 12.3s\tremaining: 4m 9s\n",
      "469:\ttotal: 12.3s\tremaining: 4m 9s\n",
      "470:\ttotal: 12.3s\tremaining: 4m 9s\n",
      "471:\ttotal: 12.4s\tremaining: 4m 9s\n",
      "472:\ttotal: 12.4s\tremaining: 4m 9s\n",
      "473:\ttotal: 12.4s\tremaining: 4m 9s\n",
      "474:\ttotal: 12.4s\tremaining: 4m 8s\n",
      "475:\ttotal: 12.4s\tremaining: 4m 8s\n",
      "476:\ttotal: 12.4s\tremaining: 4m 8s\n",
      "477:\ttotal: 12.5s\tremaining: 4m 8s\n",
      "478:\ttotal: 12.5s\tremaining: 4m 8s\n",
      "479:\ttotal: 12.5s\tremaining: 4m 8s\n",
      "480:\ttotal: 12.5s\tremaining: 4m 8s\n",
      "481:\ttotal: 12.6s\tremaining: 4m 8s\n",
      "482:\ttotal: 12.6s\tremaining: 4m 8s\n",
      "483:\ttotal: 12.6s\tremaining: 4m 8s\n",
      "484:\ttotal: 12.7s\tremaining: 4m 8s\n",
      "485:\ttotal: 12.7s\tremaining: 4m 8s\n",
      "486:\ttotal: 12.7s\tremaining: 4m 8s\n",
      "487:\ttotal: 12.7s\tremaining: 4m 8s\n",
      "488:\ttotal: 12.8s\tremaining: 4m 8s\n",
      "489:\ttotal: 12.8s\tremaining: 4m 8s\n",
      "490:\ttotal: 12.8s\tremaining: 4m 8s\n",
      "491:\ttotal: 12.8s\tremaining: 4m 8s\n",
      "492:\ttotal: 12.9s\tremaining: 4m 7s\n",
      "493:\ttotal: 12.9s\tremaining: 4m 7s\n",
      "494:\ttotal: 12.9s\tremaining: 4m 7s\n",
      "495:\ttotal: 12.9s\tremaining: 4m 7s\n",
      "496:\ttotal: 13s\tremaining: 4m 7s\n",
      "497:\ttotal: 13s\tremaining: 4m 7s\n",
      "498:\ttotal: 13s\tremaining: 4m 7s\n",
      "499:\ttotal: 13s\tremaining: 4m 7s\n",
      "500:\ttotal: 13s\tremaining: 4m 7s\n",
      "501:\ttotal: 13s\tremaining: 4m 6s\n",
      "502:\ttotal: 13.1s\tremaining: 4m 6s\n",
      "503:\ttotal: 13.1s\tremaining: 4m 6s\n",
      "504:\ttotal: 13.1s\tremaining: 4m 6s\n",
      "505:\ttotal: 13.1s\tremaining: 4m 6s\n",
      "506:\ttotal: 13.1s\tremaining: 4m 5s\n",
      "507:\ttotal: 13.2s\tremaining: 4m 5s\n",
      "508:\ttotal: 13.2s\tremaining: 4m 5s\n",
      "509:\ttotal: 13.2s\tremaining: 4m 5s\n",
      "510:\ttotal: 13.2s\tremaining: 4m 5s\n",
      "511:\ttotal: 13.2s\tremaining: 4m 5s\n",
      "512:\ttotal: 13.3s\tremaining: 4m 5s\n",
      "513:\ttotal: 13.3s\tremaining: 4m 5s\n",
      "514:\ttotal: 13.3s\tremaining: 4m 5s\n",
      "515:\ttotal: 13.3s\tremaining: 4m 5s\n",
      "516:\ttotal: 13.4s\tremaining: 4m 4s\n",
      "517:\ttotal: 13.4s\tremaining: 4m 4s\n",
      "518:\ttotal: 13.4s\tremaining: 4m 5s\n",
      "519:\ttotal: 13.4s\tremaining: 4m 4s\n",
      "520:\ttotal: 13.4s\tremaining: 4m 4s\n",
      "521:\ttotal: 13.5s\tremaining: 4m 4s\n",
      "522:\ttotal: 13.5s\tremaining: 4m 4s\n",
      "523:\ttotal: 13.5s\tremaining: 4m 4s\n",
      "524:\ttotal: 13.5s\tremaining: 4m 3s\n",
      "525:\ttotal: 13.5s\tremaining: 4m 3s\n",
      "526:\ttotal: 13.5s\tremaining: 4m 3s\n",
      "527:\ttotal: 13.6s\tremaining: 4m 3s\n",
      "528:\ttotal: 13.6s\tremaining: 4m 3s\n",
      "529:\ttotal: 13.6s\tremaining: 4m 3s\n",
      "530:\ttotal: 13.6s\tremaining: 4m 3s\n",
      "531:\ttotal: 13.7s\tremaining: 4m 2s\n",
      "532:\ttotal: 13.7s\tremaining: 4m 3s\n",
      "533:\ttotal: 13.7s\tremaining: 4m 3s\n",
      "534:\ttotal: 13.7s\tremaining: 4m 2s\n",
      "535:\ttotal: 13.7s\tremaining: 4m 2s\n",
      "536:\ttotal: 13.8s\tremaining: 4m 2s\n",
      "537:\ttotal: 13.8s\tremaining: 4m 2s\n",
      "538:\ttotal: 13.8s\tremaining: 4m 2s\n",
      "539:\ttotal: 13.8s\tremaining: 4m 2s\n",
      "540:\ttotal: 13.8s\tremaining: 4m 2s\n",
      "541:\ttotal: 13.9s\tremaining: 4m 1s\n",
      "542:\ttotal: 13.9s\tremaining: 4m 1s\n",
      "543:\ttotal: 13.9s\tremaining: 4m 1s\n",
      "544:\ttotal: 13.9s\tremaining: 4m 1s\n",
      "545:\ttotal: 13.9s\tremaining: 4m 1s\n",
      "546:\ttotal: 14s\tremaining: 4m 1s\n",
      "547:\ttotal: 14s\tremaining: 4m\n",
      "548:\ttotal: 14s\tremaining: 4m\n",
      "549:\ttotal: 14s\tremaining: 4m\n",
      "550:\ttotal: 14s\tremaining: 4m\n",
      "551:\ttotal: 14.1s\tremaining: 4m\n",
      "552:\ttotal: 14.1s\tremaining: 4m\n",
      "553:\ttotal: 14.1s\tremaining: 4m\n",
      "554:\ttotal: 14.1s\tremaining: 4m\n",
      "555:\ttotal: 14.1s\tremaining: 3m 59s\n",
      "556:\ttotal: 14.1s\tremaining: 3m 59s\n",
      "557:\ttotal: 14.2s\tremaining: 3m 59s\n",
      "558:\ttotal: 14.2s\tremaining: 3m 59s\n",
      "559:\ttotal: 14.2s\tremaining: 3m 59s\n",
      "560:\ttotal: 14.3s\tremaining: 3m 59s\n",
      "561:\ttotal: 14.3s\tremaining: 3m 59s\n",
      "562:\ttotal: 14.3s\tremaining: 3m 59s\n",
      "563:\ttotal: 14.3s\tremaining: 4m\n",
      "564:\ttotal: 14.4s\tremaining: 4m\n",
      "565:\ttotal: 14.4s\tremaining: 4m\n",
      "566:\ttotal: 14.4s\tremaining: 4m\n",
      "567:\ttotal: 14.5s\tremaining: 4m\n",
      "568:\ttotal: 14.5s\tremaining: 4m\n",
      "569:\ttotal: 14.5s\tremaining: 3m 59s\n",
      "570:\ttotal: 14.5s\tremaining: 3m 59s\n",
      "571:\ttotal: 14.6s\tremaining: 3m 59s\n",
      "572:\ttotal: 14.6s\tremaining: 3m 59s\n",
      "573:\ttotal: 14.6s\tremaining: 3m 59s\n",
      "574:\ttotal: 14.6s\tremaining: 3m 59s\n",
      "575:\ttotal: 14.7s\tremaining: 3m 59s\n",
      "576:\ttotal: 14.7s\tremaining: 3m 59s\n",
      "577:\ttotal: 14.7s\tremaining: 3m 59s\n",
      "578:\ttotal: 14.7s\tremaining: 3m 59s\n",
      "579:\ttotal: 14.7s\tremaining: 3m 59s\n",
      "580:\ttotal: 14.8s\tremaining: 3m 59s\n",
      "581:\ttotal: 14.8s\tremaining: 3m 59s\n",
      "582:\ttotal: 14.8s\tremaining: 3m 59s\n",
      "583:\ttotal: 14.8s\tremaining: 3m 59s\n",
      "584:\ttotal: 14.9s\tremaining: 3m 59s\n",
      "585:\ttotal: 14.9s\tremaining: 3m 59s\n",
      "586:\ttotal: 14.9s\tremaining: 3m 59s\n",
      "587:\ttotal: 14.9s\tremaining: 3m 58s\n",
      "588:\ttotal: 14.9s\tremaining: 3m 58s\n",
      "589:\ttotal: 15s\tremaining: 3m 59s\n",
      "590:\ttotal: 15s\tremaining: 3m 59s\n",
      "591:\ttotal: 15s\tremaining: 3m 59s\n",
      "592:\ttotal: 15.1s\tremaining: 3m 59s\n",
      "593:\ttotal: 15.1s\tremaining: 3m 58s\n",
      "594:\ttotal: 15.1s\tremaining: 3m 58s\n",
      "595:\ttotal: 15.1s\tremaining: 3m 58s\n",
      "596:\ttotal: 15.1s\tremaining: 3m 58s\n",
      "597:\ttotal: 15.2s\tremaining: 3m 58s\n",
      "598:\ttotal: 15.2s\tremaining: 3m 58s\n",
      "599:\ttotal: 15.2s\tremaining: 3m 58s\n",
      "600:\ttotal: 15.2s\tremaining: 3m 58s\n",
      "601:\ttotal: 15.3s\tremaining: 3m 58s\n",
      "602:\ttotal: 15.3s\tremaining: 3m 57s\n",
      "603:\ttotal: 15.3s\tremaining: 3m 57s\n",
      "604:\ttotal: 15.3s\tremaining: 3m 57s\n",
      "605:\ttotal: 15.3s\tremaining: 3m 57s\n",
      "606:\ttotal: 15.3s\tremaining: 3m 57s\n",
      "607:\ttotal: 15.3s\tremaining: 3m 57s\n",
      "608:\ttotal: 15.4s\tremaining: 3m 56s\n",
      "609:\ttotal: 15.4s\tremaining: 3m 56s\n",
      "610:\ttotal: 15.4s\tremaining: 3m 56s\n",
      "611:\ttotal: 15.4s\tremaining: 3m 56s\n",
      "612:\ttotal: 15.4s\tremaining: 3m 56s\n",
      "613:\ttotal: 15.5s\tremaining: 3m 56s\n",
      "614:\ttotal: 15.5s\tremaining: 3m 56s\n",
      "615:\ttotal: 15.5s\tremaining: 3m 56s\n",
      "616:\ttotal: 15.5s\tremaining: 3m 56s\n",
      "617:\ttotal: 15.6s\tremaining: 3m 56s\n",
      "618:\ttotal: 15.6s\tremaining: 3m 56s\n",
      "619:\ttotal: 15.6s\tremaining: 3m 56s\n",
      "620:\ttotal: 15.6s\tremaining: 3m 56s\n",
      "621:\ttotal: 15.7s\tremaining: 3m 56s\n",
      "622:\ttotal: 15.7s\tremaining: 3m 56s\n",
      "623:\ttotal: 15.7s\tremaining: 3m 55s\n",
      "624:\ttotal: 15.7s\tremaining: 3m 55s\n",
      "625:\ttotal: 15.7s\tremaining: 3m 55s\n",
      "626:\ttotal: 15.8s\tremaining: 3m 55s\n",
      "627:\ttotal: 15.8s\tremaining: 3m 55s\n",
      "628:\ttotal: 15.8s\tremaining: 3m 55s\n",
      "629:\ttotal: 15.8s\tremaining: 3m 55s\n",
      "630:\ttotal: 15.8s\tremaining: 3m 54s\n",
      "631:\ttotal: 15.8s\tremaining: 3m 54s\n",
      "632:\ttotal: 15.9s\tremaining: 3m 54s\n",
      "633:\ttotal: 15.9s\tremaining: 3m 54s\n",
      "634:\ttotal: 15.9s\tremaining: 3m 54s\n",
      "635:\ttotal: 15.9s\tremaining: 3m 54s\n",
      "636:\ttotal: 15.9s\tremaining: 3m 54s\n",
      "637:\ttotal: 16s\tremaining: 3m 54s\n",
      "638:\ttotal: 16s\tremaining: 3m 54s\n",
      "639:\ttotal: 16s\tremaining: 3m 53s\n",
      "640:\ttotal: 16s\tremaining: 3m 53s\n",
      "641:\ttotal: 16s\tremaining: 3m 53s\n",
      "642:\ttotal: 16.1s\tremaining: 3m 53s\n",
      "643:\ttotal: 16.1s\tremaining: 3m 53s\n",
      "644:\ttotal: 16.1s\tremaining: 3m 53s\n",
      "645:\ttotal: 16.1s\tremaining: 3m 53s\n",
      "646:\ttotal: 16.2s\tremaining: 3m 53s\n",
      "647:\ttotal: 16.2s\tremaining: 3m 53s\n",
      "648:\ttotal: 16.2s\tremaining: 3m 53s\n",
      "649:\ttotal: 16.2s\tremaining: 3m 53s\n",
      "650:\ttotal: 16.3s\tremaining: 3m 53s\n",
      "651:\ttotal: 16.3s\tremaining: 3m 53s\n",
      "652:\ttotal: 16.3s\tremaining: 3m 53s\n",
      "653:\ttotal: 16.3s\tremaining: 3m 53s\n",
      "654:\ttotal: 16.3s\tremaining: 3m 53s\n",
      "655:\ttotal: 16.4s\tremaining: 3m 53s\n",
      "656:\ttotal: 16.4s\tremaining: 3m 52s\n",
      "657:\ttotal: 16.4s\tremaining: 3m 53s\n",
      "658:\ttotal: 16.5s\tremaining: 3m 53s\n",
      "659:\ttotal: 16.5s\tremaining: 3m 53s\n",
      "660:\ttotal: 16.5s\tremaining: 3m 53s\n",
      "661:\ttotal: 16.5s\tremaining: 3m 52s\n",
      "662:\ttotal: 16.5s\tremaining: 3m 52s\n",
      "663:\ttotal: 16.5s\tremaining: 3m 52s\n",
      "664:\ttotal: 16.6s\tremaining: 3m 52s\n",
      "665:\ttotal: 16.6s\tremaining: 3m 52s\n",
      "666:\ttotal: 16.6s\tremaining: 3m 52s\n",
      "667:\ttotal: 16.6s\tremaining: 3m 51s\n",
      "668:\ttotal: 16.6s\tremaining: 3m 51s\n",
      "669:\ttotal: 16.6s\tremaining: 3m 51s\n",
      "670:\ttotal: 16.7s\tremaining: 3m 51s\n",
      "671:\ttotal: 16.7s\tremaining: 3m 51s\n",
      "672:\ttotal: 16.7s\tremaining: 3m 51s\n",
      "673:\ttotal: 16.7s\tremaining: 3m 51s\n",
      "674:\ttotal: 16.8s\tremaining: 3m 51s\n",
      "675:\ttotal: 16.8s\tremaining: 3m 51s\n",
      "676:\ttotal: 16.8s\tremaining: 3m 51s\n",
      "677:\ttotal: 16.9s\tremaining: 3m 51s\n",
      "678:\ttotal: 16.9s\tremaining: 3m 51s\n",
      "679:\ttotal: 16.9s\tremaining: 3m 51s\n",
      "680:\ttotal: 16.9s\tremaining: 3m 51s\n",
      "681:\ttotal: 16.9s\tremaining: 3m 51s\n",
      "682:\ttotal: 17s\tremaining: 3m 51s\n",
      "683:\ttotal: 17s\tremaining: 3m 51s\n",
      "684:\ttotal: 17s\tremaining: 3m 51s\n",
      "685:\ttotal: 17s\tremaining: 3m 51s\n",
      "686:\ttotal: 17.1s\tremaining: 3m 51s\n",
      "687:\ttotal: 17.1s\tremaining: 3m 50s\n",
      "688:\ttotal: 17.1s\tremaining: 3m 51s\n",
      "689:\ttotal: 17.1s\tremaining: 3m 51s\n",
      "690:\ttotal: 17.2s\tremaining: 3m 51s\n",
      "691:\ttotal: 17.2s\tremaining: 3m 50s\n",
      "692:\ttotal: 17.2s\tremaining: 3m 50s\n",
      "693:\ttotal: 17.2s\tremaining: 3m 50s\n",
      "694:\ttotal: 17.2s\tremaining: 3m 50s\n",
      "695:\ttotal: 17.2s\tremaining: 3m 50s\n",
      "696:\ttotal: 17.3s\tremaining: 3m 50s\n",
      "697:\ttotal: 17.3s\tremaining: 3m 50s\n",
      "698:\ttotal: 17.3s\tremaining: 3m 50s\n",
      "699:\ttotal: 17.4s\tremaining: 3m 50s\n",
      "700:\ttotal: 17.4s\tremaining: 3m 50s\n",
      "701:\ttotal: 17.4s\tremaining: 3m 50s\n",
      "702:\ttotal: 17.4s\tremaining: 3m 50s\n",
      "703:\ttotal: 17.4s\tremaining: 3m 50s\n",
      "704:\ttotal: 17.5s\tremaining: 3m 50s\n",
      "705:\ttotal: 17.5s\tremaining: 3m 49s\n",
      "706:\ttotal: 17.5s\tremaining: 3m 50s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707:\ttotal: 17.5s\tremaining: 3m 49s\n",
      "708:\ttotal: 17.6s\tremaining: 3m 50s\n",
      "709:\ttotal: 17.6s\tremaining: 3m 50s\n",
      "710:\ttotal: 17.6s\tremaining: 3m 50s\n",
      "711:\ttotal: 17.6s\tremaining: 3m 49s\n",
      "712:\ttotal: 17.6s\tremaining: 3m 49s\n",
      "713:\ttotal: 17.7s\tremaining: 3m 49s\n",
      "714:\ttotal: 17.7s\tremaining: 3m 49s\n",
      "715:\ttotal: 17.7s\tremaining: 3m 49s\n",
      "716:\ttotal: 17.7s\tremaining: 3m 49s\n",
      "717:\ttotal: 17.8s\tremaining: 3m 49s\n",
      "718:\ttotal: 17.8s\tremaining: 3m 49s\n",
      "719:\ttotal: 17.8s\tremaining: 3m 49s\n",
      "720:\ttotal: 17.8s\tremaining: 3m 49s\n",
      "721:\ttotal: 17.9s\tremaining: 3m 49s\n",
      "722:\ttotal: 17.9s\tremaining: 3m 49s\n",
      "723:\ttotal: 17.9s\tremaining: 3m 49s\n",
      "724:\ttotal: 17.9s\tremaining: 3m 49s\n",
      "725:\ttotal: 17.9s\tremaining: 3m 49s\n",
      "726:\ttotal: 18s\tremaining: 3m 49s\n",
      "727:\ttotal: 18s\tremaining: 3m 49s\n",
      "728:\ttotal: 18s\tremaining: 3m 49s\n",
      "729:\ttotal: 18.1s\tremaining: 3m 49s\n",
      "730:\ttotal: 18.1s\tremaining: 3m 49s\n",
      "731:\ttotal: 18.1s\tremaining: 3m 48s\n",
      "732:\ttotal: 18.1s\tremaining: 3m 48s\n",
      "733:\ttotal: 18.1s\tremaining: 3m 48s\n",
      "734:\ttotal: 18.1s\tremaining: 3m 48s\n",
      "735:\ttotal: 18.2s\tremaining: 3m 48s\n",
      "736:\ttotal: 18.2s\tremaining: 3m 48s\n",
      "737:\ttotal: 18.2s\tremaining: 3m 48s\n",
      "738:\ttotal: 18.2s\tremaining: 3m 48s\n",
      "739:\ttotal: 18.3s\tremaining: 3m 48s\n",
      "740:\ttotal: 18.3s\tremaining: 3m 48s\n",
      "741:\ttotal: 18.3s\tremaining: 3m 48s\n",
      "742:\ttotal: 18.4s\tremaining: 3m 48s\n",
      "743:\ttotal: 18.4s\tremaining: 3m 48s\n",
      "744:\ttotal: 18.4s\tremaining: 3m 48s\n",
      "745:\ttotal: 18.4s\tremaining: 3m 48s\n",
      "746:\ttotal: 18.5s\tremaining: 3m 48s\n",
      "747:\ttotal: 18.5s\tremaining: 3m 48s\n",
      "748:\ttotal: 18.5s\tremaining: 3m 48s\n",
      "749:\ttotal: 18.5s\tremaining: 3m 48s\n",
      "750:\ttotal: 18.5s\tremaining: 3m 48s\n",
      "751:\ttotal: 18.6s\tremaining: 3m 48s\n",
      "752:\ttotal: 18.6s\tremaining: 3m 48s\n",
      "753:\ttotal: 18.6s\tremaining: 3m 48s\n",
      "754:\ttotal: 18.6s\tremaining: 3m 48s\n",
      "755:\ttotal: 18.7s\tremaining: 3m 48s\n",
      "756:\ttotal: 18.7s\tremaining: 3m 48s\n",
      "757:\ttotal: 18.7s\tremaining: 3m 47s\n",
      "758:\ttotal: 18.7s\tremaining: 3m 47s\n",
      "759:\ttotal: 18.7s\tremaining: 3m 47s\n",
      "760:\ttotal: 18.7s\tremaining: 3m 47s\n",
      "761:\ttotal: 18.8s\tremaining: 3m 47s\n",
      "762:\ttotal: 18.8s\tremaining: 3m 47s\n",
      "763:\ttotal: 18.8s\tremaining: 3m 47s\n",
      "764:\ttotal: 18.8s\tremaining: 3m 47s\n",
      "765:\ttotal: 18.9s\tremaining: 3m 47s\n",
      "766:\ttotal: 18.9s\tremaining: 3m 47s\n",
      "767:\ttotal: 18.9s\tremaining: 3m 47s\n",
      "768:\ttotal: 18.9s\tremaining: 3m 47s\n",
      "769:\ttotal: 18.9s\tremaining: 3m 46s\n",
      "770:\ttotal: 18.9s\tremaining: 3m 46s\n",
      "771:\ttotal: 19s\tremaining: 3m 46s\n",
      "772:\ttotal: 19s\tremaining: 3m 46s\n",
      "773:\ttotal: 19s\tremaining: 3m 46s\n",
      "774:\ttotal: 19s\tremaining: 3m 46s\n",
      "775:\ttotal: 19s\tremaining: 3m 46s\n",
      "776:\ttotal: 19.1s\tremaining: 3m 46s\n",
      "777:\ttotal: 19.1s\tremaining: 3m 46s\n",
      "778:\ttotal: 19.1s\tremaining: 3m 45s\n",
      "779:\ttotal: 19.1s\tremaining: 3m 45s\n",
      "780:\ttotal: 19.1s\tremaining: 3m 45s\n",
      "781:\ttotal: 19.1s\tremaining: 3m 45s\n",
      "782:\ttotal: 19.1s\tremaining: 3m 45s\n",
      "783:\ttotal: 19.2s\tremaining: 3m 45s\n",
      "784:\ttotal: 19.2s\tremaining: 3m 45s\n",
      "785:\ttotal: 19.2s\tremaining: 3m 44s\n",
      "786:\ttotal: 19.2s\tremaining: 3m 45s\n",
      "787:\ttotal: 19.3s\tremaining: 3m 45s\n",
      "788:\ttotal: 19.3s\tremaining: 3m 44s\n",
      "789:\ttotal: 19.3s\tremaining: 3m 44s\n",
      "790:\ttotal: 19.3s\tremaining: 3m 44s\n",
      "791:\ttotal: 19.3s\tremaining: 3m 44s\n",
      "792:\ttotal: 19.3s\tremaining: 3m 44s\n",
      "793:\ttotal: 19.4s\tremaining: 3m 44s\n",
      "794:\ttotal: 19.4s\tremaining: 3m 44s\n",
      "795:\ttotal: 19.4s\tremaining: 3m 44s\n",
      "796:\ttotal: 19.4s\tremaining: 3m 43s\n",
      "797:\ttotal: 19.4s\tremaining: 3m 43s\n",
      "798:\ttotal: 19.5s\tremaining: 3m 44s\n",
      "799:\ttotal: 19.5s\tremaining: 3m 43s\n",
      "800:\ttotal: 19.5s\tremaining: 3m 43s\n",
      "801:\ttotal: 19.5s\tremaining: 3m 43s\n",
      "802:\ttotal: 19.5s\tremaining: 3m 43s\n",
      "803:\ttotal: 19.5s\tremaining: 3m 43s\n",
      "804:\ttotal: 19.6s\tremaining: 3m 43s\n",
      "805:\ttotal: 19.6s\tremaining: 3m 43s\n",
      "806:\ttotal: 19.6s\tremaining: 3m 43s\n",
      "807:\ttotal: 19.6s\tremaining: 3m 43s\n",
      "808:\ttotal: 19.6s\tremaining: 3m 43s\n",
      "809:\ttotal: 19.7s\tremaining: 3m 43s\n",
      "810:\ttotal: 19.7s\tremaining: 3m 43s\n",
      "811:\ttotal: 19.7s\tremaining: 3m 43s\n",
      "812:\ttotal: 19.7s\tremaining: 3m 43s\n",
      "813:\ttotal: 19.8s\tremaining: 3m 43s\n",
      "814:\ttotal: 19.8s\tremaining: 3m 43s\n",
      "815:\ttotal: 19.8s\tremaining: 3m 43s\n",
      "816:\ttotal: 19.9s\tremaining: 3m 43s\n",
      "817:\ttotal: 19.9s\tremaining: 3m 43s\n",
      "818:\ttotal: 19.9s\tremaining: 3m 42s\n",
      "819:\ttotal: 19.9s\tremaining: 3m 42s\n",
      "820:\ttotal: 19.9s\tremaining: 3m 42s\n",
      "821:\ttotal: 19.9s\tremaining: 3m 42s\n",
      "822:\ttotal: 19.9s\tremaining: 3m 42s\n",
      "823:\ttotal: 20s\tremaining: 3m 42s\n",
      "824:\ttotal: 20s\tremaining: 3m 42s\n",
      "825:\ttotal: 20s\tremaining: 3m 42s\n",
      "826:\ttotal: 20s\tremaining: 3m 41s\n",
      "827:\ttotal: 20.1s\tremaining: 3m 42s\n",
      "828:\ttotal: 20.1s\tremaining: 3m 41s\n",
      "829:\ttotal: 20.1s\tremaining: 3m 42s\n",
      "830:\ttotal: 20.1s\tremaining: 3m 42s\n",
      "831:\ttotal: 20.1s\tremaining: 3m 41s\n",
      "832:\ttotal: 20.2s\tremaining: 3m 41s\n",
      "833:\ttotal: 20.2s\tremaining: 3m 41s\n",
      "834:\ttotal: 20.2s\tremaining: 3m 41s\n",
      "835:\ttotal: 20.2s\tremaining: 3m 41s\n",
      "836:\ttotal: 20.2s\tremaining: 3m 41s\n",
      "837:\ttotal: 20.2s\tremaining: 3m 41s\n",
      "838:\ttotal: 20.2s\tremaining: 3m 41s\n",
      "839:\ttotal: 20.3s\tremaining: 3m 41s\n",
      "840:\ttotal: 20.3s\tremaining: 3m 41s\n",
      "841:\ttotal: 20.3s\tremaining: 3m 41s\n",
      "842:\ttotal: 20.3s\tremaining: 3m 40s\n",
      "843:\ttotal: 20.4s\tremaining: 3m 40s\n",
      "844:\ttotal: 20.4s\tremaining: 3m 40s\n",
      "845:\ttotal: 20.4s\tremaining: 3m 40s\n",
      "846:\ttotal: 20.4s\tremaining: 3m 40s\n",
      "847:\ttotal: 20.4s\tremaining: 3m 40s\n",
      "848:\ttotal: 20.5s\tremaining: 3m 40s\n",
      "849:\ttotal: 20.5s\tremaining: 3m 40s\n",
      "850:\ttotal: 20.5s\tremaining: 3m 40s\n",
      "851:\ttotal: 20.5s\tremaining: 3m 40s\n",
      "852:\ttotal: 20.6s\tremaining: 3m 40s\n",
      "853:\ttotal: 20.6s\tremaining: 3m 40s\n",
      "854:\ttotal: 20.6s\tremaining: 3m 40s\n",
      "855:\ttotal: 20.7s\tremaining: 3m 40s\n",
      "856:\ttotal: 20.7s\tremaining: 3m 40s\n",
      "857:\ttotal: 20.7s\tremaining: 3m 40s\n",
      "858:\ttotal: 20.7s\tremaining: 3m 40s\n",
      "859:\ttotal: 20.8s\tremaining: 3m 40s\n",
      "860:\ttotal: 20.8s\tremaining: 3m 40s\n",
      "861:\ttotal: 20.8s\tremaining: 3m 40s\n",
      "862:\ttotal: 20.8s\tremaining: 3m 40s\n",
      "863:\ttotal: 20.8s\tremaining: 3m 40s\n",
      "864:\ttotal: 20.9s\tremaining: 3m 40s\n",
      "865:\ttotal: 20.9s\tremaining: 3m 40s\n",
      "866:\ttotal: 20.9s\tremaining: 3m 40s\n",
      "867:\ttotal: 21s\tremaining: 3m 40s\n",
      "868:\ttotal: 21s\tremaining: 3m 40s\n",
      "869:\ttotal: 21s\tremaining: 3m 40s\n",
      "870:\ttotal: 21s\tremaining: 3m 40s\n",
      "871:\ttotal: 21s\tremaining: 3m 40s\n",
      "872:\ttotal: 21.1s\tremaining: 3m 40s\n",
      "873:\ttotal: 21.1s\tremaining: 3m 40s\n",
      "874:\ttotal: 21.1s\tremaining: 3m 40s\n",
      "875:\ttotal: 21.1s\tremaining: 3m 40s\n",
      "876:\ttotal: 21.2s\tremaining: 3m 40s\n",
      "877:\ttotal: 21.2s\tremaining: 3m 40s\n",
      "878:\ttotal: 21.2s\tremaining: 3m 40s\n",
      "879:\ttotal: 21.2s\tremaining: 3m 40s\n",
      "880:\ttotal: 21.2s\tremaining: 3m 39s\n",
      "881:\ttotal: 21.3s\tremaining: 3m 39s\n",
      "882:\ttotal: 21.3s\tremaining: 3m 39s\n",
      "883:\ttotal: 21.3s\tremaining: 3m 39s\n",
      "884:\ttotal: 21.3s\tremaining: 3m 39s\n",
      "885:\ttotal: 21.3s\tremaining: 3m 39s\n",
      "886:\ttotal: 21.4s\tremaining: 3m 39s\n",
      "887:\ttotal: 21.4s\tremaining: 3m 39s\n",
      "888:\ttotal: 21.4s\tremaining: 3m 39s\n",
      "889:\ttotal: 21.5s\tremaining: 3m 39s\n",
      "890:\ttotal: 21.5s\tremaining: 3m 39s\n",
      "891:\ttotal: 21.5s\tremaining: 3m 39s\n",
      "892:\ttotal: 21.5s\tremaining: 3m 39s\n",
      "893:\ttotal: 21.5s\tremaining: 3m 39s\n",
      "894:\ttotal: 21.6s\tremaining: 3m 39s\n",
      "895:\ttotal: 21.6s\tremaining: 3m 39s\n",
      "896:\ttotal: 21.6s\tremaining: 3m 39s\n",
      "897:\ttotal: 21.6s\tremaining: 3m 38s\n",
      "898:\ttotal: 21.6s\tremaining: 3m 38s\n",
      "899:\ttotal: 21.6s\tremaining: 3m 38s\n",
      "900:\ttotal: 21.7s\tremaining: 3m 38s\n",
      "901:\ttotal: 21.7s\tremaining: 3m 38s\n",
      "902:\ttotal: 21.7s\tremaining: 3m 38s\n",
      "903:\ttotal: 21.7s\tremaining: 3m 38s\n",
      "904:\ttotal: 21.7s\tremaining: 3m 38s\n",
      "905:\ttotal: 21.8s\tremaining: 3m 38s\n",
      "906:\ttotal: 21.8s\tremaining: 3m 38s\n",
      "907:\ttotal: 21.8s\tremaining: 3m 38s\n",
      "908:\ttotal: 21.8s\tremaining: 3m 38s\n",
      "909:\ttotal: 21.9s\tremaining: 3m 38s\n",
      "910:\ttotal: 21.9s\tremaining: 3m 38s\n",
      "911:\ttotal: 21.9s\tremaining: 3m 38s\n",
      "912:\ttotal: 21.9s\tremaining: 3m 38s\n",
      "913:\ttotal: 22s\tremaining: 3m 38s\n",
      "914:\ttotal: 22s\tremaining: 3m 38s\n",
      "915:\ttotal: 22s\tremaining: 3m 38s\n",
      "916:\ttotal: 22s\tremaining: 3m 38s\n",
      "917:\ttotal: 22s\tremaining: 3m 38s\n",
      "918:\ttotal: 22.1s\tremaining: 3m 37s\n",
      "919:\ttotal: 22.1s\tremaining: 3m 37s\n",
      "920:\ttotal: 22.1s\tremaining: 3m 37s\n",
      "921:\ttotal: 22.1s\tremaining: 3m 37s\n",
      "922:\ttotal: 22.1s\tremaining: 3m 37s\n",
      "923:\ttotal: 22.2s\tremaining: 3m 37s\n",
      "924:\ttotal: 22.2s\tremaining: 3m 37s\n",
      "925:\ttotal: 22.2s\tremaining: 3m 37s\n",
      "926:\ttotal: 22.2s\tremaining: 3m 37s\n",
      "927:\ttotal: 22.3s\tremaining: 3m 37s\n",
      "928:\ttotal: 22.3s\tremaining: 3m 37s\n",
      "929:\ttotal: 22.3s\tremaining: 3m 37s\n",
      "930:\ttotal: 22.4s\tremaining: 3m 37s\n",
      "931:\ttotal: 22.4s\tremaining: 3m 37s\n",
      "932:\ttotal: 22.4s\tremaining: 3m 37s\n",
      "933:\ttotal: 22.4s\tremaining: 3m 37s\n",
      "934:\ttotal: 22.5s\tremaining: 3m 37s\n",
      "935:\ttotal: 22.5s\tremaining: 3m 37s\n",
      "936:\ttotal: 22.5s\tremaining: 3m 37s\n",
      "937:\ttotal: 22.5s\tremaining: 3m 37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938:\ttotal: 22.6s\tremaining: 3m 37s\n",
      "939:\ttotal: 22.6s\tremaining: 3m 37s\n",
      "940:\ttotal: 22.6s\tremaining: 3m 38s\n",
      "941:\ttotal: 22.7s\tremaining: 3m 38s\n",
      "942:\ttotal: 22.7s\tremaining: 3m 38s\n",
      "943:\ttotal: 22.7s\tremaining: 3m 37s\n",
      "944:\ttotal: 22.7s\tremaining: 3m 37s\n",
      "945:\ttotal: 22.7s\tremaining: 3m 37s\n",
      "946:\ttotal: 22.8s\tremaining: 3m 37s\n",
      "947:\ttotal: 22.8s\tremaining: 3m 37s\n",
      "948:\ttotal: 22.8s\tremaining: 3m 37s\n",
      "949:\ttotal: 22.9s\tremaining: 3m 37s\n",
      "950:\ttotal: 22.9s\tremaining: 3m 37s\n",
      "951:\ttotal: 22.9s\tremaining: 3m 37s\n",
      "952:\ttotal: 23s\tremaining: 3m 37s\n",
      "953:\ttotal: 23s\tremaining: 3m 37s\n",
      "954:\ttotal: 23s\tremaining: 3m 38s\n",
      "955:\ttotal: 23.1s\tremaining: 3m 38s\n",
      "956:\ttotal: 23.1s\tremaining: 3m 38s\n",
      "957:\ttotal: 23.1s\tremaining: 3m 38s\n",
      "958:\ttotal: 23.2s\tremaining: 3m 38s\n",
      "959:\ttotal: 23.2s\tremaining: 3m 38s\n",
      "960:\ttotal: 23.2s\tremaining: 3m 38s\n",
      "961:\ttotal: 23.3s\tremaining: 3m 38s\n",
      "962:\ttotal: 23.3s\tremaining: 3m 38s\n",
      "963:\ttotal: 23.3s\tremaining: 3m 38s\n",
      "964:\ttotal: 23.4s\tremaining: 3m 38s\n",
      "965:\ttotal: 23.4s\tremaining: 3m 38s\n",
      "966:\ttotal: 23.4s\tremaining: 3m 38s\n",
      "967:\ttotal: 23.5s\tremaining: 3m 38s\n",
      "968:\ttotal: 23.5s\tremaining: 3m 38s\n",
      "969:\ttotal: 23.5s\tremaining: 3m 38s\n",
      "970:\ttotal: 23.5s\tremaining: 3m 38s\n",
      "971:\ttotal: 23.6s\tremaining: 3m 38s\n",
      "972:\ttotal: 23.6s\tremaining: 3m 38s\n",
      "973:\ttotal: 23.6s\tremaining: 3m 38s\n",
      "974:\ttotal: 23.6s\tremaining: 3m 38s\n",
      "975:\ttotal: 23.7s\tremaining: 3m 38s\n",
      "976:\ttotal: 23.7s\tremaining: 3m 38s\n",
      "977:\ttotal: 23.7s\tremaining: 3m 38s\n",
      "978:\ttotal: 23.7s\tremaining: 3m 38s\n",
      "979:\ttotal: 23.7s\tremaining: 3m 38s\n",
      "980:\ttotal: 23.7s\tremaining: 3m 38s\n",
      "981:\ttotal: 23.8s\tremaining: 3m 38s\n",
      "982:\ttotal: 23.8s\tremaining: 3m 38s\n",
      "983:\ttotal: 23.8s\tremaining: 3m 38s\n",
      "984:\ttotal: 23.8s\tremaining: 3m 37s\n",
      "985:\ttotal: 23.8s\tremaining: 3m 37s\n",
      "986:\ttotal: 23.9s\tremaining: 3m 37s\n",
      "987:\ttotal: 23.9s\tremaining: 3m 37s\n",
      "988:\ttotal: 23.9s\tremaining: 3m 37s\n",
      "989:\ttotal: 23.9s\tremaining: 3m 37s\n",
      "990:\ttotal: 24s\tremaining: 3m 37s\n",
      "991:\ttotal: 24s\tremaining: 3m 37s\n",
      "992:\ttotal: 24s\tremaining: 3m 37s\n",
      "993:\ttotal: 24s\tremaining: 3m 37s\n",
      "994:\ttotal: 24.1s\tremaining: 3m 37s\n",
      "995:\ttotal: 24.1s\tremaining: 3m 37s\n",
      "996:\ttotal: 24.1s\tremaining: 3m 37s\n",
      "997:\ttotal: 24.1s\tremaining: 3m 37s\n",
      "998:\ttotal: 24.2s\tremaining: 3m 37s\n",
      "999:\ttotal: 24.2s\tremaining: 3m 37s\n",
      "1000:\ttotal: 24.2s\tremaining: 3m 37s\n",
      "1001:\ttotal: 24.3s\tremaining: 3m 37s\n",
      "1002:\ttotal: 24.3s\tremaining: 3m 37s\n",
      "1003:\ttotal: 24.3s\tremaining: 3m 37s\n",
      "1004:\ttotal: 24.3s\tremaining: 3m 37s\n",
      "1005:\ttotal: 24.3s\tremaining: 3m 37s\n",
      "1006:\ttotal: 24.4s\tremaining: 3m 37s\n",
      "1007:\ttotal: 24.4s\tremaining: 3m 37s\n",
      "1008:\ttotal: 24.4s\tremaining: 3m 37s\n",
      "1009:\ttotal: 24.4s\tremaining: 3m 37s\n",
      "1010:\ttotal: 24.5s\tremaining: 3m 37s\n",
      "1011:\ttotal: 24.5s\tremaining: 3m 37s\n",
      "1012:\ttotal: 24.5s\tremaining: 3m 37s\n",
      "1013:\ttotal: 24.5s\tremaining: 3m 37s\n",
      "1014:\ttotal: 24.6s\tremaining: 3m 37s\n",
      "1015:\ttotal: 24.6s\tremaining: 3m 37s\n",
      "1016:\ttotal: 24.6s\tremaining: 3m 37s\n",
      "1017:\ttotal: 24.6s\tremaining: 3m 37s\n",
      "1018:\ttotal: 24.6s\tremaining: 3m 37s\n",
      "1019:\ttotal: 24.7s\tremaining: 3m 37s\n",
      "1020:\ttotal: 24.7s\tremaining: 3m 37s\n",
      "1021:\ttotal: 24.7s\tremaining: 3m 37s\n",
      "1022:\ttotal: 24.7s\tremaining: 3m 37s\n",
      "1023:\ttotal: 24.8s\tremaining: 3m 37s\n",
      "1024:\ttotal: 24.8s\tremaining: 3m 37s\n",
      "1025:\ttotal: 24.8s\tremaining: 3m 36s\n",
      "1026:\ttotal: 24.8s\tremaining: 3m 36s\n",
      "1027:\ttotal: 24.8s\tremaining: 3m 36s\n",
      "1028:\ttotal: 24.9s\tremaining: 3m 36s\n",
      "1029:\ttotal: 24.9s\tremaining: 3m 36s\n",
      "1030:\ttotal: 24.9s\tremaining: 3m 36s\n",
      "1031:\ttotal: 24.9s\tremaining: 3m 36s\n",
      "1032:\ttotal: 25s\tremaining: 3m 36s\n",
      "1033:\ttotal: 25s\tremaining: 3m 36s\n",
      "1034:\ttotal: 25s\tremaining: 3m 36s\n",
      "1035:\ttotal: 25s\tremaining: 3m 36s\n",
      "1036:\ttotal: 25.1s\tremaining: 3m 36s\n",
      "1037:\ttotal: 25.1s\tremaining: 3m 36s\n",
      "1038:\ttotal: 25.1s\tremaining: 3m 36s\n",
      "1039:\ttotal: 25.1s\tremaining: 3m 36s\n",
      "1040:\ttotal: 25.2s\tremaining: 3m 36s\n",
      "1041:\ttotal: 25.2s\tremaining: 3m 36s\n",
      "1042:\ttotal: 25.2s\tremaining: 3m 36s\n",
      "1043:\ttotal: 25.2s\tremaining: 3m 36s\n",
      "1044:\ttotal: 25.3s\tremaining: 3m 36s\n",
      "1045:\ttotal: 25.3s\tremaining: 3m 36s\n",
      "1046:\ttotal: 25.3s\tremaining: 3m 36s\n",
      "1047:\ttotal: 25.3s\tremaining: 3m 36s\n",
      "1048:\ttotal: 25.4s\tremaining: 3m 36s\n",
      "1049:\ttotal: 25.4s\tremaining: 3m 36s\n",
      "1050:\ttotal: 25.4s\tremaining: 3m 36s\n",
      "1051:\ttotal: 25.5s\tremaining: 3m 36s\n",
      "1052:\ttotal: 25.5s\tremaining: 3m 36s\n",
      "1053:\ttotal: 25.5s\tremaining: 3m 36s\n",
      "1054:\ttotal: 25.6s\tremaining: 3m 36s\n",
      "1055:\ttotal: 25.6s\tremaining: 3m 36s\n",
      "1056:\ttotal: 25.6s\tremaining: 3m 36s\n",
      "1057:\ttotal: 25.7s\tremaining: 3m 36s\n",
      "1058:\ttotal: 25.7s\tremaining: 3m 37s\n",
      "1059:\ttotal: 25.7s\tremaining: 3m 37s\n",
      "1060:\ttotal: 25.8s\tremaining: 3m 37s\n",
      "1061:\ttotal: 25.8s\tremaining: 3m 37s\n",
      "1062:\ttotal: 25.8s\tremaining: 3m 37s\n",
      "1063:\ttotal: 25.8s\tremaining: 3m 37s\n",
      "1064:\ttotal: 25.9s\tremaining: 3m 37s\n",
      "1065:\ttotal: 25.9s\tremaining: 3m 36s\n",
      "1066:\ttotal: 25.9s\tremaining: 3m 36s\n",
      "1067:\ttotal: 25.9s\tremaining: 3m 36s\n",
      "1068:\ttotal: 26s\tremaining: 3m 36s\n",
      "1069:\ttotal: 26s\tremaining: 3m 37s\n",
      "1070:\ttotal: 26s\tremaining: 3m 37s\n",
      "1071:\ttotal: 26.1s\tremaining: 3m 37s\n",
      "1072:\ttotal: 26.1s\tremaining: 3m 37s\n",
      "1073:\ttotal: 26.2s\tremaining: 3m 37s\n",
      "1074:\ttotal: 26.2s\tremaining: 3m 37s\n",
      "1075:\ttotal: 26.2s\tremaining: 3m 37s\n",
      "1076:\ttotal: 26.3s\tremaining: 3m 37s\n",
      "1077:\ttotal: 26.3s\tremaining: 3m 37s\n",
      "1078:\ttotal: 26.4s\tremaining: 3m 37s\n",
      "1079:\ttotal: 26.4s\tremaining: 3m 38s\n",
      "1080:\ttotal: 26.4s\tremaining: 3m 38s\n",
      "1081:\ttotal: 26.5s\tremaining: 3m 38s\n",
      "1082:\ttotal: 26.5s\tremaining: 3m 38s\n",
      "1083:\ttotal: 26.6s\tremaining: 3m 38s\n",
      "1084:\ttotal: 26.6s\tremaining: 3m 38s\n",
      "1085:\ttotal: 26.6s\tremaining: 3m 38s\n",
      "1086:\ttotal: 26.6s\tremaining: 3m 38s\n",
      "1087:\ttotal: 26.7s\tremaining: 3m 38s\n",
      "1088:\ttotal: 26.7s\tremaining: 3m 38s\n",
      "1089:\ttotal: 26.7s\tremaining: 3m 38s\n",
      "1090:\ttotal: 26.7s\tremaining: 3m 38s\n",
      "1091:\ttotal: 26.8s\tremaining: 3m 38s\n",
      "1092:\ttotal: 26.8s\tremaining: 3m 38s\n",
      "1093:\ttotal: 26.8s\tremaining: 3m 38s\n",
      "1094:\ttotal: 26.8s\tremaining: 3m 38s\n",
      "1095:\ttotal: 26.9s\tremaining: 3m 38s\n",
      "1096:\ttotal: 26.9s\tremaining: 3m 38s\n",
      "1097:\ttotal: 26.9s\tremaining: 3m 38s\n",
      "1098:\ttotal: 27s\tremaining: 3m 38s\n",
      "1099:\ttotal: 27s\tremaining: 3m 38s\n",
      "1100:\ttotal: 27s\tremaining: 3m 38s\n",
      "1101:\ttotal: 27s\tremaining: 3m 38s\n",
      "1102:\ttotal: 27.1s\tremaining: 3m 38s\n",
      "1103:\ttotal: 27.1s\tremaining: 3m 38s\n",
      "1104:\ttotal: 27.1s\tremaining: 3m 38s\n",
      "1105:\ttotal: 27.2s\tremaining: 3m 38s\n",
      "1106:\ttotal: 27.2s\tremaining: 3m 38s\n",
      "1107:\ttotal: 27.2s\tremaining: 3m 38s\n",
      "1108:\ttotal: 27.2s\tremaining: 3m 38s\n",
      "1109:\ttotal: 27.3s\tremaining: 3m 38s\n",
      "1110:\ttotal: 27.3s\tremaining: 3m 38s\n",
      "1111:\ttotal: 27.4s\tremaining: 3m 38s\n",
      "1112:\ttotal: 27.4s\tremaining: 3m 38s\n",
      "1113:\ttotal: 27.4s\tremaining: 3m 38s\n",
      "1114:\ttotal: 27.5s\tremaining: 3m 38s\n",
      "1115:\ttotal: 27.5s\tremaining: 3m 38s\n",
      "1116:\ttotal: 27.5s\tremaining: 3m 38s\n",
      "1117:\ttotal: 27.6s\tremaining: 3m 39s\n",
      "1118:\ttotal: 27.6s\tremaining: 3m 39s\n",
      "1119:\ttotal: 27.6s\tremaining: 3m 39s\n",
      "1120:\ttotal: 27.7s\tremaining: 3m 39s\n",
      "1121:\ttotal: 27.7s\tremaining: 3m 39s\n",
      "1122:\ttotal: 27.7s\tremaining: 3m 39s\n",
      "1123:\ttotal: 27.8s\tremaining: 3m 39s\n",
      "1124:\ttotal: 27.8s\tremaining: 3m 39s\n",
      "1125:\ttotal: 27.8s\tremaining: 3m 39s\n",
      "1126:\ttotal: 27.9s\tremaining: 3m 39s\n",
      "1127:\ttotal: 27.9s\tremaining: 3m 39s\n",
      "1128:\ttotal: 27.9s\tremaining: 3m 39s\n",
      "1129:\ttotal: 28s\tremaining: 3m 39s\n",
      "1130:\ttotal: 28s\tremaining: 3m 39s\n",
      "1131:\ttotal: 28s\tremaining: 3m 39s\n",
      "1132:\ttotal: 28.1s\tremaining: 3m 39s\n",
      "1133:\ttotal: 28.1s\tremaining: 3m 39s\n",
      "1134:\ttotal: 28.1s\tremaining: 3m 39s\n",
      "1135:\ttotal: 28.2s\tremaining: 3m 39s\n",
      "1136:\ttotal: 28.2s\tremaining: 3m 39s\n",
      "1137:\ttotal: 28.2s\tremaining: 3m 39s\n",
      "1138:\ttotal: 28.3s\tremaining: 3m 39s\n",
      "1139:\ttotal: 28.3s\tremaining: 3m 39s\n",
      "1140:\ttotal: 28.3s\tremaining: 3m 39s\n",
      "1141:\ttotal: 28.4s\tremaining: 3m 39s\n",
      "1142:\ttotal: 28.4s\tremaining: 3m 39s\n",
      "1143:\ttotal: 28.4s\tremaining: 3m 39s\n",
      "1144:\ttotal: 28.4s\tremaining: 3m 39s\n",
      "1145:\ttotal: 28.4s\tremaining: 3m 39s\n",
      "1146:\ttotal: 28.5s\tremaining: 3m 39s\n",
      "1147:\ttotal: 28.5s\tremaining: 3m 39s\n",
      "1148:\ttotal: 28.5s\tremaining: 3m 39s\n",
      "1149:\ttotal: 28.5s\tremaining: 3m 39s\n",
      "1150:\ttotal: 28.6s\tremaining: 3m 39s\n",
      "1151:\ttotal: 28.6s\tremaining: 3m 39s\n",
      "1152:\ttotal: 28.6s\tremaining: 3m 39s\n",
      "1153:\ttotal: 28.7s\tremaining: 3m 39s\n",
      "1154:\ttotal: 28.7s\tremaining: 3m 39s\n",
      "1155:\ttotal: 28.7s\tremaining: 3m 39s\n",
      "1156:\ttotal: 28.7s\tremaining: 3m 39s\n",
      "1157:\ttotal: 28.8s\tremaining: 3m 39s\n",
      "1158:\ttotal: 28.8s\tremaining: 3m 39s\n",
      "1159:\ttotal: 28.8s\tremaining: 3m 39s\n",
      "1160:\ttotal: 28.9s\tremaining: 3m 39s\n",
      "1161:\ttotal: 28.9s\tremaining: 3m 39s\n",
      "1162:\ttotal: 28.9s\tremaining: 3m 39s\n",
      "1163:\ttotal: 29s\tremaining: 3m 39s\n",
      "1164:\ttotal: 29s\tremaining: 3m 39s\n",
      "1165:\ttotal: 29s\tremaining: 3m 39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166:\ttotal: 29s\tremaining: 3m 39s\n",
      "1167:\ttotal: 29s\tremaining: 3m 39s\n",
      "1168:\ttotal: 29.1s\tremaining: 3m 39s\n",
      "1169:\ttotal: 29.1s\tremaining: 3m 39s\n",
      "1170:\ttotal: 29.1s\tremaining: 3m 39s\n",
      "1171:\ttotal: 29.1s\tremaining: 3m 39s\n",
      "1172:\ttotal: 29.1s\tremaining: 3m 39s\n",
      "1173:\ttotal: 29.2s\tremaining: 3m 39s\n",
      "1174:\ttotal: 29.2s\tremaining: 3m 39s\n",
      "1175:\ttotal: 29.2s\tremaining: 3m 39s\n",
      "1176:\ttotal: 29.2s\tremaining: 3m 39s\n",
      "1177:\ttotal: 29.3s\tremaining: 3m 39s\n",
      "1178:\ttotal: 29.3s\tremaining: 3m 39s\n",
      "1179:\ttotal: 29.3s\tremaining: 3m 39s\n",
      "1180:\ttotal: 29.3s\tremaining: 3m 39s\n",
      "1181:\ttotal: 29.4s\tremaining: 3m 39s\n",
      "1182:\ttotal: 29.4s\tremaining: 3m 38s\n",
      "1183:\ttotal: 29.4s\tremaining: 3m 38s\n",
      "1184:\ttotal: 29.4s\tremaining: 3m 38s\n",
      "1185:\ttotal: 29.4s\tremaining: 3m 38s\n",
      "1186:\ttotal: 29.4s\tremaining: 3m 38s\n",
      "1187:\ttotal: 29.5s\tremaining: 3m 38s\n",
      "1188:\ttotal: 29.5s\tremaining: 3m 38s\n",
      "1189:\ttotal: 29.5s\tremaining: 3m 38s\n",
      "1190:\ttotal: 29.5s\tremaining: 3m 38s\n",
      "1191:\ttotal: 29.6s\tremaining: 3m 38s\n",
      "1192:\ttotal: 29.6s\tremaining: 3m 38s\n",
      "1193:\ttotal: 29.6s\tremaining: 3m 38s\n",
      "1194:\ttotal: 29.6s\tremaining: 3m 38s\n",
      "1195:\ttotal: 29.6s\tremaining: 3m 38s\n",
      "1196:\ttotal: 29.7s\tremaining: 3m 38s\n",
      "1197:\ttotal: 29.7s\tremaining: 3m 37s\n",
      "1198:\ttotal: 29.7s\tremaining: 3m 37s\n",
      "1199:\ttotal: 29.7s\tremaining: 3m 37s\n",
      "1200:\ttotal: 29.7s\tremaining: 3m 37s\n",
      "1201:\ttotal: 29.7s\tremaining: 3m 37s\n",
      "1202:\ttotal: 29.8s\tremaining: 3m 37s\n",
      "1203:\ttotal: 29.8s\tremaining: 3m 37s\n",
      "1204:\ttotal: 29.8s\tremaining: 3m 37s\n",
      "1205:\ttotal: 29.9s\tremaining: 3m 37s\n",
      "1206:\ttotal: 29.9s\tremaining: 3m 37s\n",
      "1207:\ttotal: 29.9s\tremaining: 3m 37s\n",
      "1208:\ttotal: 29.9s\tremaining: 3m 37s\n",
      "1209:\ttotal: 30s\tremaining: 3m 37s\n",
      "1210:\ttotal: 30s\tremaining: 3m 37s\n",
      "1211:\ttotal: 30s\tremaining: 3m 37s\n",
      "1212:\ttotal: 30s\tremaining: 3m 37s\n",
      "1213:\ttotal: 30s\tremaining: 3m 37s\n",
      "1214:\ttotal: 30.1s\tremaining: 3m 37s\n",
      "1215:\ttotal: 30.1s\tremaining: 3m 37s\n",
      "1216:\ttotal: 30.1s\tremaining: 3m 37s\n",
      "1217:\ttotal: 30.1s\tremaining: 3m 37s\n",
      "1218:\ttotal: 30.2s\tremaining: 3m 37s\n",
      "1219:\ttotal: 30.2s\tremaining: 3m 37s\n",
      "1220:\ttotal: 30.2s\tremaining: 3m 37s\n",
      "1221:\ttotal: 30.2s\tremaining: 3m 37s\n",
      "1222:\ttotal: 30.2s\tremaining: 3m 36s\n",
      "1223:\ttotal: 30.3s\tremaining: 3m 36s\n",
      "1224:\ttotal: 30.3s\tremaining: 3m 36s\n",
      "1225:\ttotal: 30.3s\tremaining: 3m 36s\n",
      "1226:\ttotal: 30.3s\tremaining: 3m 36s\n",
      "1227:\ttotal: 30.3s\tremaining: 3m 36s\n",
      "1228:\ttotal: 30.3s\tremaining: 3m 36s\n",
      "1229:\ttotal: 30.4s\tremaining: 3m 36s\n",
      "1230:\ttotal: 30.4s\tremaining: 3m 36s\n",
      "1231:\ttotal: 30.4s\tremaining: 3m 36s\n",
      "1232:\ttotal: 30.4s\tremaining: 3m 36s\n",
      "1233:\ttotal: 30.4s\tremaining: 3m 36s\n",
      "1234:\ttotal: 30.5s\tremaining: 3m 36s\n",
      "1235:\ttotal: 30.5s\tremaining: 3m 36s\n",
      "1236:\ttotal: 30.5s\tremaining: 3m 35s\n",
      "1237:\ttotal: 30.5s\tremaining: 3m 35s\n",
      "1238:\ttotal: 30.5s\tremaining: 3m 35s\n",
      "1239:\ttotal: 30.6s\tremaining: 3m 35s\n",
      "1240:\ttotal: 30.6s\tremaining: 3m 35s\n",
      "1241:\ttotal: 30.6s\tremaining: 3m 35s\n",
      "1242:\ttotal: 30.6s\tremaining: 3m 35s\n",
      "1243:\ttotal: 30.7s\tremaining: 3m 35s\n",
      "1244:\ttotal: 30.7s\tremaining: 3m 35s\n",
      "1245:\ttotal: 30.7s\tremaining: 3m 35s\n",
      "1246:\ttotal: 30.8s\tremaining: 3m 35s\n",
      "1247:\ttotal: 30.8s\tremaining: 3m 36s\n",
      "1248:\ttotal: 30.9s\tremaining: 3m 36s\n",
      "1249:\ttotal: 30.9s\tremaining: 3m 36s\n",
      "1250:\ttotal: 30.9s\tremaining: 3m 36s\n",
      "1251:\ttotal: 30.9s\tremaining: 3m 36s\n",
      "1252:\ttotal: 31s\tremaining: 3m 36s\n",
      "1253:\ttotal: 31s\tremaining: 3m 36s\n",
      "1254:\ttotal: 31.1s\tremaining: 3m 36s\n",
      "1255:\ttotal: 31.1s\tremaining: 3m 36s\n",
      "1256:\ttotal: 31.1s\tremaining: 3m 36s\n",
      "1257:\ttotal: 31.2s\tremaining: 3m 36s\n",
      "1258:\ttotal: 31.2s\tremaining: 3m 36s\n",
      "1259:\ttotal: 31.3s\tremaining: 3m 36s\n",
      "1260:\ttotal: 31.3s\tremaining: 3m 36s\n",
      "1261:\ttotal: 31.3s\tremaining: 3m 36s\n",
      "1262:\ttotal: 31.4s\tremaining: 3m 36s\n",
      "1263:\ttotal: 31.4s\tremaining: 3m 36s\n",
      "1264:\ttotal: 31.4s\tremaining: 3m 37s\n",
      "1265:\ttotal: 31.5s\tremaining: 3m 37s\n",
      "1266:\ttotal: 31.5s\tremaining: 3m 37s\n",
      "1267:\ttotal: 31.5s\tremaining: 3m 37s\n",
      "1268:\ttotal: 31.6s\tremaining: 3m 37s\n",
      "1269:\ttotal: 31.6s\tremaining: 3m 37s\n",
      "1270:\ttotal: 31.6s\tremaining: 3m 37s\n",
      "1271:\ttotal: 31.7s\tremaining: 3m 37s\n",
      "1272:\ttotal: 31.7s\tremaining: 3m 37s\n",
      "1273:\ttotal: 31.7s\tremaining: 3m 37s\n",
      "1274:\ttotal: 31.8s\tremaining: 3m 37s\n",
      "1275:\ttotal: 31.8s\tremaining: 3m 37s\n",
      "1276:\ttotal: 31.8s\tremaining: 3m 37s\n",
      "1277:\ttotal: 31.9s\tremaining: 3m 37s\n",
      "1278:\ttotal: 31.9s\tremaining: 3m 37s\n",
      "1279:\ttotal: 31.9s\tremaining: 3m 37s\n",
      "1280:\ttotal: 32s\tremaining: 3m 37s\n",
      "1281:\ttotal: 32s\tremaining: 3m 37s\n",
      "1282:\ttotal: 32s\tremaining: 3m 37s\n",
      "1283:\ttotal: 32s\tremaining: 3m 37s\n",
      "1284:\ttotal: 32.1s\tremaining: 3m 37s\n",
      "1285:\ttotal: 32.1s\tremaining: 3m 37s\n",
      "1286:\ttotal: 32.1s\tremaining: 3m 37s\n",
      "1287:\ttotal: 32.2s\tremaining: 3m 37s\n",
      "1288:\ttotal: 32.2s\tremaining: 3m 37s\n",
      "1289:\ttotal: 32.2s\tremaining: 3m 37s\n",
      "1290:\ttotal: 32.3s\tremaining: 3m 37s\n",
      "1291:\ttotal: 32.3s\tremaining: 3m 37s\n",
      "1292:\ttotal: 32.3s\tremaining: 3m 37s\n",
      "1293:\ttotal: 32.4s\tremaining: 3m 37s\n",
      "1294:\ttotal: 32.4s\tremaining: 3m 37s\n",
      "1295:\ttotal: 32.4s\tremaining: 3m 37s\n",
      "1296:\ttotal: 32.5s\tremaining: 3m 37s\n",
      "1297:\ttotal: 32.5s\tremaining: 3m 37s\n",
      "1298:\ttotal: 32.5s\tremaining: 3m 37s\n",
      "1299:\ttotal: 32.6s\tremaining: 3m 38s\n",
      "1300:\ttotal: 32.6s\tremaining: 3m 37s\n",
      "1301:\ttotal: 32.6s\tremaining: 3m 38s\n",
      "1302:\ttotal: 32.7s\tremaining: 3m 38s\n",
      "1303:\ttotal: 32.7s\tremaining: 3m 38s\n",
      "1304:\ttotal: 32.8s\tremaining: 3m 38s\n",
      "1305:\ttotal: 32.8s\tremaining: 3m 38s\n",
      "1306:\ttotal: 32.8s\tremaining: 3m 38s\n",
      "1307:\ttotal: 32.9s\tremaining: 3m 38s\n",
      "1308:\ttotal: 32.9s\tremaining: 3m 38s\n",
      "1309:\ttotal: 32.9s\tremaining: 3m 38s\n",
      "1310:\ttotal: 33s\tremaining: 3m 38s\n",
      "1311:\ttotal: 33s\tremaining: 3m 38s\n",
      "1312:\ttotal: 33s\tremaining: 3m 38s\n",
      "1313:\ttotal: 33.1s\tremaining: 3m 38s\n",
      "1314:\ttotal: 33.1s\tremaining: 3m 38s\n",
      "1315:\ttotal: 33.2s\tremaining: 3m 38s\n",
      "1316:\ttotal: 33.2s\tremaining: 3m 38s\n",
      "1317:\ttotal: 33.2s\tremaining: 3m 38s\n",
      "1318:\ttotal: 33.3s\tremaining: 3m 38s\n",
      "1319:\ttotal: 33.3s\tremaining: 3m 38s\n",
      "1320:\ttotal: 33.3s\tremaining: 3m 39s\n",
      "1321:\ttotal: 33.4s\tremaining: 3m 38s\n",
      "1322:\ttotal: 33.4s\tremaining: 3m 38s\n",
      "1323:\ttotal: 33.4s\tremaining: 3m 38s\n",
      "1324:\ttotal: 33.4s\tremaining: 3m 38s\n",
      "1325:\ttotal: 33.5s\tremaining: 3m 38s\n",
      "1326:\ttotal: 33.5s\tremaining: 3m 38s\n",
      "1327:\ttotal: 33.5s\tremaining: 3m 38s\n",
      "1328:\ttotal: 33.5s\tremaining: 3m 38s\n",
      "1329:\ttotal: 33.6s\tremaining: 3m 38s\n",
      "1330:\ttotal: 33.6s\tremaining: 3m 38s\n",
      "1331:\ttotal: 33.6s\tremaining: 3m 38s\n",
      "1332:\ttotal: 33.7s\tremaining: 3m 38s\n",
      "1333:\ttotal: 33.7s\tremaining: 3m 38s\n",
      "1334:\ttotal: 33.7s\tremaining: 3m 38s\n",
      "1335:\ttotal: 33.7s\tremaining: 3m 38s\n",
      "1336:\ttotal: 33.8s\tremaining: 3m 38s\n",
      "1337:\ttotal: 33.8s\tremaining: 3m 38s\n",
      "1338:\ttotal: 33.8s\tremaining: 3m 38s\n",
      "1339:\ttotal: 33.9s\tremaining: 3m 38s\n",
      "1340:\ttotal: 33.9s\tremaining: 3m 38s\n",
      "1341:\ttotal: 33.9s\tremaining: 3m 38s\n",
      "1342:\ttotal: 33.9s\tremaining: 3m 38s\n",
      "1343:\ttotal: 34s\tremaining: 3m 38s\n",
      "1344:\ttotal: 34s\tremaining: 3m 38s\n",
      "1345:\ttotal: 34.1s\tremaining: 3m 38s\n",
      "1346:\ttotal: 34.1s\tremaining: 3m 39s\n",
      "1347:\ttotal: 34.1s\tremaining: 3m 39s\n",
      "1348:\ttotal: 34.2s\tremaining: 3m 38s\n",
      "1349:\ttotal: 34.2s\tremaining: 3m 38s\n",
      "1350:\ttotal: 34.2s\tremaining: 3m 38s\n",
      "1351:\ttotal: 34.2s\tremaining: 3m 38s\n",
      "1352:\ttotal: 34.3s\tremaining: 3m 39s\n",
      "1353:\ttotal: 34.3s\tremaining: 3m 39s\n",
      "1354:\ttotal: 34.3s\tremaining: 3m 39s\n",
      "1355:\ttotal: 34.4s\tremaining: 3m 39s\n",
      "1356:\ttotal: 34.4s\tremaining: 3m 39s\n",
      "1357:\ttotal: 34.4s\tremaining: 3m 39s\n",
      "1358:\ttotal: 34.4s\tremaining: 3m 39s\n",
      "1359:\ttotal: 34.5s\tremaining: 3m 38s\n",
      "1360:\ttotal: 34.5s\tremaining: 3m 39s\n",
      "1361:\ttotal: 34.5s\tremaining: 3m 39s\n",
      "1362:\ttotal: 34.6s\tremaining: 3m 39s\n",
      "1363:\ttotal: 34.6s\tremaining: 3m 39s\n",
      "1364:\ttotal: 34.6s\tremaining: 3m 39s\n",
      "1365:\ttotal: 34.7s\tremaining: 3m 39s\n",
      "1366:\ttotal: 34.7s\tremaining: 3m 38s\n",
      "1367:\ttotal: 34.7s\tremaining: 3m 38s\n",
      "1368:\ttotal: 34.7s\tremaining: 3m 39s\n",
      "1369:\ttotal: 34.8s\tremaining: 3m 39s\n",
      "1370:\ttotal: 34.8s\tremaining: 3m 39s\n",
      "1371:\ttotal: 34.8s\tremaining: 3m 39s\n",
      "1372:\ttotal: 34.9s\tremaining: 3m 39s\n",
      "1373:\ttotal: 34.9s\tremaining: 3m 39s\n",
      "1374:\ttotal: 34.9s\tremaining: 3m 39s\n",
      "1375:\ttotal: 35s\tremaining: 3m 39s\n",
      "1376:\ttotal: 35s\tremaining: 3m 39s\n",
      "1377:\ttotal: 35s\tremaining: 3m 39s\n",
      "1378:\ttotal: 35s\tremaining: 3m 39s\n",
      "1379:\ttotal: 35.1s\tremaining: 3m 39s\n",
      "1380:\ttotal: 35.1s\tremaining: 3m 39s\n",
      "1381:\ttotal: 35.1s\tremaining: 3m 39s\n",
      "1382:\ttotal: 35.2s\tremaining: 3m 39s\n",
      "1383:\ttotal: 35.2s\tremaining: 3m 39s\n",
      "1384:\ttotal: 35.2s\tremaining: 3m 39s\n",
      "1385:\ttotal: 35.3s\tremaining: 3m 39s\n",
      "1386:\ttotal: 35.3s\tremaining: 3m 39s\n",
      "1387:\ttotal: 35.3s\tremaining: 3m 39s\n",
      "1388:\ttotal: 35.3s\tremaining: 3m 39s\n",
      "1389:\ttotal: 35.4s\tremaining: 3m 39s\n",
      "1390:\ttotal: 35.4s\tremaining: 3m 39s\n",
      "1391:\ttotal: 35.4s\tremaining: 3m 39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1392:\ttotal: 35.5s\tremaining: 3m 39s\n",
      "1393:\ttotal: 35.5s\tremaining: 3m 39s\n",
      "1394:\ttotal: 35.6s\tremaining: 3m 39s\n",
      "1395:\ttotal: 35.6s\tremaining: 3m 39s\n",
      "1396:\ttotal: 35.6s\tremaining: 3m 39s\n",
      "1397:\ttotal: 35.6s\tremaining: 3m 39s\n",
      "1398:\ttotal: 35.7s\tremaining: 3m 39s\n",
      "1399:\ttotal: 35.7s\tremaining: 3m 39s\n",
      "1400:\ttotal: 35.7s\tremaining: 3m 39s\n",
      "1401:\ttotal: 35.8s\tremaining: 3m 39s\n",
      "1402:\ttotal: 35.8s\tremaining: 3m 39s\n",
      "1403:\ttotal: 35.8s\tremaining: 3m 39s\n",
      "1404:\ttotal: 35.9s\tremaining: 3m 39s\n",
      "1405:\ttotal: 35.9s\tremaining: 3m 39s\n",
      "1406:\ttotal: 35.9s\tremaining: 3m 39s\n",
      "1407:\ttotal: 36s\tremaining: 3m 39s\n",
      "1408:\ttotal: 36s\tremaining: 3m 39s\n",
      "1409:\ttotal: 36s\tremaining: 3m 39s\n",
      "1410:\ttotal: 36s\tremaining: 3m 39s\n",
      "1411:\ttotal: 36.1s\tremaining: 3m 39s\n",
      "1412:\ttotal: 36.1s\tremaining: 3m 39s\n",
      "1413:\ttotal: 36.1s\tremaining: 3m 39s\n",
      "1414:\ttotal: 36.1s\tremaining: 3m 39s\n",
      "1415:\ttotal: 36.1s\tremaining: 3m 39s\n",
      "1416:\ttotal: 36.2s\tremaining: 3m 39s\n",
      "1417:\ttotal: 36.2s\tremaining: 3m 39s\n",
      "1418:\ttotal: 36.2s\tremaining: 3m 39s\n",
      "1419:\ttotal: 36.3s\tremaining: 3m 39s\n",
      "1420:\ttotal: 36.3s\tremaining: 3m 39s\n",
      "1421:\ttotal: 36.3s\tremaining: 3m 39s\n",
      "1422:\ttotal: 36.3s\tremaining: 3m 39s\n",
      "1423:\ttotal: 36.4s\tremaining: 3m 39s\n",
      "1424:\ttotal: 36.4s\tremaining: 3m 38s\n",
      "1425:\ttotal: 36.4s\tremaining: 3m 38s\n",
      "1426:\ttotal: 36.5s\tremaining: 3m 38s\n",
      "1427:\ttotal: 36.5s\tremaining: 3m 38s\n",
      "1428:\ttotal: 36.5s\tremaining: 3m 38s\n",
      "1429:\ttotal: 36.5s\tremaining: 3m 38s\n",
      "1430:\ttotal: 36.6s\tremaining: 3m 38s\n",
      "1431:\ttotal: 36.6s\tremaining: 3m 38s\n",
      "1432:\ttotal: 36.6s\tremaining: 3m 38s\n",
      "1433:\ttotal: 36.6s\tremaining: 3m 38s\n",
      "1434:\ttotal: 36.7s\tremaining: 3m 38s\n",
      "1435:\ttotal: 36.7s\tremaining: 3m 38s\n",
      "1436:\ttotal: 36.8s\tremaining: 3m 38s\n",
      "1437:\ttotal: 36.8s\tremaining: 3m 39s\n",
      "1438:\ttotal: 36.8s\tremaining: 3m 39s\n",
      "1439:\ttotal: 36.9s\tremaining: 3m 39s\n",
      "1440:\ttotal: 36.9s\tremaining: 3m 39s\n",
      "1441:\ttotal: 36.9s\tremaining: 3m 39s\n",
      "1442:\ttotal: 37s\tremaining: 3m 39s\n",
      "1443:\ttotal: 37s\tremaining: 3m 39s\n",
      "1444:\ttotal: 37s\tremaining: 3m 39s\n",
      "1445:\ttotal: 37.1s\tremaining: 3m 39s\n",
      "1446:\ttotal: 37.1s\tremaining: 3m 39s\n",
      "1447:\ttotal: 37.1s\tremaining: 3m 39s\n",
      "1448:\ttotal: 37.1s\tremaining: 3m 39s\n",
      "1449:\ttotal: 37.1s\tremaining: 3m 39s\n",
      "1450:\ttotal: 37.2s\tremaining: 3m 38s\n",
      "1451:\ttotal: 37.2s\tremaining: 3m 38s\n",
      "1452:\ttotal: 37.2s\tremaining: 3m 38s\n",
      "1453:\ttotal: 37.2s\tremaining: 3m 38s\n",
      "1454:\ttotal: 37.2s\tremaining: 3m 38s\n",
      "1455:\ttotal: 37.3s\tremaining: 3m 38s\n",
      "1456:\ttotal: 37.3s\tremaining: 3m 38s\n",
      "1457:\ttotal: 37.3s\tremaining: 3m 38s\n",
      "1458:\ttotal: 37.3s\tremaining: 3m 38s\n",
      "1459:\ttotal: 37.4s\tremaining: 3m 38s\n",
      "1460:\ttotal: 37.4s\tremaining: 3m 38s\n",
      "1461:\ttotal: 37.4s\tremaining: 3m 38s\n",
      "1462:\ttotal: 37.5s\tremaining: 3m 38s\n",
      "1463:\ttotal: 37.5s\tremaining: 3m 38s\n",
      "1464:\ttotal: 37.5s\tremaining: 3m 38s\n",
      "1465:\ttotal: 37.5s\tremaining: 3m 38s\n",
      "1466:\ttotal: 37.5s\tremaining: 3m 38s\n",
      "1467:\ttotal: 37.6s\tremaining: 3m 38s\n",
      "1468:\ttotal: 37.6s\tremaining: 3m 38s\n",
      "1469:\ttotal: 37.6s\tremaining: 3m 38s\n",
      "1470:\ttotal: 37.6s\tremaining: 3m 38s\n",
      "1471:\ttotal: 37.6s\tremaining: 3m 38s\n",
      "1472:\ttotal: 37.7s\tremaining: 3m 38s\n",
      "1473:\ttotal: 37.7s\tremaining: 3m 37s\n",
      "1474:\ttotal: 37.7s\tremaining: 3m 37s\n",
      "1475:\ttotal: 37.7s\tremaining: 3m 37s\n",
      "1476:\ttotal: 37.8s\tremaining: 3m 37s\n",
      "1477:\ttotal: 37.8s\tremaining: 3m 37s\n",
      "1478:\ttotal: 37.8s\tremaining: 3m 37s\n",
      "1479:\ttotal: 37.8s\tremaining: 3m 37s\n",
      "1480:\ttotal: 37.9s\tremaining: 3m 37s\n",
      "1481:\ttotal: 37.9s\tremaining: 3m 37s\n",
      "1482:\ttotal: 37.9s\tremaining: 3m 37s\n",
      "1483:\ttotal: 37.9s\tremaining: 3m 37s\n",
      "1484:\ttotal: 38s\tremaining: 3m 37s\n",
      "1485:\ttotal: 38s\tremaining: 3m 37s\n",
      "1486:\ttotal: 38s\tremaining: 3m 37s\n",
      "1487:\ttotal: 38.1s\tremaining: 3m 37s\n",
      "1488:\ttotal: 38.1s\tremaining: 3m 37s\n",
      "1489:\ttotal: 38.1s\tremaining: 3m 37s\n",
      "1490:\ttotal: 38.1s\tremaining: 3m 37s\n",
      "1491:\ttotal: 38.1s\tremaining: 3m 37s\n",
      "1492:\ttotal: 38.2s\tremaining: 3m 37s\n",
      "1493:\ttotal: 38.2s\tremaining: 3m 37s\n",
      "1494:\ttotal: 38.3s\tremaining: 3m 37s\n",
      "1495:\ttotal: 38.3s\tremaining: 3m 37s\n",
      "1496:\ttotal: 38.3s\tremaining: 3m 37s\n",
      "1497:\ttotal: 38.3s\tremaining: 3m 37s\n",
      "1498:\ttotal: 38.4s\tremaining: 3m 37s\n",
      "1499:\ttotal: 38.4s\tremaining: 3m 37s\n",
      "1500:\ttotal: 38.4s\tremaining: 3m 37s\n",
      "1501:\ttotal: 38.5s\tremaining: 3m 37s\n",
      "1502:\ttotal: 38.5s\tremaining: 3m 37s\n",
      "1503:\ttotal: 38.5s\tremaining: 3m 37s\n",
      "1504:\ttotal: 38.5s\tremaining: 3m 37s\n",
      "1505:\ttotal: 38.6s\tremaining: 3m 37s\n",
      "1506:\ttotal: 38.6s\tremaining: 3m 37s\n",
      "1507:\ttotal: 38.6s\tremaining: 3m 37s\n",
      "1508:\ttotal: 38.7s\tremaining: 3m 37s\n",
      "1509:\ttotal: 38.7s\tremaining: 3m 37s\n",
      "1510:\ttotal: 38.7s\tremaining: 3m 37s\n",
      "1511:\ttotal: 38.7s\tremaining: 3m 37s\n",
      "1512:\ttotal: 38.8s\tremaining: 3m 37s\n",
      "1513:\ttotal: 38.8s\tremaining: 3m 37s\n",
      "1514:\ttotal: 38.8s\tremaining: 3m 37s\n",
      "1515:\ttotal: 38.9s\tremaining: 3m 37s\n",
      "1516:\ttotal: 38.9s\tremaining: 3m 37s\n",
      "1517:\ttotal: 38.9s\tremaining: 3m 37s\n",
      "1518:\ttotal: 39s\tremaining: 3m 37s\n",
      "1519:\ttotal: 39s\tremaining: 3m 37s\n",
      "1520:\ttotal: 39s\tremaining: 3m 37s\n",
      "1521:\ttotal: 39s\tremaining: 3m 37s\n",
      "1522:\ttotal: 39s\tremaining: 3m 37s\n",
      "1523:\ttotal: 39.1s\tremaining: 3m 37s\n",
      "1524:\ttotal: 39.1s\tremaining: 3m 37s\n",
      "1525:\ttotal: 39.1s\tremaining: 3m 37s\n",
      "1526:\ttotal: 39.1s\tremaining: 3m 37s\n",
      "1527:\ttotal: 39.1s\tremaining: 3m 37s\n",
      "1528:\ttotal: 39.2s\tremaining: 3m 37s\n",
      "1529:\ttotal: 39.2s\tremaining: 3m 36s\n",
      "1530:\ttotal: 39.2s\tremaining: 3m 36s\n",
      "1531:\ttotal: 39.3s\tremaining: 3m 36s\n",
      "1532:\ttotal: 39.3s\tremaining: 3m 37s\n",
      "1533:\ttotal: 39.3s\tremaining: 3m 37s\n",
      "1534:\ttotal: 39.4s\tremaining: 3m 37s\n",
      "1535:\ttotal: 39.4s\tremaining: 3m 37s\n",
      "1536:\ttotal: 39.4s\tremaining: 3m 37s\n",
      "1537:\ttotal: 39.5s\tremaining: 3m 37s\n",
      "1538:\ttotal: 39.5s\tremaining: 3m 37s\n",
      "1539:\ttotal: 39.5s\tremaining: 3m 37s\n",
      "1540:\ttotal: 39.6s\tremaining: 3m 37s\n",
      "1541:\ttotal: 39.6s\tremaining: 3m 37s\n",
      "1542:\ttotal: 39.6s\tremaining: 3m 37s\n",
      "1543:\ttotal: 39.7s\tremaining: 3m 37s\n",
      "1544:\ttotal: 39.7s\tremaining: 3m 37s\n",
      "1545:\ttotal: 39.7s\tremaining: 3m 37s\n",
      "1546:\ttotal: 39.8s\tremaining: 3m 37s\n",
      "1547:\ttotal: 39.8s\tremaining: 3m 37s\n",
      "1548:\ttotal: 39.8s\tremaining: 3m 37s\n",
      "1549:\ttotal: 39.9s\tremaining: 3m 37s\n",
      "1550:\ttotal: 39.9s\tremaining: 3m 37s\n",
      "1551:\ttotal: 39.9s\tremaining: 3m 37s\n",
      "1552:\ttotal: 40s\tremaining: 3m 37s\n",
      "1553:\ttotal: 40s\tremaining: 3m 37s\n",
      "1554:\ttotal: 40s\tremaining: 3m 37s\n",
      "1555:\ttotal: 40s\tremaining: 3m 37s\n",
      "1556:\ttotal: 40.1s\tremaining: 3m 37s\n",
      "1557:\ttotal: 40.1s\tremaining: 3m 37s\n",
      "1558:\ttotal: 40.1s\tremaining: 3m 37s\n",
      "1559:\ttotal: 40.2s\tremaining: 3m 37s\n",
      "1560:\ttotal: 40.2s\tremaining: 3m 37s\n",
      "1561:\ttotal: 40.2s\tremaining: 3m 37s\n",
      "1562:\ttotal: 40.3s\tremaining: 3m 37s\n",
      "1563:\ttotal: 40.3s\tremaining: 3m 37s\n",
      "1564:\ttotal: 40.3s\tremaining: 3m 37s\n",
      "1565:\ttotal: 40.3s\tremaining: 3m 37s\n",
      "1566:\ttotal: 40.4s\tremaining: 3m 37s\n",
      "1567:\ttotal: 40.4s\tremaining: 3m 37s\n",
      "1568:\ttotal: 40.4s\tremaining: 3m 37s\n",
      "1569:\ttotal: 40.5s\tremaining: 3m 37s\n",
      "1570:\ttotal: 40.5s\tremaining: 3m 37s\n",
      "1571:\ttotal: 40.5s\tremaining: 3m 37s\n",
      "1572:\ttotal: 40.6s\tremaining: 3m 37s\n",
      "1573:\ttotal: 40.6s\tremaining: 3m 37s\n",
      "1574:\ttotal: 40.6s\tremaining: 3m 37s\n",
      "1575:\ttotal: 40.6s\tremaining: 3m 37s\n",
      "1576:\ttotal: 40.7s\tremaining: 3m 37s\n",
      "1577:\ttotal: 40.7s\tremaining: 3m 37s\n",
      "1578:\ttotal: 40.7s\tremaining: 3m 37s\n",
      "1579:\ttotal: 40.8s\tremaining: 3m 37s\n",
      "1580:\ttotal: 40.8s\tremaining: 3m 37s\n",
      "1581:\ttotal: 40.8s\tremaining: 3m 37s\n",
      "1582:\ttotal: 40.9s\tremaining: 3m 37s\n",
      "1583:\ttotal: 40.9s\tremaining: 3m 37s\n",
      "1584:\ttotal: 41s\tremaining: 3m 37s\n",
      "1585:\ttotal: 41s\tremaining: 3m 37s\n",
      "1586:\ttotal: 41s\tremaining: 3m 37s\n",
      "1587:\ttotal: 41s\tremaining: 3m 37s\n",
      "1588:\ttotal: 41.1s\tremaining: 3m 37s\n",
      "1589:\ttotal: 41.1s\tremaining: 3m 37s\n",
      "1590:\ttotal: 41.1s\tremaining: 3m 37s\n",
      "1591:\ttotal: 41.1s\tremaining: 3m 37s\n",
      "1592:\ttotal: 41.2s\tremaining: 3m 37s\n",
      "1593:\ttotal: 41.2s\tremaining: 3m 37s\n",
      "1594:\ttotal: 41.2s\tremaining: 3m 37s\n",
      "1595:\ttotal: 41.2s\tremaining: 3m 37s\n",
      "1596:\ttotal: 41.2s\tremaining: 3m 36s\n",
      "1597:\ttotal: 41.3s\tremaining: 3m 36s\n",
      "1598:\ttotal: 41.3s\tremaining: 3m 36s\n",
      "1599:\ttotal: 41.3s\tremaining: 3m 36s\n",
      "1600:\ttotal: 41.3s\tremaining: 3m 36s\n",
      "1601:\ttotal: 41.3s\tremaining: 3m 36s\n",
      "1602:\ttotal: 41.3s\tremaining: 3m 36s\n",
      "1603:\ttotal: 41.4s\tremaining: 3m 36s\n",
      "1604:\ttotal: 41.4s\tremaining: 3m 36s\n",
      "1605:\ttotal: 41.4s\tremaining: 3m 36s\n",
      "1606:\ttotal: 41.4s\tremaining: 3m 36s\n",
      "1607:\ttotal: 41.5s\tremaining: 3m 36s\n",
      "1608:\ttotal: 41.5s\tremaining: 3m 36s\n",
      "1609:\ttotal: 41.5s\tremaining: 3m 36s\n",
      "1610:\ttotal: 41.6s\tremaining: 3m 36s\n",
      "1611:\ttotal: 41.6s\tremaining: 3m 36s\n",
      "1612:\ttotal: 41.6s\tremaining: 3m 36s\n",
      "1613:\ttotal: 41.7s\tremaining: 3m 36s\n",
      "1614:\ttotal: 41.7s\tremaining: 3m 36s\n",
      "1615:\ttotal: 41.7s\tremaining: 3m 36s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1616:\ttotal: 41.7s\tremaining: 3m 36s\n",
      "1617:\ttotal: 41.8s\tremaining: 3m 36s\n",
      "1618:\ttotal: 41.8s\tremaining: 3m 36s\n",
      "1619:\ttotal: 41.8s\tremaining: 3m 36s\n",
      "1620:\ttotal: 41.9s\tremaining: 3m 36s\n",
      "1621:\ttotal: 41.9s\tremaining: 3m 36s\n",
      "1622:\ttotal: 42s\tremaining: 3m 36s\n",
      "1623:\ttotal: 42s\tremaining: 3m 36s\n",
      "1624:\ttotal: 42s\tremaining: 3m 36s\n",
      "1625:\ttotal: 42s\tremaining: 3m 36s\n",
      "1626:\ttotal: 42.1s\tremaining: 3m 36s\n",
      "1627:\ttotal: 42.1s\tremaining: 3m 36s\n",
      "1628:\ttotal: 42.1s\tremaining: 3m 36s\n",
      "1629:\ttotal: 42.2s\tremaining: 3m 36s\n",
      "1630:\ttotal: 42.2s\tremaining: 3m 36s\n",
      "1631:\ttotal: 42.2s\tremaining: 3m 36s\n",
      "1632:\ttotal: 42.3s\tremaining: 3m 36s\n",
      "1633:\ttotal: 42.3s\tremaining: 3m 36s\n",
      "1634:\ttotal: 42.3s\tremaining: 3m 36s\n",
      "1635:\ttotal: 42.3s\tremaining: 3m 36s\n",
      "1636:\ttotal: 42.4s\tremaining: 3m 36s\n",
      "1637:\ttotal: 42.4s\tremaining: 3m 36s\n",
      "1638:\ttotal: 42.4s\tremaining: 3m 36s\n",
      "1639:\ttotal: 42.5s\tremaining: 3m 36s\n",
      "1640:\ttotal: 42.5s\tremaining: 3m 36s\n",
      "1641:\ttotal: 42.5s\tremaining: 3m 36s\n",
      "1642:\ttotal: 42.5s\tremaining: 3m 36s\n",
      "1643:\ttotal: 42.6s\tremaining: 3m 36s\n",
      "1644:\ttotal: 42.6s\tremaining: 3m 36s\n",
      "1645:\ttotal: 42.6s\tremaining: 3m 36s\n",
      "1646:\ttotal: 42.7s\tremaining: 3m 36s\n",
      "1647:\ttotal: 42.7s\tremaining: 3m 36s\n",
      "1648:\ttotal: 42.7s\tremaining: 3m 36s\n",
      "1649:\ttotal: 42.7s\tremaining: 3m 36s\n",
      "1650:\ttotal: 42.8s\tremaining: 3m 36s\n",
      "1651:\ttotal: 42.8s\tremaining: 3m 36s\n",
      "1652:\ttotal: 42.9s\tremaining: 3m 36s\n",
      "1653:\ttotal: 42.9s\tremaining: 3m 36s\n",
      "1654:\ttotal: 42.9s\tremaining: 3m 36s\n",
      "1655:\ttotal: 42.9s\tremaining: 3m 36s\n",
      "1656:\ttotal: 43s\tremaining: 3m 36s\n",
      "1657:\ttotal: 43s\tremaining: 3m 36s\n",
      "1658:\ttotal: 43s\tremaining: 3m 36s\n",
      "1659:\ttotal: 43.1s\tremaining: 3m 36s\n",
      "1660:\ttotal: 43.1s\tremaining: 3m 36s\n",
      "1661:\ttotal: 43.1s\tremaining: 3m 36s\n",
      "1662:\ttotal: 43.2s\tremaining: 3m 36s\n",
      "1663:\ttotal: 43.2s\tremaining: 3m 36s\n",
      "1664:\ttotal: 43.2s\tremaining: 3m 36s\n",
      "1665:\ttotal: 43.3s\tremaining: 3m 36s\n",
      "1666:\ttotal: 43.3s\tremaining: 3m 36s\n",
      "1667:\ttotal: 43.3s\tremaining: 3m 36s\n",
      "1668:\ttotal: 43.4s\tremaining: 3m 36s\n",
      "1669:\ttotal: 43.4s\tremaining: 3m 36s\n",
      "1670:\ttotal: 43.4s\tremaining: 3m 36s\n",
      "1671:\ttotal: 43.5s\tremaining: 3m 36s\n",
      "1672:\ttotal: 43.5s\tremaining: 3m 36s\n",
      "1673:\ttotal: 43.5s\tremaining: 3m 36s\n",
      "1674:\ttotal: 43.6s\tremaining: 3m 36s\n",
      "1675:\ttotal: 43.6s\tremaining: 3m 36s\n",
      "1676:\ttotal: 43.6s\tremaining: 3m 36s\n",
      "1677:\ttotal: 43.6s\tremaining: 3m 36s\n",
      "1678:\ttotal: 43.7s\tremaining: 3m 36s\n",
      "1679:\ttotal: 43.7s\tremaining: 3m 36s\n",
      "1680:\ttotal: 43.7s\tremaining: 3m 36s\n",
      "1681:\ttotal: 43.8s\tremaining: 3m 36s\n",
      "1682:\ttotal: 43.8s\tremaining: 3m 36s\n",
      "1683:\ttotal: 43.8s\tremaining: 3m 36s\n",
      "1684:\ttotal: 43.9s\tremaining: 3m 36s\n",
      "1685:\ttotal: 43.9s\tremaining: 3m 36s\n",
      "1686:\ttotal: 43.9s\tremaining: 3m 36s\n",
      "1687:\ttotal: 43.9s\tremaining: 3m 36s\n",
      "1688:\ttotal: 44s\tremaining: 3m 36s\n",
      "1689:\ttotal: 44s\tremaining: 3m 36s\n",
      "1690:\ttotal: 44s\tremaining: 3m 36s\n",
      "1691:\ttotal: 44s\tremaining: 3m 36s\n",
      "1692:\ttotal: 44.1s\tremaining: 3m 36s\n",
      "1693:\ttotal: 44.1s\tremaining: 3m 36s\n",
      "1694:\ttotal: 44.1s\tremaining: 3m 35s\n",
      "1695:\ttotal: 44.1s\tremaining: 3m 35s\n",
      "1696:\ttotal: 44.1s\tremaining: 3m 35s\n",
      "1697:\ttotal: 44.2s\tremaining: 3m 35s\n",
      "1698:\ttotal: 44.2s\tremaining: 3m 35s\n",
      "1699:\ttotal: 44.2s\tremaining: 3m 35s\n",
      "1700:\ttotal: 44.2s\tremaining: 3m 35s\n",
      "1701:\ttotal: 44.3s\tremaining: 3m 35s\n",
      "1702:\ttotal: 44.3s\tremaining: 3m 35s\n",
      "1703:\ttotal: 44.3s\tremaining: 3m 35s\n",
      "1704:\ttotal: 44.4s\tremaining: 3m 35s\n",
      "1705:\ttotal: 44.4s\tremaining: 3m 35s\n",
      "1706:\ttotal: 44.4s\tremaining: 3m 35s\n",
      "1707:\ttotal: 44.5s\tremaining: 3m 35s\n",
      "1708:\ttotal: 44.5s\tremaining: 3m 35s\n",
      "1709:\ttotal: 44.5s\tremaining: 3m 35s\n",
      "1710:\ttotal: 44.6s\tremaining: 3m 35s\n",
      "1711:\ttotal: 44.6s\tremaining: 3m 35s\n",
      "1712:\ttotal: 44.6s\tremaining: 3m 35s\n",
      "1713:\ttotal: 44.7s\tremaining: 3m 36s\n",
      "1714:\ttotal: 44.7s\tremaining: 3m 36s\n",
      "1715:\ttotal: 44.8s\tremaining: 3m 36s\n",
      "1716:\ttotal: 44.8s\tremaining: 3m 36s\n",
      "1717:\ttotal: 44.8s\tremaining: 3m 36s\n",
      "1718:\ttotal: 44.8s\tremaining: 3m 36s\n",
      "1719:\ttotal: 44.9s\tremaining: 3m 36s\n",
      "1720:\ttotal: 44.9s\tremaining: 3m 35s\n",
      "1721:\ttotal: 44.9s\tremaining: 3m 35s\n",
      "1722:\ttotal: 45s\tremaining: 3m 36s\n",
      "1723:\ttotal: 45s\tremaining: 3m 36s\n",
      "1724:\ttotal: 45s\tremaining: 3m 36s\n",
      "1725:\ttotal: 45.1s\tremaining: 3m 36s\n",
      "1726:\ttotal: 45.1s\tremaining: 3m 36s\n",
      "1727:\ttotal: 45.1s\tremaining: 3m 36s\n",
      "1728:\ttotal: 45.2s\tremaining: 3m 36s\n",
      "1729:\ttotal: 45.2s\tremaining: 3m 36s\n",
      "1730:\ttotal: 45.2s\tremaining: 3m 36s\n",
      "1731:\ttotal: 45.3s\tremaining: 3m 36s\n",
      "1732:\ttotal: 45.3s\tremaining: 3m 36s\n",
      "1733:\ttotal: 45.3s\tremaining: 3m 36s\n",
      "1734:\ttotal: 45.3s\tremaining: 3m 35s\n",
      "1735:\ttotal: 45.4s\tremaining: 3m 35s\n",
      "1736:\ttotal: 45.4s\tremaining: 3m 36s\n",
      "1737:\ttotal: 45.4s\tremaining: 3m 36s\n",
      "1738:\ttotal: 45.5s\tremaining: 3m 35s\n",
      "1739:\ttotal: 45.5s\tremaining: 3m 35s\n",
      "1740:\ttotal: 45.5s\tremaining: 3m 35s\n",
      "1741:\ttotal: 45.5s\tremaining: 3m 35s\n",
      "1742:\ttotal: 45.6s\tremaining: 3m 35s\n",
      "1743:\ttotal: 45.6s\tremaining: 3m 35s\n",
      "1744:\ttotal: 45.6s\tremaining: 3m 35s\n",
      "1745:\ttotal: 45.7s\tremaining: 3m 35s\n",
      "1746:\ttotal: 45.7s\tremaining: 3m 35s\n",
      "1747:\ttotal: 45.7s\tremaining: 3m 35s\n",
      "1748:\ttotal: 45.8s\tremaining: 3m 35s\n",
      "1749:\ttotal: 45.8s\tremaining: 3m 35s\n",
      "1750:\ttotal: 45.8s\tremaining: 3m 35s\n",
      "1751:\ttotal: 45.8s\tremaining: 3m 35s\n",
      "1752:\ttotal: 45.9s\tremaining: 3m 35s\n",
      "1753:\ttotal: 45.9s\tremaining: 3m 35s\n",
      "1754:\ttotal: 45.9s\tremaining: 3m 35s\n",
      "1755:\ttotal: 45.9s\tremaining: 3m 35s\n",
      "1756:\ttotal: 46s\tremaining: 3m 35s\n",
      "1757:\ttotal: 46s\tremaining: 3m 35s\n",
      "1758:\ttotal: 46s\tremaining: 3m 35s\n",
      "1759:\ttotal: 46.1s\tremaining: 3m 35s\n",
      "1760:\ttotal: 46.1s\tremaining: 3m 35s\n",
      "1761:\ttotal: 46.1s\tremaining: 3m 35s\n",
      "1762:\ttotal: 46.1s\tremaining: 3m 35s\n",
      "1763:\ttotal: 46.2s\tremaining: 3m 35s\n",
      "1764:\ttotal: 46.2s\tremaining: 3m 35s\n",
      "1765:\ttotal: 46.2s\tremaining: 3m 35s\n",
      "1766:\ttotal: 46.3s\tremaining: 3m 35s\n",
      "1767:\ttotal: 46.3s\tremaining: 3m 35s\n",
      "1768:\ttotal: 46.4s\tremaining: 3m 35s\n",
      "1769:\ttotal: 46.4s\tremaining: 3m 35s\n",
      "1770:\ttotal: 46.4s\tremaining: 3m 35s\n",
      "1771:\ttotal: 46.4s\tremaining: 3m 35s\n",
      "1772:\ttotal: 46.5s\tremaining: 3m 35s\n",
      "1773:\ttotal: 46.5s\tremaining: 3m 35s\n",
      "1774:\ttotal: 46.5s\tremaining: 3m 35s\n",
      "1775:\ttotal: 46.5s\tremaining: 3m 35s\n",
      "1776:\ttotal: 46.5s\tremaining: 3m 35s\n",
      "1777:\ttotal: 46.6s\tremaining: 3m 35s\n",
      "1778:\ttotal: 46.6s\tremaining: 3m 35s\n",
      "1779:\ttotal: 46.6s\tremaining: 3m 35s\n",
      "1780:\ttotal: 46.7s\tremaining: 3m 35s\n",
      "1781:\ttotal: 46.7s\tremaining: 3m 35s\n",
      "1782:\ttotal: 46.7s\tremaining: 3m 35s\n",
      "1783:\ttotal: 46.7s\tremaining: 3m 35s\n",
      "1784:\ttotal: 46.8s\tremaining: 3m 35s\n",
      "1785:\ttotal: 46.8s\tremaining: 3m 35s\n",
      "1786:\ttotal: 46.8s\tremaining: 3m 35s\n",
      "1787:\ttotal: 46.9s\tremaining: 3m 35s\n",
      "1788:\ttotal: 46.9s\tremaining: 3m 35s\n",
      "1789:\ttotal: 46.9s\tremaining: 3m 35s\n",
      "1790:\ttotal: 46.9s\tremaining: 3m 35s\n",
      "1791:\ttotal: 47s\tremaining: 3m 35s\n",
      "1792:\ttotal: 47s\tremaining: 3m 35s\n",
      "1793:\ttotal: 47s\tremaining: 3m 34s\n",
      "1794:\ttotal: 47s\tremaining: 3m 34s\n",
      "1795:\ttotal: 47s\tremaining: 3m 34s\n",
      "1796:\ttotal: 47.1s\tremaining: 3m 34s\n",
      "1797:\ttotal: 47.1s\tremaining: 3m 34s\n",
      "1798:\ttotal: 47.2s\tremaining: 3m 34s\n",
      "1799:\ttotal: 47.2s\tremaining: 3m 34s\n",
      "1800:\ttotal: 47.2s\tremaining: 3m 34s\n",
      "1801:\ttotal: 47.2s\tremaining: 3m 34s\n",
      "1802:\ttotal: 47.3s\tremaining: 3m 34s\n",
      "1803:\ttotal: 47.3s\tremaining: 3m 35s\n",
      "1804:\ttotal: 47.4s\tremaining: 3m 35s\n",
      "1805:\ttotal: 47.4s\tremaining: 3m 35s\n",
      "1806:\ttotal: 47.4s\tremaining: 3m 35s\n",
      "1807:\ttotal: 47.5s\tremaining: 3m 35s\n",
      "1808:\ttotal: 47.5s\tremaining: 3m 34s\n",
      "1809:\ttotal: 47.5s\tremaining: 3m 34s\n",
      "1810:\ttotal: 47.5s\tremaining: 3m 34s\n",
      "1811:\ttotal: 47.6s\tremaining: 3m 34s\n",
      "1812:\ttotal: 47.6s\tremaining: 3m 34s\n",
      "1813:\ttotal: 47.6s\tremaining: 3m 34s\n",
      "1814:\ttotal: 47.6s\tremaining: 3m 34s\n",
      "1815:\ttotal: 47.6s\tremaining: 3m 34s\n",
      "1816:\ttotal: 47.7s\tremaining: 3m 34s\n",
      "1817:\ttotal: 47.7s\tremaining: 3m 34s\n",
      "1818:\ttotal: 47.7s\tremaining: 3m 34s\n",
      "1819:\ttotal: 47.8s\tremaining: 3m 34s\n",
      "1820:\ttotal: 47.8s\tremaining: 3m 34s\n",
      "1821:\ttotal: 47.8s\tremaining: 3m 34s\n",
      "1822:\ttotal: 47.8s\tremaining: 3m 34s\n",
      "1823:\ttotal: 47.9s\tremaining: 3m 34s\n",
      "1824:\ttotal: 47.9s\tremaining: 3m 34s\n",
      "1825:\ttotal: 47.9s\tremaining: 3m 34s\n",
      "1826:\ttotal: 47.9s\tremaining: 3m 34s\n",
      "1827:\ttotal: 47.9s\tremaining: 3m 34s\n",
      "1828:\ttotal: 48s\tremaining: 3m 34s\n",
      "1829:\ttotal: 48s\tremaining: 3m 34s\n",
      "1830:\ttotal: 48s\tremaining: 3m 34s\n",
      "1831:\ttotal: 48s\tremaining: 3m 34s\n",
      "1832:\ttotal: 48.1s\tremaining: 3m 34s\n",
      "1833:\ttotal: 48.1s\tremaining: 3m 34s\n",
      "1834:\ttotal: 48.1s\tremaining: 3m 34s\n",
      "1835:\ttotal: 48.2s\tremaining: 3m 34s\n",
      "1836:\ttotal: 48.2s\tremaining: 3m 34s\n",
      "1837:\ttotal: 48.2s\tremaining: 3m 34s\n",
      "1838:\ttotal: 48.2s\tremaining: 3m 34s\n",
      "1839:\ttotal: 48.3s\tremaining: 3m 34s\n",
      "1840:\ttotal: 48.3s\tremaining: 3m 33s\n",
      "1841:\ttotal: 48.3s\tremaining: 3m 33s\n",
      "1842:\ttotal: 48.3s\tremaining: 3m 33s\n",
      "1843:\ttotal: 48.4s\tremaining: 3m 33s\n",
      "1844:\ttotal: 48.4s\tremaining: 3m 33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1845:\ttotal: 48.4s\tremaining: 3m 33s\n",
      "1846:\ttotal: 48.4s\tremaining: 3m 33s\n",
      "1847:\ttotal: 48.5s\tremaining: 3m 33s\n",
      "1848:\ttotal: 48.5s\tremaining: 3m 33s\n",
      "1849:\ttotal: 48.5s\tremaining: 3m 33s\n",
      "1850:\ttotal: 48.5s\tremaining: 3m 33s\n",
      "1851:\ttotal: 48.6s\tremaining: 3m 33s\n",
      "1852:\ttotal: 48.6s\tremaining: 3m 33s\n",
      "1853:\ttotal: 48.6s\tremaining: 3m 33s\n",
      "1854:\ttotal: 48.7s\tremaining: 3m 33s\n",
      "1855:\ttotal: 48.7s\tremaining: 3m 33s\n",
      "1856:\ttotal: 48.7s\tremaining: 3m 33s\n",
      "1857:\ttotal: 48.7s\tremaining: 3m 33s\n",
      "1858:\ttotal: 48.8s\tremaining: 3m 33s\n",
      "1859:\ttotal: 48.8s\tremaining: 3m 33s\n",
      "1860:\ttotal: 48.8s\tremaining: 3m 33s\n",
      "1861:\ttotal: 48.9s\tremaining: 3m 33s\n",
      "1862:\ttotal: 48.9s\tremaining: 3m 33s\n",
      "1863:\ttotal: 48.9s\tremaining: 3m 33s\n",
      "1864:\ttotal: 49s\tremaining: 3m 33s\n",
      "1865:\ttotal: 49s\tremaining: 3m 33s\n",
      "1866:\ttotal: 49s\tremaining: 3m 33s\n",
      "1867:\ttotal: 49.1s\tremaining: 3m 33s\n",
      "1868:\ttotal: 49.1s\tremaining: 3m 33s\n",
      "1869:\ttotal: 49.1s\tremaining: 3m 33s\n",
      "1870:\ttotal: 49.1s\tremaining: 3m 33s\n",
      "1871:\ttotal: 49.2s\tremaining: 3m 33s\n",
      "1872:\ttotal: 49.2s\tremaining: 3m 33s\n",
      "1873:\ttotal: 49.2s\tremaining: 3m 33s\n",
      "1874:\ttotal: 49.3s\tremaining: 3m 33s\n",
      "1875:\ttotal: 49.3s\tremaining: 3m 33s\n",
      "1876:\ttotal: 49.3s\tremaining: 3m 33s\n",
      "1877:\ttotal: 49.4s\tremaining: 3m 33s\n",
      "1878:\ttotal: 49.4s\tremaining: 3m 33s\n",
      "1879:\ttotal: 49.4s\tremaining: 3m 33s\n",
      "1880:\ttotal: 49.4s\tremaining: 3m 33s\n",
      "1881:\ttotal: 49.5s\tremaining: 3m 33s\n",
      "1882:\ttotal: 49.5s\tremaining: 3m 33s\n",
      "1883:\ttotal: 49.5s\tremaining: 3m 33s\n",
      "1884:\ttotal: 49.5s\tremaining: 3m 33s\n",
      "1885:\ttotal: 49.6s\tremaining: 3m 33s\n",
      "1886:\ttotal: 49.6s\tremaining: 3m 33s\n",
      "1887:\ttotal: 49.6s\tremaining: 3m 33s\n",
      "1888:\ttotal: 49.6s\tremaining: 3m 33s\n",
      "1889:\ttotal: 49.7s\tremaining: 3m 33s\n",
      "1890:\ttotal: 49.7s\tremaining: 3m 33s\n",
      "1891:\ttotal: 49.7s\tremaining: 3m 33s\n",
      "1892:\ttotal: 49.8s\tremaining: 3m 33s\n",
      "1893:\ttotal: 49.8s\tremaining: 3m 33s\n",
      "1894:\ttotal: 49.8s\tremaining: 3m 33s\n",
      "1895:\ttotal: 49.9s\tremaining: 3m 33s\n",
      "1896:\ttotal: 49.9s\tremaining: 3m 33s\n",
      "1897:\ttotal: 49.9s\tremaining: 3m 33s\n",
      "1898:\ttotal: 49.9s\tremaining: 3m 32s\n",
      "1899:\ttotal: 50s\tremaining: 3m 32s\n",
      "1900:\ttotal: 50s\tremaining: 3m 32s\n",
      "1901:\ttotal: 50s\tremaining: 3m 32s\n",
      "1902:\ttotal: 50s\tremaining: 3m 32s\n",
      "1903:\ttotal: 50s\tremaining: 3m 32s\n",
      "1904:\ttotal: 50.1s\tremaining: 3m 32s\n",
      "1905:\ttotal: 50.1s\tremaining: 3m 32s\n",
      "1906:\ttotal: 50.1s\tremaining: 3m 32s\n",
      "1907:\ttotal: 50.2s\tremaining: 3m 32s\n",
      "1908:\ttotal: 50.2s\tremaining: 3m 32s\n",
      "1909:\ttotal: 50.2s\tremaining: 3m 32s\n",
      "1910:\ttotal: 50.3s\tremaining: 3m 32s\n",
      "1911:\ttotal: 50.3s\tremaining: 3m 32s\n",
      "1912:\ttotal: 50.3s\tremaining: 3m 32s\n",
      "1913:\ttotal: 50.3s\tremaining: 3m 32s\n",
      "1914:\ttotal: 50.3s\tremaining: 3m 32s\n",
      "1915:\ttotal: 50.3s\tremaining: 3m 32s\n",
      "1916:\ttotal: 50.4s\tremaining: 3m 32s\n",
      "1917:\ttotal: 50.4s\tremaining: 3m 32s\n",
      "1918:\ttotal: 50.4s\tremaining: 3m 32s\n",
      "1919:\ttotal: 50.4s\tremaining: 3m 32s\n",
      "1920:\ttotal: 50.4s\tremaining: 3m 32s\n",
      "1921:\ttotal: 50.5s\tremaining: 3m 32s\n",
      "1922:\ttotal: 50.5s\tremaining: 3m 31s\n",
      "1923:\ttotal: 50.5s\tremaining: 3m 31s\n",
      "1924:\ttotal: 50.5s\tremaining: 3m 31s\n",
      "1925:\ttotal: 50.5s\tremaining: 3m 31s\n",
      "1926:\ttotal: 50.6s\tremaining: 3m 31s\n",
      "1927:\ttotal: 50.6s\tremaining: 3m 31s\n",
      "1928:\ttotal: 50.6s\tremaining: 3m 31s\n",
      "1929:\ttotal: 50.6s\tremaining: 3m 31s\n",
      "1930:\ttotal: 50.7s\tremaining: 3m 31s\n",
      "1931:\ttotal: 50.7s\tremaining: 3m 31s\n",
      "1932:\ttotal: 50.7s\tremaining: 3m 31s\n",
      "1933:\ttotal: 50.7s\tremaining: 3m 31s\n",
      "1934:\ttotal: 50.7s\tremaining: 3m 31s\n",
      "1935:\ttotal: 50.8s\tremaining: 3m 31s\n",
      "1936:\ttotal: 50.8s\tremaining: 3m 31s\n",
      "1937:\ttotal: 50.8s\tremaining: 3m 31s\n",
      "1938:\ttotal: 50.8s\tremaining: 3m 31s\n",
      "1939:\ttotal: 50.9s\tremaining: 3m 31s\n",
      "1940:\ttotal: 50.9s\tremaining: 3m 31s\n",
      "1941:\ttotal: 50.9s\tremaining: 3m 31s\n",
      "1942:\ttotal: 50.9s\tremaining: 3m 31s\n",
      "1943:\ttotal: 50.9s\tremaining: 3m 30s\n",
      "1944:\ttotal: 50.9s\tremaining: 3m 30s\n",
      "1945:\ttotal: 50.9s\tremaining: 3m 30s\n",
      "1946:\ttotal: 51s\tremaining: 3m 30s\n",
      "1947:\ttotal: 51s\tremaining: 3m 30s\n",
      "1948:\ttotal: 51s\tremaining: 3m 30s\n",
      "1949:\ttotal: 51s\tremaining: 3m 30s\n",
      "1950:\ttotal: 51.1s\tremaining: 3m 30s\n",
      "1951:\ttotal: 51.1s\tremaining: 3m 30s\n",
      "1952:\ttotal: 51.1s\tremaining: 3m 30s\n",
      "1953:\ttotal: 51.1s\tremaining: 3m 30s\n",
      "1954:\ttotal: 51.1s\tremaining: 3m 30s\n",
      "1955:\ttotal: 51.1s\tremaining: 3m 30s\n",
      "1956:\ttotal: 51.2s\tremaining: 3m 30s\n",
      "1957:\ttotal: 51.2s\tremaining: 3m 30s\n",
      "1958:\ttotal: 51.2s\tremaining: 3m 30s\n",
      "1959:\ttotal: 51.2s\tremaining: 3m 30s\n",
      "1960:\ttotal: 51.3s\tremaining: 3m 30s\n",
      "1961:\ttotal: 51.3s\tremaining: 3m 30s\n",
      "1962:\ttotal: 51.3s\tremaining: 3m 30s\n",
      "1963:\ttotal: 51.3s\tremaining: 3m 29s\n",
      "1964:\ttotal: 51.3s\tremaining: 3m 29s\n",
      "1965:\ttotal: 51.4s\tremaining: 3m 29s\n",
      "1966:\ttotal: 51.4s\tremaining: 3m 29s\n",
      "1967:\ttotal: 51.4s\tremaining: 3m 29s\n",
      "1968:\ttotal: 51.4s\tremaining: 3m 29s\n",
      "1969:\ttotal: 51.5s\tremaining: 3m 29s\n",
      "1970:\ttotal: 51.5s\tremaining: 3m 29s\n",
      "1971:\ttotal: 51.5s\tremaining: 3m 29s\n",
      "1972:\ttotal: 51.5s\tremaining: 3m 29s\n",
      "1973:\ttotal: 51.6s\tremaining: 3m 29s\n",
      "1974:\ttotal: 51.6s\tremaining: 3m 29s\n",
      "1975:\ttotal: 51.6s\tremaining: 3m 29s\n",
      "1976:\ttotal: 51.6s\tremaining: 3m 29s\n",
      "1977:\ttotal: 51.7s\tremaining: 3m 29s\n",
      "1978:\ttotal: 51.7s\tremaining: 3m 29s\n",
      "1979:\ttotal: 51.7s\tremaining: 3m 29s\n",
      "1980:\ttotal: 51.7s\tremaining: 3m 29s\n",
      "1981:\ttotal: 51.8s\tremaining: 3m 29s\n",
      "1982:\ttotal: 51.8s\tremaining: 3m 29s\n",
      "1983:\ttotal: 51.8s\tremaining: 3m 29s\n",
      "1984:\ttotal: 51.9s\tremaining: 3m 29s\n",
      "1985:\ttotal: 51.9s\tremaining: 3m 29s\n",
      "1986:\ttotal: 51.9s\tremaining: 3m 29s\n",
      "1987:\ttotal: 52s\tremaining: 3m 29s\n",
      "1988:\ttotal: 52s\tremaining: 3m 29s\n",
      "1989:\ttotal: 52s\tremaining: 3m 29s\n",
      "1990:\ttotal: 52.1s\tremaining: 3m 29s\n",
      "1991:\ttotal: 52.1s\tremaining: 3m 29s\n",
      "1992:\ttotal: 52.1s\tremaining: 3m 29s\n",
      "1993:\ttotal: 52.2s\tremaining: 3m 29s\n",
      "1994:\ttotal: 52.2s\tremaining: 3m 29s\n",
      "1995:\ttotal: 52.3s\tremaining: 3m 29s\n",
      "1996:\ttotal: 52.3s\tremaining: 3m 29s\n",
      "1997:\ttotal: 52.3s\tremaining: 3m 29s\n",
      "1998:\ttotal: 52.4s\tremaining: 3m 29s\n",
      "1999:\ttotal: 52.4s\tremaining: 3m 29s\n",
      "2000:\ttotal: 52.4s\tremaining: 3m 29s\n",
      "2001:\ttotal: 52.5s\tremaining: 3m 29s\n",
      "2002:\ttotal: 52.5s\tremaining: 3m 29s\n",
      "2003:\ttotal: 52.5s\tremaining: 3m 29s\n",
      "2004:\ttotal: 52.6s\tremaining: 3m 29s\n",
      "2005:\ttotal: 52.6s\tremaining: 3m 29s\n",
      "2006:\ttotal: 52.6s\tremaining: 3m 29s\n",
      "2007:\ttotal: 52.7s\tremaining: 3m 29s\n",
      "2008:\ttotal: 52.7s\tremaining: 3m 29s\n",
      "2009:\ttotal: 52.7s\tremaining: 3m 29s\n",
      "2010:\ttotal: 52.8s\tremaining: 3m 29s\n",
      "2011:\ttotal: 52.8s\tremaining: 3m 29s\n",
      "2012:\ttotal: 52.8s\tremaining: 3m 29s\n",
      "2013:\ttotal: 52.9s\tremaining: 3m 29s\n",
      "2014:\ttotal: 52.9s\tremaining: 3m 29s\n",
      "2015:\ttotal: 52.9s\tremaining: 3m 29s\n",
      "2016:\ttotal: 53s\tremaining: 3m 29s\n",
      "2017:\ttotal: 53s\tremaining: 3m 29s\n",
      "2018:\ttotal: 53s\tremaining: 3m 29s\n",
      "2019:\ttotal: 53.1s\tremaining: 3m 29s\n",
      "2020:\ttotal: 53.1s\tremaining: 3m 29s\n",
      "2021:\ttotal: 53.1s\tremaining: 3m 29s\n",
      "2022:\ttotal: 53.2s\tremaining: 3m 29s\n",
      "2023:\ttotal: 53.2s\tremaining: 3m 29s\n",
      "2024:\ttotal: 53.2s\tremaining: 3m 29s\n",
      "2025:\ttotal: 53.2s\tremaining: 3m 29s\n",
      "2026:\ttotal: 53.3s\tremaining: 3m 29s\n",
      "2027:\ttotal: 53.3s\tremaining: 3m 29s\n",
      "2028:\ttotal: 53.3s\tremaining: 3m 29s\n",
      "2029:\ttotal: 53.3s\tremaining: 3m 29s\n",
      "2030:\ttotal: 53.3s\tremaining: 3m 29s\n",
      "2031:\ttotal: 53.3s\tremaining: 3m 29s\n",
      "2032:\ttotal: 53.4s\tremaining: 3m 29s\n",
      "2033:\ttotal: 53.4s\tremaining: 3m 28s\n",
      "2034:\ttotal: 53.4s\tremaining: 3m 28s\n",
      "2035:\ttotal: 53.4s\tremaining: 3m 28s\n",
      "2036:\ttotal: 53.4s\tremaining: 3m 28s\n",
      "2037:\ttotal: 53.5s\tremaining: 3m 28s\n",
      "2038:\ttotal: 53.5s\tremaining: 3m 28s\n",
      "2039:\ttotal: 53.5s\tremaining: 3m 28s\n",
      "2040:\ttotal: 53.5s\tremaining: 3m 28s\n",
      "2041:\ttotal: 53.5s\tremaining: 3m 28s\n",
      "2042:\ttotal: 53.6s\tremaining: 3m 28s\n",
      "2043:\ttotal: 53.6s\tremaining: 3m 28s\n",
      "2044:\ttotal: 53.6s\tremaining: 3m 28s\n",
      "2045:\ttotal: 53.6s\tremaining: 3m 28s\n",
      "2046:\ttotal: 53.6s\tremaining: 3m 28s\n",
      "2047:\ttotal: 53.7s\tremaining: 3m 28s\n",
      "2048:\ttotal: 53.7s\tremaining: 3m 28s\n",
      "2049:\ttotal: 53.7s\tremaining: 3m 28s\n",
      "2050:\ttotal: 53.7s\tremaining: 3m 28s\n",
      "2051:\ttotal: 53.8s\tremaining: 3m 28s\n",
      "2052:\ttotal: 53.8s\tremaining: 3m 28s\n",
      "2053:\ttotal: 53.8s\tremaining: 3m 28s\n",
      "2054:\ttotal: 53.8s\tremaining: 3m 27s\n",
      "2055:\ttotal: 53.8s\tremaining: 3m 27s\n",
      "2056:\ttotal: 53.8s\tremaining: 3m 27s\n",
      "2057:\ttotal: 53.9s\tremaining: 3m 27s\n",
      "2058:\ttotal: 53.9s\tremaining: 3m 27s\n",
      "2059:\ttotal: 53.9s\tremaining: 3m 27s\n",
      "2060:\ttotal: 53.9s\tremaining: 3m 27s\n",
      "2061:\ttotal: 54s\tremaining: 3m 27s\n",
      "2062:\ttotal: 54s\tremaining: 3m 27s\n",
      "2063:\ttotal: 54s\tremaining: 3m 27s\n",
      "2064:\ttotal: 54s\tremaining: 3m 27s\n",
      "2065:\ttotal: 54s\tremaining: 3m 27s\n",
      "2066:\ttotal: 54.1s\tremaining: 3m 27s\n",
      "2067:\ttotal: 54.1s\tremaining: 3m 27s\n",
      "2068:\ttotal: 54.1s\tremaining: 3m 27s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2069:\ttotal: 54.1s\tremaining: 3m 27s\n",
      "2070:\ttotal: 54.1s\tremaining: 3m 27s\n",
      "2071:\ttotal: 54.2s\tremaining: 3m 27s\n",
      "2072:\ttotal: 54.2s\tremaining: 3m 27s\n",
      "2073:\ttotal: 54.2s\tremaining: 3m 27s\n",
      "2074:\ttotal: 54.2s\tremaining: 3m 27s\n",
      "2075:\ttotal: 54.2s\tremaining: 3m 26s\n",
      "2076:\ttotal: 54.3s\tremaining: 3m 26s\n",
      "2077:\ttotal: 54.3s\tremaining: 3m 26s\n",
      "2078:\ttotal: 54.3s\tremaining: 3m 26s\n",
      "2079:\ttotal: 54.3s\tremaining: 3m 26s\n",
      "2080:\ttotal: 54.3s\tremaining: 3m 26s\n",
      "2081:\ttotal: 54.3s\tremaining: 3m 26s\n",
      "2082:\ttotal: 54.3s\tremaining: 3m 26s\n",
      "2083:\ttotal: 54.4s\tremaining: 3m 26s\n",
      "2084:\ttotal: 54.4s\tremaining: 3m 26s\n",
      "2085:\ttotal: 54.4s\tremaining: 3m 26s\n",
      "2086:\ttotal: 54.4s\tremaining: 3m 26s\n",
      "2087:\ttotal: 54.4s\tremaining: 3m 26s\n",
      "2088:\ttotal: 54.5s\tremaining: 3m 26s\n",
      "2089:\ttotal: 54.5s\tremaining: 3m 26s\n",
      "2090:\ttotal: 54.5s\tremaining: 3m 26s\n",
      "2091:\ttotal: 54.5s\tremaining: 3m 26s\n",
      "2092:\ttotal: 54.5s\tremaining: 3m 25s\n",
      "2093:\ttotal: 54.5s\tremaining: 3m 25s\n",
      "2094:\ttotal: 54.6s\tremaining: 3m 25s\n",
      "2095:\ttotal: 54.6s\tremaining: 3m 25s\n",
      "2096:\ttotal: 54.6s\tremaining: 3m 25s\n",
      "2097:\ttotal: 54.6s\tremaining: 3m 25s\n",
      "2098:\ttotal: 54.6s\tremaining: 3m 25s\n",
      "2099:\ttotal: 54.6s\tremaining: 3m 25s\n",
      "2100:\ttotal: 54.6s\tremaining: 3m 25s\n",
      "2101:\ttotal: 54.7s\tremaining: 3m 25s\n",
      "2102:\ttotal: 54.7s\tremaining: 3m 25s\n",
      "2103:\ttotal: 54.7s\tremaining: 3m 25s\n",
      "2104:\ttotal: 54.7s\tremaining: 3m 25s\n",
      "2105:\ttotal: 54.7s\tremaining: 3m 25s\n",
      "2106:\ttotal: 54.8s\tremaining: 3m 25s\n",
      "2107:\ttotal: 54.8s\tremaining: 3m 25s\n",
      "2108:\ttotal: 54.8s\tremaining: 3m 24s\n",
      "2109:\ttotal: 54.8s\tremaining: 3m 24s\n",
      "2110:\ttotal: 54.8s\tremaining: 3m 24s\n",
      "2111:\ttotal: 54.8s\tremaining: 3m 24s\n",
      "2112:\ttotal: 54.9s\tremaining: 3m 24s\n",
      "2113:\ttotal: 54.9s\tremaining: 3m 24s\n",
      "2114:\ttotal: 54.9s\tremaining: 3m 24s\n",
      "2115:\ttotal: 54.9s\tremaining: 3m 24s\n",
      "2116:\ttotal: 54.9s\tremaining: 3m 24s\n",
      "2117:\ttotal: 54.9s\tremaining: 3m 24s\n",
      "2118:\ttotal: 55s\tremaining: 3m 24s\n",
      "2119:\ttotal: 55s\tremaining: 3m 24s\n",
      "2120:\ttotal: 55s\tremaining: 3m 24s\n",
      "2121:\ttotal: 55s\tremaining: 3m 24s\n",
      "2122:\ttotal: 55s\tremaining: 3m 24s\n",
      "2123:\ttotal: 55.1s\tremaining: 3m 24s\n",
      "2124:\ttotal: 55.1s\tremaining: 3m 24s\n",
      "2125:\ttotal: 55.1s\tremaining: 3m 24s\n",
      "2126:\ttotal: 55.1s\tremaining: 3m 24s\n",
      "2127:\ttotal: 55.2s\tremaining: 3m 24s\n",
      "2128:\ttotal: 55.2s\tremaining: 3m 23s\n",
      "2129:\ttotal: 55.2s\tremaining: 3m 23s\n",
      "2130:\ttotal: 55.2s\tremaining: 3m 23s\n",
      "2131:\ttotal: 55.2s\tremaining: 3m 23s\n",
      "2132:\ttotal: 55.2s\tremaining: 3m 23s\n",
      "2133:\ttotal: 55.3s\tremaining: 3m 23s\n",
      "2134:\ttotal: 55.3s\tremaining: 3m 23s\n",
      "2135:\ttotal: 55.3s\tremaining: 3m 23s\n",
      "2136:\ttotal: 55.4s\tremaining: 3m 23s\n",
      "2137:\ttotal: 55.4s\tremaining: 3m 23s\n",
      "2138:\ttotal: 55.4s\tremaining: 3m 23s\n",
      "2139:\ttotal: 55.4s\tremaining: 3m 23s\n",
      "2140:\ttotal: 55.5s\tremaining: 3m 23s\n",
      "2141:\ttotal: 55.5s\tremaining: 3m 23s\n",
      "2142:\ttotal: 55.5s\tremaining: 3m 23s\n",
      "2143:\ttotal: 55.6s\tremaining: 3m 23s\n",
      "2144:\ttotal: 55.6s\tremaining: 3m 23s\n",
      "2145:\ttotal: 55.6s\tremaining: 3m 23s\n",
      "2146:\ttotal: 55.6s\tremaining: 3m 23s\n",
      "2147:\ttotal: 55.6s\tremaining: 3m 23s\n",
      "2148:\ttotal: 55.6s\tremaining: 3m 23s\n",
      "2149:\ttotal: 55.7s\tremaining: 3m 23s\n",
      "2150:\ttotal: 55.7s\tremaining: 3m 23s\n",
      "2151:\ttotal: 55.7s\tremaining: 3m 23s\n",
      "2152:\ttotal: 55.7s\tremaining: 3m 23s\n",
      "2153:\ttotal: 55.8s\tremaining: 3m 23s\n",
      "2154:\ttotal: 55.8s\tremaining: 3m 23s\n",
      "2155:\ttotal: 55.8s\tremaining: 3m 22s\n",
      "2156:\ttotal: 55.8s\tremaining: 3m 22s\n",
      "2157:\ttotal: 55.8s\tremaining: 3m 22s\n",
      "2158:\ttotal: 55.8s\tremaining: 3m 22s\n",
      "2159:\ttotal: 55.9s\tremaining: 3m 22s\n",
      "2160:\ttotal: 55.9s\tremaining: 3m 22s\n",
      "2161:\ttotal: 55.9s\tremaining: 3m 22s\n",
      "2162:\ttotal: 55.9s\tremaining: 3m 22s\n",
      "2163:\ttotal: 55.9s\tremaining: 3m 22s\n",
      "2164:\ttotal: 56s\tremaining: 3m 22s\n",
      "2165:\ttotal: 56s\tremaining: 3m 22s\n",
      "2166:\ttotal: 56s\tremaining: 3m 22s\n",
      "2167:\ttotal: 56.1s\tremaining: 3m 22s\n",
      "2168:\ttotal: 56.1s\tremaining: 3m 22s\n",
      "2169:\ttotal: 56.1s\tremaining: 3m 22s\n",
      "2170:\ttotal: 56.1s\tremaining: 3m 22s\n",
      "2171:\ttotal: 56.2s\tremaining: 3m 22s\n",
      "2172:\ttotal: 56.2s\tremaining: 3m 22s\n",
      "2173:\ttotal: 56.2s\tremaining: 3m 22s\n",
      "2174:\ttotal: 56.2s\tremaining: 3m 22s\n",
      "2175:\ttotal: 56.3s\tremaining: 3m 22s\n",
      "2176:\ttotal: 56.3s\tremaining: 3m 22s\n",
      "2177:\ttotal: 56.3s\tremaining: 3m 22s\n",
      "2178:\ttotal: 56.3s\tremaining: 3m 22s\n",
      "2179:\ttotal: 56.4s\tremaining: 3m 22s\n",
      "2180:\ttotal: 56.4s\tremaining: 3m 22s\n",
      "2181:\ttotal: 56.4s\tremaining: 3m 22s\n",
      "2182:\ttotal: 56.4s\tremaining: 3m 22s\n",
      "2183:\ttotal: 56.5s\tremaining: 3m 22s\n",
      "2184:\ttotal: 56.5s\tremaining: 3m 21s\n",
      "2185:\ttotal: 56.5s\tremaining: 3m 21s\n",
      "2186:\ttotal: 56.5s\tremaining: 3m 21s\n",
      "2187:\ttotal: 56.5s\tremaining: 3m 21s\n",
      "2188:\ttotal: 56.5s\tremaining: 3m 21s\n",
      "2189:\ttotal: 56.5s\tremaining: 3m 21s\n",
      "2190:\ttotal: 56.6s\tremaining: 3m 21s\n",
      "2191:\ttotal: 56.6s\tremaining: 3m 21s\n",
      "2192:\ttotal: 56.6s\tremaining: 3m 21s\n",
      "2193:\ttotal: 56.7s\tremaining: 3m 21s\n",
      "2194:\ttotal: 56.7s\tremaining: 3m 21s\n",
      "2195:\ttotal: 56.7s\tremaining: 3m 21s\n",
      "2196:\ttotal: 56.7s\tremaining: 3m 21s\n",
      "2197:\ttotal: 56.8s\tremaining: 3m 21s\n",
      "2198:\ttotal: 56.8s\tremaining: 3m 21s\n",
      "2199:\ttotal: 56.8s\tremaining: 3m 21s\n",
      "2200:\ttotal: 56.8s\tremaining: 3m 21s\n",
      "2201:\ttotal: 56.8s\tremaining: 3m 21s\n",
      "2202:\ttotal: 56.9s\tremaining: 3m 21s\n",
      "2203:\ttotal: 56.9s\tremaining: 3m 21s\n",
      "2204:\ttotal: 56.9s\tremaining: 3m 21s\n",
      "2205:\ttotal: 56.9s\tremaining: 3m 21s\n",
      "2206:\ttotal: 56.9s\tremaining: 3m 21s\n",
      "2207:\ttotal: 57s\tremaining: 3m 20s\n",
      "2208:\ttotal: 57s\tremaining: 3m 20s\n",
      "2209:\ttotal: 57s\tremaining: 3m 20s\n",
      "2210:\ttotal: 57s\tremaining: 3m 20s\n",
      "2211:\ttotal: 57s\tremaining: 3m 20s\n",
      "2212:\ttotal: 57.1s\tremaining: 3m 20s\n",
      "2213:\ttotal: 57.1s\tremaining: 3m 20s\n",
      "2214:\ttotal: 57.1s\tremaining: 3m 20s\n",
      "2215:\ttotal: 57.1s\tremaining: 3m 20s\n",
      "2216:\ttotal: 57.1s\tremaining: 3m 20s\n",
      "2217:\ttotal: 57.2s\tremaining: 3m 20s\n",
      "2218:\ttotal: 57.2s\tremaining: 3m 20s\n",
      "2219:\ttotal: 57.2s\tremaining: 3m 20s\n",
      "2220:\ttotal: 57.2s\tremaining: 3m 20s\n",
      "2221:\ttotal: 57.3s\tremaining: 3m 20s\n",
      "2222:\ttotal: 57.3s\tremaining: 3m 20s\n",
      "2223:\ttotal: 57.3s\tremaining: 3m 20s\n",
      "2224:\ttotal: 57.3s\tremaining: 3m 20s\n",
      "2225:\ttotal: 57.4s\tremaining: 3m 20s\n",
      "2226:\ttotal: 57.4s\tremaining: 3m 20s\n",
      "2227:\ttotal: 57.4s\tremaining: 3m 20s\n",
      "2228:\ttotal: 57.4s\tremaining: 3m 20s\n",
      "2229:\ttotal: 57.5s\tremaining: 3m 20s\n",
      "2230:\ttotal: 57.5s\tremaining: 3m 20s\n",
      "2231:\ttotal: 57.5s\tremaining: 3m 20s\n",
      "2232:\ttotal: 57.5s\tremaining: 3m 20s\n",
      "2233:\ttotal: 57.6s\tremaining: 3m 20s\n",
      "2234:\ttotal: 57.6s\tremaining: 3m 20s\n",
      "2235:\ttotal: 57.6s\tremaining: 3m 20s\n",
      "2236:\ttotal: 57.7s\tremaining: 3m 20s\n",
      "2237:\ttotal: 57.7s\tremaining: 3m 20s\n",
      "2238:\ttotal: 57.7s\tremaining: 3m 19s\n",
      "2239:\ttotal: 57.7s\tremaining: 3m 19s\n",
      "2240:\ttotal: 57.8s\tremaining: 3m 19s\n",
      "2241:\ttotal: 57.8s\tremaining: 3m 19s\n",
      "2242:\ttotal: 57.8s\tremaining: 3m 19s\n",
      "2243:\ttotal: 57.9s\tremaining: 3m 19s\n",
      "2244:\ttotal: 57.9s\tremaining: 3m 19s\n",
      "2245:\ttotal: 57.9s\tremaining: 3m 19s\n",
      "2246:\ttotal: 58s\tremaining: 3m 19s\n",
      "2247:\ttotal: 58s\tremaining: 3m 19s\n",
      "2248:\ttotal: 58s\tremaining: 3m 19s\n",
      "2249:\ttotal: 58s\tremaining: 3m 19s\n",
      "2250:\ttotal: 58.1s\tremaining: 3m 19s\n",
      "2251:\ttotal: 58.1s\tremaining: 3m 19s\n",
      "2252:\ttotal: 58.1s\tremaining: 3m 19s\n",
      "2253:\ttotal: 58.2s\tremaining: 3m 19s\n",
      "2254:\ttotal: 58.2s\tremaining: 3m 19s\n",
      "2255:\ttotal: 58.2s\tremaining: 3m 19s\n",
      "2256:\ttotal: 58.3s\tremaining: 3m 19s\n",
      "2257:\ttotal: 58.3s\tremaining: 3m 19s\n",
      "2258:\ttotal: 58.3s\tremaining: 3m 19s\n",
      "2259:\ttotal: 58.4s\tremaining: 3m 19s\n",
      "2260:\ttotal: 58.4s\tremaining: 3m 19s\n",
      "2261:\ttotal: 58.4s\tremaining: 3m 19s\n",
      "2262:\ttotal: 58.5s\tremaining: 3m 19s\n",
      "2263:\ttotal: 58.5s\tremaining: 3m 19s\n",
      "2264:\ttotal: 58.5s\tremaining: 3m 19s\n",
      "2265:\ttotal: 58.6s\tremaining: 3m 19s\n",
      "2266:\ttotal: 58.6s\tremaining: 3m 19s\n",
      "2267:\ttotal: 58.6s\tremaining: 3m 19s\n",
      "2268:\ttotal: 58.7s\tremaining: 3m 19s\n",
      "2269:\ttotal: 58.7s\tremaining: 3m 19s\n",
      "2270:\ttotal: 58.7s\tremaining: 3m 19s\n",
      "2271:\ttotal: 58.7s\tremaining: 3m 19s\n",
      "2272:\ttotal: 58.8s\tremaining: 3m 19s\n",
      "2273:\ttotal: 58.8s\tremaining: 3m 19s\n",
      "2274:\ttotal: 58.8s\tremaining: 3m 19s\n",
      "2275:\ttotal: 58.9s\tremaining: 3m 19s\n",
      "2276:\ttotal: 58.9s\tremaining: 3m 19s\n",
      "2277:\ttotal: 58.9s\tremaining: 3m 19s\n",
      "2278:\ttotal: 59s\tremaining: 3m 19s\n",
      "2279:\ttotal: 59s\tremaining: 3m 19s\n",
      "2280:\ttotal: 59s\tremaining: 3m 19s\n",
      "2281:\ttotal: 59s\tremaining: 3m 19s\n",
      "2282:\ttotal: 59.1s\tremaining: 3m 19s\n",
      "2283:\ttotal: 59.1s\tremaining: 3m 19s\n",
      "2284:\ttotal: 59.1s\tremaining: 3m 19s\n",
      "2285:\ttotal: 59.1s\tremaining: 3m 19s\n",
      "2286:\ttotal: 59.2s\tremaining: 3m 19s\n",
      "2287:\ttotal: 59.2s\tremaining: 3m 19s\n",
      "2288:\ttotal: 59.2s\tremaining: 3m 19s\n",
      "2289:\ttotal: 59.2s\tremaining: 3m 19s\n",
      "2290:\ttotal: 59.3s\tremaining: 3m 19s\n",
      "2291:\ttotal: 59.3s\tremaining: 3m 19s\n",
      "2292:\ttotal: 59.3s\tremaining: 3m 19s\n",
      "2293:\ttotal: 59.3s\tremaining: 3m 19s\n",
      "2294:\ttotal: 59.3s\tremaining: 3m 19s\n",
      "2295:\ttotal: 59.4s\tremaining: 3m 19s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2296:\ttotal: 59.4s\tremaining: 3m 19s\n",
      "2297:\ttotal: 59.4s\tremaining: 3m 19s\n",
      "2298:\ttotal: 59.5s\tremaining: 3m 19s\n",
      "2299:\ttotal: 59.5s\tremaining: 3m 19s\n",
      "2300:\ttotal: 59.5s\tremaining: 3m 19s\n",
      "2301:\ttotal: 59.6s\tremaining: 3m 19s\n",
      "2302:\ttotal: 59.6s\tremaining: 3m 19s\n",
      "2303:\ttotal: 59.6s\tremaining: 3m 19s\n",
      "2304:\ttotal: 59.7s\tremaining: 3m 19s\n",
      "2305:\ttotal: 59.7s\tremaining: 3m 19s\n",
      "2306:\ttotal: 59.7s\tremaining: 3m 19s\n",
      "2307:\ttotal: 59.7s\tremaining: 3m 19s\n",
      "2308:\ttotal: 59.8s\tremaining: 3m 19s\n",
      "2309:\ttotal: 59.8s\tremaining: 3m 19s\n",
      "2310:\ttotal: 59.8s\tremaining: 3m 19s\n",
      "2311:\ttotal: 59.9s\tremaining: 3m 19s\n",
      "2312:\ttotal: 59.9s\tremaining: 3m 19s\n",
      "2313:\ttotal: 60s\tremaining: 3m 19s\n",
      "2314:\ttotal: 60s\tremaining: 3m 19s\n",
      "2315:\ttotal: 1m\tremaining: 3m 19s\n",
      "2316:\ttotal: 1m\tremaining: 3m 19s\n",
      "2317:\ttotal: 1m\tremaining: 3m 19s\n",
      "2318:\ttotal: 1m\tremaining: 3m 19s\n",
      "2319:\ttotal: 1m\tremaining: 3m 19s\n",
      "2320:\ttotal: 1m\tremaining: 3m 19s\n",
      "2321:\ttotal: 1m\tremaining: 3m 18s\n",
      "2322:\ttotal: 1m\tremaining: 3m 18s\n",
      "2323:\ttotal: 1m\tremaining: 3m 18s\n",
      "2324:\ttotal: 1m\tremaining: 3m 18s\n",
      "2325:\ttotal: 1m\tremaining: 3m 18s\n",
      "2326:\ttotal: 1m\tremaining: 3m 18s\n",
      "2327:\ttotal: 1m\tremaining: 3m 18s\n",
      "2328:\ttotal: 1m\tremaining: 3m 18s\n",
      "2329:\ttotal: 1m\tremaining: 3m 18s\n",
      "2330:\ttotal: 1m\tremaining: 3m 18s\n",
      "2331:\ttotal: 1m\tremaining: 3m 18s\n",
      "2332:\ttotal: 1m\tremaining: 3m 18s\n",
      "2333:\ttotal: 1m\tremaining: 3m 18s\n",
      "2334:\ttotal: 1m\tremaining: 3m 18s\n",
      "2335:\ttotal: 1m\tremaining: 3m 18s\n",
      "2336:\ttotal: 1m\tremaining: 3m 18s\n",
      "2337:\ttotal: 1m\tremaining: 3m 18s\n",
      "2338:\ttotal: 1m\tremaining: 3m 18s\n",
      "2339:\ttotal: 1m\tremaining: 3m 18s\n",
      "2340:\ttotal: 1m\tremaining: 3m 18s\n",
      "2341:\ttotal: 1m\tremaining: 3m 18s\n",
      "2342:\ttotal: 1m\tremaining: 3m 18s\n",
      "2343:\ttotal: 1m\tremaining: 3m 18s\n",
      "2344:\ttotal: 1m\tremaining: 3m 18s\n",
      "2345:\ttotal: 1m\tremaining: 3m 18s\n",
      "2346:\ttotal: 1m\tremaining: 3m 18s\n",
      "2347:\ttotal: 1m\tremaining: 3m 18s\n",
      "2348:\ttotal: 1m\tremaining: 3m 18s\n",
      "2349:\ttotal: 1m\tremaining: 3m 18s\n",
      "2350:\ttotal: 1m\tremaining: 3m 18s\n",
      "2351:\ttotal: 1m\tremaining: 3m 18s\n",
      "2352:\ttotal: 1m\tremaining: 3m 18s\n",
      "2353:\ttotal: 1m\tremaining: 3m 18s\n",
      "2354:\ttotal: 1m 1s\tremaining: 3m 18s\n",
      "2355:\ttotal: 1m 1s\tremaining: 3m 18s\n",
      "2356:\ttotal: 1m 1s\tremaining: 3m 18s\n",
      "2357:\ttotal: 1m 1s\tremaining: 3m 18s\n",
      "2358:\ttotal: 1m 1s\tremaining: 3m 18s\n",
      "2359:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2360:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2361:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2362:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2363:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2364:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2365:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2366:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2367:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2368:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2369:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2370:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2371:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2372:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2373:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2374:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2375:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2376:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2377:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2378:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2379:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2380:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2381:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2382:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2383:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2384:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2385:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2386:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2387:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2388:\ttotal: 1m 1s\tremaining: 3m 17s\n",
      "2389:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2390:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2391:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2392:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2393:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2394:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2395:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2396:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2397:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2398:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2399:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2400:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2401:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2402:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2403:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2404:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2405:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2406:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2407:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2408:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2409:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2410:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2411:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2412:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2413:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2414:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2415:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2416:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2417:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2418:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2419:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2420:\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2421:\ttotal: 1m 2s\tremaining: 3m 16s\n",
      "2422:\ttotal: 1m 2s\tremaining: 3m 16s\n",
      "2423:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2424:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2425:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2426:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2427:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2428:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2429:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2430:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2431:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2432:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2433:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2434:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2435:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2436:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2437:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2438:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2439:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2440:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2441:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2442:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2443:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2444:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2445:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2446:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2447:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2448:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2449:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2450:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2451:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2452:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2453:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2454:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2455:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2456:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2457:\ttotal: 1m 3s\tremaining: 3m 16s\n",
      "2458:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2459:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2460:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2461:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2462:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2463:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2464:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2465:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2466:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2467:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2468:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2469:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2470:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2471:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2472:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2473:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2474:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2475:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2476:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2477:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2478:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2479:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2480:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2481:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2482:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2483:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2484:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2485:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2486:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2487:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2488:\ttotal: 1m 4s\tremaining: 3m 16s\n",
      "2489:\ttotal: 1m 5s\tremaining: 3m 16s\n",
      "2490:\ttotal: 1m 5s\tremaining: 3m 16s\n",
      "2491:\ttotal: 1m 5s\tremaining: 3m 16s\n",
      "2492:\ttotal: 1m 5s\tremaining: 3m 16s\n",
      "2493:\ttotal: 1m 5s\tremaining: 3m 16s\n",
      "2494:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2495:\ttotal: 1m 5s\tremaining: 3m 16s\n",
      "2496:\ttotal: 1m 5s\tremaining: 3m 16s\n",
      "2497:\ttotal: 1m 5s\tremaining: 3m 16s\n",
      "2498:\ttotal: 1m 5s\tremaining: 3m 16s\n",
      "2499:\ttotal: 1m 5s\tremaining: 3m 16s\n",
      "2500:\ttotal: 1m 5s\tremaining: 3m 16s\n",
      "2501:\ttotal: 1m 5s\tremaining: 3m 16s\n",
      "2502:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2503:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2504:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2505:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2506:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2507:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2508:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2509:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2510:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2511:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2512:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2513:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2514:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2515:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2516:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2517:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2518:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2519:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2520:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2521:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2522:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2523:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2524:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2525:\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2526:\ttotal: 1m 5s\tremaining: 3m 15s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2527:\ttotal: 1m 6s\tremaining: 3m 15s\n",
      "2528:\ttotal: 1m 6s\tremaining: 3m 15s\n",
      "2529:\ttotal: 1m 6s\tremaining: 3m 15s\n",
      "2530:\ttotal: 1m 6s\tremaining: 3m 15s\n",
      "2531:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2532:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2533:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2534:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2535:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2536:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2537:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2538:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2539:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2540:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2541:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2542:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2543:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2544:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2545:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2546:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2547:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2548:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2549:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2550:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2551:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2552:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2553:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2554:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2555:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2556:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2557:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2558:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2559:\ttotal: 1m 6s\tremaining: 3m 14s\n",
      "2560:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2561:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2562:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2563:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2564:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2565:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2566:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2567:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2568:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2569:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2570:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2571:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2572:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2573:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2574:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2575:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2576:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2577:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2578:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2579:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2580:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2581:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2582:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2583:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2584:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2585:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2586:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2587:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2588:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2589:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2590:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2591:\ttotal: 1m 7s\tremaining: 3m 14s\n",
      "2592:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2593:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2594:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2595:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2596:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2597:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2598:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2599:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2600:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2601:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2602:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2603:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2604:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2605:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2606:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2607:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2608:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2609:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2610:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2611:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2612:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2613:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2614:\ttotal: 1m 8s\tremaining: 3m 13s\n",
      "2615:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2616:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2617:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2618:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2619:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2620:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2621:\ttotal: 1m 8s\tremaining: 3m 14s\n",
      "2622:\ttotal: 1m 9s\tremaining: 3m 14s\n",
      "2623:\ttotal: 1m 9s\tremaining: 3m 14s\n",
      "2624:\ttotal: 1m 9s\tremaining: 3m 14s\n",
      "2625:\ttotal: 1m 9s\tremaining: 3m 14s\n",
      "2626:\ttotal: 1m 9s\tremaining: 3m 14s\n",
      "2627:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2628:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2629:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2630:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2631:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2632:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2633:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2634:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2635:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2636:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2637:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2638:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2639:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2640:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2641:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2642:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2643:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2644:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2645:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2646:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2647:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2648:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2649:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2650:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2651:\ttotal: 1m 9s\tremaining: 3m 13s\n",
      "2652:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2653:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2654:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2655:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2656:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2657:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2658:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2659:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2660:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2661:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2662:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2663:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2664:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2665:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2666:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2667:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2668:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2669:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2670:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2671:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2672:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2673:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2674:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2675:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2676:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2677:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2678:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2679:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2680:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2681:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2682:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2683:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2684:\ttotal: 1m 10s\tremaining: 3m 13s\n",
      "2685:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2686:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2687:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2688:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2689:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2690:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2691:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2692:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2693:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2694:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2695:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2696:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2697:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2698:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2699:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2700:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2701:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2702:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2703:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2704:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2705:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2706:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2707:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2708:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2709:\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2710:\ttotal: 1m 11s\tremaining: 3m 12s\n",
      "2711:\ttotal: 1m 11s\tremaining: 3m 12s\n",
      "2712:\ttotal: 1m 11s\tremaining: 3m 12s\n",
      "2713:\ttotal: 1m 11s\tremaining: 3m 12s\n",
      "2714:\ttotal: 1m 11s\tremaining: 3m 12s\n",
      "2715:\ttotal: 1m 11s\tremaining: 3m 12s\n",
      "2716:\ttotal: 1m 11s\tremaining: 3m 12s\n",
      "2717:\ttotal: 1m 11s\tremaining: 3m 12s\n",
      "2718:\ttotal: 1m 11s\tremaining: 3m 12s\n",
      "2719:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2720:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2721:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2722:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2723:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2724:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2725:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2726:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2727:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2728:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2729:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2730:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2731:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2732:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2733:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2734:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2735:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2736:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2737:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2738:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2739:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2740:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2741:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2742:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2743:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2744:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2745:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2746:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2747:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2748:\ttotal: 1m 12s\tremaining: 3m 12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2749:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2750:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2751:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2752:\ttotal: 1m 12s\tremaining: 3m 12s\n",
      "2753:\ttotal: 1m 13s\tremaining: 3m 12s\n",
      "2754:\ttotal: 1m 13s\tremaining: 3m 12s\n",
      "2755:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2756:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2757:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2758:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2759:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2760:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2761:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2762:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2763:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2764:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2765:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2766:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2767:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2768:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2769:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2770:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2771:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2772:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2773:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2774:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2775:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2776:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2777:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2778:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2779:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2780:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2781:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2782:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2783:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2784:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2785:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2786:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2787:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2788:\ttotal: 1m 13s\tremaining: 3m 11s\n",
      "2789:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2790:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2791:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2792:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2793:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2794:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2795:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2796:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2797:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2798:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2799:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2800:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2801:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2802:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2803:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2804:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2805:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2806:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2807:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2808:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2809:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2810:\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2811:\ttotal: 1m 14s\tremaining: 3m 10s\n",
      "2812:\ttotal: 1m 14s\tremaining: 3m 10s\n",
      "2813:\ttotal: 1m 14s\tremaining: 3m 10s\n",
      "2814:\ttotal: 1m 14s\tremaining: 3m 10s\n",
      "2815:\ttotal: 1m 14s\tremaining: 3m 10s\n",
      "2816:\ttotal: 1m 14s\tremaining: 3m 10s\n",
      "2817:\ttotal: 1m 14s\tremaining: 3m 10s\n",
      "2818:\ttotal: 1m 14s\tremaining: 3m 10s\n",
      "2819:\ttotal: 1m 14s\tremaining: 3m 10s\n",
      "2820:\ttotal: 1m 14s\tremaining: 3m 10s\n",
      "2821:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2822:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2823:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2824:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2825:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2826:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2827:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2828:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2829:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2830:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2831:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2832:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2833:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2834:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2835:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2836:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2837:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2838:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2839:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2840:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2841:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2842:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2843:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2844:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2845:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2846:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2847:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2848:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2849:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2850:\ttotal: 1m 15s\tremaining: 3m 10s\n",
      "2851:\ttotal: 1m 15s\tremaining: 3m 9s\n",
      "2852:\ttotal: 1m 15s\tremaining: 3m 9s\n",
      "2853:\ttotal: 1m 15s\tremaining: 3m 9s\n",
      "2854:\ttotal: 1m 15s\tremaining: 3m 9s\n",
      "2855:\ttotal: 1m 15s\tremaining: 3m 9s\n",
      "2856:\ttotal: 1m 15s\tremaining: 3m 9s\n",
      "2857:\ttotal: 1m 15s\tremaining: 3m 9s\n",
      "2858:\ttotal: 1m 15s\tremaining: 3m 9s\n",
      "2859:\ttotal: 1m 15s\tremaining: 3m 9s\n",
      "2860:\ttotal: 1m 15s\tremaining: 3m 9s\n",
      "2861:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2862:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2863:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2864:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2865:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2866:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2867:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2868:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2869:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2870:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2871:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2872:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2873:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2874:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2875:\ttotal: 1m 16s\tremaining: 3m 9s\n",
      "2876:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2877:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2878:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2879:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2880:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2881:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2882:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2883:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2884:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2885:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2886:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2887:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2888:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2889:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2890:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2891:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2892:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2893:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2894:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2895:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2896:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2897:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2898:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2899:\ttotal: 1m 16s\tremaining: 3m 8s\n",
      "2900:\ttotal: 1m 16s\tremaining: 3m 7s\n",
      "2901:\ttotal: 1m 16s\tremaining: 3m 7s\n",
      "2902:\ttotal: 1m 16s\tremaining: 3m 7s\n",
      "2903:\ttotal: 1m 16s\tremaining: 3m 7s\n",
      "2904:\ttotal: 1m 16s\tremaining: 3m 7s\n",
      "2905:\ttotal: 1m 16s\tremaining: 3m 7s\n",
      "2906:\ttotal: 1m 16s\tremaining: 3m 7s\n",
      "2907:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2908:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2909:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2910:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2911:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2912:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2913:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2914:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2915:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2916:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2917:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2918:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2919:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2920:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2921:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2922:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2923:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2924:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2925:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2926:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2927:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2928:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2929:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2930:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2931:\ttotal: 1m 17s\tremaining: 3m 7s\n",
      "2932:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2933:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2934:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2935:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2936:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2937:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2938:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2939:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2940:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2941:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2942:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2943:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2944:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2945:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2946:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2947:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2948:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2949:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2950:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2951:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2952:\ttotal: 1m 17s\tremaining: 3m 6s\n",
      "2953:\ttotal: 1m 17s\tremaining: 3m 5s\n",
      "2954:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2955:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2956:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2957:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2958:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2959:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2960:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2961:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2962:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2963:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2964:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2965:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2966:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2967:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2968:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2969:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2970:\ttotal: 1m 18s\tremaining: 3m 5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2971:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2972:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2973:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2974:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2975:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2976:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2977:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2978:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2979:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2980:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2981:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2982:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2983:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2984:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2985:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2986:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2987:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2988:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2989:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2990:\ttotal: 1m 18s\tremaining: 3m 5s\n",
      "2991:\ttotal: 1m 18s\tremaining: 3m 4s\n",
      "2992:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "2993:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "2994:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "2995:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "2996:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "2997:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "2998:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "2999:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3000:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3001:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3002:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3003:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3004:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3005:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3006:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3007:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3008:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3009:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3010:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3011:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3012:\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3013:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3014:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3015:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3016:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3017:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3018:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3019:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3020:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3021:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3022:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3023:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3024:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3025:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3026:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3027:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3028:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3029:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3030:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3031:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3032:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3033:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3034:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3035:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3036:\ttotal: 1m 19s\tremaining: 3m 3s\n",
      "3037:\ttotal: 1m 19s\tremaining: 3m 2s\n",
      "3038:\ttotal: 1m 19s\tremaining: 3m 2s\n",
      "3039:\ttotal: 1m 19s\tremaining: 3m 2s\n",
      "3040:\ttotal: 1m 19s\tremaining: 3m 2s\n",
      "3041:\ttotal: 1m 19s\tremaining: 3m 2s\n",
      "3042:\ttotal: 1m 19s\tremaining: 3m 2s\n",
      "3043:\ttotal: 1m 19s\tremaining: 3m 2s\n",
      "3044:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3045:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3046:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3047:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3048:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3049:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3050:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3051:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3052:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3053:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3054:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3055:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3056:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3057:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3058:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3059:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3060:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3061:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3062:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3063:\ttotal: 1m 20s\tremaining: 3m 2s\n",
      "3064:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3065:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3066:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3067:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3068:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3069:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3070:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3071:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3072:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3073:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3074:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3075:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3076:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3077:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3078:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3079:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3080:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3081:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3082:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3083:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3084:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3085:\ttotal: 1m 20s\tremaining: 3m 1s\n",
      "3086:\ttotal: 1m 20s\tremaining: 3m\n",
      "3087:\ttotal: 1m 20s\tremaining: 3m\n",
      "3088:\ttotal: 1m 20s\tremaining: 3m\n",
      "3089:\ttotal: 1m 20s\tremaining: 3m\n",
      "3090:\ttotal: 1m 20s\tremaining: 3m\n",
      "3091:\ttotal: 1m 20s\tremaining: 3m\n",
      "3092:\ttotal: 1m 20s\tremaining: 3m\n",
      "3093:\ttotal: 1m 21s\tremaining: 3m\n",
      "3094:\ttotal: 1m 21s\tremaining: 3m\n",
      "3095:\ttotal: 1m 21s\tremaining: 3m\n",
      "3096:\ttotal: 1m 21s\tremaining: 3m\n",
      "3097:\ttotal: 1m 21s\tremaining: 3m\n",
      "3098:\ttotal: 1m 21s\tremaining: 3m\n",
      "3099:\ttotal: 1m 21s\tremaining: 3m\n",
      "3100:\ttotal: 1m 21s\tremaining: 3m\n",
      "3101:\ttotal: 1m 21s\tremaining: 3m\n",
      "3102:\ttotal: 1m 21s\tremaining: 3m\n",
      "3103:\ttotal: 1m 21s\tremaining: 3m\n",
      "3104:\ttotal: 1m 21s\tremaining: 3m\n",
      "3105:\ttotal: 1m 21s\tremaining: 3m\n",
      "3106:\ttotal: 1m 21s\tremaining: 3m\n",
      "3107:\ttotal: 1m 21s\tremaining: 3m\n",
      "3108:\ttotal: 1m 21s\tremaining: 3m\n",
      "3109:\ttotal: 1m 21s\tremaining: 3m\n",
      "3110:\ttotal: 1m 21s\tremaining: 3m\n",
      "3111:\ttotal: 1m 21s\tremaining: 3m\n",
      "3112:\ttotal: 1m 21s\tremaining: 3m\n",
      "3113:\ttotal: 1m 21s\tremaining: 3m\n",
      "3114:\ttotal: 1m 21s\tremaining: 3m\n",
      "3115:\ttotal: 1m 21s\tremaining: 3m\n",
      "3116:\ttotal: 1m 21s\tremaining: 3m\n",
      "3117:\ttotal: 1m 21s\tremaining: 3m\n",
      "3118:\ttotal: 1m 21s\tremaining: 3m\n",
      "3119:\ttotal: 1m 21s\tremaining: 3m\n",
      "3120:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3121:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3122:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3123:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3124:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3125:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3126:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3127:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3128:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3129:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3130:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3131:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3132:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3133:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3134:\ttotal: 1m 21s\tremaining: 2m 59s\n",
      "3135:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3136:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3137:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3138:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3139:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3140:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3141:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3142:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3143:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3144:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3145:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3146:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3147:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3148:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3149:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3150:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3151:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3152:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3153:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3154:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3155:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3156:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3157:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3158:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3159:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3160:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3161:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3162:\ttotal: 1m 22s\tremaining: 2m 59s\n",
      "3163:\ttotal: 1m 22s\tremaining: 2m 58s\n",
      "3164:\ttotal: 1m 22s\tremaining: 2m 58s\n",
      "3165:\ttotal: 1m 22s\tremaining: 2m 58s\n",
      "3166:\ttotal: 1m 22s\tremaining: 2m 58s\n",
      "3167:\ttotal: 1m 22s\tremaining: 2m 58s\n",
      "3168:\ttotal: 1m 22s\tremaining: 2m 58s\n",
      "3169:\ttotal: 1m 22s\tremaining: 2m 58s\n",
      "3170:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3171:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3172:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3173:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3174:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3175:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3176:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3177:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3178:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3179:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3180:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3181:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3182:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3183:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3184:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3185:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3186:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3187:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3188:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3189:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3190:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3191:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3192:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3193:\ttotal: 1m 23s\tremaining: 2m 58s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3194:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3195:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3196:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3197:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3198:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3199:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3200:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3201:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3202:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3203:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3204:\ttotal: 1m 23s\tremaining: 2m 58s\n",
      "3205:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3206:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3207:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3208:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3209:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3210:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3211:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3212:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3213:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3214:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3215:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3216:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3217:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3218:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3219:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3220:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3221:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3222:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3223:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3224:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3225:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3226:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3227:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3228:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3229:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3230:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3231:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3232:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3233:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3234:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3235:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3236:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3237:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3238:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3239:\ttotal: 1m 24s\tremaining: 2m 57s\n",
      "3240:\ttotal: 1m 25s\tremaining: 2m 57s\n",
      "3241:\ttotal: 1m 25s\tremaining: 2m 57s\n",
      "3242:\ttotal: 1m 25s\tremaining: 2m 57s\n",
      "3243:\ttotal: 1m 25s\tremaining: 2m 57s\n",
      "3244:\ttotal: 1m 25s\tremaining: 2m 57s\n",
      "3245:\ttotal: 1m 25s\tremaining: 2m 57s\n",
      "3246:\ttotal: 1m 25s\tremaining: 2m 57s\n",
      "3247:\ttotal: 1m 25s\tremaining: 2m 57s\n",
      "3248:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3249:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3250:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3251:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3252:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3253:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3254:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3255:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3256:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3257:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3258:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3259:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3260:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3261:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3262:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3263:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3264:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3265:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3266:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3267:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3268:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3269:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3270:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3271:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3272:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3273:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3274:\ttotal: 1m 25s\tremaining: 2m 56s\n",
      "3275:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3276:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3277:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3278:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3279:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3280:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3281:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3282:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3283:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3284:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3285:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3286:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3287:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3288:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3289:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3290:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3291:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3292:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3293:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3294:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3295:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3296:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3297:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3298:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3299:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3300:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3301:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3302:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3303:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3304:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3305:\ttotal: 1m 26s\tremaining: 2m 56s\n",
      "3306:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3307:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3308:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3309:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3310:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3311:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3312:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3313:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3314:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3315:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3316:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3317:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3318:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3319:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3320:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3321:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3322:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3323:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3324:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3325:\ttotal: 1m 27s\tremaining: 2m 55s\n",
      "3326:\ttotal: 1m 27s\tremaining: 2m 55s\n",
      "3327:\ttotal: 1m 27s\tremaining: 2m 55s\n",
      "3328:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3329:\ttotal: 1m 27s\tremaining: 2m 55s\n",
      "3330:\ttotal: 1m 27s\tremaining: 2m 56s\n",
      "3331:\ttotal: 1m 27s\tremaining: 2m 55s\n",
      "3332:\ttotal: 1m 27s\tremaining: 2m 55s\n",
      "3333:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3334:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3335:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3336:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3337:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3338:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3339:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3340:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3341:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3342:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3343:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3344:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3345:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3346:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3347:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3348:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3349:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3350:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3351:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3352:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3353:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3354:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3355:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3356:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3357:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3358:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3359:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3360:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3361:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3362:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3363:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3364:\ttotal: 1m 28s\tremaining: 2m 55s\n",
      "3365:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3366:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3367:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3368:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3369:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3370:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3371:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3372:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3373:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3374:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3375:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3376:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3377:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3378:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3379:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3380:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3381:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3382:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3383:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3384:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3385:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3386:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3387:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3388:\ttotal: 1m 29s\tremaining: 2m 55s\n",
      "3389:\ttotal: 1m 29s\tremaining: 2m 54s\n",
      "3390:\ttotal: 1m 29s\tremaining: 2m 54s\n",
      "3391:\ttotal: 1m 29s\tremaining: 2m 54s\n",
      "3392:\ttotal: 1m 29s\tremaining: 2m 54s\n",
      "3393:\ttotal: 1m 29s\tremaining: 2m 54s\n",
      "3394:\ttotal: 1m 29s\tremaining: 2m 54s\n",
      "3395:\ttotal: 1m 29s\tremaining: 2m 54s\n",
      "3396:\ttotal: 1m 29s\tremaining: 2m 54s\n",
      "3397:\ttotal: 1m 29s\tremaining: 2m 54s\n",
      "3398:\ttotal: 1m 29s\tremaining: 2m 54s\n",
      "3399:\ttotal: 1m 29s\tremaining: 2m 54s\n",
      "3400:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3401:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3402:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3403:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3404:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3405:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3406:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3407:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3408:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3409:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3410:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3411:\ttotal: 1m 30s\tremaining: 2m 54s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3412:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3413:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3414:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3415:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3416:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3417:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3418:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3419:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3420:\ttotal: 1m 30s\tremaining: 2m 54s\n",
      "3421:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3422:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3423:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3424:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3425:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3426:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3427:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3428:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3429:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3430:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3431:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3432:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3433:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3434:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3435:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3436:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3437:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3438:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3439:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3440:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3441:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3442:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3443:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3444:\ttotal: 1m 30s\tremaining: 2m 53s\n",
      "3445:\ttotal: 1m 31s\tremaining: 2m 53s\n",
      "3446:\ttotal: 1m 31s\tremaining: 2m 53s\n",
      "3447:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3448:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3449:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3450:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3451:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3452:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3453:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3454:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3455:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3456:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3457:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3458:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3459:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3460:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3461:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3462:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3463:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3464:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3465:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3466:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3467:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3468:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3469:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3470:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3471:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3472:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3473:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3474:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3475:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3476:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3477:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3478:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3479:\ttotal: 1m 31s\tremaining: 2m 52s\n",
      "3480:\ttotal: 1m 32s\tremaining: 2m 52s\n",
      "3481:\ttotal: 1m 32s\tremaining: 2m 52s\n",
      "3482:\ttotal: 1m 32s\tremaining: 2m 52s\n",
      "3483:\ttotal: 1m 32s\tremaining: 2m 52s\n",
      "3484:\ttotal: 1m 32s\tremaining: 2m 52s\n",
      "3485:\ttotal: 1m 32s\tremaining: 2m 52s\n",
      "3486:\ttotal: 1m 32s\tremaining: 2m 52s\n",
      "3487:\ttotal: 1m 32s\tremaining: 2m 52s\n",
      "3488:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3489:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3490:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3491:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3492:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3493:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3494:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3495:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3496:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3497:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3498:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3499:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3500:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3501:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3502:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3503:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3504:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3505:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3506:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3507:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3508:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3509:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3510:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3511:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3512:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3513:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3514:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3515:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3516:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3517:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3518:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3519:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3520:\ttotal: 1m 32s\tremaining: 2m 51s\n",
      "3521:\ttotal: 1m 33s\tremaining: 2m 51s\n",
      "3522:\ttotal: 1m 33s\tremaining: 2m 51s\n",
      "3523:\ttotal: 1m 33s\tremaining: 2m 51s\n",
      "3524:\ttotal: 1m 33s\tremaining: 2m 51s\n",
      "3525:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3526:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3527:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3528:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3529:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3530:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3531:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3532:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3533:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3534:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3535:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3536:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3537:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3538:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3539:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3540:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3541:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3542:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3543:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3544:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3545:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3546:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3547:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3548:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3549:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3550:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3551:\ttotal: 1m 33s\tremaining: 2m 50s\n",
      "3552:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3553:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3554:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3555:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3556:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3557:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3558:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3559:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3560:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3561:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3562:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3563:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3564:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3565:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3566:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3567:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3568:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3569:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3570:\ttotal: 1m 34s\tremaining: 2m 50s\n",
      "3571:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3572:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3573:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3574:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3575:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3576:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3577:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3578:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3579:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3580:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3581:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3582:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3583:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3584:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3585:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3586:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3587:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3588:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3589:\ttotal: 1m 34s\tremaining: 2m 49s\n",
      "3590:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3591:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3592:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3593:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3594:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3595:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3596:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3597:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3598:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3599:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3600:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3601:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3602:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3603:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3604:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3605:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3606:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3607:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3608:\ttotal: 1m 35s\tremaining: 2m 49s\n",
      "3609:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3610:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3611:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3612:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3613:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3614:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3615:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3616:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3617:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3618:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3619:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3620:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3621:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3622:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3623:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3624:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3625:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3626:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3627:\ttotal: 1m 35s\tremaining: 2m 48s\n",
      "3628:\ttotal: 1m 35s\tremaining: 2m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3629:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3630:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3631:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3632:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3633:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3634:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3635:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3636:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3637:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3638:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3639:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3640:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3641:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3642:\ttotal: 1m 36s\tremaining: 2m 48s\n",
      "3643:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3644:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3645:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3646:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3647:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3648:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3649:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3650:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3651:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3652:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3653:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3654:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3655:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3656:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3657:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3658:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3659:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3660:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3661:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3662:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3663:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3664:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3665:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3666:\ttotal: 1m 36s\tremaining: 2m 47s\n",
      "3667:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3668:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3669:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3670:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3671:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3672:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3673:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3674:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3675:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3676:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3677:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3678:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3679:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3680:\ttotal: 1m 36s\tremaining: 2m 46s\n",
      "3681:\ttotal: 1m 37s\tremaining: 2m 46s\n",
      "3682:\ttotal: 1m 37s\tremaining: 2m 46s\n",
      "3683:\ttotal: 1m 37s\tremaining: 2m 46s\n",
      "3684:\ttotal: 1m 37s\tremaining: 2m 46s\n",
      "3685:\ttotal: 1m 37s\tremaining: 2m 46s\n",
      "3686:\ttotal: 1m 37s\tremaining: 2m 46s\n",
      "3687:\ttotal: 1m 37s\tremaining: 2m 46s\n",
      "3688:\ttotal: 1m 37s\tremaining: 2m 46s\n",
      "3689:\ttotal: 1m 37s\tremaining: 2m 46s\n",
      "3690:\ttotal: 1m 37s\tremaining: 2m 46s\n",
      "3691:\ttotal: 1m 37s\tremaining: 2m 46s\n",
      "3692:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3693:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3694:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3695:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3696:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3697:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3698:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3699:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3700:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3701:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3702:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3703:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3704:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3705:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3706:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3707:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3708:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3709:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3710:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3711:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3712:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3713:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3714:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3715:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3716:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3717:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3718:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3719:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3720:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3721:\ttotal: 1m 37s\tremaining: 2m 45s\n",
      "3722:\ttotal: 1m 37s\tremaining: 2m 44s\n",
      "3723:\ttotal: 1m 37s\tremaining: 2m 44s\n",
      "3724:\ttotal: 1m 37s\tremaining: 2m 44s\n",
      "3725:\ttotal: 1m 37s\tremaining: 2m 44s\n",
      "3726:\ttotal: 1m 37s\tremaining: 2m 44s\n",
      "3727:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3728:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3729:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3730:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3731:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3732:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3733:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3734:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3735:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3736:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3737:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3738:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3739:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3740:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3741:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3742:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3743:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3744:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3745:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3746:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3747:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3748:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3749:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3750:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3751:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3752:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3753:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3754:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3755:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3756:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3757:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3758:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3759:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3760:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3761:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3762:\ttotal: 1m 38s\tremaining: 2m 44s\n",
      "3763:\ttotal: 1m 39s\tremaining: 2m 44s\n",
      "3764:\ttotal: 1m 39s\tremaining: 2m 44s\n",
      "3765:\ttotal: 1m 39s\tremaining: 2m 44s\n",
      "3766:\ttotal: 1m 39s\tremaining: 2m 44s\n",
      "3767:\ttotal: 1m 39s\tremaining: 2m 44s\n",
      "3768:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3769:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3770:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3771:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3772:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3773:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3774:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3775:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3776:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3777:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3778:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3779:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3780:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3781:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3782:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3783:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3784:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3785:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3786:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3787:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3788:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3789:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3790:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3791:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3792:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3793:\ttotal: 1m 39s\tremaining: 2m 43s\n",
      "3794:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3795:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3796:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3797:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3798:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3799:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3800:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3801:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3802:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3803:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3804:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3805:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3806:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3807:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3808:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3809:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3810:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3811:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3812:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3813:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3814:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3815:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3816:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3817:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3818:\ttotal: 1m 40s\tremaining: 2m 43s\n",
      "3819:\ttotal: 1m 40s\tremaining: 2m 42s\n",
      "3820:\ttotal: 1m 40s\tremaining: 2m 42s\n",
      "3821:\ttotal: 1m 40s\tremaining: 2m 42s\n",
      "3822:\ttotal: 1m 40s\tremaining: 2m 42s\n",
      "3823:\ttotal: 1m 40s\tremaining: 2m 42s\n",
      "3824:\ttotal: 1m 40s\tremaining: 2m 42s\n",
      "3825:\ttotal: 1m 40s\tremaining: 2m 42s\n",
      "3826:\ttotal: 1m 40s\tremaining: 2m 42s\n",
      "3827:\ttotal: 1m 40s\tremaining: 2m 42s\n",
      "3828:\ttotal: 1m 40s\tremaining: 2m 42s\n",
      "3829:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3830:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3831:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3832:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3833:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3834:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3835:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3836:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3837:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3838:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3839:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3840:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3841:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3842:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3843:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3844:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3845:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3846:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3847:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3848:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3849:\ttotal: 1m 41s\tremaining: 2m 42s\n",
      "3850:\ttotal: 1m 41s\tremaining: 2m 41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3851:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3852:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3853:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3854:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3855:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3856:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3857:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3858:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3859:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3860:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3861:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3862:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3863:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3864:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3865:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3866:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3867:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3868:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3869:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3870:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3871:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3872:\ttotal: 1m 41s\tremaining: 2m 41s\n",
      "3873:\ttotal: 1m 42s\tremaining: 2m 41s\n",
      "3874:\ttotal: 1m 42s\tremaining: 2m 41s\n",
      "3875:\ttotal: 1m 42s\tremaining: 2m 41s\n",
      "3876:\ttotal: 1m 42s\tremaining: 2m 41s\n",
      "3877:\ttotal: 1m 42s\tremaining: 2m 41s\n",
      "3878:\ttotal: 1m 42s\tremaining: 2m 41s\n",
      "3879:\ttotal: 1m 42s\tremaining: 2m 41s\n",
      "3880:\ttotal: 1m 42s\tremaining: 2m 41s\n",
      "3881:\ttotal: 1m 42s\tremaining: 2m 41s\n",
      "3882:\ttotal: 1m 42s\tremaining: 2m 41s\n",
      "3883:\ttotal: 1m 42s\tremaining: 2m 41s\n",
      "3884:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3885:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3886:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3887:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3888:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3889:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3890:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3891:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3892:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3893:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3894:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3895:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3896:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3897:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3898:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3899:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3900:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3901:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3902:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3903:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3904:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3905:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3906:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3907:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3908:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3909:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3910:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3911:\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "3912:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3913:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3914:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3915:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3916:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3917:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3918:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3919:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3920:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3921:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3922:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3923:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3924:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3925:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3926:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3927:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3928:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3929:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3930:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3931:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3932:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3933:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3934:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3935:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3936:\ttotal: 1m 43s\tremaining: 2m 40s\n",
      "3937:\ttotal: 1m 44s\tremaining: 2m 40s\n",
      "3938:\ttotal: 1m 44s\tremaining: 2m 40s\n",
      "3939:\ttotal: 1m 44s\tremaining: 2m 40s\n",
      "3940:\ttotal: 1m 44s\tremaining: 2m 40s\n",
      "3941:\ttotal: 1m 44s\tremaining: 2m 40s\n",
      "3942:\ttotal: 1m 44s\tremaining: 2m 40s\n",
      "3943:\ttotal: 1m 44s\tremaining: 2m 40s\n",
      "3944:\ttotal: 1m 44s\tremaining: 2m 40s\n",
      "3945:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3946:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3947:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3948:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3949:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3950:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3951:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3952:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3953:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3954:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3955:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3956:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3957:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3958:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3959:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3960:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3961:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3962:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3963:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3964:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3965:\ttotal: 1m 44s\tremaining: 2m 39s\n",
      "3966:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3967:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3968:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3969:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3970:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3971:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3972:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3973:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3974:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3975:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3976:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3977:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3978:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3979:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3980:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3981:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3982:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3983:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3984:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3985:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3986:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3987:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3988:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3989:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3990:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3991:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3992:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3993:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3994:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3995:\ttotal: 1m 45s\tremaining: 2m 39s\n",
      "3996:\ttotal: 1m 45s\tremaining: 2m 38s\n",
      "3997:\ttotal: 1m 45s\tremaining: 2m 38s\n",
      "3998:\ttotal: 1m 45s\tremaining: 2m 38s\n",
      "3999:\ttotal: 1m 45s\tremaining: 2m 38s\n",
      "4000:\ttotal: 1m 45s\tremaining: 2m 38s\n",
      "4001:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4002:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4003:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4004:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4005:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4006:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4007:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4008:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4009:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4010:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4011:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4012:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4013:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4014:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4015:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4016:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4017:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4018:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4019:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4020:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4021:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4022:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4023:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4024:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4025:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4026:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4027:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4028:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4029:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4030:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4031:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4032:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4033:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4034:\ttotal: 1m 46s\tremaining: 2m 38s\n",
      "4035:\ttotal: 1m 47s\tremaining: 2m 38s\n",
      "4036:\ttotal: 1m 47s\tremaining: 2m 38s\n",
      "4037:\ttotal: 1m 47s\tremaining: 2m 38s\n",
      "4038:\ttotal: 1m 47s\tremaining: 2m 38s\n",
      "4039:\ttotal: 1m 47s\tremaining: 2m 38s\n",
      "4040:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4041:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4042:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4043:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4044:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4045:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4046:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4047:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4048:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4049:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4050:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4051:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4052:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4053:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4054:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4055:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4056:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4057:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4058:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4059:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4060:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4061:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4062:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4063:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4064:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4065:\ttotal: 1m 47s\tremaining: 2m 37s\n",
      "4066:\ttotal: 1m 47s\tremaining: 2m 37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4068:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4069:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4070:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4071:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4072:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4073:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4074:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4075:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4076:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4077:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4078:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4079:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4080:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4081:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4082:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4083:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4084:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4085:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4086:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4087:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4088:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4089:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4090:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4091:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4092:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4093:\ttotal: 1m 48s\tremaining: 2m 37s\n",
      "4094:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4095:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4096:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4097:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4098:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4099:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4100:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4101:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4102:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4103:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4104:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4105:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4106:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4107:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4108:\ttotal: 1m 49s\tremaining: 2m 37s\n",
      "4109:\ttotal: 1m 49s\tremaining: 2m 36s\n",
      "4110:\ttotal: 1m 49s\tremaining: 2m 36s\n",
      "4111:\ttotal: 1m 49s\tremaining: 2m 36s\n",
      "4112:\ttotal: 1m 49s\tremaining: 2m 36s\n",
      "4113:\ttotal: 1m 49s\tremaining: 2m 36s\n",
      "4114:\ttotal: 1m 49s\tremaining: 2m 36s\n",
      "4115:\ttotal: 1m 49s\tremaining: 2m 36s\n",
      "4116:\ttotal: 1m 49s\tremaining: 2m 36s\n",
      "4117:\ttotal: 1m 49s\tremaining: 2m 36s\n",
      "4118:\ttotal: 1m 49s\tremaining: 2m 36s\n",
      "4119:\ttotal: 1m 49s\tremaining: 2m 36s\n",
      "4120:\ttotal: 1m 49s\tremaining: 2m 36s\n",
      "4121:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4122:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4123:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4124:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4125:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4126:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4127:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4128:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4129:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4130:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4131:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4132:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4133:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4134:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4135:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4136:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4137:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4138:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4139:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4140:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4141:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4142:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4143:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4144:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4145:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4146:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4147:\ttotal: 1m 50s\tremaining: 2m 36s\n",
      "4148:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4149:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4150:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4151:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4152:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4153:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4154:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4155:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4156:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4157:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4158:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4159:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4160:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4161:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4162:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4163:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4164:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4165:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4166:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4167:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4168:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4169:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4170:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4171:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4172:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4173:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4174:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4175:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4176:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4177:\ttotal: 1m 51s\tremaining: 2m 36s\n",
      "4178:\ttotal: 1m 52s\tremaining: 2m 36s\n",
      "4179:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4180:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4181:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4182:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4183:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4184:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4185:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4186:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4187:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4188:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4189:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4190:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4191:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4192:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4193:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4194:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4195:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4196:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4197:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4198:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4199:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4200:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4201:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4202:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4203:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4204:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4205:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4206:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4207:\ttotal: 1m 52s\tremaining: 2m 35s\n",
      "4208:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4209:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4210:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4211:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4212:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4213:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4214:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4215:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4216:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4217:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4218:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4219:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4220:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4221:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4222:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4223:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4224:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4225:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4226:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4227:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4228:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4229:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4230:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4231:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4232:\ttotal: 1m 53s\tremaining: 2m 35s\n",
      "4233:\ttotal: 1m 53s\tremaining: 2m 34s\n",
      "4234:\ttotal: 1m 53s\tremaining: 2m 34s\n",
      "4235:\ttotal: 1m 53s\tremaining: 2m 34s\n",
      "4236:\ttotal: 1m 53s\tremaining: 2m 34s\n",
      "4237:\ttotal: 1m 53s\tremaining: 2m 34s\n",
      "4238:\ttotal: 1m 53s\tremaining: 2m 34s\n",
      "4239:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4240:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4241:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4242:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4243:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4244:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4245:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4246:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4247:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4248:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4249:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4250:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4251:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4252:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4253:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4254:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4255:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4256:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4257:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4258:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4259:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4260:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4261:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4262:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4263:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4264:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4265:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4266:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4267:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4268:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4269:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4270:\ttotal: 1m 54s\tremaining: 2m 34s\n",
      "4271:\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "4272:\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "4273:\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "4274:\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "4275:\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "4276:\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "4277:\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "4278:\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "4279:\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "4280:\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "4281:\ttotal: 1m 55s\tremaining: 2m 34s\n",
      "4282:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4283:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4284:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4285:\ttotal: 1m 55s\tremaining: 2m 33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4286:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4287:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4288:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4289:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4290:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4291:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4292:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4293:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4294:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4295:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4296:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4297:\ttotal: 1m 55s\tremaining: 2m 33s\n",
      "4298:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4299:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4300:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4301:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4302:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4303:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4304:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4305:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4306:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4307:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4308:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4309:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4310:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4311:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4312:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4313:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4314:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4315:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4316:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4317:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4318:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4319:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4320:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4321:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4322:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4323:\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4324:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4325:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4326:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4327:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4328:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4329:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4330:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4331:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4332:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4333:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4334:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4335:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4336:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4337:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4338:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4339:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4340:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4341:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4342:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4343:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4344:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4345:\ttotal: 1m 57s\tremaining: 2m 33s\n",
      "4346:\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "4347:\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "4348:\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "4349:\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "4350:\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "4351:\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "4352:\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "4353:\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "4354:\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "4355:\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "4356:\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "4357:\ttotal: 1m 57s\tremaining: 2m 32s\n",
      "4358:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4359:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4360:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4361:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4362:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4363:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4364:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4365:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4366:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4367:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4368:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4369:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4370:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4371:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4372:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4373:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4374:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4375:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4376:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4377:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4378:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4379:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4380:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4381:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4382:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4383:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4384:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4385:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4386:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4387:\ttotal: 1m 58s\tremaining: 2m 32s\n",
      "4388:\ttotal: 1m 59s\tremaining: 2m 32s\n",
      "4389:\ttotal: 1m 59s\tremaining: 2m 32s\n",
      "4390:\ttotal: 1m 59s\tremaining: 2m 32s\n",
      "4391:\ttotal: 1m 59s\tremaining: 2m 32s\n",
      "4392:\ttotal: 1m 59s\tremaining: 2m 32s\n",
      "4393:\ttotal: 1m 59s\tremaining: 2m 32s\n",
      "4394:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4395:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4396:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4397:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4398:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4399:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4400:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4401:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4402:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4403:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4404:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4405:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4406:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4407:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4408:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4409:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4410:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4411:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4412:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4413:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4414:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4415:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4416:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4417:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4418:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4419:\ttotal: 1m 59s\tremaining: 2m 31s\n",
      "4420:\ttotal: 2m\tremaining: 2m 31s\n",
      "4421:\ttotal: 2m\tremaining: 2m 31s\n",
      "4422:\ttotal: 2m\tremaining: 2m 31s\n",
      "4423:\ttotal: 2m\tremaining: 2m 31s\n",
      "4424:\ttotal: 2m\tremaining: 2m 31s\n",
      "4425:\ttotal: 2m\tremaining: 2m 31s\n",
      "4426:\ttotal: 2m\tremaining: 2m 31s\n",
      "4427:\ttotal: 2m\tremaining: 2m 31s\n",
      "4428:\ttotal: 2m\tremaining: 2m 31s\n",
      "4429:\ttotal: 2m\tremaining: 2m 31s\n",
      "4430:\ttotal: 2m\tremaining: 2m 31s\n",
      "4431:\ttotal: 2m\tremaining: 2m 31s\n",
      "4432:\ttotal: 2m\tremaining: 2m 31s\n",
      "4433:\ttotal: 2m\tremaining: 2m 31s\n",
      "4434:\ttotal: 2m\tremaining: 2m 31s\n",
      "4435:\ttotal: 2m\tremaining: 2m 31s\n",
      "4436:\ttotal: 2m\tremaining: 2m 31s\n",
      "4437:\ttotal: 2m\tremaining: 2m 31s\n",
      "4438:\ttotal: 2m\tremaining: 2m 31s\n",
      "4439:\ttotal: 2m\tremaining: 2m 31s\n",
      "4440:\ttotal: 2m\tremaining: 2m 31s\n",
      "4441:\ttotal: 2m\tremaining: 2m 31s\n",
      "4442:\ttotal: 2m\tremaining: 2m 31s\n",
      "4443:\ttotal: 2m\tremaining: 2m 31s\n",
      "4444:\ttotal: 2m\tremaining: 2m 31s\n",
      "4445:\ttotal: 2m\tremaining: 2m 31s\n",
      "4446:\ttotal: 2m\tremaining: 2m 30s\n",
      "4447:\ttotal: 2m\tremaining: 2m 30s\n",
      "4448:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4449:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4450:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4451:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4452:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4453:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4454:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4455:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4456:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4457:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4458:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4459:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4460:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4461:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4462:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4463:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4464:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4465:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4466:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4467:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4468:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4469:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4470:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4471:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4472:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4473:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4474:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4475:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4476:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4477:\ttotal: 2m 1s\tremaining: 2m 30s\n",
      "4478:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4479:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4480:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4481:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4482:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4483:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4484:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4485:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4486:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4487:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4488:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4489:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4490:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4491:\ttotal: 2m 2s\tremaining: 2m 30s\n",
      "4492:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4493:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4494:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4495:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4496:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4497:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4498:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4499:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4500:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4501:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4502:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4503:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4504:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4505:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4506:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4507:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4508:\ttotal: 2m 2s\tremaining: 2m 29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4509:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4510:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4511:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4512:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4513:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4514:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4515:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4516:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4517:\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4518:\ttotal: 2m 3s\tremaining: 2m 29s\n",
      "4519:\ttotal: 2m 3s\tremaining: 2m 29s\n",
      "4520:\ttotal: 2m 3s\tremaining: 2m 29s\n",
      "4521:\ttotal: 2m 3s\tremaining: 2m 29s\n",
      "4522:\ttotal: 2m 3s\tremaining: 2m 29s\n",
      "4523:\ttotal: 2m 3s\tremaining: 2m 29s\n",
      "4524:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4525:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4526:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4527:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4528:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4529:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4530:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4531:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4532:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4533:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4534:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4535:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4536:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4537:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4538:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4539:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4540:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4541:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4542:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4543:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4544:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4545:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4546:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4547:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4548:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4549:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4550:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4551:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4552:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4553:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4554:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4555:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4556:\ttotal: 2m 3s\tremaining: 2m 28s\n",
      "4557:\ttotal: 2m 4s\tremaining: 2m 28s\n",
      "4558:\ttotal: 2m 4s\tremaining: 2m 28s\n",
      "4559:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4560:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4561:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4562:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4563:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4564:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4565:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4566:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4567:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4568:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4569:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4570:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4571:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4572:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4573:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4574:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4575:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4576:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4577:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4578:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4579:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4580:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4581:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4582:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4583:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4584:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4585:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4586:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4587:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4588:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4589:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4590:\ttotal: 2m 4s\tremaining: 2m 27s\n",
      "4591:\ttotal: 2m 5s\tremaining: 2m 27s\n",
      "4592:\ttotal: 2m 5s\tremaining: 2m 27s\n",
      "4593:\ttotal: 2m 5s\tremaining: 2m 27s\n",
      "4594:\ttotal: 2m 5s\tremaining: 2m 27s\n",
      "4595:\ttotal: 2m 5s\tremaining: 2m 27s\n",
      "4596:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4597:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4598:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4599:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4600:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4601:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4602:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4603:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4604:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4605:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4606:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4607:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4608:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4609:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4610:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4611:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4612:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4613:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4614:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4615:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4616:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4617:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4618:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4619:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4620:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4621:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4622:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4623:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4624:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4625:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4626:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4627:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4628:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4629:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4630:\ttotal: 2m 5s\tremaining: 2m 26s\n",
      "4631:\ttotal: 2m 6s\tremaining: 2m 26s\n",
      "4632:\ttotal: 2m 6s\tremaining: 2m 26s\n",
      "4633:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4634:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4635:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4636:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4637:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4638:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4639:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4640:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4641:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4642:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4643:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4644:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4645:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4646:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4647:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4648:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4649:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4650:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4651:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4652:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4653:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4654:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4655:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4656:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4657:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4658:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4659:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4660:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4661:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4662:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4663:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4664:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4665:\ttotal: 2m 6s\tremaining: 2m 25s\n",
      "4666:\ttotal: 2m 7s\tremaining: 2m 25s\n",
      "4667:\ttotal: 2m 7s\tremaining: 2m 25s\n",
      "4668:\ttotal: 2m 7s\tremaining: 2m 25s\n",
      "4669:\ttotal: 2m 7s\tremaining: 2m 25s\n",
      "4670:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4671:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4672:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4673:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4674:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4675:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4676:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4677:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4678:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4679:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4680:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4681:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4682:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4683:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4684:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4685:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4686:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4687:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4688:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4689:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4690:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4691:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4692:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4693:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4694:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4695:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4696:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4697:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4698:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4699:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4700:\ttotal: 2m 7s\tremaining: 2m 24s\n",
      "4701:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4702:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4703:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4704:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4705:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4706:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4707:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4708:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4709:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4710:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4711:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4712:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4713:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4714:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4715:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4716:\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4717:\ttotal: 2m 8s\tremaining: 2m 23s\n",
      "4718:\ttotal: 2m 8s\tremaining: 2m 23s\n",
      "4719:\ttotal: 2m 8s\tremaining: 2m 23s\n",
      "4720:\ttotal: 2m 8s\tremaining: 2m 23s\n",
      "4721:\ttotal: 2m 8s\tremaining: 2m 23s\n",
      "4722:\ttotal: 2m 8s\tremaining: 2m 23s\n",
      "4723:\ttotal: 2m 8s\tremaining: 2m 23s\n",
      "4724:\ttotal: 2m 8s\tremaining: 2m 23s\n",
      "4725:\ttotal: 2m 8s\tremaining: 2m 23s\n",
      "4726:\ttotal: 2m 8s\tremaining: 2m 23s\n",
      "4727:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4728:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4729:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4730:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4731:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4732:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4733:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4734:\ttotal: 2m 9s\tremaining: 2m 23s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4735:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4736:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4737:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4738:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4739:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4740:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4741:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4742:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4743:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4744:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4745:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4746:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4747:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4748:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4749:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4750:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4751:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4752:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4753:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4754:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4755:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4756:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4757:\ttotal: 2m 9s\tremaining: 2m 23s\n",
      "4758:\ttotal: 2m 9s\tremaining: 2m 22s\n",
      "4759:\ttotal: 2m 9s\tremaining: 2m 22s\n",
      "4760:\ttotal: 2m 9s\tremaining: 2m 22s\n",
      "4761:\ttotal: 2m 9s\tremaining: 2m 22s\n",
      "4762:\ttotal: 2m 9s\tremaining: 2m 22s\n",
      "4763:\ttotal: 2m 9s\tremaining: 2m 22s\n",
      "4764:\ttotal: 2m 9s\tremaining: 2m 22s\n",
      "4765:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4766:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4767:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4768:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4769:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4770:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4771:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4772:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4773:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4774:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4775:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4776:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4777:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4778:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4779:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4780:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4781:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4782:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4783:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4784:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4785:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4786:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4787:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4788:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4789:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4790:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4791:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4792:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4793:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4794:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4795:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4796:\ttotal: 2m 10s\tremaining: 2m 22s\n",
      "4797:\ttotal: 2m 10s\tremaining: 2m 21s\n",
      "4798:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4799:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4800:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4801:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4802:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4803:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4804:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4805:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4806:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4807:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4808:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4809:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4810:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4811:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4812:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4813:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4814:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4815:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4816:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4817:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4818:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4819:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4820:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4821:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4822:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4823:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4824:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4825:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4826:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4827:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4828:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4829:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4830:\ttotal: 2m 11s\tremaining: 2m 21s\n",
      "4831:\ttotal: 2m 12s\tremaining: 2m 21s\n",
      "4832:\ttotal: 2m 12s\tremaining: 2m 21s\n",
      "4833:\ttotal: 2m 12s\tremaining: 2m 21s\n",
      "4834:\ttotal: 2m 12s\tremaining: 2m 21s\n",
      "4835:\ttotal: 2m 12s\tremaining: 2m 21s\n",
      "4836:\ttotal: 2m 12s\tremaining: 2m 21s\n",
      "4837:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4838:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4839:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4840:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4841:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4842:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4843:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4844:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4845:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4846:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4847:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4848:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4849:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4850:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4851:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4852:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4853:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4854:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4855:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4856:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4857:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4858:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4859:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4860:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4861:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4862:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4863:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4864:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4865:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4866:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4867:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4868:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4869:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4870:\ttotal: 2m 12s\tremaining: 2m 20s\n",
      "4871:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4872:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4873:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4874:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4875:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4876:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4877:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4878:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4879:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4880:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4881:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4882:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4883:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4884:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4885:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4886:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4887:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4888:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4889:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4890:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4891:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4892:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4893:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4894:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4895:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4896:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4897:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4898:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4899:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4900:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4901:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4902:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4903:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4904:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4905:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4906:\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "4907:\ttotal: 2m 14s\tremaining: 2m 19s\n",
      "4908:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4909:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4910:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4911:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4912:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4913:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4914:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4915:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4916:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4917:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4918:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4919:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4920:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4921:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4922:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4923:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4924:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4925:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4926:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4927:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4928:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4929:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4930:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4931:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4932:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4933:\ttotal: 2m 14s\tremaining: 2m 18s\n",
      "4934:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4935:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4936:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4937:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4938:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4939:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4940:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4941:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4942:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4943:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4944:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4945:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4946:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4947:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4948:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4949:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4950:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4951:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4952:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4953:\ttotal: 2m 15s\tremaining: 2m 18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4954:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4955:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4956:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4957:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4958:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4959:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4960:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4961:\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "4962:\ttotal: 2m 16s\tremaining: 2m 18s\n",
      "4963:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4964:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4965:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4966:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4967:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4968:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4969:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4970:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4971:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4972:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4973:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4974:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4975:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4976:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4977:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4978:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4979:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4980:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4981:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4982:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4983:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4984:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4985:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4986:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4987:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4988:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4989:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4990:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4991:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4992:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4993:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4994:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4995:\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "4996:\ttotal: 2m 17s\tremaining: 2m 17s\n",
      "4997:\ttotal: 2m 17s\tremaining: 2m 17s\n",
      "4998:\ttotal: 2m 17s\tremaining: 2m 17s\n",
      "4999:\ttotal: 2m 17s\tremaining: 2m 17s\n",
      "5000:\ttotal: 2m 17s\tremaining: 2m 17s\n",
      "5001:\ttotal: 2m 17s\tremaining: 2m 17s\n",
      "5002:\ttotal: 2m 17s\tremaining: 2m 17s\n",
      "5003:\ttotal: 2m 17s\tremaining: 2m 17s\n",
      "5004:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5005:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5006:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5007:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5008:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5009:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5010:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5011:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5012:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5013:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5014:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5015:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5016:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5017:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5018:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5019:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5020:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5021:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5022:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5023:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5024:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5025:\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "5026:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5027:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5028:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5029:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5030:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5031:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5032:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5033:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5034:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5035:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5036:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5037:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5038:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5039:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5040:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5041:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5042:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5043:\ttotal: 2m 18s\tremaining: 2m 16s\n",
      "5044:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5045:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5046:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5047:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5048:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5049:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5050:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5051:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5052:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5053:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5054:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5055:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5056:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5057:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5058:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5059:\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "5060:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5061:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5062:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5063:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5064:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5065:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5066:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5067:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5068:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5069:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5070:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5071:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5072:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5073:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5074:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5075:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5076:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5077:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5078:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5079:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5080:\ttotal: 2m 19s\tremaining: 2m 15s\n",
      "5081:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5082:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5083:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5084:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5085:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5086:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5087:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5088:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5089:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5090:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5091:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5092:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5093:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5094:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5095:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5096:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5097:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5098:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5099:\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5100:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5101:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5102:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5103:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5104:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5105:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5106:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5107:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5108:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5109:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5110:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5111:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5112:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5113:\ttotal: 2m 20s\tremaining: 2m 14s\n",
      "5114:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5115:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5116:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5117:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5118:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5119:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5120:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5121:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5122:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5123:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5124:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5125:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5126:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5127:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5128:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5129:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5130:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5131:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5132:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5133:\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "5134:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5135:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5136:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5137:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5138:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5139:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5140:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5141:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5142:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5143:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5144:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5145:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5146:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5147:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5148:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5149:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5150:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5151:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5152:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5153:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5154:\ttotal: 2m 21s\tremaining: 2m 13s\n",
      "5155:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5156:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5157:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5158:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5159:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5160:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5161:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5162:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5163:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5164:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5165:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5166:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5167:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5168:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5169:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5170:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5171:\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "5172:\ttotal: 2m 22s\tremaining: 2m 12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5173:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5174:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5175:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5176:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5177:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5178:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5179:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5180:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5181:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5182:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5183:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5184:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5185:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5186:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5187:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5188:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5189:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5190:\ttotal: 2m 22s\tremaining: 2m 12s\n",
      "5191:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5192:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5193:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5194:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5195:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5196:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5197:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5198:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5199:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5200:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5201:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5202:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5203:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5204:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5205:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5206:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5207:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5208:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5209:\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5210:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5211:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5212:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5213:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5214:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5215:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5216:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5217:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5218:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5219:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5220:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5221:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5222:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5223:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5224:\ttotal: 2m 23s\tremaining: 2m 11s\n",
      "5225:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5226:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5227:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5228:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5229:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5230:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5231:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5232:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5233:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5234:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5235:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5236:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5237:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5238:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5239:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5240:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5241:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5242:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5243:\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "5244:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5245:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5246:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5247:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5248:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5249:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5250:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5251:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5252:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5253:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5254:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5255:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5256:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5257:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5258:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5259:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5260:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5261:\ttotal: 2m 24s\tremaining: 2m 10s\n",
      "5262:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5263:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5264:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5265:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5266:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5267:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5268:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5269:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5270:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5271:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5272:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5273:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5274:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5275:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5276:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5277:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5278:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5279:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5280:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5281:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5282:\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "5283:\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5284:\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5285:\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5286:\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5287:\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5288:\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5289:\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5290:\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5291:\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5292:\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5293:\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5294:\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5295:\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5296:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5297:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5298:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5299:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5300:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5301:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5302:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5303:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5304:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5305:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5306:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5307:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5308:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5309:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5310:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5311:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5312:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5313:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5314:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5315:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5316:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5317:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5318:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5319:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5320:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5321:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5322:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5323:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5324:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5325:\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "5326:\ttotal: 2m 25s\tremaining: 2m 7s\n",
      "5327:\ttotal: 2m 25s\tremaining: 2m 7s\n",
      "5328:\ttotal: 2m 25s\tremaining: 2m 7s\n",
      "5329:\ttotal: 2m 25s\tremaining: 2m 7s\n",
      "5330:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5331:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5332:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5333:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5334:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5335:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5336:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5337:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5338:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5339:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5340:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5341:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5342:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5343:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5344:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5345:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5346:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5347:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5348:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5349:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5350:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5351:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5352:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5353:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5354:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5355:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5356:\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "5357:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5358:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5359:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5360:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5361:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5362:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5363:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5364:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5365:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5366:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5367:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5368:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5369:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5370:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5371:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5372:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5373:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5374:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5375:\ttotal: 2m 26s\tremaining: 2m 6s\n",
      "5376:\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "5377:\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "5378:\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "5379:\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "5380:\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "5381:\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "5382:\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "5383:\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "5384:\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "5385:\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "5386:\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "5387:\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "5388:\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "5389:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5390:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5391:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5392:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5393:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5394:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5395:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5396:\ttotal: 2m 27s\tremaining: 2m 5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5397:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5398:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5399:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5400:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5401:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5402:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5403:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5404:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5405:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5406:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5407:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5408:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5409:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5410:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5411:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5412:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5413:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5414:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5415:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5416:\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5417:\ttotal: 2m 28s\tremaining: 2m 5s\n",
      "5418:\ttotal: 2m 28s\tremaining: 2m 5s\n",
      "5419:\ttotal: 2m 28s\tremaining: 2m 5s\n",
      "5420:\ttotal: 2m 28s\tremaining: 2m 5s\n",
      "5421:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5422:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5423:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5424:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5425:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5426:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5427:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5428:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5429:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5430:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5431:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5432:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5433:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5434:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5435:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5436:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5437:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5438:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5439:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5440:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5441:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5442:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5443:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5444:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5445:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5446:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5447:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5448:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5449:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5450:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5451:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5452:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5453:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5454:\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "5455:\ttotal: 2m 28s\tremaining: 2m 3s\n",
      "5456:\ttotal: 2m 28s\tremaining: 2m 3s\n",
      "5457:\ttotal: 2m 28s\tremaining: 2m 3s\n",
      "5458:\ttotal: 2m 28s\tremaining: 2m 3s\n",
      "5459:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5460:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5461:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5462:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5463:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5464:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5465:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5466:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5467:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5468:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5469:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5470:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5471:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5472:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5473:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5474:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5475:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5476:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5477:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5478:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5479:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5480:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5481:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5482:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5483:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5484:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5485:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5486:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5487:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5488:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5489:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5490:\ttotal: 2m 29s\tremaining: 2m 3s\n",
      "5491:\ttotal: 2m 29s\tremaining: 2m 2s\n",
      "5492:\ttotal: 2m 29s\tremaining: 2m 2s\n",
      "5493:\ttotal: 2m 29s\tremaining: 2m 2s\n",
      "5494:\ttotal: 2m 29s\tremaining: 2m 2s\n",
      "5495:\ttotal: 2m 29s\tremaining: 2m 2s\n",
      "5496:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5497:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5498:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5499:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5500:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5501:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5502:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5503:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5504:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5505:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5506:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5507:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5508:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5509:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5510:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5511:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5512:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5513:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5514:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5515:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5516:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5517:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5518:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5519:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5520:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5521:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5522:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5523:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5524:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5525:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5526:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5527:\ttotal: 2m 30s\tremaining: 2m 2s\n",
      "5528:\ttotal: 2m 31s\tremaining: 2m 2s\n",
      "5529:\ttotal: 2m 31s\tremaining: 2m 2s\n",
      "5530:\ttotal: 2m 31s\tremaining: 2m 2s\n",
      "5531:\ttotal: 2m 31s\tremaining: 2m 2s\n",
      "5532:\ttotal: 2m 31s\tremaining: 2m 2s\n",
      "5533:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5534:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5535:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5536:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5537:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5538:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5539:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5540:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5541:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5542:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5543:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5544:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5545:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5546:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5547:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5548:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5549:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5550:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5551:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5552:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5553:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5554:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5555:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5556:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5557:\ttotal: 2m 31s\tremaining: 2m 1s\n",
      "5558:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5559:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5560:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5561:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5562:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5563:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5564:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5565:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5566:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5567:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5568:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5569:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5570:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5571:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5572:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5573:\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "5574:\ttotal: 2m 32s\tremaining: 2m\n",
      "5575:\ttotal: 2m 32s\tremaining: 2m\n",
      "5576:\ttotal: 2m 32s\tremaining: 2m\n",
      "5577:\ttotal: 2m 32s\tremaining: 2m\n",
      "5578:\ttotal: 2m 32s\tremaining: 2m\n",
      "5579:\ttotal: 2m 32s\tremaining: 2m\n",
      "5580:\ttotal: 2m 32s\tremaining: 2m\n",
      "5581:\ttotal: 2m 32s\tremaining: 2m\n",
      "5582:\ttotal: 2m 32s\tremaining: 2m\n",
      "5583:\ttotal: 2m 32s\tremaining: 2m\n",
      "5584:\ttotal: 2m 32s\tremaining: 2m\n",
      "5585:\ttotal: 2m 32s\tremaining: 2m\n",
      "5586:\ttotal: 2m 32s\tremaining: 2m\n",
      "5587:\ttotal: 2m 32s\tremaining: 2m\n",
      "5588:\ttotal: 2m 32s\tremaining: 2m\n",
      "5589:\ttotal: 2m 32s\tremaining: 2m\n",
      "5590:\ttotal: 2m 32s\tremaining: 2m\n",
      "5591:\ttotal: 2m 33s\tremaining: 2m\n",
      "5592:\ttotal: 2m 33s\tremaining: 2m\n",
      "5593:\ttotal: 2m 33s\tremaining: 2m\n",
      "5594:\ttotal: 2m 33s\tremaining: 2m\n",
      "5595:\ttotal: 2m 33s\tremaining: 2m\n",
      "5596:\ttotal: 2m 33s\tremaining: 2m\n",
      "5597:\ttotal: 2m 33s\tremaining: 2m\n",
      "5598:\ttotal: 2m 33s\tremaining: 2m\n",
      "5599:\ttotal: 2m 33s\tremaining: 2m\n",
      "5600:\ttotal: 2m 33s\tremaining: 2m\n",
      "5601:\ttotal: 2m 33s\tremaining: 2m\n",
      "5602:\ttotal: 2m 33s\tremaining: 2m\n",
      "5603:\ttotal: 2m 33s\tremaining: 2m\n",
      "5604:\ttotal: 2m 33s\tremaining: 2m\n",
      "5605:\ttotal: 2m 33s\tremaining: 2m\n",
      "5606:\ttotal: 2m 33s\tremaining: 2m\n",
      "5607:\ttotal: 2m 33s\tremaining: 2m\n",
      "5608:\ttotal: 2m 33s\tremaining: 2m\n",
      "5609:\ttotal: 2m 33s\tremaining: 2m\n",
      "5610:\ttotal: 2m 33s\tremaining: 2m\n",
      "5611:\ttotal: 2m 33s\tremaining: 2m\n",
      "5612:\ttotal: 2m 33s\tremaining: 2m\n",
      "5613:\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "5614:\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "5615:\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "5616:\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "5617:\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "5618:\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "5619:\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "5620:\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "5621:\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "5622:\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "5623:\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "5624:\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "5625:\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "5626:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5627:\ttotal: 2m 34s\tremaining: 1m 59s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5628:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5629:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5630:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5631:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5632:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5633:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5634:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5635:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5636:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5637:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5638:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5639:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5640:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5641:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5642:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5643:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5644:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5645:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5646:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5647:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5648:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5649:\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "5650:\ttotal: 2m 34s\tremaining: 1m 58s\n",
      "5651:\ttotal: 2m 34s\tremaining: 1m 58s\n",
      "5652:\ttotal: 2m 34s\tremaining: 1m 58s\n",
      "5653:\ttotal: 2m 34s\tremaining: 1m 58s\n",
      "5654:\ttotal: 2m 34s\tremaining: 1m 58s\n",
      "5655:\ttotal: 2m 34s\tremaining: 1m 58s\n",
      "5656:\ttotal: 2m 34s\tremaining: 1m 58s\n",
      "5657:\ttotal: 2m 34s\tremaining: 1m 58s\n",
      "5658:\ttotal: 2m 34s\tremaining: 1m 58s\n",
      "5659:\ttotal: 2m 34s\tremaining: 1m 58s\n",
      "5660:\ttotal: 2m 34s\tremaining: 1m 58s\n",
      "5661:\ttotal: 2m 34s\tremaining: 1m 58s\n",
      "5662:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5663:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5664:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5665:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5666:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5667:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5668:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5669:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5670:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5671:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5672:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5673:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5674:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5675:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5676:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5677:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5678:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5679:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5680:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5681:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5682:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5683:\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "5684:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5685:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5686:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5687:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5688:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5689:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5690:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5691:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5692:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5693:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5694:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5695:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5696:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5697:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5698:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5699:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5700:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5701:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5702:\ttotal: 2m 35s\tremaining: 1m 57s\n",
      "5703:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5704:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5705:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5706:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5707:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5708:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5709:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5710:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5711:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5712:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5713:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5714:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5715:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5716:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5717:\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5718:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5719:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5720:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5721:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5722:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5723:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5724:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5725:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5726:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5727:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5728:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5729:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5730:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5731:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5732:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5733:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5734:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5735:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5736:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5737:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5738:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5739:\ttotal: 2m 36s\tremaining: 1m 56s\n",
      "5740:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5741:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5742:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5743:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5744:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5745:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5746:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5747:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5748:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5749:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5750:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5751:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5752:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5753:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5754:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5755:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5756:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5757:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5758:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5759:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5760:\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "5761:\ttotal: 2m 37s\tremaining: 1m 55s\n",
      "5762:\ttotal: 2m 37s\tremaining: 1m 55s\n",
      "5763:\ttotal: 2m 37s\tremaining: 1m 55s\n",
      "5764:\ttotal: 2m 37s\tremaining: 1m 55s\n",
      "5765:\ttotal: 2m 37s\tremaining: 1m 55s\n",
      "5766:\ttotal: 2m 37s\tremaining: 1m 55s\n",
      "5767:\ttotal: 2m 37s\tremaining: 1m 55s\n",
      "5768:\ttotal: 2m 37s\tremaining: 1m 55s\n",
      "5769:\ttotal: 2m 37s\tremaining: 1m 55s\n",
      "5770:\ttotal: 2m 37s\tremaining: 1m 55s\n",
      "5771:\ttotal: 2m 37s\tremaining: 1m 55s\n",
      "5772:\ttotal: 2m 37s\tremaining: 1m 55s\n",
      "5773:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5774:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5775:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5776:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5777:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5778:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5779:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5780:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5781:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5782:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5783:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5784:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5785:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5786:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5787:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5788:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5789:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5790:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5791:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5792:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5793:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5794:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5795:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5796:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5797:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5798:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5799:\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "5800:\ttotal: 2m 38s\tremaining: 1m 54s\n",
      "5801:\ttotal: 2m 38s\tremaining: 1m 54s\n",
      "5802:\ttotal: 2m 38s\tremaining: 1m 54s\n",
      "5803:\ttotal: 2m 38s\tremaining: 1m 54s\n",
      "5804:\ttotal: 2m 38s\tremaining: 1m 54s\n",
      "5805:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5806:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5807:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5808:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5809:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5810:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5811:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5812:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5813:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5814:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5815:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5816:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5817:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5818:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5819:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5820:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5821:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5822:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5823:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5824:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5825:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5826:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5827:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5828:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5829:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5830:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5831:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5832:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5833:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5834:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5835:\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "5836:\ttotal: 2m 39s\tremaining: 1m 53s\n",
      "5837:\ttotal: 2m 39s\tremaining: 1m 53s\n",
      "5838:\ttotal: 2m 39s\tremaining: 1m 53s\n",
      "5839:\ttotal: 2m 39s\tremaining: 1m 53s\n",
      "5840:\ttotal: 2m 39s\tremaining: 1m 53s\n",
      "5841:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5842:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5843:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5844:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5845:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5846:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5847:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5848:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5849:\ttotal: 2m 40s\tremaining: 1m 53s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5850:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5851:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5852:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5853:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5854:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5855:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5856:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5857:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5858:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5859:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5860:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5861:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5862:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5863:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5864:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5865:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5866:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5867:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5868:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5869:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5870:\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "5871:\ttotal: 2m 41s\tremaining: 1m 53s\n",
      "5872:\ttotal: 2m 41s\tremaining: 1m 53s\n",
      "5873:\ttotal: 2m 41s\tremaining: 1m 53s\n",
      "5874:\ttotal: 2m 41s\tremaining: 1m 53s\n",
      "5875:\ttotal: 2m 41s\tremaining: 1m 53s\n",
      "5876:\ttotal: 2m 41s\tremaining: 1m 53s\n",
      "5877:\ttotal: 2m 41s\tremaining: 1m 53s\n",
      "5878:\ttotal: 2m 41s\tremaining: 1m 53s\n",
      "5879:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5880:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5881:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5882:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5883:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5884:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5885:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5886:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5887:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5888:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5889:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5890:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5891:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5892:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5893:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5894:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5895:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5896:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5897:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5898:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5899:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5900:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5901:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5902:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5903:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5904:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5905:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5906:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5907:\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "5908:\ttotal: 2m 42s\tremaining: 1m 52s\n",
      "5909:\ttotal: 2m 42s\tremaining: 1m 52s\n",
      "5910:\ttotal: 2m 42s\tremaining: 1m 52s\n",
      "5911:\ttotal: 2m 42s\tremaining: 1m 52s\n",
      "5912:\ttotal: 2m 42s\tremaining: 1m 52s\n",
      "5913:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5914:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5915:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5916:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5917:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5918:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5919:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5920:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5921:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5922:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5923:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5924:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5925:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5926:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5927:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5928:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5929:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5930:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5931:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5932:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5933:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5934:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5935:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5936:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5937:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5938:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5939:\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "5940:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5941:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5942:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5943:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5944:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5945:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5946:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5947:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5948:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5949:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5950:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5951:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5952:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5953:\ttotal: 2m 43s\tremaining: 1m 51s\n",
      "5954:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5955:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5956:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5957:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5958:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5959:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5960:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5961:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5962:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5963:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5964:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5965:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5966:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5967:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5968:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5969:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5970:\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "5971:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5972:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5973:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5974:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5975:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5976:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5977:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5978:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5979:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5980:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5981:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5982:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5983:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5984:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5985:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5986:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5987:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5988:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5989:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5990:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5991:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5992:\ttotal: 2m 44s\tremaining: 1m 50s\n",
      "5993:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "5994:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "5995:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "5996:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "5997:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "5998:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "5999:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "6000:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "6001:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "6002:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "6003:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "6004:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "6005:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "6006:\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "6007:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6008:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6009:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6010:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6011:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6012:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6013:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6014:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6015:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6016:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6017:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6018:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6019:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6020:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6021:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6022:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6023:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6024:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6025:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6026:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6027:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6028:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6029:\ttotal: 2m 45s\tremaining: 1m 49s\n",
      "6030:\ttotal: 2m 45s\tremaining: 1m 48s\n",
      "6031:\ttotal: 2m 45s\tremaining: 1m 48s\n",
      "6032:\ttotal: 2m 45s\tremaining: 1m 48s\n",
      "6033:\ttotal: 2m 45s\tremaining: 1m 48s\n",
      "6034:\ttotal: 2m 45s\tremaining: 1m 48s\n",
      "6035:\ttotal: 2m 45s\tremaining: 1m 48s\n",
      "6036:\ttotal: 2m 45s\tremaining: 1m 48s\n",
      "6037:\ttotal: 2m 45s\tremaining: 1m 48s\n",
      "6038:\ttotal: 2m 45s\tremaining: 1m 48s\n",
      "6039:\ttotal: 2m 45s\tremaining: 1m 48s\n",
      "6040:\ttotal: 2m 45s\tremaining: 1m 48s\n",
      "6041:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6042:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6043:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6044:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6045:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6046:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6047:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6048:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6049:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6050:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6051:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6052:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6053:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6054:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6055:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6056:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6057:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6058:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6059:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6060:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6061:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6062:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6063:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6064:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6065:\ttotal: 2m 46s\tremaining: 1m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6066:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6067:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6068:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6069:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6070:\ttotal: 2m 46s\tremaining: 1m 48s\n",
      "6071:\ttotal: 2m 46s\tremaining: 1m 47s\n",
      "6072:\ttotal: 2m 46s\tremaining: 1m 47s\n",
      "6073:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6074:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6075:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6076:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6077:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6078:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6079:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6080:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6081:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6082:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6083:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6084:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6085:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6086:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6087:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6088:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6089:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6090:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6091:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6092:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6093:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6094:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6095:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6096:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6097:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6098:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6099:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6100:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6101:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6102:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6103:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6104:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6105:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6106:\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6107:\ttotal: 2m 47s\tremaining: 1m 46s\n",
      "6108:\ttotal: 2m 47s\tremaining: 1m 46s\n",
      "6109:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6110:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6111:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6112:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6113:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6114:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6115:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6116:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6117:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6118:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6119:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6120:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6121:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6122:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6123:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6124:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6125:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6126:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6127:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6128:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6129:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6130:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6131:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6132:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6133:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6134:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6135:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6136:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6137:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6138:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6139:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6140:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6141:\ttotal: 2m 48s\tremaining: 1m 46s\n",
      "6142:\ttotal: 2m 49s\tremaining: 1m 46s\n",
      "6143:\ttotal: 2m 49s\tremaining: 1m 46s\n",
      "6144:\ttotal: 2m 49s\tremaining: 1m 46s\n",
      "6145:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6146:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6147:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6148:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6149:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6150:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6151:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6152:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6153:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6154:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6155:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6156:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6157:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6158:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6159:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6160:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6161:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6162:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6163:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6164:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6165:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6166:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6167:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6168:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6169:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6170:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6171:\ttotal: 2m 49s\tremaining: 1m 45s\n",
      "6172:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6173:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6174:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6175:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6176:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6177:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6178:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6179:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6180:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6181:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6182:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6183:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6184:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6185:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6186:\ttotal: 2m 50s\tremaining: 1m 45s\n",
      "6187:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6188:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6189:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6190:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6191:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6192:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6193:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6194:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6195:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6196:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6197:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6198:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6199:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6200:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6201:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6202:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6203:\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6204:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6205:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6206:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6207:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6208:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6209:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6210:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6211:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6212:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6213:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6214:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6215:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6216:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6217:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6218:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6219:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6220:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6221:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6222:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6223:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6224:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6225:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6226:\ttotal: 2m 51s\tremaining: 1m 44s\n",
      "6227:\ttotal: 2m 51s\tremaining: 1m 43s\n",
      "6228:\ttotal: 2m 51s\tremaining: 1m 43s\n",
      "6229:\ttotal: 2m 51s\tremaining: 1m 43s\n",
      "6230:\ttotal: 2m 51s\tremaining: 1m 43s\n",
      "6231:\ttotal: 2m 51s\tremaining: 1m 43s\n",
      "6232:\ttotal: 2m 51s\tremaining: 1m 43s\n",
      "6233:\ttotal: 2m 51s\tremaining: 1m 43s\n",
      "6234:\ttotal: 2m 51s\tremaining: 1m 43s\n",
      "6235:\ttotal: 2m 51s\tremaining: 1m 43s\n",
      "6236:\ttotal: 2m 51s\tremaining: 1m 43s\n",
      "6237:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6238:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6239:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6240:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6241:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6242:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6243:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6244:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6245:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6246:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6247:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6248:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6249:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6250:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6251:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6252:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6253:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6254:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6255:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6256:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6257:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6258:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6259:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6260:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6261:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6262:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6263:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6264:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6265:\ttotal: 2m 52s\tremaining: 1m 43s\n",
      "6266:\ttotal: 2m 52s\tremaining: 1m 42s\n",
      "6267:\ttotal: 2m 52s\tremaining: 1m 42s\n",
      "6268:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6269:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6270:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6271:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6272:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6273:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6274:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6275:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6276:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6277:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6278:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6279:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6280:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6281:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6282:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6283:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6284:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6285:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6286:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6287:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6288:\ttotal: 2m 53s\tremaining: 1m 42s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6289:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6290:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6291:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6292:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6293:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6294:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6295:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6296:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6297:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6298:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6299:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6300:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6301:\ttotal: 2m 53s\tremaining: 1m 42s\n",
      "6302:\ttotal: 2m 53s\tremaining: 1m 41s\n",
      "6303:\ttotal: 2m 53s\tremaining: 1m 41s\n",
      "6304:\ttotal: 2m 53s\tremaining: 1m 41s\n",
      "6305:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6306:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6307:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6308:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6309:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6310:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6311:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6312:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6313:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6314:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6315:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6316:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6317:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6318:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6319:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6320:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6321:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6322:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6323:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6324:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6325:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6326:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6327:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6328:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6329:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6330:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6331:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6332:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6333:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6334:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6335:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6336:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6337:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6338:\ttotal: 2m 54s\tremaining: 1m 41s\n",
      "6339:\ttotal: 2m 54s\tremaining: 1m 40s\n",
      "6340:\ttotal: 2m 54s\tremaining: 1m 40s\n",
      "6341:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6342:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6343:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6344:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6345:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6346:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6347:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6348:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6349:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6350:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6351:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6352:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6353:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6354:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6355:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6356:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6357:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6358:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6359:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6360:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6361:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6362:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6363:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6364:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6365:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6366:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6367:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6368:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6369:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6370:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6371:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6372:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6373:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6374:\ttotal: 2m 55s\tremaining: 1m 40s\n",
      "6375:\ttotal: 2m 56s\tremaining: 1m 40s\n",
      "6376:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6377:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6378:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6379:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6380:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6381:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6382:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6383:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6384:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6385:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6386:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6387:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6388:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6389:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6390:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6391:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6392:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6393:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6394:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6395:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6396:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6397:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6398:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6399:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6400:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6401:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6402:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6403:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6404:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6405:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6406:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6407:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6408:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6409:\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6410:\ttotal: 2m 57s\tremaining: 1m 39s\n",
      "6411:\ttotal: 2m 57s\tremaining: 1m 39s\n",
      "6412:\ttotal: 2m 57s\tremaining: 1m 39s\n",
      "6413:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6414:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6415:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6416:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6417:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6418:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6419:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6420:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6421:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6422:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6423:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6424:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6425:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6426:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6427:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6428:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6429:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6430:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6431:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6432:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6433:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6434:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6435:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6436:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6437:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6438:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6439:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6440:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6441:\ttotal: 2m 57s\tremaining: 1m 38s\n",
      "6442:\ttotal: 2m 58s\tremaining: 1m 38s\n",
      "6443:\ttotal: 2m 58s\tremaining: 1m 38s\n",
      "6444:\ttotal: 2m 58s\tremaining: 1m 38s\n",
      "6445:\ttotal: 2m 58s\tremaining: 1m 38s\n",
      "6446:\ttotal: 2m 58s\tremaining: 1m 38s\n",
      "6447:\ttotal: 2m 58s\tremaining: 1m 38s\n",
      "6448:\ttotal: 2m 58s\tremaining: 1m 38s\n",
      "6449:\ttotal: 2m 58s\tremaining: 1m 38s\n",
      "6450:\ttotal: 2m 58s\tremaining: 1m 38s\n",
      "6451:\ttotal: 2m 58s\tremaining: 1m 38s\n",
      "6452:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6453:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6454:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6455:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6456:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6457:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6458:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6459:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6460:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6461:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6462:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6463:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6464:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6465:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6466:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6467:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6468:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6469:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6470:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6471:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6472:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6473:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6474:\ttotal: 2m 58s\tremaining: 1m 37s\n",
      "6475:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6476:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6477:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6478:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6479:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6480:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6481:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6482:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6483:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6484:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6485:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6486:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6487:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6488:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6489:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6490:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6491:\ttotal: 2m 59s\tremaining: 1m 37s\n",
      "6492:\ttotal: 2m 59s\tremaining: 1m 36s\n",
      "6493:\ttotal: 2m 59s\tremaining: 1m 36s\n",
      "6494:\ttotal: 2m 59s\tremaining: 1m 36s\n",
      "6495:\ttotal: 2m 59s\tremaining: 1m 36s\n",
      "6496:\ttotal: 2m 59s\tremaining: 1m 36s\n",
      "6497:\ttotal: 2m 59s\tremaining: 1m 36s\n",
      "6498:\ttotal: 2m 59s\tremaining: 1m 36s\n",
      "6499:\ttotal: 2m 59s\tremaining: 1m 36s\n",
      "6500:\ttotal: 2m 59s\tremaining: 1m 36s\n",
      "6501:\ttotal: 3m\tremaining: 1m 36s\n",
      "6502:\ttotal: 3m\tremaining: 1m 36s\n",
      "6503:\ttotal: 3m\tremaining: 1m 36s\n",
      "6504:\ttotal: 3m\tremaining: 1m 36s\n",
      "6505:\ttotal: 3m\tremaining: 1m 36s\n",
      "6506:\ttotal: 3m\tremaining: 1m 36s\n",
      "6507:\ttotal: 3m\tremaining: 1m 36s\n",
      "6508:\ttotal: 3m\tremaining: 1m 36s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6509:\ttotal: 3m\tremaining: 1m 36s\n",
      "6510:\ttotal: 3m\tremaining: 1m 36s\n",
      "6511:\ttotal: 3m\tremaining: 1m 36s\n",
      "6512:\ttotal: 3m\tremaining: 1m 36s\n",
      "6513:\ttotal: 3m\tremaining: 1m 36s\n",
      "6514:\ttotal: 3m\tremaining: 1m 36s\n",
      "6515:\ttotal: 3m\tremaining: 1m 36s\n",
      "6516:\ttotal: 3m\tremaining: 1m 36s\n",
      "6517:\ttotal: 3m\tremaining: 1m 36s\n",
      "6518:\ttotal: 3m\tremaining: 1m 36s\n",
      "6519:\ttotal: 3m\tremaining: 1m 36s\n",
      "6520:\ttotal: 3m\tremaining: 1m 36s\n",
      "6521:\ttotal: 3m\tremaining: 1m 36s\n",
      "6522:\ttotal: 3m\tremaining: 1m 36s\n",
      "6523:\ttotal: 3m\tremaining: 1m 36s\n",
      "6524:\ttotal: 3m\tremaining: 1m 36s\n",
      "6525:\ttotal: 3m\tremaining: 1m 36s\n",
      "6526:\ttotal: 3m\tremaining: 1m 36s\n",
      "6527:\ttotal: 3m\tremaining: 1m 36s\n",
      "6528:\ttotal: 3m\tremaining: 1m 36s\n",
      "6529:\ttotal: 3m\tremaining: 1m 36s\n",
      "6530:\ttotal: 3m\tremaining: 1m 36s\n",
      "6531:\ttotal: 3m 1s\tremaining: 1m 36s\n",
      "6532:\ttotal: 3m 1s\tremaining: 1m 36s\n",
      "6533:\ttotal: 3m 1s\tremaining: 1m 36s\n",
      "6534:\ttotal: 3m 1s\tremaining: 1m 36s\n",
      "6535:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6536:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6537:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6538:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6539:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6540:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6541:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6542:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6543:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6544:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6545:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6546:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6547:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6548:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6549:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6550:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6551:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6552:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6553:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6554:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6555:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6556:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6557:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6558:\ttotal: 3m 1s\tremaining: 1m 35s\n",
      "6559:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6560:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6561:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6562:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6563:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6564:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6565:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6566:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6567:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6568:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6569:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6570:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6571:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6572:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6573:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6574:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6575:\ttotal: 3m 2s\tremaining: 1m 35s\n",
      "6576:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6577:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6578:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6579:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6580:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6581:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6582:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6583:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6584:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6585:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6586:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6587:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6588:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6589:\ttotal: 3m 2s\tremaining: 1m 34s\n",
      "6590:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6591:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6592:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6593:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6594:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6595:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6596:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6597:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6598:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6599:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6600:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6601:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6602:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6603:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6604:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6605:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6606:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6607:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6608:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6609:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6610:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6611:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6612:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6613:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6614:\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6615:\ttotal: 3m 3s\tremaining: 1m 33s\n",
      "6616:\ttotal: 3m 3s\tremaining: 1m 33s\n",
      "6617:\ttotal: 3m 3s\tremaining: 1m 33s\n",
      "6618:\ttotal: 3m 3s\tremaining: 1m 33s\n",
      "6619:\ttotal: 3m 3s\tremaining: 1m 33s\n",
      "6620:\ttotal: 3m 3s\tremaining: 1m 33s\n",
      "6621:\ttotal: 3m 3s\tremaining: 1m 33s\n",
      "6622:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6623:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6624:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6625:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6626:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6627:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6628:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6629:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6630:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6631:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6632:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6633:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6634:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6635:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6636:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6637:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6638:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6639:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6640:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6641:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6642:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6643:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6644:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6645:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6646:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6647:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6648:\ttotal: 3m 4s\tremaining: 1m 33s\n",
      "6649:\ttotal: 3m 5s\tremaining: 1m 33s\n",
      "6650:\ttotal: 3m 5s\tremaining: 1m 33s\n",
      "6651:\ttotal: 3m 5s\tremaining: 1m 33s\n",
      "6652:\ttotal: 3m 5s\tremaining: 1m 33s\n",
      "6653:\ttotal: 3m 5s\tremaining: 1m 33s\n",
      "6654:\ttotal: 3m 5s\tremaining: 1m 33s\n",
      "6655:\ttotal: 3m 5s\tremaining: 1m 33s\n",
      "6656:\ttotal: 3m 5s\tremaining: 1m 33s\n",
      "6657:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6658:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6659:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6660:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6661:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6662:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6663:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6664:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6665:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6666:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6667:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6668:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6669:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6670:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6671:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6672:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6673:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6674:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6675:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6676:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6677:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6678:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6679:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6680:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6681:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6682:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6683:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6684:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6685:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6686:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6687:\ttotal: 3m 5s\tremaining: 1m 32s\n",
      "6688:\ttotal: 3m 6s\tremaining: 1m 32s\n",
      "6689:\ttotal: 3m 6s\tremaining: 1m 32s\n",
      "6690:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6691:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6692:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6693:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6694:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6695:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6696:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6697:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6698:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6699:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6700:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6701:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6702:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6703:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6704:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6705:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6706:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6707:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6708:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6709:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6710:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6711:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6712:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6713:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6714:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6715:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6716:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6717:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6718:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6719:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6720:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6721:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6722:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6723:\ttotal: 3m 6s\tremaining: 1m 31s\n",
      "6724:\ttotal: 3m 6s\tremaining: 1m 30s\n",
      "6725:\ttotal: 3m 6s\tremaining: 1m 30s\n",
      "6726:\ttotal: 3m 6s\tremaining: 1m 30s\n",
      "6727:\ttotal: 3m 6s\tremaining: 1m 30s\n",
      "6728:\ttotal: 3m 6s\tremaining: 1m 30s\n",
      "6729:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6730:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6731:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6732:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6733:\ttotal: 3m 7s\tremaining: 1m 30s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6734:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6735:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6736:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6737:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6738:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6739:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6740:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6741:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6742:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6743:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6744:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6745:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6746:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6747:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6748:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6749:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6750:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6751:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6752:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6753:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6754:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6755:\ttotal: 3m 7s\tremaining: 1m 30s\n",
      "6756:\ttotal: 3m 8s\tremaining: 1m 30s\n",
      "6757:\ttotal: 3m 8s\tremaining: 1m 30s\n",
      "6758:\ttotal: 3m 8s\tremaining: 1m 30s\n",
      "6759:\ttotal: 3m 8s\tremaining: 1m 30s\n",
      "6760:\ttotal: 3m 8s\tremaining: 1m 30s\n",
      "6761:\ttotal: 3m 8s\tremaining: 1m 30s\n",
      "6762:\ttotal: 3m 8s\tremaining: 1m 30s\n",
      "6763:\ttotal: 3m 8s\tremaining: 1m 30s\n",
      "6764:\ttotal: 3m 8s\tremaining: 1m 30s\n",
      "6765:\ttotal: 3m 8s\tremaining: 1m 30s\n",
      "6766:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6767:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6768:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6769:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6770:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6771:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6772:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6773:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6774:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6775:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6776:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6777:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6778:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6779:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6780:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6781:\ttotal: 3m 8s\tremaining: 1m 29s\n",
      "6782:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6783:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6784:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6785:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6786:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6787:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6788:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6789:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6790:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6791:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6792:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6793:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6794:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6795:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6796:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6797:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6798:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6799:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6800:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6801:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6802:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6803:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6804:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6805:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6806:\ttotal: 3m 9s\tremaining: 1m 29s\n",
      "6807:\ttotal: 3m 10s\tremaining: 1m 29s\n",
      "6808:\ttotal: 3m 10s\tremaining: 1m 29s\n",
      "6809:\ttotal: 3m 10s\tremaining: 1m 29s\n",
      "6810:\ttotal: 3m 10s\tremaining: 1m 29s\n",
      "6811:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6812:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6813:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6814:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6815:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6816:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6817:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6818:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6819:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6820:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6821:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6822:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6823:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6824:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6825:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6826:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6827:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6828:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6829:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6830:\ttotal: 3m 10s\tremaining: 1m 28s\n",
      "6831:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6832:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6833:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6834:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6835:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6836:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6837:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6838:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6839:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6840:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6841:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6842:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6843:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6844:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6845:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6846:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6847:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6848:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6849:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6850:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6851:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6852:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6853:\ttotal: 3m 11s\tremaining: 1m 28s\n",
      "6854:\ttotal: 3m 11s\tremaining: 1m 27s\n",
      "6855:\ttotal: 3m 11s\tremaining: 1m 27s\n",
      "6856:\ttotal: 3m 11s\tremaining: 1m 27s\n",
      "6857:\ttotal: 3m 11s\tremaining: 1m 27s\n",
      "6858:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6859:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6860:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6861:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6862:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6863:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6864:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6865:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6866:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6867:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6868:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6869:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6870:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6871:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6872:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6873:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6874:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6875:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6876:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6877:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6878:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6879:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6880:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6881:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6882:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6883:\ttotal: 3m 12s\tremaining: 1m 27s\n",
      "6884:\ttotal: 3m 13s\tremaining: 1m 27s\n",
      "6885:\ttotal: 3m 13s\tremaining: 1m 27s\n",
      "6886:\ttotal: 3m 13s\tremaining: 1m 27s\n",
      "6887:\ttotal: 3m 13s\tremaining: 1m 27s\n",
      "6888:\ttotal: 3m 13s\tremaining: 1m 27s\n",
      "6889:\ttotal: 3m 13s\tremaining: 1m 27s\n",
      "6890:\ttotal: 3m 13s\tremaining: 1m 27s\n",
      "6891:\ttotal: 3m 13s\tremaining: 1m 27s\n",
      "6892:\ttotal: 3m 13s\tremaining: 1m 27s\n",
      "6893:\ttotal: 3m 13s\tremaining: 1m 27s\n",
      "6894:\ttotal: 3m 13s\tremaining: 1m 27s\n",
      "6895:\ttotal: 3m 13s\tremaining: 1m 27s\n",
      "6896:\ttotal: 3m 13s\tremaining: 1m 27s\n",
      "6897:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6898:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6899:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6900:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6901:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6902:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6903:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6904:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6905:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6906:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6907:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6908:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6909:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6910:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6911:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6912:\ttotal: 3m 13s\tremaining: 1m 26s\n",
      "6913:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6914:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6915:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6916:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6917:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6918:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6919:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6920:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6921:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6922:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6923:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6924:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6925:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6926:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6927:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6928:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6929:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6930:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6931:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6932:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6933:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6934:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6935:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6936:\ttotal: 3m 14s\tremaining: 1m 26s\n",
      "6937:\ttotal: 3m 14s\tremaining: 1m 25s\n",
      "6938:\ttotal: 3m 14s\tremaining: 1m 25s\n",
      "6939:\ttotal: 3m 14s\tremaining: 1m 25s\n",
      "6940:\ttotal: 3m 14s\tremaining: 1m 25s\n",
      "6941:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6942:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6943:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6944:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6945:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6946:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6947:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6948:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6949:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6950:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6951:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6952:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6953:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6954:\ttotal: 3m 15s\tremaining: 1m 25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6955:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6956:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6957:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6958:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6959:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6960:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6961:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6962:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6963:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6964:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6965:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6966:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6967:\ttotal: 3m 15s\tremaining: 1m 25s\n",
      "6968:\ttotal: 3m 16s\tremaining: 1m 25s\n",
      "6969:\ttotal: 3m 16s\tremaining: 1m 25s\n",
      "6970:\ttotal: 3m 16s\tremaining: 1m 25s\n",
      "6971:\ttotal: 3m 16s\tremaining: 1m 25s\n",
      "6972:\ttotal: 3m 16s\tremaining: 1m 25s\n",
      "6973:\ttotal: 3m 16s\tremaining: 1m 25s\n",
      "6974:\ttotal: 3m 16s\tremaining: 1m 25s\n",
      "6975:\ttotal: 3m 16s\tremaining: 1m 25s\n",
      "6976:\ttotal: 3m 16s\tremaining: 1m 25s\n",
      "6977:\ttotal: 3m 16s\tremaining: 1m 25s\n",
      "6978:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6979:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6980:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6981:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6982:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6983:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6984:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6985:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6986:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6987:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6988:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6989:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6990:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6991:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6992:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6993:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6994:\ttotal: 3m 16s\tremaining: 1m 24s\n",
      "6995:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "6996:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "6997:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "6998:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "6999:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7000:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7001:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7002:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7003:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7004:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7005:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7006:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7007:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7008:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7009:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7010:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7011:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7012:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7013:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7014:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7015:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7016:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7017:\ttotal: 3m 17s\tremaining: 1m 24s\n",
      "7018:\ttotal: 3m 18s\tremaining: 1m 24s\n",
      "7019:\ttotal: 3m 18s\tremaining: 1m 24s\n",
      "7020:\ttotal: 3m 18s\tremaining: 1m 24s\n",
      "7021:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7022:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7023:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7024:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7025:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7026:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7027:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7028:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7029:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7030:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7031:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7032:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7033:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7034:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7035:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7036:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7037:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7038:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7039:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7040:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7041:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7042:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7043:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7044:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7045:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7046:\ttotal: 3m 18s\tremaining: 1m 23s\n",
      "7047:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7048:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7049:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7050:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7051:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7052:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7053:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7054:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7055:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7056:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7057:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7058:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7059:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7060:\ttotal: 3m 19s\tremaining: 1m 23s\n",
      "7061:\ttotal: 3m 19s\tremaining: 1m 22s\n",
      "7062:\ttotal: 3m 19s\tremaining: 1m 22s\n",
      "7063:\ttotal: 3m 19s\tremaining: 1m 22s\n",
      "7064:\ttotal: 3m 19s\tremaining: 1m 22s\n",
      "7065:\ttotal: 3m 19s\tremaining: 1m 22s\n",
      "7066:\ttotal: 3m 19s\tremaining: 1m 22s\n",
      "7067:\ttotal: 3m 19s\tremaining: 1m 22s\n",
      "7068:\ttotal: 3m 19s\tremaining: 1m 22s\n",
      "7069:\ttotal: 3m 19s\tremaining: 1m 22s\n",
      "7070:\ttotal: 3m 19s\tremaining: 1m 22s\n",
      "7071:\ttotal: 3m 19s\tremaining: 1m 22s\n",
      "7072:\ttotal: 3m 19s\tremaining: 1m 22s\n",
      "7073:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7074:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7075:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7076:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7077:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7078:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7079:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7080:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7081:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7082:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7083:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7084:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7085:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7086:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7087:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7088:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7089:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7090:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7091:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7092:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7093:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7094:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7095:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7096:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7097:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7098:\ttotal: 3m 20s\tremaining: 1m 22s\n",
      "7099:\ttotal: 3m 21s\tremaining: 1m 22s\n",
      "7100:\ttotal: 3m 21s\tremaining: 1m 22s\n",
      "7101:\ttotal: 3m 21s\tremaining: 1m 22s\n",
      "7102:\ttotal: 3m 21s\tremaining: 1m 22s\n",
      "7103:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7104:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7105:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7106:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7107:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7108:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7109:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7110:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7111:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7112:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7113:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7114:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7115:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7116:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7117:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7118:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7119:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7120:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7121:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7122:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7123:\ttotal: 3m 21s\tremaining: 1m 21s\n",
      "7124:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7125:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7126:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7127:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7128:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7129:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7130:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7131:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7132:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7133:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7134:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7135:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7136:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7137:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7138:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7139:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7140:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7141:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7142:\ttotal: 3m 22s\tremaining: 1m 21s\n",
      "7143:\ttotal: 3m 22s\tremaining: 1m 20s\n",
      "7144:\ttotal: 3m 22s\tremaining: 1m 20s\n",
      "7145:\ttotal: 3m 22s\tremaining: 1m 20s\n",
      "7146:\ttotal: 3m 22s\tremaining: 1m 20s\n",
      "7147:\ttotal: 3m 22s\tremaining: 1m 20s\n",
      "7148:\ttotal: 3m 22s\tremaining: 1m 20s\n",
      "7149:\ttotal: 3m 22s\tremaining: 1m 20s\n",
      "7150:\ttotal: 3m 22s\tremaining: 1m 20s\n",
      "7151:\ttotal: 3m 22s\tremaining: 1m 20s\n",
      "7152:\ttotal: 3m 22s\tremaining: 1m 20s\n",
      "7153:\ttotal: 3m 22s\tremaining: 1m 20s\n",
      "7154:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7155:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7156:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7157:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7158:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7159:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7160:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7161:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7162:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7163:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7164:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7165:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7166:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7167:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7168:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7169:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7170:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7171:\ttotal: 3m 23s\tremaining: 1m 20s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7173:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7174:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7175:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7176:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7177:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7178:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7179:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7180:\ttotal: 3m 23s\tremaining: 1m 20s\n",
      "7181:\ttotal: 3m 23s\tremaining: 1m 19s\n",
      "7182:\ttotal: 3m 23s\tremaining: 1m 19s\n",
      "7183:\ttotal: 3m 23s\tremaining: 1m 19s\n",
      "7184:\ttotal: 3m 23s\tremaining: 1m 19s\n",
      "7185:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7186:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7187:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7188:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7189:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7190:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7191:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7192:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7193:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7194:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7195:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7196:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7197:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7198:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7199:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7200:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7201:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7202:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7203:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7204:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7205:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7206:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7207:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7208:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7209:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7210:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7211:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7212:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7213:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7214:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7215:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7216:\ttotal: 3m 24s\tremaining: 1m 19s\n",
      "7217:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7218:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7219:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7220:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7221:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7222:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7223:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7224:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7225:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7226:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7227:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7228:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7229:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7230:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7231:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7232:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7233:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7234:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7235:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7236:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7237:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7238:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7239:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7240:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7241:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7242:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7243:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7244:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7245:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7246:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7247:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7248:\ttotal: 3m 25s\tremaining: 1m 18s\n",
      "7249:\ttotal: 3m 26s\tremaining: 1m 18s\n",
      "7250:\ttotal: 3m 26s\tremaining: 1m 18s\n",
      "7251:\ttotal: 3m 26s\tremaining: 1m 18s\n",
      "7252:\ttotal: 3m 26s\tremaining: 1m 18s\n",
      "7253:\ttotal: 3m 26s\tremaining: 1m 18s\n",
      "7254:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7255:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7256:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7257:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7258:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7259:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7260:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7261:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7262:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7263:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7264:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7265:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7266:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7267:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7268:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7269:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7270:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7271:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7272:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7273:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7274:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7275:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7276:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7277:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7278:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7279:\ttotal: 3m 26s\tremaining: 1m 17s\n",
      "7280:\ttotal: 3m 27s\tremaining: 1m 17s\n",
      "7281:\ttotal: 3m 27s\tremaining: 1m 17s\n",
      "7282:\ttotal: 3m 27s\tremaining: 1m 17s\n",
      "7283:\ttotal: 3m 27s\tremaining: 1m 17s\n",
      "7284:\ttotal: 3m 27s\tremaining: 1m 17s\n",
      "7285:\ttotal: 3m 27s\tremaining: 1m 17s\n",
      "7286:\ttotal: 3m 27s\tremaining: 1m 17s\n",
      "7287:\ttotal: 3m 27s\tremaining: 1m 17s\n",
      "7288:\ttotal: 3m 27s\tremaining: 1m 17s\n",
      "7289:\ttotal: 3m 27s\tremaining: 1m 17s\n",
      "7290:\ttotal: 3m 27s\tremaining: 1m 17s\n",
      "7291:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7292:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7293:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7294:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7295:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7296:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7297:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7298:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7299:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7300:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7301:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7302:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7303:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7304:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7305:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7306:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7307:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7308:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7309:\ttotal: 3m 27s\tremaining: 1m 16s\n",
      "7310:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7311:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7312:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7313:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7314:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7315:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7316:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7317:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7318:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7319:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7320:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7321:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7322:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7323:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7324:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7325:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7326:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7327:\ttotal: 3m 28s\tremaining: 1m 16s\n",
      "7328:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7329:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7330:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7331:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7332:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7333:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7334:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7335:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7336:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7337:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7338:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7339:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7340:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7341:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7342:\ttotal: 3m 28s\tremaining: 1m 15s\n",
      "7343:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7344:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7345:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7346:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7347:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7348:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7349:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7350:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7351:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7352:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7353:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7354:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7355:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7356:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7357:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7358:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7359:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7360:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7361:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7362:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7363:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7364:\ttotal: 3m 29s\tremaining: 1m 15s\n",
      "7365:\ttotal: 3m 29s\tremaining: 1m 14s\n",
      "7366:\ttotal: 3m 29s\tremaining: 1m 14s\n",
      "7367:\ttotal: 3m 29s\tremaining: 1m 14s\n",
      "7368:\ttotal: 3m 29s\tremaining: 1m 14s\n",
      "7369:\ttotal: 3m 29s\tremaining: 1m 14s\n",
      "7370:\ttotal: 3m 29s\tremaining: 1m 14s\n",
      "7371:\ttotal: 3m 29s\tremaining: 1m 14s\n",
      "7372:\ttotal: 3m 29s\tremaining: 1m 14s\n",
      "7373:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7374:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7375:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7376:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7377:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7378:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7379:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7380:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7381:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7382:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7383:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7384:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7385:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7386:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7387:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7388:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7389:\ttotal: 3m 30s\tremaining: 1m 14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7390:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7391:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7392:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7393:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7394:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7395:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7396:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7397:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7398:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7399:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7400:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7401:\ttotal: 3m 30s\tremaining: 1m 14s\n",
      "7402:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7403:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7404:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7405:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7406:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7407:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7408:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7409:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7410:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7411:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7412:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7413:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7414:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7415:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7416:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7417:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7418:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7419:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7420:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7421:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7422:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7423:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7424:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7425:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7426:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7427:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7428:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7429:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7430:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7431:\ttotal: 3m 31s\tremaining: 1m 13s\n",
      "7432:\ttotal: 3m 32s\tremaining: 1m 13s\n",
      "7433:\ttotal: 3m 32s\tremaining: 1m 13s\n",
      "7434:\ttotal: 3m 32s\tremaining: 1m 13s\n",
      "7435:\ttotal: 3m 32s\tremaining: 1m 13s\n",
      "7436:\ttotal: 3m 32s\tremaining: 1m 13s\n",
      "7437:\ttotal: 3m 32s\tremaining: 1m 13s\n",
      "7438:\ttotal: 3m 32s\tremaining: 1m 13s\n",
      "7439:\ttotal: 3m 32s\tremaining: 1m 13s\n",
      "7440:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7441:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7442:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7443:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7444:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7445:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7446:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7447:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7448:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7449:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7450:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7451:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7452:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7453:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7454:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7455:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7456:\ttotal: 3m 32s\tremaining: 1m 12s\n",
      "7457:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7458:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7459:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7460:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7461:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7462:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7463:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7464:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7465:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7466:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7467:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7468:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7469:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7470:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7471:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7472:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7473:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7474:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7475:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7476:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7477:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7478:\ttotal: 3m 33s\tremaining: 1m 12s\n",
      "7479:\ttotal: 3m 33s\tremaining: 1m 11s\n",
      "7480:\ttotal: 3m 33s\tremaining: 1m 11s\n",
      "7481:\ttotal: 3m 33s\tremaining: 1m 11s\n",
      "7482:\ttotal: 3m 33s\tremaining: 1m 11s\n",
      "7483:\ttotal: 3m 33s\tremaining: 1m 11s\n",
      "7484:\ttotal: 3m 33s\tremaining: 1m 11s\n",
      "7485:\ttotal: 3m 33s\tremaining: 1m 11s\n",
      "7486:\ttotal: 3m 33s\tremaining: 1m 11s\n",
      "7487:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7488:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7489:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7490:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7491:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7492:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7493:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7494:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7495:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7496:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7497:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7498:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7499:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7500:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7501:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7502:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7503:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7504:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7505:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7506:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7507:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7508:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7509:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7510:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7511:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7512:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7513:\ttotal: 3m 34s\tremaining: 1m 11s\n",
      "7514:\ttotal: 3m 34s\tremaining: 1m 10s\n",
      "7515:\ttotal: 3m 34s\tremaining: 1m 10s\n",
      "7516:\ttotal: 3m 34s\tremaining: 1m 10s\n",
      "7517:\ttotal: 3m 34s\tremaining: 1m 10s\n",
      "7518:\ttotal: 3m 34s\tremaining: 1m 10s\n",
      "7519:\ttotal: 3m 34s\tremaining: 1m 10s\n",
      "7520:\ttotal: 3m 34s\tremaining: 1m 10s\n",
      "7521:\ttotal: 3m 34s\tremaining: 1m 10s\n",
      "7522:\ttotal: 3m 34s\tremaining: 1m 10s\n",
      "7523:\ttotal: 3m 34s\tremaining: 1m 10s\n",
      "7524:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7525:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7526:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7527:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7528:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7529:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7530:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7531:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7532:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7533:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7534:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7535:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7536:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7537:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7538:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7539:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7540:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7541:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7542:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7543:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7544:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7545:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7546:\ttotal: 3m 35s\tremaining: 1m 10s\n",
      "7547:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7548:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7549:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7550:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7551:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7552:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7553:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7554:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7555:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7556:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7557:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7558:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7559:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7560:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7561:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7562:\ttotal: 3m 35s\tremaining: 1m 9s\n",
      "7563:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7564:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7565:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7566:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7567:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7568:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7569:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7570:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7571:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7572:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7573:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7574:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7575:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7576:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7577:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7578:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7579:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7580:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7581:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7582:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7583:\ttotal: 3m 36s\tremaining: 1m 9s\n",
      "7584:\ttotal: 3m 36s\tremaining: 1m 8s\n",
      "7585:\ttotal: 3m 36s\tremaining: 1m 8s\n",
      "7586:\ttotal: 3m 36s\tremaining: 1m 8s\n",
      "7587:\ttotal: 3m 36s\tremaining: 1m 8s\n",
      "7588:\ttotal: 3m 36s\tremaining: 1m 8s\n",
      "7589:\ttotal: 3m 36s\tremaining: 1m 8s\n",
      "7590:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7591:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7592:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7593:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7594:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7595:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7596:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7597:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7598:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7599:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7600:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7601:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7602:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7603:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7604:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7605:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7606:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7607:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7608:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7609:\ttotal: 3m 37s\tremaining: 1m 8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7610:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7611:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7612:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7613:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7614:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7615:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7616:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7617:\ttotal: 3m 37s\tremaining: 1m 8s\n",
      "7618:\ttotal: 3m 38s\tremaining: 1m 8s\n",
      "7619:\ttotal: 3m 38s\tremaining: 1m 8s\n",
      "7620:\ttotal: 3m 38s\tremaining: 1m 8s\n",
      "7621:\ttotal: 3m 38s\tremaining: 1m 8s\n",
      "7622:\ttotal: 3m 38s\tremaining: 1m 8s\n",
      "7623:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7624:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7625:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7626:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7627:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7628:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7629:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7630:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7631:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7632:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7633:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7634:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7635:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7636:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7637:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7638:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7639:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7640:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7641:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7642:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7643:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7644:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7645:\ttotal: 3m 38s\tremaining: 1m 7s\n",
      "7646:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7647:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7648:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7649:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7650:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7651:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7652:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7653:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7654:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7655:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7656:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7657:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7658:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7659:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7660:\ttotal: 3m 39s\tremaining: 1m 7s\n",
      "7661:\ttotal: 3m 39s\tremaining: 1m 6s\n",
      "7662:\ttotal: 3m 39s\tremaining: 1m 6s\n",
      "7663:\ttotal: 3m 39s\tremaining: 1m 6s\n",
      "7664:\ttotal: 3m 39s\tremaining: 1m 6s\n",
      "7665:\ttotal: 3m 39s\tremaining: 1m 6s\n",
      "7666:\ttotal: 3m 39s\tremaining: 1m 6s\n",
      "7667:\ttotal: 3m 39s\tremaining: 1m 6s\n",
      "7668:\ttotal: 3m 39s\tremaining: 1m 6s\n",
      "7669:\ttotal: 3m 39s\tremaining: 1m 6s\n",
      "7670:\ttotal: 3m 39s\tremaining: 1m 6s\n",
      "7671:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7672:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7673:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7674:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7675:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7676:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7677:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7678:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7679:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7680:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7681:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7682:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7683:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7684:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7685:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7686:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7687:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7688:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7689:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7690:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7691:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7692:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7693:\ttotal: 3m 40s\tremaining: 1m 6s\n",
      "7694:\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "7695:\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "7696:\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "7697:\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "7698:\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "7699:\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "7700:\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "7701:\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "7702:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7703:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7704:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7705:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7706:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7707:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7708:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7709:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7710:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7711:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7712:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7713:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7714:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7715:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7716:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7717:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7718:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7719:\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "7720:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7721:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7722:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7723:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7724:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7725:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7726:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7727:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7728:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7729:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7730:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7731:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7732:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7733:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7734:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7735:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7736:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7737:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7738:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7739:\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "7740:\ttotal: 3m 42s\tremaining: 1m 4s\n",
      "7741:\ttotal: 3m 42s\tremaining: 1m 4s\n",
      "7742:\ttotal: 3m 42s\tremaining: 1m 4s\n",
      "7743:\ttotal: 3m 42s\tremaining: 1m 4s\n",
      "7744:\ttotal: 3m 42s\tremaining: 1m 4s\n",
      "7745:\ttotal: 3m 42s\tremaining: 1m 4s\n",
      "7746:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7747:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7748:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7749:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7750:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7751:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7752:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7753:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7754:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7755:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7756:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7757:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7758:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7759:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7760:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7761:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7762:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7763:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7764:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7765:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7766:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7767:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7768:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7769:\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "7770:\ttotal: 3m 44s\tremaining: 1m 4s\n",
      "7771:\ttotal: 3m 44s\tremaining: 1m 4s\n",
      "7772:\ttotal: 3m 44s\tremaining: 1m 4s\n",
      "7773:\ttotal: 3m 44s\tremaining: 1m 4s\n",
      "7774:\ttotal: 3m 44s\tremaining: 1m 4s\n",
      "7775:\ttotal: 3m 44s\tremaining: 1m 4s\n",
      "7776:\ttotal: 3m 44s\tremaining: 1m 4s\n",
      "7777:\ttotal: 3m 44s\tremaining: 1m 4s\n",
      "7778:\ttotal: 3m 44s\tremaining: 1m 4s\n",
      "7779:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7780:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7781:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7782:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7783:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7784:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7785:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7786:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7787:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7788:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7789:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7790:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7791:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7792:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7793:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7794:\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7795:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7796:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7797:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7798:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7799:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7800:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7801:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7802:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7803:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7804:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7805:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7806:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7807:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7808:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7809:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7810:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7811:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7812:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7813:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7814:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7815:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7816:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7817:\ttotal: 3m 45s\tremaining: 1m 3s\n",
      "7818:\ttotal: 3m 45s\tremaining: 1m 2s\n",
      "7819:\ttotal: 3m 45s\tremaining: 1m 2s\n",
      "7820:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7821:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7822:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7823:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7824:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7825:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7826:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7827:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7828:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7829:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7830:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7831:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7832:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7833:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7834:\ttotal: 3m 46s\tremaining: 1m 2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7835:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7836:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7837:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7838:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7839:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7840:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7841:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7842:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7843:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7844:\ttotal: 3m 46s\tremaining: 1m 2s\n",
      "7845:\ttotal: 3m 47s\tremaining: 1m 2s\n",
      "7846:\ttotal: 3m 47s\tremaining: 1m 2s\n",
      "7847:\ttotal: 3m 47s\tremaining: 1m 2s\n",
      "7848:\ttotal: 3m 47s\tremaining: 1m 2s\n",
      "7849:\ttotal: 3m 47s\tremaining: 1m 2s\n",
      "7850:\ttotal: 3m 47s\tremaining: 1m 2s\n",
      "7851:\ttotal: 3m 47s\tremaining: 1m 2s\n",
      "7852:\ttotal: 3m 47s\tremaining: 1m 2s\n",
      "7853:\ttotal: 3m 47s\tremaining: 1m 2s\n",
      "7854:\ttotal: 3m 47s\tremaining: 1m 2s\n",
      "7855:\ttotal: 3m 47s\tremaining: 1m 2s\n",
      "7856:\ttotal: 3m 47s\tremaining: 1m 2s\n",
      "7857:\ttotal: 3m 47s\tremaining: 1m 1s\n",
      "7858:\ttotal: 3m 47s\tremaining: 1m 1s\n",
      "7859:\ttotal: 3m 47s\tremaining: 1m 1s\n",
      "7860:\ttotal: 3m 47s\tremaining: 1m 1s\n",
      "7861:\ttotal: 3m 47s\tremaining: 1m 1s\n",
      "7862:\ttotal: 3m 47s\tremaining: 1m 1s\n",
      "7863:\ttotal: 3m 47s\tremaining: 1m 1s\n",
      "7864:\ttotal: 3m 47s\tremaining: 1m 1s\n",
      "7865:\ttotal: 3m 47s\tremaining: 1m 1s\n",
      "7866:\ttotal: 3m 47s\tremaining: 1m 1s\n",
      "7867:\ttotal: 3m 47s\tremaining: 1m 1s\n",
      "7868:\ttotal: 3m 47s\tremaining: 1m 1s\n",
      "7869:\ttotal: 3m 47s\tremaining: 1m 1s\n",
      "7870:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7871:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7872:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7873:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7874:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7875:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7876:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7877:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7878:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7879:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7880:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7881:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7882:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7883:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7884:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7885:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7886:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7887:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7888:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7889:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7890:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7891:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7892:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7893:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7894:\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "7895:\ttotal: 3m 48s\tremaining: 1m\n",
      "7896:\ttotal: 3m 49s\tremaining: 1m\n",
      "7897:\ttotal: 3m 49s\tremaining: 1m\n",
      "7898:\ttotal: 3m 49s\tremaining: 1m\n",
      "7899:\ttotal: 3m 49s\tremaining: 1m\n",
      "7900:\ttotal: 3m 49s\tremaining: 1m\n",
      "7901:\ttotal: 3m 49s\tremaining: 1m\n",
      "7902:\ttotal: 3m 49s\tremaining: 1m\n",
      "7903:\ttotal: 3m 49s\tremaining: 1m\n",
      "7904:\ttotal: 3m 49s\tremaining: 1m\n",
      "7905:\ttotal: 3m 49s\tremaining: 1m\n",
      "7906:\ttotal: 3m 49s\tremaining: 1m\n",
      "7907:\ttotal: 3m 49s\tremaining: 1m\n",
      "7908:\ttotal: 3m 49s\tremaining: 1m\n",
      "7909:\ttotal: 3m 49s\tremaining: 1m\n",
      "7910:\ttotal: 3m 49s\tremaining: 1m\n",
      "7911:\ttotal: 3m 49s\tremaining: 1m\n",
      "7912:\ttotal: 3m 49s\tremaining: 1m\n",
      "7913:\ttotal: 3m 49s\tremaining: 1m\n",
      "7914:\ttotal: 3m 49s\tremaining: 1m\n",
      "7915:\ttotal: 3m 49s\tremaining: 1m\n",
      "7916:\ttotal: 3m 49s\tremaining: 1m\n",
      "7917:\ttotal: 3m 49s\tremaining: 1m\n",
      "7918:\ttotal: 3m 49s\tremaining: 1m\n",
      "7919:\ttotal: 3m 49s\tremaining: 1m\n",
      "7920:\ttotal: 3m 50s\tremaining: 1m\n",
      "7921:\ttotal: 3m 50s\tremaining: 1m\n",
      "7922:\ttotal: 3m 50s\tremaining: 1m\n",
      "7923:\ttotal: 3m 50s\tremaining: 1m\n",
      "7924:\ttotal: 3m 50s\tremaining: 1m\n",
      "7925:\ttotal: 3m 50s\tremaining: 1m\n",
      "7926:\ttotal: 3m 50s\tremaining: 1m\n",
      "7927:\ttotal: 3m 50s\tremaining: 1m\n",
      "7928:\ttotal: 3m 50s\tremaining: 1m\n",
      "7929:\ttotal: 3m 50s\tremaining: 1m\n",
      "7930:\ttotal: 3m 50s\tremaining: 1m\n",
      "7931:\ttotal: 3m 50s\tremaining: 1m\n",
      "7932:\ttotal: 3m 50s\tremaining: 1m\n",
      "7933:\ttotal: 3m 50s\tremaining: 1m\n",
      "7934:\ttotal: 3m 50s\tremaining: 60s\n",
      "7935:\ttotal: 3m 50s\tremaining: 60s\n",
      "7936:\ttotal: 3m 50s\tremaining: 59.9s\n",
      "7937:\ttotal: 3m 50s\tremaining: 59.9s\n",
      "7938:\ttotal: 3m 50s\tremaining: 59.9s\n",
      "7939:\ttotal: 3m 50s\tremaining: 59.9s\n",
      "7940:\ttotal: 3m 50s\tremaining: 59.8s\n",
      "7941:\ttotal: 3m 50s\tremaining: 59.8s\n",
      "7942:\ttotal: 3m 50s\tremaining: 59.8s\n",
      "7943:\ttotal: 3m 50s\tremaining: 59.7s\n",
      "7944:\ttotal: 3m 50s\tremaining: 59.7s\n",
      "7945:\ttotal: 3m 50s\tremaining: 59.7s\n",
      "7946:\ttotal: 3m 51s\tremaining: 59.6s\n",
      "7947:\ttotal: 3m 51s\tremaining: 59.6s\n",
      "7948:\ttotal: 3m 51s\tremaining: 59.6s\n",
      "7949:\ttotal: 3m 51s\tremaining: 59.6s\n",
      "7950:\ttotal: 3m 51s\tremaining: 59.5s\n",
      "7951:\ttotal: 3m 51s\tremaining: 59.5s\n",
      "7952:\ttotal: 3m 51s\tremaining: 59.5s\n",
      "7953:\ttotal: 3m 51s\tremaining: 59.4s\n",
      "7954:\ttotal: 3m 51s\tremaining: 59.4s\n",
      "7955:\ttotal: 3m 51s\tremaining: 59.4s\n",
      "7956:\ttotal: 3m 51s\tremaining: 59.4s\n",
      "7957:\ttotal: 3m 51s\tremaining: 59.3s\n",
      "7958:\ttotal: 3m 51s\tremaining: 59.3s\n",
      "7959:\ttotal: 3m 51s\tremaining: 59.3s\n",
      "7960:\ttotal: 3m 51s\tremaining: 59.3s\n",
      "7961:\ttotal: 3m 51s\tremaining: 59.2s\n",
      "7962:\ttotal: 3m 51s\tremaining: 59.2s\n",
      "7963:\ttotal: 3m 51s\tremaining: 59.2s\n",
      "7964:\ttotal: 3m 51s\tremaining: 59.1s\n",
      "7965:\ttotal: 3m 51s\tremaining: 59.1s\n",
      "7966:\ttotal: 3m 51s\tremaining: 59.1s\n",
      "7967:\ttotal: 3m 51s\tremaining: 59.1s\n",
      "7968:\ttotal: 3m 51s\tremaining: 59s\n",
      "7969:\ttotal: 3m 51s\tremaining: 59s\n",
      "7970:\ttotal: 3m 51s\tremaining: 59s\n",
      "7971:\ttotal: 3m 51s\tremaining: 58.9s\n",
      "7972:\ttotal: 3m 51s\tremaining: 58.9s\n",
      "7973:\ttotal: 3m 51s\tremaining: 58.9s\n",
      "7974:\ttotal: 3m 51s\tremaining: 58.9s\n",
      "7975:\ttotal: 3m 51s\tremaining: 58.8s\n",
      "7976:\ttotal: 3m 51s\tremaining: 58.8s\n",
      "7977:\ttotal: 3m 52s\tremaining: 58.8s\n",
      "7978:\ttotal: 3m 52s\tremaining: 58.8s\n",
      "7979:\ttotal: 3m 52s\tremaining: 58.7s\n",
      "7980:\ttotal: 3m 52s\tremaining: 58.7s\n",
      "7981:\ttotal: 3m 52s\tremaining: 58.7s\n",
      "7982:\ttotal: 3m 52s\tremaining: 58.6s\n",
      "7983:\ttotal: 3m 52s\tremaining: 58.6s\n",
      "7984:\ttotal: 3m 52s\tremaining: 58.6s\n",
      "7985:\ttotal: 3m 52s\tremaining: 58.6s\n",
      "7986:\ttotal: 3m 52s\tremaining: 58.5s\n",
      "7987:\ttotal: 3m 52s\tremaining: 58.5s\n",
      "7988:\ttotal: 3m 52s\tremaining: 58.5s\n",
      "7989:\ttotal: 3m 52s\tremaining: 58.4s\n",
      "7990:\ttotal: 3m 52s\tremaining: 58.4s\n",
      "7991:\ttotal: 3m 52s\tremaining: 58.4s\n",
      "7992:\ttotal: 3m 52s\tremaining: 58.4s\n",
      "7993:\ttotal: 3m 52s\tremaining: 58.3s\n",
      "7994:\ttotal: 3m 52s\tremaining: 58.3s\n",
      "7995:\ttotal: 3m 52s\tremaining: 58.3s\n",
      "7996:\ttotal: 3m 52s\tremaining: 58.2s\n",
      "7997:\ttotal: 3m 52s\tremaining: 58.2s\n",
      "7998:\ttotal: 3m 52s\tremaining: 58.2s\n",
      "7999:\ttotal: 3m 52s\tremaining: 58.2s\n",
      "8000:\ttotal: 3m 52s\tremaining: 58.1s\n",
      "8001:\ttotal: 3m 52s\tremaining: 58.1s\n",
      "8002:\ttotal: 3m 52s\tremaining: 58.1s\n",
      "8003:\ttotal: 3m 52s\tremaining: 58s\n",
      "8004:\ttotal: 3m 52s\tremaining: 58s\n",
      "8005:\ttotal: 3m 52s\tremaining: 58s\n",
      "8006:\ttotal: 3m 52s\tremaining: 58s\n",
      "8007:\ttotal: 3m 52s\tremaining: 57.9s\n",
      "8008:\ttotal: 3m 53s\tremaining: 57.9s\n",
      "8009:\ttotal: 3m 53s\tremaining: 57.9s\n",
      "8010:\ttotal: 3m 53s\tremaining: 57.8s\n",
      "8011:\ttotal: 3m 53s\tremaining: 57.8s\n",
      "8012:\ttotal: 3m 53s\tremaining: 57.8s\n",
      "8013:\ttotal: 3m 53s\tremaining: 57.8s\n",
      "8014:\ttotal: 3m 53s\tremaining: 57.7s\n",
      "8015:\ttotal: 3m 53s\tremaining: 57.7s\n",
      "8016:\ttotal: 3m 53s\tremaining: 57.7s\n",
      "8017:\ttotal: 3m 53s\tremaining: 57.7s\n",
      "8018:\ttotal: 3m 53s\tremaining: 57.6s\n",
      "8019:\ttotal: 3m 53s\tremaining: 57.6s\n",
      "8020:\ttotal: 3m 53s\tremaining: 57.6s\n",
      "8021:\ttotal: 3m 53s\tremaining: 57.5s\n",
      "8022:\ttotal: 3m 53s\tremaining: 57.5s\n",
      "8023:\ttotal: 3m 53s\tremaining: 57.5s\n",
      "8024:\ttotal: 3m 53s\tremaining: 57.4s\n",
      "8025:\ttotal: 3m 53s\tremaining: 57.4s\n",
      "8026:\ttotal: 3m 53s\tremaining: 57.4s\n",
      "8027:\ttotal: 3m 53s\tremaining: 57.4s\n",
      "8028:\ttotal: 3m 53s\tremaining: 57.3s\n",
      "8029:\ttotal: 3m 53s\tremaining: 57.3s\n",
      "8030:\ttotal: 3m 53s\tremaining: 57.3s\n",
      "8031:\ttotal: 3m 53s\tremaining: 57.3s\n",
      "8032:\ttotal: 3m 53s\tremaining: 57.2s\n",
      "8033:\ttotal: 3m 53s\tremaining: 57.2s\n",
      "8034:\ttotal: 3m 53s\tremaining: 57.2s\n",
      "8035:\ttotal: 3m 53s\tremaining: 57.1s\n",
      "8036:\ttotal: 3m 53s\tremaining: 57.1s\n",
      "8037:\ttotal: 3m 53s\tremaining: 57.1s\n",
      "8038:\ttotal: 3m 53s\tremaining: 57s\n",
      "8039:\ttotal: 3m 54s\tremaining: 57s\n",
      "8040:\ttotal: 3m 54s\tremaining: 57s\n",
      "8041:\ttotal: 3m 54s\tremaining: 57s\n",
      "8042:\ttotal: 3m 54s\tremaining: 56.9s\n",
      "8043:\ttotal: 3m 54s\tremaining: 56.9s\n",
      "8044:\ttotal: 3m 54s\tremaining: 56.9s\n",
      "8045:\ttotal: 3m 54s\tremaining: 56.8s\n",
      "8046:\ttotal: 3m 54s\tremaining: 56.8s\n",
      "8047:\ttotal: 3m 54s\tremaining: 56.8s\n",
      "8048:\ttotal: 3m 54s\tremaining: 56.8s\n",
      "8049:\ttotal: 3m 54s\tremaining: 56.7s\n",
      "8050:\ttotal: 3m 54s\tremaining: 56.7s\n",
      "8051:\ttotal: 3m 54s\tremaining: 56.7s\n",
      "8052:\ttotal: 3m 54s\tremaining: 56.6s\n",
      "8053:\ttotal: 3m 54s\tremaining: 56.6s\n",
      "8054:\ttotal: 3m 54s\tremaining: 56.6s\n",
      "8055:\ttotal: 3m 54s\tremaining: 56.6s\n",
      "8056:\ttotal: 3m 54s\tremaining: 56.5s\n",
      "8057:\ttotal: 3m 54s\tremaining: 56.5s\n",
      "8058:\ttotal: 3m 54s\tremaining: 56.5s\n",
      "8059:\ttotal: 3m 54s\tremaining: 56.4s\n",
      "8060:\ttotal: 3m 54s\tremaining: 56.4s\n",
      "8061:\ttotal: 3m 54s\tremaining: 56.4s\n",
      "8062:\ttotal: 3m 54s\tremaining: 56.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8063:\ttotal: 3m 54s\tremaining: 56.3s\n",
      "8064:\ttotal: 3m 54s\tremaining: 56.3s\n",
      "8065:\ttotal: 3m 54s\tremaining: 56.3s\n",
      "8066:\ttotal: 3m 54s\tremaining: 56.2s\n",
      "8067:\ttotal: 3m 54s\tremaining: 56.2s\n",
      "8068:\ttotal: 3m 54s\tremaining: 56.2s\n",
      "8069:\ttotal: 3m 54s\tremaining: 56.2s\n",
      "8070:\ttotal: 3m 54s\tremaining: 56.1s\n",
      "8071:\ttotal: 3m 55s\tremaining: 56.1s\n",
      "8072:\ttotal: 3m 55s\tremaining: 56.1s\n",
      "8073:\ttotal: 3m 55s\tremaining: 56s\n",
      "8074:\ttotal: 3m 55s\tremaining: 56s\n",
      "8075:\ttotal: 3m 55s\tremaining: 56s\n",
      "8076:\ttotal: 3m 55s\tremaining: 56s\n",
      "8077:\ttotal: 3m 55s\tremaining: 55.9s\n",
      "8078:\ttotal: 3m 55s\tremaining: 55.9s\n",
      "8079:\ttotal: 3m 55s\tremaining: 55.9s\n",
      "8080:\ttotal: 3m 55s\tremaining: 55.8s\n",
      "8081:\ttotal: 3m 55s\tremaining: 55.8s\n",
      "8082:\ttotal: 3m 55s\tremaining: 55.8s\n",
      "8083:\ttotal: 3m 55s\tremaining: 55.7s\n",
      "8084:\ttotal: 3m 55s\tremaining: 55.7s\n",
      "8085:\ttotal: 3m 55s\tremaining: 55.7s\n",
      "8086:\ttotal: 3m 55s\tremaining: 55.7s\n",
      "8087:\ttotal: 3m 55s\tremaining: 55.6s\n",
      "8088:\ttotal: 3m 55s\tremaining: 55.6s\n",
      "8089:\ttotal: 3m 55s\tremaining: 55.6s\n",
      "8090:\ttotal: 3m 55s\tremaining: 55.5s\n",
      "8091:\ttotal: 3m 55s\tremaining: 55.5s\n",
      "8092:\ttotal: 3m 55s\tremaining: 55.5s\n",
      "8093:\ttotal: 3m 55s\tremaining: 55.4s\n",
      "8094:\ttotal: 3m 55s\tremaining: 55.4s\n",
      "8095:\ttotal: 3m 55s\tremaining: 55.4s\n",
      "8096:\ttotal: 3m 55s\tremaining: 55.4s\n",
      "8097:\ttotal: 3m 55s\tremaining: 55.3s\n",
      "8098:\ttotal: 3m 55s\tremaining: 55.3s\n",
      "8099:\ttotal: 3m 55s\tremaining: 55.3s\n",
      "8100:\ttotal: 3m 55s\tremaining: 55.2s\n",
      "8101:\ttotal: 3m 55s\tremaining: 55.2s\n",
      "8102:\ttotal: 3m 55s\tremaining: 55.2s\n",
      "8103:\ttotal: 3m 55s\tremaining: 55.1s\n",
      "8104:\ttotal: 3m 55s\tremaining: 55.1s\n",
      "8105:\ttotal: 3m 55s\tremaining: 55.1s\n",
      "8106:\ttotal: 3m 55s\tremaining: 55.1s\n",
      "8107:\ttotal: 3m 55s\tremaining: 55s\n",
      "8108:\ttotal: 3m 55s\tremaining: 55s\n",
      "8109:\ttotal: 3m 55s\tremaining: 55s\n",
      "8110:\ttotal: 3m 55s\tremaining: 54.9s\n",
      "8111:\ttotal: 3m 56s\tremaining: 54.9s\n",
      "8112:\ttotal: 3m 56s\tremaining: 54.9s\n",
      "8113:\ttotal: 3m 56s\tremaining: 54.8s\n",
      "8114:\ttotal: 3m 56s\tremaining: 54.8s\n",
      "8115:\ttotal: 3m 56s\tremaining: 54.8s\n",
      "8116:\ttotal: 3m 56s\tremaining: 54.8s\n",
      "8117:\ttotal: 3m 56s\tremaining: 54.7s\n",
      "8118:\ttotal: 3m 56s\tremaining: 54.7s\n",
      "8119:\ttotal: 3m 56s\tremaining: 54.7s\n",
      "8120:\ttotal: 3m 56s\tremaining: 54.6s\n",
      "8121:\ttotal: 3m 56s\tremaining: 54.6s\n",
      "8122:\ttotal: 3m 56s\tremaining: 54.6s\n",
      "8123:\ttotal: 3m 56s\tremaining: 54.5s\n",
      "8124:\ttotal: 3m 56s\tremaining: 54.5s\n",
      "8125:\ttotal: 3m 56s\tremaining: 54.5s\n",
      "8126:\ttotal: 3m 56s\tremaining: 54.4s\n",
      "8127:\ttotal: 3m 56s\tremaining: 54.4s\n",
      "8128:\ttotal: 3m 56s\tremaining: 54.4s\n",
      "8129:\ttotal: 3m 56s\tremaining: 54.3s\n",
      "8130:\ttotal: 3m 56s\tremaining: 54.3s\n",
      "8131:\ttotal: 3m 56s\tremaining: 54.3s\n",
      "8132:\ttotal: 3m 56s\tremaining: 54.3s\n",
      "8133:\ttotal: 3m 56s\tremaining: 54.2s\n",
      "8134:\ttotal: 3m 56s\tremaining: 54.2s\n",
      "8135:\ttotal: 3m 56s\tremaining: 54.2s\n",
      "8136:\ttotal: 3m 56s\tremaining: 54.1s\n",
      "8137:\ttotal: 3m 56s\tremaining: 54.1s\n",
      "8138:\ttotal: 3m 56s\tremaining: 54.1s\n",
      "8139:\ttotal: 3m 56s\tremaining: 54.1s\n",
      "8140:\ttotal: 3m 56s\tremaining: 54s\n",
      "8141:\ttotal: 3m 56s\tremaining: 54s\n",
      "8142:\ttotal: 3m 56s\tremaining: 54s\n",
      "8143:\ttotal: 3m 56s\tremaining: 53.9s\n",
      "8144:\ttotal: 3m 56s\tremaining: 53.9s\n",
      "8145:\ttotal: 3m 56s\tremaining: 53.9s\n",
      "8146:\ttotal: 3m 56s\tremaining: 53.9s\n",
      "8147:\ttotal: 3m 56s\tremaining: 53.8s\n",
      "8148:\ttotal: 3m 57s\tremaining: 53.8s\n",
      "8149:\ttotal: 3m 57s\tremaining: 53.8s\n",
      "8150:\ttotal: 3m 57s\tremaining: 53.8s\n",
      "8151:\ttotal: 3m 57s\tremaining: 53.7s\n",
      "8152:\ttotal: 3m 57s\tremaining: 53.7s\n",
      "8153:\ttotal: 3m 57s\tremaining: 53.7s\n",
      "8154:\ttotal: 3m 57s\tremaining: 53.7s\n",
      "8155:\ttotal: 3m 57s\tremaining: 53.6s\n",
      "8156:\ttotal: 3m 57s\tremaining: 53.6s\n",
      "8157:\ttotal: 3m 57s\tremaining: 53.6s\n",
      "8158:\ttotal: 3m 57s\tremaining: 53.5s\n",
      "8159:\ttotal: 3m 57s\tremaining: 53.5s\n",
      "8160:\ttotal: 3m 57s\tremaining: 53.5s\n",
      "8161:\ttotal: 3m 57s\tremaining: 53.5s\n",
      "8162:\ttotal: 3m 57s\tremaining: 53.4s\n",
      "8163:\ttotal: 3m 57s\tremaining: 53.4s\n",
      "8164:\ttotal: 3m 57s\tremaining: 53.4s\n",
      "8165:\ttotal: 3m 57s\tremaining: 53.3s\n",
      "8166:\ttotal: 3m 57s\tremaining: 53.3s\n",
      "8167:\ttotal: 3m 57s\tremaining: 53.3s\n",
      "8168:\ttotal: 3m 57s\tremaining: 53.3s\n",
      "8169:\ttotal: 3m 57s\tremaining: 53.2s\n",
      "8170:\ttotal: 3m 57s\tremaining: 53.2s\n",
      "8171:\ttotal: 3m 57s\tremaining: 53.2s\n",
      "8172:\ttotal: 3m 57s\tremaining: 53.1s\n",
      "8173:\ttotal: 3m 57s\tremaining: 53.1s\n",
      "8174:\ttotal: 3m 57s\tremaining: 53.1s\n",
      "8175:\ttotal: 3m 58s\tremaining: 53.1s\n",
      "8176:\ttotal: 3m 58s\tremaining: 53s\n",
      "8177:\ttotal: 3m 58s\tremaining: 53s\n",
      "8178:\ttotal: 3m 58s\tremaining: 53s\n",
      "8179:\ttotal: 3m 58s\tremaining: 53s\n",
      "8180:\ttotal: 3m 58s\tremaining: 52.9s\n",
      "8181:\ttotal: 3m 58s\tremaining: 52.9s\n",
      "8182:\ttotal: 3m 58s\tremaining: 52.9s\n",
      "8183:\ttotal: 3m 58s\tremaining: 52.9s\n",
      "8184:\ttotal: 3m 58s\tremaining: 52.8s\n",
      "8185:\ttotal: 3m 58s\tremaining: 52.8s\n",
      "8186:\ttotal: 3m 58s\tremaining: 52.8s\n",
      "8187:\ttotal: 3m 58s\tremaining: 52.8s\n",
      "8188:\ttotal: 3m 58s\tremaining: 52.7s\n",
      "8189:\ttotal: 3m 58s\tremaining: 52.7s\n",
      "8190:\ttotal: 3m 58s\tremaining: 52.7s\n",
      "8191:\ttotal: 3m 58s\tremaining: 52.6s\n",
      "8192:\ttotal: 3m 58s\tremaining: 52.6s\n",
      "8193:\ttotal: 3m 58s\tremaining: 52.6s\n",
      "8194:\ttotal: 3m 58s\tremaining: 52.6s\n",
      "8195:\ttotal: 3m 58s\tremaining: 52.5s\n",
      "8196:\ttotal: 3m 58s\tremaining: 52.5s\n",
      "8197:\ttotal: 3m 58s\tremaining: 52.5s\n",
      "8198:\ttotal: 3m 58s\tremaining: 52.5s\n",
      "8199:\ttotal: 3m 58s\tremaining: 52.4s\n",
      "8200:\ttotal: 3m 58s\tremaining: 52.4s\n",
      "8201:\ttotal: 3m 59s\tremaining: 52.4s\n",
      "8202:\ttotal: 3m 59s\tremaining: 52.3s\n",
      "8203:\ttotal: 3m 59s\tremaining: 52.3s\n",
      "8204:\ttotal: 3m 59s\tremaining: 52.3s\n",
      "8205:\ttotal: 3m 59s\tremaining: 52.3s\n",
      "8206:\ttotal: 3m 59s\tremaining: 52.2s\n",
      "8207:\ttotal: 3m 59s\tremaining: 52.2s\n",
      "8208:\ttotal: 3m 59s\tremaining: 52.2s\n",
      "8209:\ttotal: 3m 59s\tremaining: 52.1s\n",
      "8210:\ttotal: 3m 59s\tremaining: 52.1s\n",
      "8211:\ttotal: 3m 59s\tremaining: 52.1s\n",
      "8212:\ttotal: 3m 59s\tremaining: 52s\n",
      "8213:\ttotal: 3m 59s\tremaining: 52s\n",
      "8214:\ttotal: 3m 59s\tremaining: 52s\n",
      "8215:\ttotal: 3m 59s\tremaining: 52s\n",
      "8216:\ttotal: 3m 59s\tremaining: 51.9s\n",
      "8217:\ttotal: 3m 59s\tremaining: 51.9s\n",
      "8218:\ttotal: 3m 59s\tremaining: 51.9s\n",
      "8219:\ttotal: 3m 59s\tremaining: 51.8s\n",
      "8220:\ttotal: 3m 59s\tremaining: 51.8s\n",
      "8221:\ttotal: 3m 59s\tremaining: 51.8s\n",
      "8222:\ttotal: 3m 59s\tremaining: 51.7s\n",
      "8223:\ttotal: 3m 59s\tremaining: 51.7s\n",
      "8224:\ttotal: 3m 59s\tremaining: 51.7s\n",
      "8225:\ttotal: 3m 59s\tremaining: 51.6s\n",
      "8226:\ttotal: 3m 59s\tremaining: 51.6s\n",
      "8227:\ttotal: 3m 59s\tremaining: 51.6s\n",
      "8228:\ttotal: 3m 59s\tremaining: 51.6s\n",
      "8229:\ttotal: 3m 59s\tremaining: 51.5s\n",
      "8230:\ttotal: 3m 59s\tremaining: 51.5s\n",
      "8231:\ttotal: 3m 59s\tremaining: 51.5s\n",
      "8232:\ttotal: 3m 59s\tremaining: 51.4s\n",
      "8233:\ttotal: 3m 59s\tremaining: 51.4s\n",
      "8234:\ttotal: 3m 59s\tremaining: 51.4s\n",
      "8235:\ttotal: 3m 59s\tremaining: 51.4s\n",
      "8236:\ttotal: 3m 59s\tremaining: 51.3s\n",
      "8237:\ttotal: 4m\tremaining: 51.3s\n",
      "8238:\ttotal: 4m\tremaining: 51.3s\n",
      "8239:\ttotal: 4m\tremaining: 51.2s\n",
      "8240:\ttotal: 4m\tremaining: 51.2s\n",
      "8241:\ttotal: 4m\tremaining: 51.2s\n",
      "8242:\ttotal: 4m\tremaining: 51.2s\n",
      "8243:\ttotal: 4m\tremaining: 51.1s\n",
      "8244:\ttotal: 4m\tremaining: 51.1s\n",
      "8245:\ttotal: 4m\tremaining: 51.1s\n",
      "8246:\ttotal: 4m\tremaining: 51s\n",
      "8247:\ttotal: 4m\tremaining: 51s\n",
      "8248:\ttotal: 4m\tremaining: 51s\n",
      "8249:\ttotal: 4m\tremaining: 50.9s\n",
      "8250:\ttotal: 4m\tremaining: 50.9s\n",
      "8251:\ttotal: 4m\tremaining: 50.9s\n",
      "8252:\ttotal: 4m\tremaining: 50.9s\n",
      "8253:\ttotal: 4m\tremaining: 50.8s\n",
      "8254:\ttotal: 4m\tremaining: 50.8s\n",
      "8255:\ttotal: 4m\tremaining: 50.8s\n",
      "8256:\ttotal: 4m\tremaining: 50.7s\n",
      "8257:\ttotal: 4m\tremaining: 50.7s\n",
      "8258:\ttotal: 4m\tremaining: 50.7s\n",
      "8259:\ttotal: 4m\tremaining: 50.6s\n",
      "8260:\ttotal: 4m\tremaining: 50.6s\n",
      "8261:\ttotal: 4m\tremaining: 50.6s\n",
      "8262:\ttotal: 4m\tremaining: 50.6s\n",
      "8263:\ttotal: 4m\tremaining: 50.5s\n",
      "8264:\ttotal: 4m\tremaining: 50.5s\n",
      "8265:\ttotal: 4m\tremaining: 50.5s\n",
      "8266:\ttotal: 4m\tremaining: 50.4s\n",
      "8267:\ttotal: 4m\tremaining: 50.4s\n",
      "8268:\ttotal: 4m\tremaining: 50.4s\n",
      "8269:\ttotal: 4m\tremaining: 50.3s\n",
      "8270:\ttotal: 4m\tremaining: 50.3s\n",
      "8271:\ttotal: 4m\tremaining: 50.3s\n",
      "8272:\ttotal: 4m\tremaining: 50.3s\n",
      "8273:\ttotal: 4m\tremaining: 50.2s\n",
      "8274:\ttotal: 4m\tremaining: 50.2s\n",
      "8275:\ttotal: 4m\tremaining: 50.2s\n",
      "8276:\ttotal: 4m 1s\tremaining: 50.1s\n",
      "8277:\ttotal: 4m 1s\tremaining: 50.1s\n",
      "8278:\ttotal: 4m 1s\tremaining: 50.1s\n",
      "8279:\ttotal: 4m 1s\tremaining: 50.1s\n",
      "8280:\ttotal: 4m 1s\tremaining: 50s\n",
      "8281:\ttotal: 4m 1s\tremaining: 50s\n",
      "8282:\ttotal: 4m 1s\tremaining: 50s\n",
      "8283:\ttotal: 4m 1s\tremaining: 49.9s\n",
      "8284:\ttotal: 4m 1s\tremaining: 49.9s\n",
      "8285:\ttotal: 4m 1s\tremaining: 49.9s\n",
      "8286:\ttotal: 4m 1s\tremaining: 49.8s\n",
      "8287:\ttotal: 4m 1s\tremaining: 49.8s\n",
      "8288:\ttotal: 4m 1s\tremaining: 49.8s\n",
      "8289:\ttotal: 4m 1s\tremaining: 49.8s\n",
      "8290:\ttotal: 4m 1s\tremaining: 49.7s\n",
      "8291:\ttotal: 4m 1s\tremaining: 49.7s\n",
      "8292:\ttotal: 4m 1s\tremaining: 49.7s\n",
      "8293:\ttotal: 4m 1s\tremaining: 49.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8294:\ttotal: 4m 1s\tremaining: 49.6s\n",
      "8295:\ttotal: 4m 1s\tremaining: 49.6s\n",
      "8296:\ttotal: 4m 1s\tremaining: 49.6s\n",
      "8297:\ttotal: 4m 1s\tremaining: 49.5s\n",
      "8298:\ttotal: 4m 1s\tremaining: 49.5s\n",
      "8299:\ttotal: 4m 1s\tremaining: 49.5s\n",
      "8300:\ttotal: 4m 1s\tremaining: 49.4s\n",
      "8301:\ttotal: 4m 1s\tremaining: 49.4s\n",
      "8302:\ttotal: 4m 1s\tremaining: 49.4s\n",
      "8303:\ttotal: 4m 1s\tremaining: 49.3s\n",
      "8304:\ttotal: 4m 1s\tremaining: 49.3s\n",
      "8305:\ttotal: 4m 1s\tremaining: 49.3s\n",
      "8306:\ttotal: 4m 1s\tremaining: 49.3s\n",
      "8307:\ttotal: 4m 1s\tremaining: 49.2s\n",
      "8308:\ttotal: 4m 1s\tremaining: 49.2s\n",
      "8309:\ttotal: 4m 1s\tremaining: 49.2s\n",
      "8310:\ttotal: 4m 1s\tremaining: 49.1s\n",
      "8311:\ttotal: 4m 1s\tremaining: 49.1s\n",
      "8312:\ttotal: 4m 2s\tremaining: 49.1s\n",
      "8313:\ttotal: 4m 2s\tremaining: 49.1s\n",
      "8314:\ttotal: 4m 2s\tremaining: 49s\n",
      "8315:\ttotal: 4m 2s\tremaining: 49s\n",
      "8316:\ttotal: 4m 2s\tremaining: 49s\n",
      "8317:\ttotal: 4m 2s\tremaining: 48.9s\n",
      "8318:\ttotal: 4m 2s\tremaining: 48.9s\n",
      "8319:\ttotal: 4m 2s\tremaining: 48.9s\n",
      "8320:\ttotal: 4m 2s\tremaining: 48.8s\n",
      "8321:\ttotal: 4m 2s\tremaining: 48.8s\n",
      "8322:\ttotal: 4m 2s\tremaining: 48.8s\n",
      "8323:\ttotal: 4m 2s\tremaining: 48.8s\n",
      "8324:\ttotal: 4m 2s\tremaining: 48.7s\n",
      "8325:\ttotal: 4m 2s\tremaining: 48.7s\n",
      "8326:\ttotal: 4m 2s\tremaining: 48.7s\n",
      "8327:\ttotal: 4m 2s\tremaining: 48.6s\n",
      "8328:\ttotal: 4m 2s\tremaining: 48.6s\n",
      "8329:\ttotal: 4m 2s\tremaining: 48.6s\n",
      "8330:\ttotal: 4m 2s\tremaining: 48.5s\n",
      "8331:\ttotal: 4m 2s\tremaining: 48.5s\n",
      "8332:\ttotal: 4m 2s\tremaining: 48.5s\n",
      "8333:\ttotal: 4m 2s\tremaining: 48.5s\n",
      "8334:\ttotal: 4m 2s\tremaining: 48.4s\n",
      "8335:\ttotal: 4m 2s\tremaining: 48.4s\n",
      "8336:\ttotal: 4m 2s\tremaining: 48.4s\n",
      "8337:\ttotal: 4m 2s\tremaining: 48.4s\n",
      "8338:\ttotal: 4m 2s\tremaining: 48.3s\n",
      "8339:\ttotal: 4m 2s\tremaining: 48.3s\n",
      "8340:\ttotal: 4m 2s\tremaining: 48.3s\n",
      "8341:\ttotal: 4m 2s\tremaining: 48.2s\n",
      "8342:\ttotal: 4m 2s\tremaining: 48.2s\n",
      "8343:\ttotal: 4m 2s\tremaining: 48.2s\n",
      "8344:\ttotal: 4m 2s\tremaining: 48.2s\n",
      "8345:\ttotal: 4m 3s\tremaining: 48.1s\n",
      "8346:\ttotal: 4m 3s\tremaining: 48.1s\n",
      "8347:\ttotal: 4m 3s\tremaining: 48.1s\n",
      "8348:\ttotal: 4m 3s\tremaining: 48s\n",
      "8349:\ttotal: 4m 3s\tremaining: 48s\n",
      "8350:\ttotal: 4m 3s\tremaining: 48s\n",
      "8351:\ttotal: 4m 3s\tremaining: 48s\n",
      "8352:\ttotal: 4m 3s\tremaining: 47.9s\n",
      "8353:\ttotal: 4m 3s\tremaining: 47.9s\n",
      "8354:\ttotal: 4m 3s\tremaining: 47.9s\n",
      "8355:\ttotal: 4m 3s\tremaining: 47.9s\n",
      "8356:\ttotal: 4m 3s\tremaining: 47.8s\n",
      "8357:\ttotal: 4m 3s\tremaining: 47.8s\n",
      "8358:\ttotal: 4m 3s\tremaining: 47.8s\n",
      "8359:\ttotal: 4m 3s\tremaining: 47.7s\n",
      "8360:\ttotal: 4m 3s\tremaining: 47.7s\n",
      "8361:\ttotal: 4m 3s\tremaining: 47.7s\n",
      "8362:\ttotal: 4m 3s\tremaining: 47.7s\n",
      "8363:\ttotal: 4m 3s\tremaining: 47.6s\n",
      "8364:\ttotal: 4m 3s\tremaining: 47.6s\n",
      "8365:\ttotal: 4m 3s\tremaining: 47.6s\n",
      "8366:\ttotal: 4m 3s\tremaining: 47.5s\n",
      "8367:\ttotal: 4m 3s\tremaining: 47.5s\n",
      "8368:\ttotal: 4m 3s\tremaining: 47.5s\n",
      "8369:\ttotal: 4m 3s\tremaining: 47.5s\n",
      "8370:\ttotal: 4m 3s\tremaining: 47.4s\n",
      "8371:\ttotal: 4m 3s\tremaining: 47.4s\n",
      "8372:\ttotal: 4m 4s\tremaining: 47.4s\n",
      "8373:\ttotal: 4m 4s\tremaining: 47.4s\n",
      "8374:\ttotal: 4m 4s\tremaining: 47.3s\n",
      "8375:\ttotal: 4m 4s\tremaining: 47.3s\n",
      "8376:\ttotal: 4m 4s\tremaining: 47.3s\n",
      "8377:\ttotal: 4m 4s\tremaining: 47.3s\n",
      "8378:\ttotal: 4m 4s\tremaining: 47.2s\n",
      "8379:\ttotal: 4m 4s\tremaining: 47.2s\n",
      "8380:\ttotal: 4m 4s\tremaining: 47.2s\n",
      "8381:\ttotal: 4m 4s\tremaining: 47.1s\n",
      "8382:\ttotal: 4m 4s\tremaining: 47.1s\n",
      "8383:\ttotal: 4m 4s\tremaining: 47.1s\n",
      "8384:\ttotal: 4m 4s\tremaining: 47.1s\n",
      "8385:\ttotal: 4m 4s\tremaining: 47s\n",
      "8386:\ttotal: 4m 4s\tremaining: 47s\n",
      "8387:\ttotal: 4m 4s\tremaining: 47s\n",
      "8388:\ttotal: 4m 4s\tremaining: 47s\n",
      "8389:\ttotal: 4m 4s\tremaining: 46.9s\n",
      "8390:\ttotal: 4m 4s\tremaining: 46.9s\n",
      "8391:\ttotal: 4m 4s\tremaining: 46.9s\n",
      "8392:\ttotal: 4m 4s\tremaining: 46.9s\n",
      "8393:\ttotal: 4m 4s\tremaining: 46.8s\n",
      "8394:\ttotal: 4m 4s\tremaining: 46.8s\n",
      "8395:\ttotal: 4m 4s\tremaining: 46.8s\n",
      "8396:\ttotal: 4m 5s\tremaining: 46.7s\n",
      "8397:\ttotal: 4m 5s\tremaining: 46.7s\n",
      "8398:\ttotal: 4m 5s\tremaining: 46.7s\n",
      "8399:\ttotal: 4m 5s\tremaining: 46.7s\n",
      "8400:\ttotal: 4m 5s\tremaining: 46.6s\n",
      "8401:\ttotal: 4m 5s\tremaining: 46.6s\n",
      "8402:\ttotal: 4m 5s\tremaining: 46.6s\n",
      "8403:\ttotal: 4m 5s\tremaining: 46.6s\n",
      "8404:\ttotal: 4m 5s\tremaining: 46.5s\n",
      "8405:\ttotal: 4m 5s\tremaining: 46.5s\n",
      "8406:\ttotal: 4m 5s\tremaining: 46.5s\n",
      "8407:\ttotal: 4m 5s\tremaining: 46.4s\n",
      "8408:\ttotal: 4m 5s\tremaining: 46.4s\n",
      "8409:\ttotal: 4m 5s\tremaining: 46.4s\n",
      "8410:\ttotal: 4m 5s\tremaining: 46.4s\n",
      "8411:\ttotal: 4m 5s\tremaining: 46.3s\n",
      "8412:\ttotal: 4m 5s\tremaining: 46.3s\n",
      "8413:\ttotal: 4m 5s\tremaining: 46.3s\n",
      "8414:\ttotal: 4m 5s\tremaining: 46.2s\n",
      "8415:\ttotal: 4m 5s\tremaining: 46.2s\n",
      "8416:\ttotal: 4m 5s\tremaining: 46.2s\n",
      "8417:\ttotal: 4m 5s\tremaining: 46.2s\n",
      "8418:\ttotal: 4m 5s\tremaining: 46.1s\n",
      "8419:\ttotal: 4m 5s\tremaining: 46.1s\n",
      "8420:\ttotal: 4m 5s\tremaining: 46.1s\n",
      "8421:\ttotal: 4m 5s\tremaining: 46s\n",
      "8422:\ttotal: 4m 5s\tremaining: 46s\n",
      "8423:\ttotal: 4m 5s\tremaining: 46s\n",
      "8424:\ttotal: 4m 5s\tremaining: 45.9s\n",
      "8425:\ttotal: 4m 5s\tremaining: 45.9s\n",
      "8426:\ttotal: 4m 5s\tremaining: 45.9s\n",
      "8427:\ttotal: 4m 5s\tremaining: 45.8s\n",
      "8428:\ttotal: 4m 6s\tremaining: 45.8s\n",
      "8429:\ttotal: 4m 6s\tremaining: 45.8s\n",
      "8430:\ttotal: 4m 6s\tremaining: 45.8s\n",
      "8431:\ttotal: 4m 6s\tremaining: 45.7s\n",
      "8432:\ttotal: 4m 6s\tremaining: 45.7s\n",
      "8433:\ttotal: 4m 6s\tremaining: 45.7s\n",
      "8434:\ttotal: 4m 6s\tremaining: 45.6s\n",
      "8435:\ttotal: 4m 6s\tremaining: 45.6s\n",
      "8436:\ttotal: 4m 6s\tremaining: 45.6s\n",
      "8437:\ttotal: 4m 6s\tremaining: 45.5s\n",
      "8438:\ttotal: 4m 6s\tremaining: 45.5s\n",
      "8439:\ttotal: 4m 6s\tremaining: 45.5s\n",
      "8440:\ttotal: 4m 6s\tremaining: 45.5s\n",
      "8441:\ttotal: 4m 6s\tremaining: 45.4s\n",
      "8442:\ttotal: 4m 6s\tremaining: 45.4s\n",
      "8443:\ttotal: 4m 6s\tremaining: 45.4s\n",
      "8444:\ttotal: 4m 6s\tremaining: 45.3s\n",
      "8445:\ttotal: 4m 6s\tremaining: 45.3s\n",
      "8446:\ttotal: 4m 6s\tremaining: 45.3s\n",
      "8447:\ttotal: 4m 6s\tremaining: 45.2s\n",
      "8448:\ttotal: 4m 6s\tremaining: 45.2s\n",
      "8449:\ttotal: 4m 6s\tremaining: 45.2s\n",
      "8450:\ttotal: 4m 6s\tremaining: 45.2s\n",
      "8451:\ttotal: 4m 6s\tremaining: 45.1s\n",
      "8452:\ttotal: 4m 6s\tremaining: 45.1s\n",
      "8453:\ttotal: 4m 6s\tremaining: 45.1s\n",
      "8454:\ttotal: 4m 6s\tremaining: 45s\n",
      "8455:\ttotal: 4m 6s\tremaining: 45s\n",
      "8456:\ttotal: 4m 6s\tremaining: 45s\n",
      "8457:\ttotal: 4m 6s\tremaining: 44.9s\n",
      "8458:\ttotal: 4m 6s\tremaining: 44.9s\n",
      "8459:\ttotal: 4m 6s\tremaining: 44.9s\n",
      "8460:\ttotal: 4m 6s\tremaining: 44.9s\n",
      "8461:\ttotal: 4m 6s\tremaining: 44.8s\n",
      "8462:\ttotal: 4m 6s\tremaining: 44.8s\n",
      "8463:\ttotal: 4m 6s\tremaining: 44.8s\n",
      "8464:\ttotal: 4m 6s\tremaining: 44.7s\n",
      "8465:\ttotal: 4m 6s\tremaining: 44.7s\n",
      "8466:\ttotal: 4m 6s\tremaining: 44.7s\n",
      "8467:\ttotal: 4m 6s\tremaining: 44.6s\n",
      "8468:\ttotal: 4m 6s\tremaining: 44.6s\n",
      "8469:\ttotal: 4m 7s\tremaining: 44.6s\n",
      "8470:\ttotal: 4m 7s\tremaining: 44.6s\n",
      "8471:\ttotal: 4m 7s\tremaining: 44.5s\n",
      "8472:\ttotal: 4m 7s\tremaining: 44.5s\n",
      "8473:\ttotal: 4m 7s\tremaining: 44.5s\n",
      "8474:\ttotal: 4m 7s\tremaining: 44.5s\n",
      "8475:\ttotal: 4m 7s\tremaining: 44.4s\n",
      "8476:\ttotal: 4m 7s\tremaining: 44.4s\n",
      "8477:\ttotal: 4m 7s\tremaining: 44.4s\n",
      "8478:\ttotal: 4m 7s\tremaining: 44.3s\n",
      "8479:\ttotal: 4m 7s\tremaining: 44.3s\n",
      "8480:\ttotal: 4m 7s\tremaining: 44.3s\n",
      "8481:\ttotal: 4m 7s\tremaining: 44.3s\n",
      "8482:\ttotal: 4m 7s\tremaining: 44.2s\n",
      "8483:\ttotal: 4m 7s\tremaining: 44.2s\n",
      "8484:\ttotal: 4m 7s\tremaining: 44.2s\n",
      "8485:\ttotal: 4m 7s\tremaining: 44.2s\n",
      "8486:\ttotal: 4m 7s\tremaining: 44.1s\n",
      "8487:\ttotal: 4m 7s\tremaining: 44.1s\n",
      "8488:\ttotal: 4m 7s\tremaining: 44.1s\n",
      "8489:\ttotal: 4m 7s\tremaining: 44s\n",
      "8490:\ttotal: 4m 7s\tremaining: 44s\n",
      "8491:\ttotal: 4m 7s\tremaining: 44s\n",
      "8492:\ttotal: 4m 7s\tremaining: 43.9s\n",
      "8493:\ttotal: 4m 7s\tremaining: 43.9s\n",
      "8494:\ttotal: 4m 7s\tremaining: 43.9s\n",
      "8495:\ttotal: 4m 7s\tremaining: 43.8s\n",
      "8496:\ttotal: 4m 7s\tremaining: 43.8s\n",
      "8497:\ttotal: 4m 7s\tremaining: 43.8s\n",
      "8498:\ttotal: 4m 7s\tremaining: 43.8s\n",
      "8499:\ttotal: 4m 7s\tremaining: 43.7s\n",
      "8500:\ttotal: 4m 7s\tremaining: 43.7s\n",
      "8501:\ttotal: 4m 7s\tremaining: 43.7s\n",
      "8502:\ttotal: 4m 7s\tremaining: 43.6s\n",
      "8503:\ttotal: 4m 7s\tremaining: 43.6s\n",
      "8504:\ttotal: 4m 8s\tremaining: 43.6s\n",
      "8505:\ttotal: 4m 8s\tremaining: 43.5s\n",
      "8506:\ttotal: 4m 8s\tremaining: 43.5s\n",
      "8507:\ttotal: 4m 8s\tremaining: 43.5s\n",
      "8508:\ttotal: 4m 8s\tremaining: 43.4s\n",
      "8509:\ttotal: 4m 8s\tremaining: 43.4s\n",
      "8510:\ttotal: 4m 8s\tremaining: 43.4s\n",
      "8511:\ttotal: 4m 8s\tremaining: 43.4s\n",
      "8512:\ttotal: 4m 8s\tremaining: 43.3s\n",
      "8513:\ttotal: 4m 8s\tremaining: 43.3s\n",
      "8514:\ttotal: 4m 8s\tremaining: 43.3s\n",
      "8515:\ttotal: 4m 8s\tremaining: 43.2s\n",
      "8516:\ttotal: 4m 8s\tremaining: 43.2s\n",
      "8517:\ttotal: 4m 8s\tremaining: 43.2s\n",
      "8518:\ttotal: 4m 8s\tremaining: 43.1s\n",
      "8519:\ttotal: 4m 8s\tremaining: 43.1s\n",
      "8520:\ttotal: 4m 8s\tremaining: 43.1s\n",
      "8521:\ttotal: 4m 8s\tremaining: 43.1s\n",
      "8522:\ttotal: 4m 8s\tremaining: 43s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8523:\ttotal: 4m 8s\tremaining: 43s\n",
      "8524:\ttotal: 4m 8s\tremaining: 43s\n",
      "8525:\ttotal: 4m 8s\tremaining: 42.9s\n",
      "8526:\ttotal: 4m 8s\tremaining: 42.9s\n",
      "8527:\ttotal: 4m 8s\tremaining: 42.9s\n",
      "8528:\ttotal: 4m 8s\tremaining: 42.8s\n",
      "8529:\ttotal: 4m 8s\tremaining: 42.8s\n",
      "8530:\ttotal: 4m 8s\tremaining: 42.8s\n",
      "8531:\ttotal: 4m 8s\tremaining: 42.8s\n",
      "8532:\ttotal: 4m 8s\tremaining: 42.7s\n",
      "8533:\ttotal: 4m 8s\tremaining: 42.7s\n",
      "8534:\ttotal: 4m 8s\tremaining: 42.7s\n",
      "8535:\ttotal: 4m 8s\tremaining: 42.6s\n",
      "8536:\ttotal: 4m 8s\tremaining: 42.6s\n",
      "8537:\ttotal: 4m 8s\tremaining: 42.6s\n",
      "8538:\ttotal: 4m 8s\tremaining: 42.5s\n",
      "8539:\ttotal: 4m 8s\tremaining: 42.5s\n",
      "8540:\ttotal: 4m 8s\tremaining: 42.5s\n",
      "8541:\ttotal: 4m 8s\tremaining: 42.5s\n",
      "8542:\ttotal: 4m 8s\tremaining: 42.4s\n",
      "8543:\ttotal: 4m 8s\tremaining: 42.4s\n",
      "8544:\ttotal: 4m 9s\tremaining: 42.4s\n",
      "8545:\ttotal: 4m 9s\tremaining: 42.3s\n",
      "8546:\ttotal: 4m 9s\tremaining: 42.3s\n",
      "8547:\ttotal: 4m 9s\tremaining: 42.3s\n",
      "8548:\ttotal: 4m 9s\tremaining: 42.2s\n",
      "8549:\ttotal: 4m 9s\tremaining: 42.2s\n",
      "8550:\ttotal: 4m 9s\tremaining: 42.2s\n",
      "8551:\ttotal: 4m 9s\tremaining: 42.2s\n",
      "8552:\ttotal: 4m 9s\tremaining: 42.1s\n",
      "8553:\ttotal: 4m 9s\tremaining: 42.1s\n",
      "8554:\ttotal: 4m 9s\tremaining: 42.1s\n",
      "8555:\ttotal: 4m 9s\tremaining: 42s\n",
      "8556:\ttotal: 4m 9s\tremaining: 42s\n",
      "8557:\ttotal: 4m 9s\tremaining: 42s\n",
      "8558:\ttotal: 4m 9s\tremaining: 42s\n",
      "8559:\ttotal: 4m 9s\tremaining: 41.9s\n",
      "8560:\ttotal: 4m 9s\tremaining: 41.9s\n",
      "8561:\ttotal: 4m 9s\tremaining: 41.9s\n",
      "8562:\ttotal: 4m 9s\tremaining: 41.8s\n",
      "8563:\ttotal: 4m 9s\tremaining: 41.8s\n",
      "8564:\ttotal: 4m 9s\tremaining: 41.8s\n",
      "8565:\ttotal: 4m 9s\tremaining: 41.8s\n",
      "8566:\ttotal: 4m 9s\tremaining: 41.7s\n",
      "8567:\ttotal: 4m 9s\tremaining: 41.7s\n",
      "8568:\ttotal: 4m 9s\tremaining: 41.7s\n",
      "8569:\ttotal: 4m 9s\tremaining: 41.6s\n",
      "8570:\ttotal: 4m 9s\tremaining: 41.6s\n",
      "8571:\ttotal: 4m 9s\tremaining: 41.6s\n",
      "8572:\ttotal: 4m 9s\tremaining: 41.5s\n",
      "8573:\ttotal: 4m 9s\tremaining: 41.5s\n",
      "8574:\ttotal: 4m 9s\tremaining: 41.5s\n",
      "8575:\ttotal: 4m 9s\tremaining: 41.5s\n",
      "8576:\ttotal: 4m 9s\tremaining: 41.4s\n",
      "8577:\ttotal: 4m 9s\tremaining: 41.4s\n",
      "8578:\ttotal: 4m 9s\tremaining: 41.4s\n",
      "8579:\ttotal: 4m 9s\tremaining: 41.3s\n",
      "8580:\ttotal: 4m 10s\tremaining: 41.3s\n",
      "8581:\ttotal: 4m 10s\tremaining: 41.3s\n",
      "8582:\ttotal: 4m 10s\tremaining: 41.3s\n",
      "8583:\ttotal: 4m 10s\tremaining: 41.2s\n",
      "8584:\ttotal: 4m 10s\tremaining: 41.2s\n",
      "8585:\ttotal: 4m 10s\tremaining: 41.2s\n",
      "8586:\ttotal: 4m 10s\tremaining: 41.1s\n",
      "8587:\ttotal: 4m 10s\tremaining: 41.1s\n",
      "8588:\ttotal: 4m 10s\tremaining: 41.1s\n",
      "8589:\ttotal: 4m 10s\tremaining: 41s\n",
      "8590:\ttotal: 4m 10s\tremaining: 41s\n",
      "8591:\ttotal: 4m 10s\tremaining: 41s\n",
      "8592:\ttotal: 4m 10s\tremaining: 41s\n",
      "8593:\ttotal: 4m 10s\tremaining: 40.9s\n",
      "8594:\ttotal: 4m 10s\tremaining: 40.9s\n",
      "8595:\ttotal: 4m 10s\tremaining: 40.9s\n",
      "8596:\ttotal: 4m 10s\tremaining: 40.8s\n",
      "8597:\ttotal: 4m 10s\tremaining: 40.8s\n",
      "8598:\ttotal: 4m 10s\tremaining: 40.8s\n",
      "8599:\ttotal: 4m 10s\tremaining: 40.8s\n",
      "8600:\ttotal: 4m 10s\tremaining: 40.7s\n",
      "8601:\ttotal: 4m 10s\tremaining: 40.7s\n",
      "8602:\ttotal: 4m 10s\tremaining: 40.7s\n",
      "8603:\ttotal: 4m 10s\tremaining: 40.6s\n",
      "8604:\ttotal: 4m 10s\tremaining: 40.6s\n",
      "8605:\ttotal: 4m 10s\tremaining: 40.6s\n",
      "8606:\ttotal: 4m 10s\tremaining: 40.5s\n",
      "8607:\ttotal: 4m 10s\tremaining: 40.5s\n",
      "8608:\ttotal: 4m 10s\tremaining: 40.5s\n",
      "8609:\ttotal: 4m 10s\tremaining: 40.5s\n",
      "8610:\ttotal: 4m 10s\tremaining: 40.4s\n",
      "8611:\ttotal: 4m 10s\tremaining: 40.4s\n",
      "8612:\ttotal: 4m 10s\tremaining: 40.4s\n",
      "8613:\ttotal: 4m 10s\tremaining: 40.3s\n",
      "8614:\ttotal: 4m 10s\tremaining: 40.3s\n",
      "8615:\ttotal: 4m 10s\tremaining: 40.3s\n",
      "8616:\ttotal: 4m 11s\tremaining: 40.3s\n",
      "8617:\ttotal: 4m 11s\tremaining: 40.2s\n",
      "8618:\ttotal: 4m 11s\tremaining: 40.2s\n",
      "8619:\ttotal: 4m 11s\tremaining: 40.2s\n",
      "8620:\ttotal: 4m 11s\tremaining: 40.1s\n",
      "8621:\ttotal: 4m 11s\tremaining: 40.1s\n",
      "8622:\ttotal: 4m 11s\tremaining: 40.1s\n",
      "8623:\ttotal: 4m 11s\tremaining: 40.1s\n",
      "8624:\ttotal: 4m 11s\tremaining: 40s\n",
      "8625:\ttotal: 4m 11s\tremaining: 40s\n",
      "8626:\ttotal: 4m 11s\tremaining: 40s\n",
      "8627:\ttotal: 4m 11s\tremaining: 40s\n",
      "8628:\ttotal: 4m 11s\tremaining: 39.9s\n",
      "8629:\ttotal: 4m 11s\tremaining: 39.9s\n",
      "8630:\ttotal: 4m 11s\tremaining: 39.9s\n",
      "8631:\ttotal: 4m 11s\tremaining: 39.8s\n",
      "8632:\ttotal: 4m 11s\tremaining: 39.8s\n",
      "8633:\ttotal: 4m 11s\tremaining: 39.8s\n",
      "8634:\ttotal: 4m 11s\tremaining: 39.7s\n",
      "8635:\ttotal: 4m 11s\tremaining: 39.7s\n",
      "8636:\ttotal: 4m 11s\tremaining: 39.7s\n",
      "8637:\ttotal: 4m 11s\tremaining: 39.6s\n",
      "8638:\ttotal: 4m 11s\tremaining: 39.6s\n",
      "8639:\ttotal: 4m 11s\tremaining: 39.6s\n",
      "8640:\ttotal: 4m 11s\tremaining: 39.6s\n",
      "8641:\ttotal: 4m 11s\tremaining: 39.5s\n",
      "8642:\ttotal: 4m 11s\tremaining: 39.5s\n",
      "8643:\ttotal: 4m 11s\tremaining: 39.5s\n",
      "8644:\ttotal: 4m 11s\tremaining: 39.4s\n",
      "8645:\ttotal: 4m 11s\tremaining: 39.4s\n",
      "8646:\ttotal: 4m 11s\tremaining: 39.4s\n",
      "8647:\ttotal: 4m 11s\tremaining: 39.4s\n",
      "8648:\ttotal: 4m 11s\tremaining: 39.3s\n",
      "8649:\ttotal: 4m 12s\tremaining: 39.3s\n",
      "8650:\ttotal: 4m 12s\tremaining: 39.3s\n",
      "8651:\ttotal: 4m 12s\tremaining: 39.2s\n",
      "8652:\ttotal: 4m 12s\tremaining: 39.2s\n",
      "8653:\ttotal: 4m 12s\tremaining: 39.2s\n",
      "8654:\ttotal: 4m 12s\tremaining: 39.2s\n",
      "8655:\ttotal: 4m 12s\tremaining: 39.1s\n",
      "8656:\ttotal: 4m 12s\tremaining: 39.1s\n",
      "8657:\ttotal: 4m 12s\tremaining: 39.1s\n",
      "8658:\ttotal: 4m 12s\tremaining: 39s\n",
      "8659:\ttotal: 4m 12s\tremaining: 39s\n",
      "8660:\ttotal: 4m 12s\tremaining: 39s\n",
      "8661:\ttotal: 4m 12s\tremaining: 39s\n",
      "8662:\ttotal: 4m 12s\tremaining: 38.9s\n",
      "8663:\ttotal: 4m 12s\tremaining: 38.9s\n",
      "8664:\ttotal: 4m 12s\tremaining: 38.9s\n",
      "8665:\ttotal: 4m 12s\tremaining: 38.9s\n",
      "8666:\ttotal: 4m 12s\tremaining: 38.8s\n",
      "8667:\ttotal: 4m 12s\tremaining: 38.8s\n",
      "8668:\ttotal: 4m 12s\tremaining: 38.8s\n",
      "8669:\ttotal: 4m 12s\tremaining: 38.7s\n",
      "8670:\ttotal: 4m 12s\tremaining: 38.7s\n",
      "8671:\ttotal: 4m 12s\tremaining: 38.7s\n",
      "8672:\ttotal: 4m 12s\tremaining: 38.7s\n",
      "8673:\ttotal: 4m 12s\tremaining: 38.6s\n",
      "8674:\ttotal: 4m 12s\tremaining: 38.6s\n",
      "8675:\ttotal: 4m 12s\tremaining: 38.6s\n",
      "8676:\ttotal: 4m 13s\tremaining: 38.5s\n",
      "8677:\ttotal: 4m 13s\tremaining: 38.5s\n",
      "8678:\ttotal: 4m 13s\tremaining: 38.5s\n",
      "8679:\ttotal: 4m 13s\tremaining: 38.5s\n",
      "8680:\ttotal: 4m 13s\tremaining: 38.4s\n",
      "8681:\ttotal: 4m 13s\tremaining: 38.4s\n",
      "8682:\ttotal: 4m 13s\tremaining: 38.4s\n",
      "8683:\ttotal: 4m 13s\tremaining: 38.4s\n",
      "8684:\ttotal: 4m 13s\tremaining: 38.3s\n",
      "8685:\ttotal: 4m 13s\tremaining: 38.3s\n",
      "8686:\ttotal: 4m 13s\tremaining: 38.3s\n",
      "8687:\ttotal: 4m 13s\tremaining: 38.2s\n",
      "8688:\ttotal: 4m 13s\tremaining: 38.2s\n",
      "8689:\ttotal: 4m 13s\tremaining: 38.2s\n",
      "8690:\ttotal: 4m 13s\tremaining: 38.2s\n",
      "8691:\ttotal: 4m 13s\tremaining: 38.1s\n",
      "8692:\ttotal: 4m 13s\tremaining: 38.1s\n",
      "8693:\ttotal: 4m 13s\tremaining: 38.1s\n",
      "8694:\ttotal: 4m 13s\tremaining: 38.1s\n",
      "8695:\ttotal: 4m 13s\tremaining: 38s\n",
      "8696:\ttotal: 4m 13s\tremaining: 38s\n",
      "8697:\ttotal: 4m 13s\tremaining: 38s\n",
      "8698:\ttotal: 4m 13s\tremaining: 37.9s\n",
      "8699:\ttotal: 4m 13s\tremaining: 37.9s\n",
      "8700:\ttotal: 4m 13s\tremaining: 37.9s\n",
      "8701:\ttotal: 4m 14s\tremaining: 37.9s\n",
      "8702:\ttotal: 4m 14s\tremaining: 37.8s\n",
      "8703:\ttotal: 4m 14s\tremaining: 37.8s\n",
      "8704:\ttotal: 4m 14s\tremaining: 37.8s\n",
      "8705:\ttotal: 4m 14s\tremaining: 37.8s\n",
      "8706:\ttotal: 4m 14s\tremaining: 37.7s\n",
      "8707:\ttotal: 4m 14s\tremaining: 37.7s\n",
      "8708:\ttotal: 4m 14s\tremaining: 37.7s\n",
      "8709:\ttotal: 4m 14s\tremaining: 37.6s\n",
      "8710:\ttotal: 4m 14s\tremaining: 37.6s\n",
      "8711:\ttotal: 4m 14s\tremaining: 37.6s\n",
      "8712:\ttotal: 4m 14s\tremaining: 37.6s\n",
      "8713:\ttotal: 4m 14s\tremaining: 37.5s\n",
      "8714:\ttotal: 4m 14s\tremaining: 37.5s\n",
      "8715:\ttotal: 4m 14s\tremaining: 37.5s\n",
      "8716:\ttotal: 4m 14s\tremaining: 37.4s\n",
      "8717:\ttotal: 4m 14s\tremaining: 37.4s\n",
      "8718:\ttotal: 4m 14s\tremaining: 37.4s\n",
      "8719:\ttotal: 4m 14s\tremaining: 37.3s\n",
      "8720:\ttotal: 4m 14s\tremaining: 37.3s\n",
      "8721:\ttotal: 4m 14s\tremaining: 37.3s\n",
      "8722:\ttotal: 4m 14s\tremaining: 37.3s\n",
      "8723:\ttotal: 4m 14s\tremaining: 37.2s\n",
      "8724:\ttotal: 4m 14s\tremaining: 37.2s\n",
      "8725:\ttotal: 4m 14s\tremaining: 37.2s\n",
      "8726:\ttotal: 4m 14s\tremaining: 37.1s\n",
      "8727:\ttotal: 4m 14s\tremaining: 37.1s\n",
      "8728:\ttotal: 4m 14s\tremaining: 37.1s\n",
      "8729:\ttotal: 4m 14s\tremaining: 37s\n",
      "8730:\ttotal: 4m 14s\tremaining: 37s\n",
      "8731:\ttotal: 4m 14s\tremaining: 37s\n",
      "8732:\ttotal: 4m 15s\tremaining: 37s\n",
      "8733:\ttotal: 4m 15s\tremaining: 36.9s\n",
      "8734:\ttotal: 4m 15s\tremaining: 36.9s\n",
      "8735:\ttotal: 4m 15s\tremaining: 36.9s\n",
      "8736:\ttotal: 4m 15s\tremaining: 36.9s\n",
      "8737:\ttotal: 4m 15s\tremaining: 36.8s\n",
      "8738:\ttotal: 4m 15s\tremaining: 36.8s\n",
      "8739:\ttotal: 4m 15s\tremaining: 36.8s\n",
      "8740:\ttotal: 4m 15s\tremaining: 36.7s\n",
      "8741:\ttotal: 4m 15s\tremaining: 36.7s\n",
      "8742:\ttotal: 4m 15s\tremaining: 36.7s\n",
      "8743:\ttotal: 4m 15s\tremaining: 36.6s\n",
      "8744:\ttotal: 4m 15s\tremaining: 36.6s\n",
      "8745:\ttotal: 4m 15s\tremaining: 36.6s\n",
      "8746:\ttotal: 4m 15s\tremaining: 36.6s\n",
      "8747:\ttotal: 4m 15s\tremaining: 36.5s\n",
      "8748:\ttotal: 4m 15s\tremaining: 36.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8749:\ttotal: 4m 15s\tremaining: 36.5s\n",
      "8750:\ttotal: 4m 15s\tremaining: 36.4s\n",
      "8751:\ttotal: 4m 15s\tremaining: 36.4s\n",
      "8752:\ttotal: 4m 15s\tremaining: 36.4s\n",
      "8753:\ttotal: 4m 15s\tremaining: 36.4s\n",
      "8754:\ttotal: 4m 15s\tremaining: 36.3s\n",
      "8755:\ttotal: 4m 15s\tremaining: 36.3s\n",
      "8756:\ttotal: 4m 15s\tremaining: 36.3s\n",
      "8757:\ttotal: 4m 15s\tremaining: 36.2s\n",
      "8758:\ttotal: 4m 15s\tremaining: 36.2s\n",
      "8759:\ttotal: 4m 15s\tremaining: 36.2s\n",
      "8760:\ttotal: 4m 15s\tremaining: 36.2s\n",
      "8761:\ttotal: 4m 15s\tremaining: 36.1s\n",
      "8762:\ttotal: 4m 15s\tremaining: 36.1s\n",
      "8763:\ttotal: 4m 15s\tremaining: 36.1s\n",
      "8764:\ttotal: 4m 16s\tremaining: 36s\n",
      "8765:\ttotal: 4m 16s\tremaining: 36s\n",
      "8766:\ttotal: 4m 16s\tremaining: 36s\n",
      "8767:\ttotal: 4m 16s\tremaining: 36s\n",
      "8768:\ttotal: 4m 16s\tremaining: 35.9s\n",
      "8769:\ttotal: 4m 16s\tremaining: 35.9s\n",
      "8770:\ttotal: 4m 16s\tremaining: 35.9s\n",
      "8771:\ttotal: 4m 16s\tremaining: 35.8s\n",
      "8772:\ttotal: 4m 16s\tremaining: 35.8s\n",
      "8773:\ttotal: 4m 16s\tremaining: 35.8s\n",
      "8774:\ttotal: 4m 16s\tremaining: 35.8s\n",
      "8775:\ttotal: 4m 16s\tremaining: 35.7s\n",
      "8776:\ttotal: 4m 16s\tremaining: 35.7s\n",
      "8777:\ttotal: 4m 16s\tremaining: 35.7s\n",
      "8778:\ttotal: 4m 16s\tremaining: 35.7s\n",
      "8779:\ttotal: 4m 16s\tremaining: 35.6s\n",
      "8780:\ttotal: 4m 16s\tremaining: 35.6s\n",
      "8781:\ttotal: 4m 16s\tremaining: 35.6s\n",
      "8782:\ttotal: 4m 16s\tremaining: 35.5s\n",
      "8783:\ttotal: 4m 16s\tremaining: 35.5s\n",
      "8784:\ttotal: 4m 16s\tremaining: 35.5s\n",
      "8785:\ttotal: 4m 16s\tremaining: 35.5s\n",
      "8786:\ttotal: 4m 16s\tremaining: 35.4s\n",
      "8787:\ttotal: 4m 16s\tremaining: 35.4s\n",
      "8788:\ttotal: 4m 16s\tremaining: 35.4s\n",
      "8789:\ttotal: 4m 16s\tremaining: 35.3s\n",
      "8790:\ttotal: 4m 16s\tremaining: 35.3s\n",
      "8791:\ttotal: 4m 17s\tremaining: 35.3s\n",
      "8792:\ttotal: 4m 17s\tremaining: 35.3s\n",
      "8793:\ttotal: 4m 17s\tremaining: 35.2s\n",
      "8794:\ttotal: 4m 17s\tremaining: 35.2s\n",
      "8795:\ttotal: 4m 17s\tremaining: 35.2s\n",
      "8796:\ttotal: 4m 17s\tremaining: 35.1s\n",
      "8797:\ttotal: 4m 17s\tremaining: 35.1s\n",
      "8798:\ttotal: 4m 17s\tremaining: 35.1s\n",
      "8799:\ttotal: 4m 17s\tremaining: 35s\n",
      "8800:\ttotal: 4m 17s\tremaining: 35s\n",
      "8801:\ttotal: 4m 17s\tremaining: 35s\n",
      "8802:\ttotal: 4m 17s\tremaining: 35s\n",
      "8803:\ttotal: 4m 17s\tremaining: 34.9s\n",
      "8804:\ttotal: 4m 17s\tremaining: 34.9s\n",
      "8805:\ttotal: 4m 17s\tremaining: 34.9s\n",
      "8806:\ttotal: 4m 17s\tremaining: 34.8s\n",
      "8807:\ttotal: 4m 17s\tremaining: 34.8s\n",
      "8808:\ttotal: 4m 17s\tremaining: 34.8s\n",
      "8809:\ttotal: 4m 17s\tremaining: 34.7s\n",
      "8810:\ttotal: 4m 17s\tremaining: 34.7s\n",
      "8811:\ttotal: 4m 17s\tremaining: 34.7s\n",
      "8812:\ttotal: 4m 17s\tremaining: 34.7s\n",
      "8813:\ttotal: 4m 17s\tremaining: 34.6s\n",
      "8814:\ttotal: 4m 17s\tremaining: 34.6s\n",
      "8815:\ttotal: 4m 17s\tremaining: 34.6s\n",
      "8816:\ttotal: 4m 17s\tremaining: 34.5s\n",
      "8817:\ttotal: 4m 17s\tremaining: 34.5s\n",
      "8818:\ttotal: 4m 17s\tremaining: 34.5s\n",
      "8819:\ttotal: 4m 17s\tremaining: 34.4s\n",
      "8820:\ttotal: 4m 17s\tremaining: 34.4s\n",
      "8821:\ttotal: 4m 17s\tremaining: 34.4s\n",
      "8822:\ttotal: 4m 17s\tremaining: 34.4s\n",
      "8823:\ttotal: 4m 17s\tremaining: 34.3s\n",
      "8824:\ttotal: 4m 17s\tremaining: 34.3s\n",
      "8825:\ttotal: 4m 17s\tremaining: 34.3s\n",
      "8826:\ttotal: 4m 17s\tremaining: 34.2s\n",
      "8827:\ttotal: 4m 17s\tremaining: 34.2s\n",
      "8828:\ttotal: 4m 17s\tremaining: 34.2s\n",
      "8829:\ttotal: 4m 17s\tremaining: 34.2s\n",
      "8830:\ttotal: 4m 18s\tremaining: 34.1s\n",
      "8831:\ttotal: 4m 18s\tremaining: 34.1s\n",
      "8832:\ttotal: 4m 18s\tremaining: 34.1s\n",
      "8833:\ttotal: 4m 18s\tremaining: 34s\n",
      "8834:\ttotal: 4m 18s\tremaining: 34s\n",
      "8835:\ttotal: 4m 18s\tremaining: 34s\n",
      "8836:\ttotal: 4m 18s\tremaining: 33.9s\n",
      "8837:\ttotal: 4m 18s\tremaining: 33.9s\n",
      "8838:\ttotal: 4m 18s\tremaining: 33.9s\n",
      "8839:\ttotal: 4m 18s\tremaining: 33.9s\n",
      "8840:\ttotal: 4m 18s\tremaining: 33.8s\n",
      "8841:\ttotal: 4m 18s\tremaining: 33.8s\n",
      "8842:\ttotal: 4m 18s\tremaining: 33.8s\n",
      "8843:\ttotal: 4m 18s\tremaining: 33.7s\n",
      "8844:\ttotal: 4m 18s\tremaining: 33.7s\n",
      "8845:\ttotal: 4m 18s\tremaining: 33.7s\n",
      "8846:\ttotal: 4m 18s\tremaining: 33.6s\n",
      "8847:\ttotal: 4m 18s\tremaining: 33.6s\n",
      "8848:\ttotal: 4m 18s\tremaining: 33.6s\n",
      "8849:\ttotal: 4m 18s\tremaining: 33.6s\n",
      "8850:\ttotal: 4m 18s\tremaining: 33.5s\n",
      "8851:\ttotal: 4m 18s\tremaining: 33.5s\n",
      "8852:\ttotal: 4m 18s\tremaining: 33.5s\n",
      "8853:\ttotal: 4m 18s\tremaining: 33.4s\n",
      "8854:\ttotal: 4m 18s\tremaining: 33.4s\n",
      "8855:\ttotal: 4m 18s\tremaining: 33.4s\n",
      "8856:\ttotal: 4m 18s\tremaining: 33.3s\n",
      "8857:\ttotal: 4m 18s\tremaining: 33.3s\n",
      "8858:\ttotal: 4m 18s\tremaining: 33.3s\n",
      "8859:\ttotal: 4m 18s\tremaining: 33.2s\n",
      "8860:\ttotal: 4m 18s\tremaining: 33.2s\n",
      "8861:\ttotal: 4m 18s\tremaining: 33.2s\n",
      "8862:\ttotal: 4m 18s\tremaining: 33.2s\n",
      "8863:\ttotal: 4m 18s\tremaining: 33.1s\n",
      "8864:\ttotal: 4m 18s\tremaining: 33.1s\n",
      "8865:\ttotal: 4m 18s\tremaining: 33.1s\n",
      "8866:\ttotal: 4m 18s\tremaining: 33s\n",
      "8867:\ttotal: 4m 18s\tremaining: 33s\n",
      "8868:\ttotal: 4m 18s\tremaining: 33s\n",
      "8869:\ttotal: 4m 18s\tremaining: 32.9s\n",
      "8870:\ttotal: 4m 18s\tremaining: 32.9s\n",
      "8871:\ttotal: 4m 18s\tremaining: 32.9s\n",
      "8872:\ttotal: 4m 18s\tremaining: 32.9s\n",
      "8873:\ttotal: 4m 18s\tremaining: 32.8s\n",
      "8874:\ttotal: 4m 18s\tremaining: 32.8s\n",
      "8875:\ttotal: 4m 19s\tremaining: 32.8s\n",
      "8876:\ttotal: 4m 19s\tremaining: 32.7s\n",
      "8877:\ttotal: 4m 19s\tremaining: 32.7s\n",
      "8878:\ttotal: 4m 19s\tremaining: 32.7s\n",
      "8879:\ttotal: 4m 19s\tremaining: 32.7s\n",
      "8880:\ttotal: 4m 19s\tremaining: 32.6s\n",
      "8881:\ttotal: 4m 19s\tremaining: 32.6s\n",
      "8882:\ttotal: 4m 19s\tremaining: 32.6s\n",
      "8883:\ttotal: 4m 19s\tremaining: 32.5s\n",
      "8884:\ttotal: 4m 19s\tremaining: 32.5s\n",
      "8885:\ttotal: 4m 19s\tremaining: 32.5s\n",
      "8886:\ttotal: 4m 19s\tremaining: 32.4s\n",
      "8887:\ttotal: 4m 19s\tremaining: 32.4s\n",
      "8888:\ttotal: 4m 19s\tremaining: 32.4s\n",
      "8889:\ttotal: 4m 19s\tremaining: 32.4s\n",
      "8890:\ttotal: 4m 19s\tremaining: 32.3s\n",
      "8891:\ttotal: 4m 19s\tremaining: 32.3s\n",
      "8892:\ttotal: 4m 19s\tremaining: 32.3s\n",
      "8893:\ttotal: 4m 19s\tremaining: 32.2s\n",
      "8894:\ttotal: 4m 19s\tremaining: 32.2s\n",
      "8895:\ttotal: 4m 19s\tremaining: 32.2s\n",
      "8896:\ttotal: 4m 19s\tremaining: 32.1s\n",
      "8897:\ttotal: 4m 19s\tremaining: 32.1s\n",
      "8898:\ttotal: 4m 19s\tremaining: 32.1s\n",
      "8899:\ttotal: 4m 19s\tremaining: 32.1s\n",
      "8900:\ttotal: 4m 19s\tremaining: 32s\n",
      "8901:\ttotal: 4m 19s\tremaining: 32s\n",
      "8902:\ttotal: 4m 19s\tremaining: 32s\n",
      "8903:\ttotal: 4m 19s\tremaining: 31.9s\n",
      "8904:\ttotal: 4m 19s\tremaining: 31.9s\n",
      "8905:\ttotal: 4m 19s\tremaining: 31.9s\n",
      "8906:\ttotal: 4m 19s\tremaining: 31.9s\n",
      "8907:\ttotal: 4m 19s\tremaining: 31.8s\n",
      "8908:\ttotal: 4m 19s\tremaining: 31.8s\n",
      "8909:\ttotal: 4m 20s\tremaining: 31.8s\n",
      "8910:\ttotal: 4m 20s\tremaining: 31.7s\n",
      "8911:\ttotal: 4m 20s\tremaining: 31.7s\n",
      "8912:\ttotal: 4m 20s\tremaining: 31.7s\n",
      "8913:\ttotal: 4m 20s\tremaining: 31.7s\n",
      "8914:\ttotal: 4m 20s\tremaining: 31.6s\n",
      "8915:\ttotal: 4m 20s\tremaining: 31.6s\n",
      "8916:\ttotal: 4m 20s\tremaining: 31.6s\n",
      "8917:\ttotal: 4m 20s\tremaining: 31.5s\n",
      "8918:\ttotal: 4m 20s\tremaining: 31.5s\n",
      "8919:\ttotal: 4m 20s\tremaining: 31.5s\n",
      "8920:\ttotal: 4m 20s\tremaining: 31.5s\n",
      "8921:\ttotal: 4m 20s\tremaining: 31.4s\n",
      "8922:\ttotal: 4m 20s\tremaining: 31.4s\n",
      "8923:\ttotal: 4m 20s\tremaining: 31.4s\n",
      "8924:\ttotal: 4m 20s\tremaining: 31.3s\n",
      "8925:\ttotal: 4m 20s\tremaining: 31.3s\n",
      "8926:\ttotal: 4m 20s\tremaining: 31.3s\n",
      "8927:\ttotal: 4m 20s\tremaining: 31.3s\n",
      "8928:\ttotal: 4m 20s\tremaining: 31.2s\n",
      "8929:\ttotal: 4m 20s\tremaining: 31.2s\n",
      "8930:\ttotal: 4m 20s\tremaining: 31.2s\n",
      "8931:\ttotal: 4m 20s\tremaining: 31.1s\n",
      "8932:\ttotal: 4m 20s\tremaining: 31.1s\n",
      "8933:\ttotal: 4m 20s\tremaining: 31.1s\n",
      "8934:\ttotal: 4m 20s\tremaining: 31.1s\n",
      "8935:\ttotal: 4m 20s\tremaining: 31s\n",
      "8936:\ttotal: 4m 20s\tremaining: 31s\n",
      "8937:\ttotal: 4m 20s\tremaining: 31s\n",
      "8938:\ttotal: 4m 20s\tremaining: 30.9s\n",
      "8939:\ttotal: 4m 20s\tremaining: 30.9s\n",
      "8940:\ttotal: 4m 20s\tremaining: 30.9s\n",
      "8941:\ttotal: 4m 20s\tremaining: 30.9s\n",
      "8942:\ttotal: 4m 21s\tremaining: 30.8s\n",
      "8943:\ttotal: 4m 21s\tremaining: 30.8s\n",
      "8944:\ttotal: 4m 21s\tremaining: 30.8s\n",
      "8945:\ttotal: 4m 21s\tremaining: 30.7s\n",
      "8946:\ttotal: 4m 21s\tremaining: 30.7s\n",
      "8947:\ttotal: 4m 21s\tremaining: 30.7s\n",
      "8948:\ttotal: 4m 21s\tremaining: 30.6s\n",
      "8949:\ttotal: 4m 21s\tremaining: 30.6s\n",
      "8950:\ttotal: 4m 21s\tremaining: 30.6s\n",
      "8951:\ttotal: 4m 21s\tremaining: 30.6s\n",
      "8952:\ttotal: 4m 21s\tremaining: 30.5s\n",
      "8953:\ttotal: 4m 21s\tremaining: 30.5s\n",
      "8954:\ttotal: 4m 21s\tremaining: 30.5s\n",
      "8955:\ttotal: 4m 21s\tremaining: 30.5s\n",
      "8956:\ttotal: 4m 21s\tremaining: 30.4s\n",
      "8957:\ttotal: 4m 21s\tremaining: 30.4s\n",
      "8958:\ttotal: 4m 21s\tremaining: 30.4s\n",
      "8959:\ttotal: 4m 21s\tremaining: 30.3s\n",
      "8960:\ttotal: 4m 21s\tremaining: 30.3s\n",
      "8961:\ttotal: 4m 21s\tremaining: 30.3s\n",
      "8962:\ttotal: 4m 21s\tremaining: 30.3s\n",
      "8963:\ttotal: 4m 21s\tremaining: 30.2s\n",
      "8964:\ttotal: 4m 21s\tremaining: 30.2s\n",
      "8965:\ttotal: 4m 21s\tremaining: 30.2s\n",
      "8966:\ttotal: 4m 21s\tremaining: 30.1s\n",
      "8967:\ttotal: 4m 21s\tremaining: 30.1s\n",
      "8968:\ttotal: 4m 22s\tremaining: 30.1s\n",
      "8969:\ttotal: 4m 22s\tremaining: 30.1s\n",
      "8970:\ttotal: 4m 22s\tremaining: 30s\n",
      "8971:\ttotal: 4m 22s\tremaining: 30s\n",
      "8972:\ttotal: 4m 22s\tremaining: 30s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8973:\ttotal: 4m 22s\tremaining: 29.9s\n",
      "8974:\ttotal: 4m 22s\tremaining: 29.9s\n",
      "8975:\ttotal: 4m 22s\tremaining: 29.9s\n",
      "8976:\ttotal: 4m 22s\tremaining: 29.9s\n",
      "8977:\ttotal: 4m 22s\tremaining: 29.8s\n",
      "8978:\ttotal: 4m 22s\tremaining: 29.8s\n",
      "8979:\ttotal: 4m 22s\tremaining: 29.8s\n",
      "8980:\ttotal: 4m 22s\tremaining: 29.7s\n",
      "8981:\ttotal: 4m 22s\tremaining: 29.7s\n",
      "8982:\ttotal: 4m 22s\tremaining: 29.7s\n",
      "8983:\ttotal: 4m 22s\tremaining: 29.7s\n",
      "8984:\ttotal: 4m 22s\tremaining: 29.6s\n",
      "8985:\ttotal: 4m 22s\tremaining: 29.6s\n",
      "8986:\ttotal: 4m 22s\tremaining: 29.6s\n",
      "8987:\ttotal: 4m 22s\tremaining: 29.6s\n",
      "8988:\ttotal: 4m 22s\tremaining: 29.5s\n",
      "8989:\ttotal: 4m 22s\tremaining: 29.5s\n",
      "8990:\ttotal: 4m 22s\tremaining: 29.5s\n",
      "8991:\ttotal: 4m 22s\tremaining: 29.4s\n",
      "8992:\ttotal: 4m 22s\tremaining: 29.4s\n",
      "8993:\ttotal: 4m 22s\tremaining: 29.4s\n",
      "8994:\ttotal: 4m 22s\tremaining: 29.3s\n",
      "8995:\ttotal: 4m 22s\tremaining: 29.3s\n",
      "8996:\ttotal: 4m 23s\tremaining: 29.3s\n",
      "8997:\ttotal: 4m 23s\tremaining: 29.3s\n",
      "8998:\ttotal: 4m 23s\tremaining: 29.2s\n",
      "8999:\ttotal: 4m 23s\tremaining: 29.2s\n",
      "9000:\ttotal: 4m 23s\tremaining: 29.2s\n",
      "9001:\ttotal: 4m 23s\tremaining: 29.2s\n",
      "9002:\ttotal: 4m 23s\tremaining: 29.1s\n",
      "9003:\ttotal: 4m 23s\tremaining: 29.1s\n",
      "9004:\ttotal: 4m 23s\tremaining: 29.1s\n",
      "9005:\ttotal: 4m 23s\tremaining: 29s\n",
      "9006:\ttotal: 4m 23s\tremaining: 29s\n",
      "9007:\ttotal: 4m 23s\tremaining: 29s\n",
      "9008:\ttotal: 4m 23s\tremaining: 28.9s\n",
      "9009:\ttotal: 4m 23s\tremaining: 28.9s\n",
      "9010:\ttotal: 4m 23s\tremaining: 28.9s\n",
      "9011:\ttotal: 4m 23s\tremaining: 28.9s\n",
      "9012:\ttotal: 4m 23s\tremaining: 28.8s\n",
      "9013:\ttotal: 4m 23s\tremaining: 28.8s\n",
      "9014:\ttotal: 4m 23s\tremaining: 28.8s\n",
      "9015:\ttotal: 4m 23s\tremaining: 28.8s\n",
      "9016:\ttotal: 4m 23s\tremaining: 28.7s\n",
      "9017:\ttotal: 4m 23s\tremaining: 28.7s\n",
      "9018:\ttotal: 4m 23s\tremaining: 28.7s\n",
      "9019:\ttotal: 4m 23s\tremaining: 28.6s\n",
      "9020:\ttotal: 4m 23s\tremaining: 28.6s\n",
      "9021:\ttotal: 4m 23s\tremaining: 28.6s\n",
      "9022:\ttotal: 4m 24s\tremaining: 28.6s\n",
      "9023:\ttotal: 4m 24s\tremaining: 28.5s\n",
      "9024:\ttotal: 4m 24s\tremaining: 28.5s\n",
      "9025:\ttotal: 4m 24s\tremaining: 28.5s\n",
      "9026:\ttotal: 4m 24s\tremaining: 28.4s\n",
      "9027:\ttotal: 4m 24s\tremaining: 28.4s\n",
      "9028:\ttotal: 4m 24s\tremaining: 28.4s\n",
      "9029:\ttotal: 4m 24s\tremaining: 28.4s\n",
      "9030:\ttotal: 4m 24s\tremaining: 28.3s\n",
      "9031:\ttotal: 4m 24s\tremaining: 28.3s\n",
      "9032:\ttotal: 4m 24s\tremaining: 28.3s\n",
      "9033:\ttotal: 4m 24s\tremaining: 28.2s\n",
      "9034:\ttotal: 4m 24s\tremaining: 28.2s\n",
      "9035:\ttotal: 4m 24s\tremaining: 28.2s\n",
      "9036:\ttotal: 4m 24s\tremaining: 28.2s\n",
      "9037:\ttotal: 4m 24s\tremaining: 28.1s\n",
      "9038:\ttotal: 4m 24s\tremaining: 28.1s\n",
      "9039:\ttotal: 4m 24s\tremaining: 28.1s\n",
      "9040:\ttotal: 4m 24s\tremaining: 28s\n",
      "9041:\ttotal: 4m 24s\tremaining: 28s\n",
      "9042:\ttotal: 4m 24s\tremaining: 28s\n",
      "9043:\ttotal: 4m 24s\tremaining: 28s\n",
      "9044:\ttotal: 4m 24s\tremaining: 27.9s\n",
      "9045:\ttotal: 4m 24s\tremaining: 27.9s\n",
      "9046:\ttotal: 4m 24s\tremaining: 27.9s\n",
      "9047:\ttotal: 4m 24s\tremaining: 27.8s\n",
      "9048:\ttotal: 4m 24s\tremaining: 27.8s\n",
      "9049:\ttotal: 4m 24s\tremaining: 27.8s\n",
      "9050:\ttotal: 4m 25s\tremaining: 27.8s\n",
      "9051:\ttotal: 4m 25s\tremaining: 27.7s\n",
      "9052:\ttotal: 4m 25s\tremaining: 27.7s\n",
      "9053:\ttotal: 4m 25s\tremaining: 27.7s\n",
      "9054:\ttotal: 4m 25s\tremaining: 27.6s\n",
      "9055:\ttotal: 4m 25s\tremaining: 27.6s\n",
      "9056:\ttotal: 4m 25s\tremaining: 27.6s\n",
      "9057:\ttotal: 4m 25s\tremaining: 27.6s\n",
      "9058:\ttotal: 4m 25s\tremaining: 27.5s\n",
      "9059:\ttotal: 4m 25s\tremaining: 27.5s\n",
      "9060:\ttotal: 4m 25s\tremaining: 27.5s\n",
      "9061:\ttotal: 4m 25s\tremaining: 27.4s\n",
      "9062:\ttotal: 4m 25s\tremaining: 27.4s\n",
      "9063:\ttotal: 4m 25s\tremaining: 27.4s\n",
      "9064:\ttotal: 4m 25s\tremaining: 27.4s\n",
      "9065:\ttotal: 4m 25s\tremaining: 27.3s\n",
      "9066:\ttotal: 4m 25s\tremaining: 27.3s\n",
      "9067:\ttotal: 4m 25s\tremaining: 27.3s\n",
      "9068:\ttotal: 4m 25s\tremaining: 27.2s\n",
      "9069:\ttotal: 4m 25s\tremaining: 27.2s\n",
      "9070:\ttotal: 4m 25s\tremaining: 27.2s\n",
      "9071:\ttotal: 4m 25s\tremaining: 27.2s\n",
      "9072:\ttotal: 4m 25s\tremaining: 27.1s\n",
      "9073:\ttotal: 4m 25s\tremaining: 27.1s\n",
      "9074:\ttotal: 4m 25s\tremaining: 27.1s\n",
      "9075:\ttotal: 4m 25s\tremaining: 27s\n",
      "9076:\ttotal: 4m 25s\tremaining: 27s\n",
      "9077:\ttotal: 4m 26s\tremaining: 27s\n",
      "9078:\ttotal: 4m 26s\tremaining: 27s\n",
      "9079:\ttotal: 4m 26s\tremaining: 26.9s\n",
      "9080:\ttotal: 4m 26s\tremaining: 26.9s\n",
      "9081:\ttotal: 4m 26s\tremaining: 26.9s\n",
      "9082:\ttotal: 4m 26s\tremaining: 26.8s\n",
      "9083:\ttotal: 4m 26s\tremaining: 26.8s\n",
      "9084:\ttotal: 4m 26s\tremaining: 26.8s\n",
      "9085:\ttotal: 4m 26s\tremaining: 26.8s\n",
      "9086:\ttotal: 4m 26s\tremaining: 26.7s\n",
      "9087:\ttotal: 4m 26s\tremaining: 26.7s\n",
      "9088:\ttotal: 4m 26s\tremaining: 26.7s\n",
      "9089:\ttotal: 4m 26s\tremaining: 26.6s\n",
      "9090:\ttotal: 4m 26s\tremaining: 26.6s\n",
      "9091:\ttotal: 4m 26s\tremaining: 26.6s\n",
      "9092:\ttotal: 4m 26s\tremaining: 26.6s\n",
      "9093:\ttotal: 4m 26s\tremaining: 26.5s\n",
      "9094:\ttotal: 4m 26s\tremaining: 26.5s\n",
      "9095:\ttotal: 4m 26s\tremaining: 26.5s\n",
      "9096:\ttotal: 4m 26s\tremaining: 26.5s\n",
      "9097:\ttotal: 4m 26s\tremaining: 26.4s\n",
      "9098:\ttotal: 4m 26s\tremaining: 26.4s\n",
      "9099:\ttotal: 4m 26s\tremaining: 26.4s\n",
      "9100:\ttotal: 4m 26s\tremaining: 26.3s\n",
      "9101:\ttotal: 4m 26s\tremaining: 26.3s\n",
      "9102:\ttotal: 4m 27s\tremaining: 26.3s\n",
      "9103:\ttotal: 4m 27s\tremaining: 26.3s\n",
      "9104:\ttotal: 4m 27s\tremaining: 26.2s\n",
      "9105:\ttotal: 4m 27s\tremaining: 26.2s\n",
      "9106:\ttotal: 4m 27s\tremaining: 26.2s\n",
      "9107:\ttotal: 4m 27s\tremaining: 26.1s\n",
      "9108:\ttotal: 4m 27s\tremaining: 26.1s\n",
      "9109:\ttotal: 4m 27s\tremaining: 26.1s\n",
      "9110:\ttotal: 4m 27s\tremaining: 26.1s\n",
      "9111:\ttotal: 4m 27s\tremaining: 26s\n",
      "9112:\ttotal: 4m 27s\tremaining: 26s\n",
      "9113:\ttotal: 4m 27s\tremaining: 26s\n",
      "9114:\ttotal: 4m 27s\tremaining: 25.9s\n",
      "9115:\ttotal: 4m 27s\tremaining: 25.9s\n",
      "9116:\ttotal: 4m 27s\tremaining: 25.9s\n",
      "9117:\ttotal: 4m 27s\tremaining: 25.9s\n",
      "9118:\ttotal: 4m 27s\tremaining: 25.8s\n",
      "9119:\ttotal: 4m 27s\tremaining: 25.8s\n",
      "9120:\ttotal: 4m 27s\tremaining: 25.8s\n",
      "9121:\ttotal: 4m 27s\tremaining: 25.7s\n",
      "9122:\ttotal: 4m 27s\tremaining: 25.7s\n",
      "9123:\ttotal: 4m 27s\tremaining: 25.7s\n",
      "9124:\ttotal: 4m 27s\tremaining: 25.7s\n",
      "9125:\ttotal: 4m 27s\tremaining: 25.6s\n",
      "9126:\ttotal: 4m 27s\tremaining: 25.6s\n",
      "9127:\ttotal: 4m 27s\tremaining: 25.6s\n",
      "9128:\ttotal: 4m 27s\tremaining: 25.5s\n",
      "9129:\ttotal: 4m 27s\tremaining: 25.5s\n",
      "9130:\ttotal: 4m 28s\tremaining: 25.5s\n",
      "9131:\ttotal: 4m 28s\tremaining: 25.4s\n",
      "9132:\ttotal: 4m 28s\tremaining: 25.4s\n",
      "9133:\ttotal: 4m 28s\tremaining: 25.4s\n",
      "9134:\ttotal: 4m 28s\tremaining: 25.4s\n",
      "9135:\ttotal: 4m 28s\tremaining: 25.3s\n",
      "9136:\ttotal: 4m 28s\tremaining: 25.3s\n",
      "9137:\ttotal: 4m 28s\tremaining: 25.3s\n",
      "9138:\ttotal: 4m 28s\tremaining: 25.2s\n",
      "9139:\ttotal: 4m 28s\tremaining: 25.2s\n",
      "9140:\ttotal: 4m 28s\tremaining: 25.2s\n",
      "9141:\ttotal: 4m 28s\tremaining: 25.1s\n",
      "9142:\ttotal: 4m 28s\tremaining: 25.1s\n",
      "9143:\ttotal: 4m 28s\tremaining: 25.1s\n",
      "9144:\ttotal: 4m 28s\tremaining: 25.1s\n",
      "9145:\ttotal: 4m 28s\tremaining: 25s\n",
      "9146:\ttotal: 4m 28s\tremaining: 25s\n",
      "9147:\ttotal: 4m 28s\tremaining: 25s\n",
      "9148:\ttotal: 4m 28s\tremaining: 24.9s\n",
      "9149:\ttotal: 4m 28s\tremaining: 24.9s\n",
      "9150:\ttotal: 4m 28s\tremaining: 24.9s\n",
      "9151:\ttotal: 4m 28s\tremaining: 24.9s\n",
      "9152:\ttotal: 4m 28s\tremaining: 24.8s\n",
      "9153:\ttotal: 4m 28s\tremaining: 24.8s\n",
      "9154:\ttotal: 4m 28s\tremaining: 24.8s\n",
      "9155:\ttotal: 4m 28s\tremaining: 24.7s\n",
      "9156:\ttotal: 4m 28s\tremaining: 24.7s\n",
      "9157:\ttotal: 4m 28s\tremaining: 24.7s\n",
      "9158:\ttotal: 4m 28s\tremaining: 24.7s\n",
      "9159:\ttotal: 4m 28s\tremaining: 24.6s\n",
      "9160:\ttotal: 4m 28s\tremaining: 24.6s\n",
      "9161:\ttotal: 4m 28s\tremaining: 24.6s\n",
      "9162:\ttotal: 4m 28s\tremaining: 24.5s\n",
      "9163:\ttotal: 4m 29s\tremaining: 24.5s\n",
      "9164:\ttotal: 4m 29s\tremaining: 24.5s\n",
      "9165:\ttotal: 4m 29s\tremaining: 24.5s\n",
      "9166:\ttotal: 4m 29s\tremaining: 24.4s\n",
      "9167:\ttotal: 4m 29s\tremaining: 24.4s\n",
      "9168:\ttotal: 4m 29s\tremaining: 24.4s\n",
      "9169:\ttotal: 4m 29s\tremaining: 24.3s\n",
      "9170:\ttotal: 4m 29s\tremaining: 24.3s\n",
      "9171:\ttotal: 4m 29s\tremaining: 24.3s\n",
      "9172:\ttotal: 4m 29s\tremaining: 24.3s\n",
      "9173:\ttotal: 4m 29s\tremaining: 24.2s\n",
      "9174:\ttotal: 4m 29s\tremaining: 24.2s\n",
      "9175:\ttotal: 4m 29s\tremaining: 24.2s\n",
      "9176:\ttotal: 4m 29s\tremaining: 24.1s\n",
      "9177:\ttotal: 4m 29s\tremaining: 24.1s\n",
      "9178:\ttotal: 4m 29s\tremaining: 24.1s\n",
      "9179:\ttotal: 4m 29s\tremaining: 24s\n",
      "9180:\ttotal: 4m 29s\tremaining: 24s\n",
      "9181:\ttotal: 4m 29s\tremaining: 24s\n",
      "9182:\ttotal: 4m 29s\tremaining: 24s\n",
      "9183:\ttotal: 4m 29s\tremaining: 23.9s\n",
      "9184:\ttotal: 4m 29s\tremaining: 23.9s\n",
      "9185:\ttotal: 4m 29s\tremaining: 23.9s\n",
      "9186:\ttotal: 4m 29s\tremaining: 23.9s\n",
      "9187:\ttotal: 4m 29s\tremaining: 23.8s\n",
      "9188:\ttotal: 4m 29s\tremaining: 23.8s\n",
      "9189:\ttotal: 4m 29s\tremaining: 23.8s\n",
      "9190:\ttotal: 4m 29s\tremaining: 23.7s\n",
      "9191:\ttotal: 4m 30s\tremaining: 23.7s\n",
      "9192:\ttotal: 4m 30s\tremaining: 23.7s\n",
      "9193:\ttotal: 4m 30s\tremaining: 23.6s\n",
      "9194:\ttotal: 4m 30s\tremaining: 23.6s\n",
      "9195:\ttotal: 4m 30s\tremaining: 23.6s\n",
      "9196:\ttotal: 4m 30s\tremaining: 23.6s\n",
      "9197:\ttotal: 4m 30s\tremaining: 23.5s\n",
      "9198:\ttotal: 4m 30s\tremaining: 23.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9199:\ttotal: 4m 30s\tremaining: 23.5s\n",
      "9200:\ttotal: 4m 30s\tremaining: 23.4s\n",
      "9201:\ttotal: 4m 30s\tremaining: 23.4s\n",
      "9202:\ttotal: 4m 30s\tremaining: 23.4s\n",
      "9203:\ttotal: 4m 30s\tremaining: 23.4s\n",
      "9204:\ttotal: 4m 30s\tremaining: 23.3s\n",
      "9205:\ttotal: 4m 30s\tremaining: 23.3s\n",
      "9206:\ttotal: 4m 30s\tremaining: 23.3s\n",
      "9207:\ttotal: 4m 30s\tremaining: 23.2s\n",
      "9208:\ttotal: 4m 30s\tremaining: 23.2s\n",
      "9209:\ttotal: 4m 30s\tremaining: 23.2s\n",
      "9210:\ttotal: 4m 30s\tremaining: 23.2s\n",
      "9211:\ttotal: 4m 30s\tremaining: 23.1s\n",
      "9212:\ttotal: 4m 30s\tremaining: 23.1s\n",
      "9213:\ttotal: 4m 30s\tremaining: 23.1s\n",
      "9214:\ttotal: 4m 30s\tremaining: 23s\n",
      "9215:\ttotal: 4m 30s\tremaining: 23s\n",
      "9216:\ttotal: 4m 30s\tremaining: 23s\n",
      "9217:\ttotal: 4m 30s\tremaining: 22.9s\n",
      "9218:\ttotal: 4m 30s\tremaining: 22.9s\n",
      "9219:\ttotal: 4m 30s\tremaining: 22.9s\n",
      "9220:\ttotal: 4m 30s\tremaining: 22.9s\n",
      "9221:\ttotal: 4m 30s\tremaining: 22.8s\n",
      "9222:\ttotal: 4m 30s\tremaining: 22.8s\n",
      "9223:\ttotal: 4m 31s\tremaining: 22.8s\n",
      "9224:\ttotal: 4m 31s\tremaining: 22.7s\n",
      "9225:\ttotal: 4m 31s\tremaining: 22.7s\n",
      "9226:\ttotal: 4m 31s\tremaining: 22.7s\n",
      "9227:\ttotal: 4m 31s\tremaining: 22.7s\n",
      "9228:\ttotal: 4m 31s\tremaining: 22.6s\n",
      "9229:\ttotal: 4m 31s\tremaining: 22.6s\n",
      "9230:\ttotal: 4m 31s\tremaining: 22.6s\n",
      "9231:\ttotal: 4m 31s\tremaining: 22.5s\n",
      "9232:\ttotal: 4m 31s\tremaining: 22.5s\n",
      "9233:\ttotal: 4m 31s\tremaining: 22.5s\n",
      "9234:\ttotal: 4m 31s\tremaining: 22.4s\n",
      "9235:\ttotal: 4m 31s\tremaining: 22.4s\n",
      "9236:\ttotal: 4m 31s\tremaining: 22.4s\n",
      "9237:\ttotal: 4m 31s\tremaining: 22.4s\n",
      "9238:\ttotal: 4m 31s\tremaining: 22.3s\n",
      "9239:\ttotal: 4m 31s\tremaining: 22.3s\n",
      "9240:\ttotal: 4m 31s\tremaining: 22.3s\n",
      "9241:\ttotal: 4m 31s\tremaining: 22.2s\n",
      "9242:\ttotal: 4m 31s\tremaining: 22.2s\n",
      "9243:\ttotal: 4m 31s\tremaining: 22.2s\n",
      "9244:\ttotal: 4m 31s\tremaining: 22.2s\n",
      "9245:\ttotal: 4m 31s\tremaining: 22.1s\n",
      "9246:\ttotal: 4m 31s\tremaining: 22.1s\n",
      "9247:\ttotal: 4m 31s\tremaining: 22.1s\n",
      "9248:\ttotal: 4m 31s\tremaining: 22s\n",
      "9249:\ttotal: 4m 31s\tremaining: 22s\n",
      "9250:\ttotal: 4m 31s\tremaining: 22s\n",
      "9251:\ttotal: 4m 31s\tremaining: 22s\n",
      "9252:\ttotal: 4m 31s\tremaining: 21.9s\n",
      "9253:\ttotal: 4m 31s\tremaining: 21.9s\n",
      "9254:\ttotal: 4m 32s\tremaining: 21.9s\n",
      "9255:\ttotal: 4m 32s\tremaining: 21.8s\n",
      "9256:\ttotal: 4m 32s\tremaining: 21.8s\n",
      "9257:\ttotal: 4m 32s\tremaining: 21.8s\n",
      "9258:\ttotal: 4m 32s\tremaining: 21.8s\n",
      "9259:\ttotal: 4m 32s\tremaining: 21.7s\n",
      "9260:\ttotal: 4m 32s\tremaining: 21.7s\n",
      "9261:\ttotal: 4m 32s\tremaining: 21.7s\n",
      "9262:\ttotal: 4m 32s\tremaining: 21.6s\n",
      "9263:\ttotal: 4m 32s\tremaining: 21.6s\n",
      "9264:\ttotal: 4m 32s\tremaining: 21.6s\n",
      "9265:\ttotal: 4m 32s\tremaining: 21.6s\n",
      "9266:\ttotal: 4m 32s\tremaining: 21.5s\n",
      "9267:\ttotal: 4m 32s\tremaining: 21.5s\n",
      "9268:\ttotal: 4m 32s\tremaining: 21.5s\n",
      "9269:\ttotal: 4m 32s\tremaining: 21.4s\n",
      "9270:\ttotal: 4m 32s\tremaining: 21.4s\n",
      "9271:\ttotal: 4m 32s\tremaining: 21.4s\n",
      "9272:\ttotal: 4m 32s\tremaining: 21.4s\n",
      "9273:\ttotal: 4m 32s\tremaining: 21.3s\n",
      "9274:\ttotal: 4m 32s\tremaining: 21.3s\n",
      "9275:\ttotal: 4m 32s\tremaining: 21.3s\n",
      "9276:\ttotal: 4m 32s\tremaining: 21.2s\n",
      "9277:\ttotal: 4m 33s\tremaining: 21.2s\n",
      "9278:\ttotal: 4m 33s\tremaining: 21.2s\n",
      "9279:\ttotal: 4m 33s\tremaining: 21.2s\n",
      "9280:\ttotal: 4m 33s\tremaining: 21.1s\n",
      "9281:\ttotal: 4m 33s\tremaining: 21.1s\n",
      "9282:\ttotal: 4m 33s\tremaining: 21.1s\n",
      "9283:\ttotal: 4m 33s\tremaining: 21s\n",
      "9284:\ttotal: 4m 33s\tremaining: 21s\n",
      "9285:\ttotal: 4m 33s\tremaining: 21s\n",
      "9286:\ttotal: 4m 33s\tremaining: 21s\n",
      "9287:\ttotal: 4m 33s\tremaining: 20.9s\n",
      "9288:\ttotal: 4m 33s\tremaining: 20.9s\n",
      "9289:\ttotal: 4m 33s\tremaining: 20.9s\n",
      "9290:\ttotal: 4m 33s\tremaining: 20.8s\n",
      "9291:\ttotal: 4m 33s\tremaining: 20.8s\n",
      "9292:\ttotal: 4m 33s\tremaining: 20.8s\n",
      "9293:\ttotal: 4m 33s\tremaining: 20.8s\n",
      "9294:\ttotal: 4m 33s\tremaining: 20.7s\n",
      "9295:\ttotal: 4m 33s\tremaining: 20.7s\n",
      "9296:\ttotal: 4m 33s\tremaining: 20.7s\n",
      "9297:\ttotal: 4m 33s\tremaining: 20.6s\n",
      "9298:\ttotal: 4m 33s\tremaining: 20.6s\n",
      "9299:\ttotal: 4m 33s\tremaining: 20.6s\n",
      "9300:\ttotal: 4m 33s\tremaining: 20.6s\n",
      "9301:\ttotal: 4m 33s\tremaining: 20.5s\n",
      "9302:\ttotal: 4m 33s\tremaining: 20.5s\n",
      "9303:\ttotal: 4m 34s\tremaining: 20.5s\n",
      "9304:\ttotal: 4m 34s\tremaining: 20.4s\n",
      "9305:\ttotal: 4m 34s\tremaining: 20.4s\n",
      "9306:\ttotal: 4m 34s\tremaining: 20.4s\n",
      "9307:\ttotal: 4m 34s\tremaining: 20.4s\n",
      "9308:\ttotal: 4m 34s\tremaining: 20.3s\n",
      "9309:\ttotal: 4m 34s\tremaining: 20.3s\n",
      "9310:\ttotal: 4m 34s\tremaining: 20.3s\n",
      "9311:\ttotal: 4m 34s\tremaining: 20.2s\n",
      "9312:\ttotal: 4m 34s\tremaining: 20.2s\n",
      "9313:\ttotal: 4m 34s\tremaining: 20.2s\n",
      "9314:\ttotal: 4m 34s\tremaining: 20.1s\n",
      "9315:\ttotal: 4m 34s\tremaining: 20.1s\n",
      "9316:\ttotal: 4m 34s\tremaining: 20.1s\n",
      "9317:\ttotal: 4m 34s\tremaining: 20.1s\n",
      "9318:\ttotal: 4m 34s\tremaining: 20s\n",
      "9319:\ttotal: 4m 34s\tremaining: 20s\n",
      "9320:\ttotal: 4m 34s\tremaining: 20s\n",
      "9321:\ttotal: 4m 34s\tremaining: 19.9s\n",
      "9322:\ttotal: 4m 34s\tremaining: 19.9s\n",
      "9323:\ttotal: 4m 34s\tremaining: 19.9s\n",
      "9324:\ttotal: 4m 34s\tremaining: 19.9s\n",
      "9325:\ttotal: 4m 34s\tremaining: 19.8s\n",
      "9326:\ttotal: 4m 34s\tremaining: 19.8s\n",
      "9327:\ttotal: 4m 34s\tremaining: 19.8s\n",
      "9328:\ttotal: 4m 34s\tremaining: 19.7s\n",
      "9329:\ttotal: 4m 34s\tremaining: 19.7s\n",
      "9330:\ttotal: 4m 35s\tremaining: 19.7s\n",
      "9331:\ttotal: 4m 35s\tremaining: 19.7s\n",
      "9332:\ttotal: 4m 35s\tremaining: 19.6s\n",
      "9333:\ttotal: 4m 35s\tremaining: 19.6s\n",
      "9334:\ttotal: 4m 35s\tremaining: 19.6s\n",
      "9335:\ttotal: 4m 35s\tremaining: 19.5s\n",
      "9336:\ttotal: 4m 35s\tremaining: 19.5s\n",
      "9337:\ttotal: 4m 35s\tremaining: 19.5s\n",
      "9338:\ttotal: 4m 35s\tremaining: 19.5s\n",
      "9339:\ttotal: 4m 35s\tremaining: 19.4s\n",
      "9340:\ttotal: 4m 35s\tremaining: 19.4s\n",
      "9341:\ttotal: 4m 35s\tremaining: 19.4s\n",
      "9342:\ttotal: 4m 35s\tremaining: 19.3s\n",
      "9343:\ttotal: 4m 35s\tremaining: 19.3s\n",
      "9344:\ttotal: 4m 35s\tremaining: 19.3s\n",
      "9345:\ttotal: 4m 35s\tremaining: 19.3s\n",
      "9346:\ttotal: 4m 35s\tremaining: 19.2s\n",
      "9347:\ttotal: 4m 35s\tremaining: 19.2s\n",
      "9348:\ttotal: 4m 35s\tremaining: 19.2s\n",
      "9349:\ttotal: 4m 35s\tremaining: 19.1s\n",
      "9350:\ttotal: 4m 35s\tremaining: 19.1s\n",
      "9351:\ttotal: 4m 35s\tremaining: 19.1s\n",
      "9352:\ttotal: 4m 35s\tremaining: 19.1s\n",
      "9353:\ttotal: 4m 36s\tremaining: 19s\n",
      "9354:\ttotal: 4m 36s\tremaining: 19s\n",
      "9355:\ttotal: 4m 36s\tremaining: 19s\n",
      "9356:\ttotal: 4m 36s\tremaining: 18.9s\n",
      "9357:\ttotal: 4m 36s\tremaining: 18.9s\n",
      "9358:\ttotal: 4m 36s\tremaining: 18.9s\n",
      "9359:\ttotal: 4m 36s\tremaining: 18.9s\n",
      "9360:\ttotal: 4m 36s\tremaining: 18.8s\n",
      "9361:\ttotal: 4m 36s\tremaining: 18.8s\n",
      "9362:\ttotal: 4m 36s\tremaining: 18.8s\n",
      "9363:\ttotal: 4m 36s\tremaining: 18.7s\n",
      "9364:\ttotal: 4m 36s\tremaining: 18.7s\n",
      "9365:\ttotal: 4m 36s\tremaining: 18.7s\n",
      "9366:\ttotal: 4m 36s\tremaining: 18.6s\n",
      "9367:\ttotal: 4m 36s\tremaining: 18.6s\n",
      "9368:\ttotal: 4m 36s\tremaining: 18.6s\n",
      "9369:\ttotal: 4m 36s\tremaining: 18.6s\n",
      "9370:\ttotal: 4m 36s\tremaining: 18.5s\n",
      "9371:\ttotal: 4m 36s\tremaining: 18.5s\n",
      "9372:\ttotal: 4m 36s\tremaining: 18.5s\n",
      "9373:\ttotal: 4m 36s\tremaining: 18.4s\n",
      "9374:\ttotal: 4m 36s\tremaining: 18.4s\n",
      "9375:\ttotal: 4m 36s\tremaining: 18.4s\n",
      "9376:\ttotal: 4m 36s\tremaining: 18.4s\n",
      "9377:\ttotal: 4m 36s\tremaining: 18.3s\n",
      "9378:\ttotal: 4m 36s\tremaining: 18.3s\n",
      "9379:\ttotal: 4m 36s\tremaining: 18.3s\n",
      "9380:\ttotal: 4m 36s\tremaining: 18.2s\n",
      "9381:\ttotal: 4m 36s\tremaining: 18.2s\n",
      "9382:\ttotal: 4m 36s\tremaining: 18.2s\n",
      "9383:\ttotal: 4m 36s\tremaining: 18.1s\n",
      "9384:\ttotal: 4m 36s\tremaining: 18.1s\n",
      "9385:\ttotal: 4m 36s\tremaining: 18.1s\n",
      "9386:\ttotal: 4m 36s\tremaining: 18.1s\n",
      "9387:\ttotal: 4m 37s\tremaining: 18s\n",
      "9388:\ttotal: 4m 37s\tremaining: 18s\n",
      "9389:\ttotal: 4m 37s\tremaining: 18s\n",
      "9390:\ttotal: 4m 37s\tremaining: 17.9s\n",
      "9391:\ttotal: 4m 37s\tremaining: 17.9s\n",
      "9392:\ttotal: 4m 37s\tremaining: 17.9s\n",
      "9393:\ttotal: 4m 37s\tremaining: 17.9s\n",
      "9394:\ttotal: 4m 37s\tremaining: 17.8s\n",
      "9395:\ttotal: 4m 37s\tremaining: 17.8s\n",
      "9396:\ttotal: 4m 37s\tremaining: 17.8s\n",
      "9397:\ttotal: 4m 37s\tremaining: 17.7s\n",
      "9398:\ttotal: 4m 37s\tremaining: 17.7s\n",
      "9399:\ttotal: 4m 37s\tremaining: 17.7s\n",
      "9400:\ttotal: 4m 37s\tremaining: 17.6s\n",
      "9401:\ttotal: 4m 37s\tremaining: 17.6s\n",
      "9402:\ttotal: 4m 37s\tremaining: 17.6s\n",
      "9403:\ttotal: 4m 37s\tremaining: 17.6s\n",
      "9404:\ttotal: 4m 37s\tremaining: 17.5s\n",
      "9405:\ttotal: 4m 37s\tremaining: 17.5s\n",
      "9406:\ttotal: 4m 37s\tremaining: 17.5s\n",
      "9407:\ttotal: 4m 37s\tremaining: 17.4s\n",
      "9408:\ttotal: 4m 37s\tremaining: 17.4s\n",
      "9409:\ttotal: 4m 37s\tremaining: 17.4s\n",
      "9410:\ttotal: 4m 37s\tremaining: 17.3s\n",
      "9411:\ttotal: 4m 37s\tremaining: 17.3s\n",
      "9412:\ttotal: 4m 37s\tremaining: 17.3s\n",
      "9413:\ttotal: 4m 37s\tremaining: 17.3s\n",
      "9414:\ttotal: 4m 37s\tremaining: 17.2s\n",
      "9415:\ttotal: 4m 37s\tremaining: 17.2s\n",
      "9416:\ttotal: 4m 37s\tremaining: 17.2s\n",
      "9417:\ttotal: 4m 37s\tremaining: 17.1s\n",
      "9418:\ttotal: 4m 37s\tremaining: 17.1s\n",
      "9419:\ttotal: 4m 37s\tremaining: 17.1s\n",
      "9420:\ttotal: 4m 37s\tremaining: 17s\n",
      "9421:\ttotal: 4m 37s\tremaining: 17s\n",
      "9422:\ttotal: 4m 37s\tremaining: 17s\n",
      "9423:\ttotal: 4m 37s\tremaining: 17s\n",
      "9424:\ttotal: 4m 37s\tremaining: 16.9s\n",
      "9425:\ttotal: 4m 38s\tremaining: 16.9s\n",
      "9426:\ttotal: 4m 38s\tremaining: 16.9s\n",
      "9427:\ttotal: 4m 38s\tremaining: 16.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9428:\ttotal: 4m 38s\tremaining: 16.8s\n",
      "9429:\ttotal: 4m 38s\tremaining: 16.8s\n",
      "9430:\ttotal: 4m 38s\tremaining: 16.8s\n",
      "9431:\ttotal: 4m 38s\tremaining: 16.7s\n",
      "9432:\ttotal: 4m 38s\tremaining: 16.7s\n",
      "9433:\ttotal: 4m 38s\tremaining: 16.7s\n",
      "9434:\ttotal: 4m 38s\tremaining: 16.6s\n",
      "9435:\ttotal: 4m 38s\tremaining: 16.6s\n",
      "9436:\ttotal: 4m 38s\tremaining: 16.6s\n",
      "9437:\ttotal: 4m 38s\tremaining: 16.5s\n",
      "9438:\ttotal: 4m 38s\tremaining: 16.5s\n",
      "9439:\ttotal: 4m 38s\tremaining: 16.5s\n",
      "9440:\ttotal: 4m 38s\tremaining: 16.5s\n",
      "9441:\ttotal: 4m 38s\tremaining: 16.4s\n",
      "9442:\ttotal: 4m 38s\tremaining: 16.4s\n",
      "9443:\ttotal: 4m 38s\tremaining: 16.4s\n",
      "9444:\ttotal: 4m 38s\tremaining: 16.3s\n",
      "9445:\ttotal: 4m 38s\tremaining: 16.3s\n",
      "9446:\ttotal: 4m 38s\tremaining: 16.3s\n",
      "9447:\ttotal: 4m 38s\tremaining: 16.3s\n",
      "9448:\ttotal: 4m 38s\tremaining: 16.2s\n",
      "9449:\ttotal: 4m 38s\tremaining: 16.2s\n",
      "9450:\ttotal: 4m 38s\tremaining: 16.2s\n",
      "9451:\ttotal: 4m 38s\tremaining: 16.1s\n",
      "9452:\ttotal: 4m 38s\tremaining: 16.1s\n",
      "9453:\ttotal: 4m 38s\tremaining: 16.1s\n",
      "9454:\ttotal: 4m 38s\tremaining: 16s\n",
      "9455:\ttotal: 4m 38s\tremaining: 16s\n",
      "9456:\ttotal: 4m 38s\tremaining: 16s\n",
      "9457:\ttotal: 4m 38s\tremaining: 16s\n",
      "9458:\ttotal: 4m 39s\tremaining: 15.9s\n",
      "9459:\ttotal: 4m 39s\tremaining: 15.9s\n",
      "9460:\ttotal: 4m 39s\tremaining: 15.9s\n",
      "9461:\ttotal: 4m 39s\tremaining: 15.8s\n",
      "9462:\ttotal: 4m 39s\tremaining: 15.8s\n",
      "9463:\ttotal: 4m 39s\tremaining: 15.8s\n",
      "9464:\ttotal: 4m 39s\tremaining: 15.7s\n",
      "9465:\ttotal: 4m 39s\tremaining: 15.7s\n",
      "9466:\ttotal: 4m 39s\tremaining: 15.7s\n",
      "9467:\ttotal: 4m 39s\tremaining: 15.7s\n",
      "9468:\ttotal: 4m 39s\tremaining: 15.6s\n",
      "9469:\ttotal: 4m 39s\tremaining: 15.6s\n",
      "9470:\ttotal: 4m 39s\tremaining: 15.6s\n",
      "9471:\ttotal: 4m 39s\tremaining: 15.5s\n",
      "9472:\ttotal: 4m 39s\tremaining: 15.5s\n",
      "9473:\ttotal: 4m 39s\tremaining: 15.5s\n",
      "9474:\ttotal: 4m 39s\tremaining: 15.5s\n",
      "9475:\ttotal: 4m 39s\tremaining: 15.4s\n",
      "9476:\ttotal: 4m 39s\tremaining: 15.4s\n",
      "9477:\ttotal: 4m 39s\tremaining: 15.4s\n",
      "9478:\ttotal: 4m 39s\tremaining: 15.3s\n",
      "9479:\ttotal: 4m 39s\tremaining: 15.3s\n",
      "9480:\ttotal: 4m 39s\tremaining: 15.3s\n",
      "9481:\ttotal: 4m 39s\tremaining: 15.2s\n",
      "9482:\ttotal: 4m 39s\tremaining: 15.2s\n",
      "9483:\ttotal: 4m 39s\tremaining: 15.2s\n",
      "9484:\ttotal: 4m 39s\tremaining: 15.2s\n",
      "9485:\ttotal: 4m 39s\tremaining: 15.1s\n",
      "9486:\ttotal: 4m 39s\tremaining: 15.1s\n",
      "9487:\ttotal: 4m 39s\tremaining: 15.1s\n",
      "9488:\ttotal: 4m 39s\tremaining: 15s\n",
      "9489:\ttotal: 4m 39s\tremaining: 15s\n",
      "9490:\ttotal: 4m 39s\tremaining: 15s\n",
      "9491:\ttotal: 4m 39s\tremaining: 14.9s\n",
      "9492:\ttotal: 4m 39s\tremaining: 14.9s\n",
      "9493:\ttotal: 4m 39s\tremaining: 14.9s\n",
      "9494:\ttotal: 4m 40s\tremaining: 14.9s\n",
      "9495:\ttotal: 4m 40s\tremaining: 14.8s\n",
      "9496:\ttotal: 4m 40s\tremaining: 14.8s\n",
      "9497:\ttotal: 4m 40s\tremaining: 14.8s\n",
      "9498:\ttotal: 4m 40s\tremaining: 14.7s\n",
      "9499:\ttotal: 4m 40s\tremaining: 14.7s\n",
      "9500:\ttotal: 4m 40s\tremaining: 14.7s\n",
      "9501:\ttotal: 4m 40s\tremaining: 14.7s\n",
      "9502:\ttotal: 4m 40s\tremaining: 14.6s\n",
      "9503:\ttotal: 4m 40s\tremaining: 14.6s\n",
      "9504:\ttotal: 4m 40s\tremaining: 14.6s\n",
      "9505:\ttotal: 4m 40s\tremaining: 14.5s\n",
      "9506:\ttotal: 4m 40s\tremaining: 14.5s\n",
      "9507:\ttotal: 4m 40s\tremaining: 14.5s\n",
      "9508:\ttotal: 4m 40s\tremaining: 14.4s\n",
      "9509:\ttotal: 4m 40s\tremaining: 14.4s\n",
      "9510:\ttotal: 4m 40s\tremaining: 14.4s\n",
      "9511:\ttotal: 4m 40s\tremaining: 14.4s\n",
      "9512:\ttotal: 4m 40s\tremaining: 14.3s\n",
      "9513:\ttotal: 4m 40s\tremaining: 14.3s\n",
      "9514:\ttotal: 4m 40s\tremaining: 14.3s\n",
      "9515:\ttotal: 4m 40s\tremaining: 14.2s\n",
      "9516:\ttotal: 4m 40s\tremaining: 14.2s\n",
      "9517:\ttotal: 4m 40s\tremaining: 14.2s\n",
      "9518:\ttotal: 4m 40s\tremaining: 14.2s\n",
      "9519:\ttotal: 4m 40s\tremaining: 14.1s\n",
      "9520:\ttotal: 4m 40s\tremaining: 14.1s\n",
      "9521:\ttotal: 4m 40s\tremaining: 14.1s\n",
      "9522:\ttotal: 4m 40s\tremaining: 14s\n",
      "9523:\ttotal: 4m 40s\tremaining: 14s\n",
      "9524:\ttotal: 4m 40s\tremaining: 14s\n",
      "9525:\ttotal: 4m 40s\tremaining: 13.9s\n",
      "9526:\ttotal: 4m 40s\tremaining: 13.9s\n",
      "9527:\ttotal: 4m 40s\tremaining: 13.9s\n",
      "9528:\ttotal: 4m 40s\tremaining: 13.9s\n",
      "9529:\ttotal: 4m 40s\tremaining: 13.8s\n",
      "9530:\ttotal: 4m 40s\tremaining: 13.8s\n",
      "9531:\ttotal: 4m 40s\tremaining: 13.8s\n",
      "9532:\ttotal: 4m 40s\tremaining: 13.7s\n",
      "9533:\ttotal: 4m 41s\tremaining: 13.7s\n",
      "9534:\ttotal: 4m 41s\tremaining: 13.7s\n",
      "9535:\ttotal: 4m 41s\tremaining: 13.6s\n",
      "9536:\ttotal: 4m 41s\tremaining: 13.6s\n",
      "9537:\ttotal: 4m 41s\tremaining: 13.6s\n",
      "9538:\ttotal: 4m 41s\tremaining: 13.6s\n",
      "9539:\ttotal: 4m 41s\tremaining: 13.5s\n",
      "9540:\ttotal: 4m 41s\tremaining: 13.5s\n",
      "9541:\ttotal: 4m 41s\tremaining: 13.5s\n",
      "9542:\ttotal: 4m 41s\tremaining: 13.4s\n",
      "9543:\ttotal: 4m 41s\tremaining: 13.4s\n",
      "9544:\ttotal: 4m 41s\tremaining: 13.4s\n",
      "9545:\ttotal: 4m 41s\tremaining: 13.4s\n",
      "9546:\ttotal: 4m 41s\tremaining: 13.3s\n",
      "9547:\ttotal: 4m 41s\tremaining: 13.3s\n",
      "9548:\ttotal: 4m 41s\tremaining: 13.3s\n",
      "9549:\ttotal: 4m 41s\tremaining: 13.2s\n",
      "9550:\ttotal: 4m 41s\tremaining: 13.2s\n",
      "9551:\ttotal: 4m 41s\tremaining: 13.2s\n",
      "9552:\ttotal: 4m 41s\tremaining: 13.1s\n",
      "9553:\ttotal: 4m 41s\tremaining: 13.1s\n",
      "9554:\ttotal: 4m 41s\tremaining: 13.1s\n",
      "9555:\ttotal: 4m 41s\tremaining: 13.1s\n",
      "9556:\ttotal: 4m 41s\tremaining: 13s\n",
      "9557:\ttotal: 4m 41s\tremaining: 13s\n",
      "9558:\ttotal: 4m 41s\tremaining: 13s\n",
      "9559:\ttotal: 4m 41s\tremaining: 12.9s\n",
      "9560:\ttotal: 4m 41s\tremaining: 12.9s\n",
      "9561:\ttotal: 4m 41s\tremaining: 12.9s\n",
      "9562:\ttotal: 4m 41s\tremaining: 12.8s\n",
      "9563:\ttotal: 4m 41s\tremaining: 12.8s\n",
      "9564:\ttotal: 4m 41s\tremaining: 12.8s\n",
      "9565:\ttotal: 4m 41s\tremaining: 12.8s\n",
      "9566:\ttotal: 4m 41s\tremaining: 12.7s\n",
      "9567:\ttotal: 4m 42s\tremaining: 12.7s\n",
      "9568:\ttotal: 4m 42s\tremaining: 12.7s\n",
      "9569:\ttotal: 4m 42s\tremaining: 12.6s\n",
      "9570:\ttotal: 4m 42s\tremaining: 12.6s\n",
      "9571:\ttotal: 4m 42s\tremaining: 12.6s\n",
      "9572:\ttotal: 4m 42s\tremaining: 12.6s\n",
      "9573:\ttotal: 4m 42s\tremaining: 12.5s\n",
      "9574:\ttotal: 4m 42s\tremaining: 12.5s\n",
      "9575:\ttotal: 4m 42s\tremaining: 12.5s\n",
      "9576:\ttotal: 4m 42s\tremaining: 12.4s\n",
      "9577:\ttotal: 4m 42s\tremaining: 12.4s\n",
      "9578:\ttotal: 4m 42s\tremaining: 12.4s\n",
      "9579:\ttotal: 4m 42s\tremaining: 12.3s\n",
      "9580:\ttotal: 4m 42s\tremaining: 12.3s\n",
      "9581:\ttotal: 4m 42s\tremaining: 12.3s\n",
      "9582:\ttotal: 4m 42s\tremaining: 12.3s\n",
      "9583:\ttotal: 4m 42s\tremaining: 12.2s\n",
      "9584:\ttotal: 4m 42s\tremaining: 12.2s\n",
      "9585:\ttotal: 4m 42s\tremaining: 12.2s\n",
      "9586:\ttotal: 4m 42s\tremaining: 12.1s\n",
      "9587:\ttotal: 4m 42s\tremaining: 12.1s\n",
      "9588:\ttotal: 4m 42s\tremaining: 12.1s\n",
      "9589:\ttotal: 4m 42s\tremaining: 12.1s\n",
      "9590:\ttotal: 4m 42s\tremaining: 12s\n",
      "9591:\ttotal: 4m 42s\tremaining: 12s\n",
      "9592:\ttotal: 4m 42s\tremaining: 12s\n",
      "9593:\ttotal: 4m 42s\tremaining: 11.9s\n",
      "9594:\ttotal: 4m 42s\tremaining: 11.9s\n",
      "9595:\ttotal: 4m 42s\tremaining: 11.9s\n",
      "9596:\ttotal: 4m 42s\tremaining: 11.8s\n",
      "9597:\ttotal: 4m 42s\tremaining: 11.8s\n",
      "9598:\ttotal: 4m 42s\tremaining: 11.8s\n",
      "9599:\ttotal: 4m 42s\tremaining: 11.8s\n",
      "9600:\ttotal: 4m 42s\tremaining: 11.7s\n",
      "9601:\ttotal: 4m 42s\tremaining: 11.7s\n",
      "9602:\ttotal: 4m 42s\tremaining: 11.7s\n",
      "9603:\ttotal: 4m 42s\tremaining: 11.6s\n",
      "9604:\ttotal: 4m 42s\tremaining: 11.6s\n",
      "9605:\ttotal: 4m 42s\tremaining: 11.6s\n",
      "9606:\ttotal: 4m 42s\tremaining: 11.5s\n",
      "9607:\ttotal: 4m 43s\tremaining: 11.5s\n",
      "9608:\ttotal: 4m 43s\tremaining: 11.5s\n",
      "9609:\ttotal: 4m 43s\tremaining: 11.5s\n",
      "9610:\ttotal: 4m 43s\tremaining: 11.4s\n",
      "9611:\ttotal: 4m 43s\tremaining: 11.4s\n",
      "9612:\ttotal: 4m 43s\tremaining: 11.4s\n",
      "9613:\ttotal: 4m 43s\tremaining: 11.3s\n",
      "9614:\ttotal: 4m 43s\tremaining: 11.3s\n",
      "9615:\ttotal: 4m 43s\tremaining: 11.3s\n",
      "9616:\ttotal: 4m 43s\tremaining: 11.2s\n",
      "9617:\ttotal: 4m 43s\tremaining: 11.2s\n",
      "9618:\ttotal: 4m 43s\tremaining: 11.2s\n",
      "9619:\ttotal: 4m 43s\tremaining: 11.2s\n",
      "9620:\ttotal: 4m 43s\tremaining: 11.1s\n",
      "9621:\ttotal: 4m 43s\tremaining: 11.1s\n",
      "9622:\ttotal: 4m 43s\tremaining: 11.1s\n",
      "9623:\ttotal: 4m 43s\tremaining: 11s\n",
      "9624:\ttotal: 4m 43s\tremaining: 11s\n",
      "9625:\ttotal: 4m 43s\tremaining: 11s\n",
      "9626:\ttotal: 4m 43s\tremaining: 11s\n",
      "9627:\ttotal: 4m 43s\tremaining: 10.9s\n",
      "9628:\ttotal: 4m 43s\tremaining: 10.9s\n",
      "9629:\ttotal: 4m 43s\tremaining: 10.9s\n",
      "9630:\ttotal: 4m 43s\tremaining: 10.8s\n",
      "9631:\ttotal: 4m 43s\tremaining: 10.8s\n",
      "9632:\ttotal: 4m 43s\tremaining: 10.8s\n",
      "9633:\ttotal: 4m 43s\tremaining: 10.7s\n",
      "9634:\ttotal: 4m 43s\tremaining: 10.7s\n",
      "9635:\ttotal: 4m 43s\tremaining: 10.7s\n",
      "9636:\ttotal: 4m 43s\tremaining: 10.7s\n",
      "9637:\ttotal: 4m 43s\tremaining: 10.6s\n",
      "9638:\ttotal: 4m 43s\tremaining: 10.6s\n",
      "9639:\ttotal: 4m 43s\tremaining: 10.6s\n",
      "9640:\ttotal: 4m 43s\tremaining: 10.5s\n",
      "9641:\ttotal: 4m 43s\tremaining: 10.5s\n",
      "9642:\ttotal: 4m 43s\tremaining: 10.5s\n",
      "9643:\ttotal: 4m 43s\tremaining: 10.5s\n",
      "9644:\ttotal: 4m 44s\tremaining: 10.4s\n",
      "9645:\ttotal: 4m 44s\tremaining: 10.4s\n",
      "9646:\ttotal: 4m 44s\tremaining: 10.4s\n",
      "9647:\ttotal: 4m 44s\tremaining: 10.3s\n",
      "9648:\ttotal: 4m 44s\tremaining: 10.3s\n",
      "9649:\ttotal: 4m 44s\tremaining: 10.3s\n",
      "9650:\ttotal: 4m 44s\tremaining: 10.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9651:\ttotal: 4m 44s\tremaining: 10.2s\n",
      "9652:\ttotal: 4m 44s\tremaining: 10.2s\n",
      "9653:\ttotal: 4m 44s\tremaining: 10.2s\n",
      "9654:\ttotal: 4m 44s\tremaining: 10.1s\n",
      "9655:\ttotal: 4m 44s\tremaining: 10.1s\n",
      "9656:\ttotal: 4m 44s\tremaining: 10.1s\n",
      "9657:\ttotal: 4m 44s\tremaining: 10s\n",
      "9658:\ttotal: 4m 44s\tremaining: 10s\n",
      "9659:\ttotal: 4m 44s\tremaining: 9.98s\n",
      "9660:\ttotal: 4m 44s\tremaining: 9.95s\n",
      "9661:\ttotal: 4m 44s\tremaining: 9.92s\n",
      "9662:\ttotal: 4m 44s\tremaining: 9.89s\n",
      "9663:\ttotal: 4m 44s\tremaining: 9.86s\n",
      "9664:\ttotal: 4m 44s\tremaining: 9.83s\n",
      "9665:\ttotal: 4m 44s\tremaining: 9.8s\n",
      "9666:\ttotal: 4m 44s\tremaining: 9.77s\n",
      "9667:\ttotal: 4m 44s\tremaining: 9.74s\n",
      "9668:\ttotal: 4m 44s\tremaining: 9.71s\n",
      "9669:\ttotal: 4m 44s\tremaining: 9.68s\n",
      "9670:\ttotal: 4m 44s\tremaining: 9.65s\n",
      "9671:\ttotal: 4m 44s\tremaining: 9.62s\n",
      "9672:\ttotal: 4m 44s\tremaining: 9.59s\n",
      "9673:\ttotal: 4m 44s\tremaining: 9.56s\n",
      "9674:\ttotal: 4m 44s\tremaining: 9.54s\n",
      "9675:\ttotal: 4m 44s\tremaining: 9.51s\n",
      "9676:\ttotal: 4m 44s\tremaining: 9.47s\n",
      "9677:\ttotal: 4m 44s\tremaining: 9.45s\n",
      "9678:\ttotal: 4m 44s\tremaining: 9.42s\n",
      "9679:\ttotal: 4m 44s\tremaining: 9.39s\n",
      "9680:\ttotal: 4m 44s\tremaining: 9.36s\n",
      "9681:\ttotal: 4m 44s\tremaining: 9.33s\n",
      "9682:\ttotal: 4m 44s\tremaining: 9.3s\n",
      "9683:\ttotal: 4m 44s\tremaining: 9.27s\n",
      "9684:\ttotal: 4m 44s\tremaining: 9.24s\n",
      "9685:\ttotal: 4m 45s\tremaining: 9.21s\n",
      "9686:\ttotal: 4m 45s\tremaining: 9.18s\n",
      "9687:\ttotal: 4m 45s\tremaining: 9.15s\n",
      "9688:\ttotal: 4m 45s\tremaining: 9.12s\n",
      "9689:\ttotal: 4m 45s\tremaining: 9.1s\n",
      "9690:\ttotal: 4m 45s\tremaining: 9.07s\n",
      "9691:\ttotal: 4m 45s\tremaining: 9.04s\n",
      "9692:\ttotal: 4m 45s\tremaining: 9.01s\n",
      "9693:\ttotal: 4m 45s\tremaining: 8.98s\n",
      "9694:\ttotal: 4m 45s\tremaining: 8.95s\n",
      "9695:\ttotal: 4m 45s\tremaining: 8.92s\n",
      "9696:\ttotal: 4m 45s\tremaining: 8.89s\n",
      "9697:\ttotal: 4m 45s\tremaining: 8.86s\n",
      "9698:\ttotal: 4m 45s\tremaining: 8.83s\n",
      "9699:\ttotal: 4m 45s\tremaining: 8.8s\n",
      "9700:\ttotal: 4m 45s\tremaining: 8.77s\n",
      "9701:\ttotal: 4m 45s\tremaining: 8.74s\n",
      "9702:\ttotal: 4m 45s\tremaining: 8.71s\n",
      "9703:\ttotal: 4m 45s\tremaining: 8.69s\n",
      "9704:\ttotal: 4m 45s\tremaining: 8.66s\n",
      "9705:\ttotal: 4m 45s\tremaining: 8.63s\n",
      "9706:\ttotal: 4m 45s\tremaining: 8.6s\n",
      "9707:\ttotal: 4m 45s\tremaining: 8.57s\n",
      "9708:\ttotal: 4m 45s\tremaining: 8.54s\n",
      "9709:\ttotal: 4m 45s\tremaining: 8.51s\n",
      "9710:\ttotal: 4m 45s\tremaining: 8.48s\n",
      "9711:\ttotal: 4m 45s\tremaining: 8.45s\n",
      "9712:\ttotal: 4m 46s\tremaining: 8.42s\n",
      "9713:\ttotal: 4m 46s\tremaining: 8.39s\n",
      "9714:\ttotal: 4m 46s\tremaining: 8.36s\n",
      "9715:\ttotal: 4m 46s\tremaining: 8.33s\n",
      "9716:\ttotal: 4m 46s\tremaining: 8.3s\n",
      "9717:\ttotal: 4m 46s\tremaining: 8.27s\n",
      "9718:\ttotal: 4m 46s\tremaining: 8.24s\n",
      "9719:\ttotal: 4m 46s\tremaining: 8.21s\n",
      "9720:\ttotal: 4m 46s\tremaining: 8.19s\n",
      "9721:\ttotal: 4m 46s\tremaining: 8.16s\n",
      "9722:\ttotal: 4m 46s\tremaining: 8.13s\n",
      "9723:\ttotal: 4m 46s\tremaining: 8.1s\n",
      "9724:\ttotal: 4m 46s\tremaining: 8.07s\n",
      "9725:\ttotal: 4m 46s\tremaining: 8.04s\n",
      "9726:\ttotal: 4m 46s\tremaining: 8.01s\n",
      "9727:\ttotal: 4m 46s\tremaining: 7.98s\n",
      "9728:\ttotal: 4m 46s\tremaining: 7.95s\n",
      "9729:\ttotal: 4m 46s\tremaining: 7.92s\n",
      "9730:\ttotal: 4m 46s\tremaining: 7.89s\n",
      "9731:\ttotal: 4m 46s\tremaining: 7.86s\n",
      "9732:\ttotal: 4m 46s\tremaining: 7.83s\n",
      "9733:\ttotal: 4m 46s\tremaining: 7.8s\n",
      "9734:\ttotal: 4m 46s\tremaining: 7.77s\n",
      "9735:\ttotal: 4m 46s\tremaining: 7.74s\n",
      "9736:\ttotal: 4m 46s\tremaining: 7.71s\n",
      "9737:\ttotal: 4m 46s\tremaining: 7.68s\n",
      "9738:\ttotal: 4m 46s\tremaining: 7.66s\n",
      "9739:\ttotal: 4m 46s\tremaining: 7.63s\n",
      "9740:\ttotal: 4m 46s\tremaining: 7.6s\n",
      "9741:\ttotal: 4m 46s\tremaining: 7.57s\n",
      "9742:\ttotal: 4m 46s\tremaining: 7.54s\n",
      "9743:\ttotal: 4m 46s\tremaining: 7.51s\n",
      "9744:\ttotal: 4m 46s\tremaining: 7.48s\n",
      "9745:\ttotal: 4m 46s\tremaining: 7.45s\n",
      "9746:\ttotal: 4m 47s\tremaining: 7.42s\n",
      "9747:\ttotal: 4m 47s\tremaining: 7.39s\n",
      "9748:\ttotal: 4m 47s\tremaining: 7.36s\n",
      "9749:\ttotal: 4m 47s\tremaining: 7.33s\n",
      "9750:\ttotal: 4m 47s\tremaining: 7.3s\n",
      "9751:\ttotal: 4m 47s\tremaining: 7.27s\n",
      "9752:\ttotal: 4m 47s\tremaining: 7.24s\n",
      "9753:\ttotal: 4m 47s\tremaining: 7.21s\n",
      "9754:\ttotal: 4m 47s\tremaining: 7.18s\n",
      "9755:\ttotal: 4m 47s\tremaining: 7.16s\n",
      "9756:\ttotal: 4m 47s\tremaining: 7.13s\n",
      "9757:\ttotal: 4m 47s\tremaining: 7.1s\n",
      "9758:\ttotal: 4m 47s\tremaining: 7.07s\n",
      "9759:\ttotal: 4m 47s\tremaining: 7.04s\n",
      "9760:\ttotal: 4m 47s\tremaining: 7.01s\n",
      "9761:\ttotal: 4m 47s\tremaining: 6.98s\n",
      "9762:\ttotal: 4m 47s\tremaining: 6.95s\n",
      "9763:\ttotal: 4m 47s\tremaining: 6.92s\n",
      "9764:\ttotal: 4m 47s\tremaining: 6.89s\n",
      "9765:\ttotal: 4m 47s\tremaining: 6.86s\n",
      "9766:\ttotal: 4m 47s\tremaining: 6.83s\n",
      "9767:\ttotal: 4m 47s\tremaining: 6.8s\n",
      "9768:\ttotal: 4m 47s\tremaining: 6.77s\n",
      "9769:\ttotal: 4m 47s\tremaining: 6.75s\n",
      "9770:\ttotal: 4m 47s\tremaining: 6.71s\n",
      "9771:\ttotal: 4m 47s\tremaining: 6.68s\n",
      "9772:\ttotal: 4m 47s\tremaining: 6.66s\n",
      "9773:\ttotal: 4m 47s\tremaining: 6.63s\n",
      "9774:\ttotal: 4m 47s\tremaining: 6.6s\n",
      "9775:\ttotal: 4m 47s\tremaining: 6.57s\n",
      "9776:\ttotal: 4m 47s\tremaining: 6.54s\n",
      "9777:\ttotal: 4m 47s\tremaining: 6.51s\n",
      "9778:\ttotal: 4m 47s\tremaining: 6.48s\n",
      "9779:\ttotal: 4m 47s\tremaining: 6.45s\n",
      "9780:\ttotal: 4m 48s\tremaining: 6.42s\n",
      "9781:\ttotal: 4m 48s\tremaining: 6.39s\n",
      "9782:\ttotal: 4m 48s\tremaining: 6.36s\n",
      "9783:\ttotal: 4m 48s\tremaining: 6.33s\n",
      "9784:\ttotal: 4m 48s\tremaining: 6.3s\n",
      "9785:\ttotal: 4m 48s\tremaining: 6.27s\n",
      "9786:\ttotal: 4m 48s\tremaining: 6.24s\n",
      "9787:\ttotal: 4m 48s\tremaining: 6.21s\n",
      "9788:\ttotal: 4m 48s\tremaining: 6.18s\n",
      "9789:\ttotal: 4m 48s\tremaining: 6.15s\n",
      "9790:\ttotal: 4m 48s\tremaining: 6.12s\n",
      "9791:\ttotal: 4m 48s\tremaining: 6.09s\n",
      "9792:\ttotal: 4m 48s\tremaining: 6.06s\n",
      "9793:\ttotal: 4m 48s\tremaining: 6.03s\n",
      "9794:\ttotal: 4m 48s\tremaining: 6s\n",
      "9795:\ttotal: 4m 48s\tremaining: 5.97s\n",
      "9796:\ttotal: 4m 48s\tremaining: 5.95s\n",
      "9797:\ttotal: 4m 48s\tremaining: 5.92s\n",
      "9798:\ttotal: 4m 48s\tremaining: 5.89s\n",
      "9799:\ttotal: 4m 48s\tremaining: 5.86s\n",
      "9800:\ttotal: 4m 48s\tremaining: 5.83s\n",
      "9801:\ttotal: 4m 48s\tremaining: 5.8s\n",
      "9802:\ttotal: 4m 48s\tremaining: 5.77s\n",
      "9803:\ttotal: 4m 48s\tremaining: 5.74s\n",
      "9804:\ttotal: 4m 48s\tremaining: 5.71s\n",
      "9805:\ttotal: 4m 48s\tremaining: 5.68s\n",
      "9806:\ttotal: 4m 48s\tremaining: 5.65s\n",
      "9807:\ttotal: 4m 48s\tremaining: 5.62s\n",
      "9808:\ttotal: 4m 48s\tremaining: 5.59s\n",
      "9809:\ttotal: 4m 48s\tremaining: 5.56s\n",
      "9810:\ttotal: 4m 48s\tremaining: 5.54s\n",
      "9811:\ttotal: 4m 48s\tremaining: 5.51s\n",
      "9812:\ttotal: 4m 48s\tremaining: 5.48s\n",
      "9813:\ttotal: 4m 48s\tremaining: 5.45s\n",
      "9814:\ttotal: 4m 48s\tremaining: 5.42s\n",
      "9815:\ttotal: 4m 49s\tremaining: 5.39s\n",
      "9816:\ttotal: 4m 49s\tremaining: 5.36s\n",
      "9817:\ttotal: 4m 49s\tremaining: 5.33s\n",
      "9818:\ttotal: 4m 49s\tremaining: 5.3s\n",
      "9819:\ttotal: 4m 49s\tremaining: 5.27s\n",
      "9820:\ttotal: 4m 49s\tremaining: 5.24s\n",
      "9821:\ttotal: 4m 49s\tremaining: 5.21s\n",
      "9822:\ttotal: 4m 49s\tremaining: 5.18s\n",
      "9823:\ttotal: 4m 49s\tremaining: 5.15s\n",
      "9824:\ttotal: 4m 49s\tremaining: 5.12s\n",
      "9825:\ttotal: 4m 49s\tremaining: 5.09s\n",
      "9826:\ttotal: 4m 49s\tremaining: 5.06s\n",
      "9827:\ttotal: 4m 49s\tremaining: 5.04s\n",
      "9828:\ttotal: 4m 49s\tremaining: 5s\n",
      "9829:\ttotal: 4m 49s\tremaining: 4.98s\n",
      "9830:\ttotal: 4m 49s\tremaining: 4.95s\n",
      "9831:\ttotal: 4m 49s\tremaining: 4.92s\n",
      "9832:\ttotal: 4m 49s\tremaining: 4.89s\n",
      "9833:\ttotal: 4m 49s\tremaining: 4.86s\n",
      "9834:\ttotal: 4m 49s\tremaining: 4.83s\n",
      "9835:\ttotal: 4m 49s\tremaining: 4.8s\n",
      "9836:\ttotal: 4m 49s\tremaining: 4.77s\n",
      "9837:\ttotal: 4m 49s\tremaining: 4.74s\n",
      "9838:\ttotal: 4m 49s\tremaining: 4.71s\n",
      "9839:\ttotal: 4m 49s\tremaining: 4.68s\n",
      "9840:\ttotal: 4m 49s\tremaining: 4.65s\n",
      "9841:\ttotal: 4m 49s\tremaining: 4.62s\n",
      "9842:\ttotal: 4m 49s\tremaining: 4.59s\n",
      "9843:\ttotal: 4m 49s\tremaining: 4.56s\n",
      "9844:\ttotal: 4m 49s\tremaining: 4.53s\n",
      "9845:\ttotal: 4m 49s\tremaining: 4.5s\n",
      "9846:\ttotal: 4m 49s\tremaining: 4.47s\n",
      "9847:\ttotal: 4m 49s\tremaining: 4.45s\n",
      "9848:\ttotal: 4m 49s\tremaining: 4.42s\n",
      "9849:\ttotal: 4m 49s\tremaining: 4.39s\n",
      "9850:\ttotal: 4m 49s\tremaining: 4.36s\n",
      "9851:\ttotal: 4m 50s\tremaining: 4.33s\n",
      "9852:\ttotal: 4m 50s\tremaining: 4.3s\n",
      "9853:\ttotal: 4m 50s\tremaining: 4.27s\n",
      "9854:\ttotal: 4m 50s\tremaining: 4.24s\n",
      "9855:\ttotal: 4m 50s\tremaining: 4.21s\n",
      "9856:\ttotal: 4m 50s\tremaining: 4.18s\n",
      "9857:\ttotal: 4m 50s\tremaining: 4.15s\n",
      "9858:\ttotal: 4m 50s\tremaining: 4.12s\n",
      "9859:\ttotal: 4m 50s\tremaining: 4.09s\n",
      "9860:\ttotal: 4m 50s\tremaining: 4.06s\n",
      "9861:\ttotal: 4m 50s\tremaining: 4.03s\n",
      "9862:\ttotal: 4m 50s\tremaining: 4s\n",
      "9863:\ttotal: 4m 50s\tremaining: 3.97s\n",
      "9864:\ttotal: 4m 50s\tremaining: 3.94s\n",
      "9865:\ttotal: 4m 50s\tremaining: 3.92s\n",
      "9866:\ttotal: 4m 50s\tremaining: 3.88s\n",
      "9867:\ttotal: 4m 50s\tremaining: 3.86s\n",
      "9868:\ttotal: 4m 50s\tremaining: 3.83s\n",
      "9869:\ttotal: 4m 50s\tremaining: 3.8s\n",
      "9870:\ttotal: 4m 50s\tremaining: 3.77s\n",
      "9871:\ttotal: 4m 50s\tremaining: 3.74s\n",
      "9872:\ttotal: 4m 50s\tremaining: 3.71s\n",
      "9873:\ttotal: 4m 50s\tremaining: 3.68s\n",
      "9874:\ttotal: 4m 50s\tremaining: 3.65s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9875:\ttotal: 4m 50s\tremaining: 3.62s\n",
      "9876:\ttotal: 4m 50s\tremaining: 3.59s\n",
      "9877:\ttotal: 4m 50s\tremaining: 3.56s\n",
      "9878:\ttotal: 4m 50s\tremaining: 3.53s\n",
      "9879:\ttotal: 4m 50s\tremaining: 3.5s\n",
      "9880:\ttotal: 4m 50s\tremaining: 3.47s\n",
      "9881:\ttotal: 4m 50s\tremaining: 3.44s\n",
      "9882:\ttotal: 4m 50s\tremaining: 3.41s\n",
      "9883:\ttotal: 4m 50s\tremaining: 3.38s\n",
      "9884:\ttotal: 4m 50s\tremaining: 3.35s\n",
      "9885:\ttotal: 4m 51s\tremaining: 3.33s\n",
      "9886:\ttotal: 4m 51s\tremaining: 3.3s\n",
      "9887:\ttotal: 4m 51s\tremaining: 3.27s\n",
      "9888:\ttotal: 4m 51s\tremaining: 3.24s\n",
      "9889:\ttotal: 4m 51s\tremaining: 3.21s\n",
      "9890:\ttotal: 4m 51s\tremaining: 3.18s\n",
      "9891:\ttotal: 4m 51s\tremaining: 3.15s\n",
      "9892:\ttotal: 4m 51s\tremaining: 3.12s\n",
      "9893:\ttotal: 4m 51s\tremaining: 3.09s\n",
      "9894:\ttotal: 4m 51s\tremaining: 3.06s\n",
      "9895:\ttotal: 4m 51s\tremaining: 3.03s\n",
      "9896:\ttotal: 4m 51s\tremaining: 3s\n",
      "9897:\ttotal: 4m 51s\tremaining: 2.97s\n",
      "9898:\ttotal: 4m 51s\tremaining: 2.94s\n",
      "9899:\ttotal: 4m 51s\tremaining: 2.91s\n",
      "9900:\ttotal: 4m 51s\tremaining: 2.88s\n",
      "9901:\ttotal: 4m 51s\tremaining: 2.85s\n",
      "9902:\ttotal: 4m 51s\tremaining: 2.83s\n",
      "9903:\ttotal: 4m 51s\tremaining: 2.79s\n",
      "9904:\ttotal: 4m 51s\tremaining: 2.77s\n",
      "9905:\ttotal: 4m 51s\tremaining: 2.74s\n",
      "9906:\ttotal: 4m 51s\tremaining: 2.71s\n",
      "9907:\ttotal: 4m 51s\tremaining: 2.68s\n",
      "9908:\ttotal: 4m 51s\tremaining: 2.65s\n",
      "9909:\ttotal: 4m 51s\tremaining: 2.62s\n",
      "9910:\ttotal: 4m 51s\tremaining: 2.59s\n",
      "9911:\ttotal: 4m 51s\tremaining: 2.56s\n",
      "9912:\ttotal: 4m 51s\tremaining: 2.53s\n",
      "9913:\ttotal: 4m 51s\tremaining: 2.5s\n",
      "9914:\ttotal: 4m 51s\tremaining: 2.47s\n",
      "9915:\ttotal: 4m 51s\tremaining: 2.44s\n",
      "9916:\ttotal: 4m 51s\tremaining: 2.41s\n",
      "9917:\ttotal: 4m 51s\tremaining: 2.38s\n",
      "9918:\ttotal: 4m 51s\tremaining: 2.35s\n",
      "9919:\ttotal: 4m 51s\tremaining: 2.32s\n",
      "9920:\ttotal: 4m 51s\tremaining: 2.29s\n",
      "9921:\ttotal: 4m 51s\tremaining: 2.27s\n",
      "9922:\ttotal: 4m 52s\tremaining: 2.24s\n",
      "9923:\ttotal: 4m 52s\tremaining: 2.21s\n",
      "9924:\ttotal: 4m 52s\tremaining: 2.18s\n",
      "9925:\ttotal: 4m 52s\tremaining: 2.15s\n",
      "9926:\ttotal: 4m 52s\tremaining: 2.12s\n",
      "9927:\ttotal: 4m 52s\tremaining: 2.09s\n",
      "9928:\ttotal: 4m 52s\tremaining: 2.06s\n",
      "9929:\ttotal: 4m 52s\tremaining: 2.03s\n",
      "9930:\ttotal: 4m 52s\tremaining: 2s\n",
      "9931:\ttotal: 4m 52s\tremaining: 1.97s\n",
      "9932:\ttotal: 4m 52s\tremaining: 1.94s\n",
      "9933:\ttotal: 4m 52s\tremaining: 1.91s\n",
      "9934:\ttotal: 4m 52s\tremaining: 1.88s\n",
      "9935:\ttotal: 4m 52s\tremaining: 1.85s\n",
      "9936:\ttotal: 4m 52s\tremaining: 1.82s\n",
      "9937:\ttotal: 4m 52s\tremaining: 1.79s\n",
      "9938:\ttotal: 4m 52s\tremaining: 1.76s\n",
      "9939:\ttotal: 4m 52s\tremaining: 1.74s\n",
      "9940:\ttotal: 4m 52s\tremaining: 1.71s\n",
      "9941:\ttotal: 4m 52s\tremaining: 1.68s\n",
      "9942:\ttotal: 4m 52s\tremaining: 1.65s\n",
      "9943:\ttotal: 4m 52s\tremaining: 1.62s\n",
      "9944:\ttotal: 4m 52s\tremaining: 1.59s\n",
      "9945:\ttotal: 4m 52s\tremaining: 1.56s\n",
      "9946:\ttotal: 4m 52s\tremaining: 1.53s\n",
      "9947:\ttotal: 4m 52s\tremaining: 1.5s\n",
      "9948:\ttotal: 4m 52s\tremaining: 1.47s\n",
      "9949:\ttotal: 4m 52s\tremaining: 1.44s\n",
      "9950:\ttotal: 4m 52s\tremaining: 1.41s\n",
      "9951:\ttotal: 4m 52s\tremaining: 1.38s\n",
      "9952:\ttotal: 4m 52s\tremaining: 1.35s\n",
      "9953:\ttotal: 4m 52s\tremaining: 1.32s\n",
      "9954:\ttotal: 4m 52s\tremaining: 1.29s\n",
      "9955:\ttotal: 4m 52s\tremaining: 1.26s\n",
      "9956:\ttotal: 4m 52s\tremaining: 1.24s\n",
      "9957:\ttotal: 4m 53s\tremaining: 1.21s\n",
      "9958:\ttotal: 4m 53s\tremaining: 1.18s\n",
      "9959:\ttotal: 4m 53s\tremaining: 1.15s\n",
      "9960:\ttotal: 4m 53s\tremaining: 1.12s\n",
      "9961:\ttotal: 4m 53s\tremaining: 1.09s\n",
      "9962:\ttotal: 4m 53s\tremaining: 1.06s\n",
      "9963:\ttotal: 4m 53s\tremaining: 1.03s\n",
      "9964:\ttotal: 4m 53s\tremaining: 1s\n",
      "9965:\ttotal: 4m 53s\tremaining: 971ms\n",
      "9966:\ttotal: 4m 53s\tremaining: 942ms\n",
      "9967:\ttotal: 4m 53s\tremaining: 912ms\n",
      "9968:\ttotal: 4m 53s\tremaining: 883ms\n",
      "9969:\ttotal: 4m 53s\tremaining: 853ms\n",
      "9970:\ttotal: 4m 53s\tremaining: 824ms\n",
      "9971:\ttotal: 4m 53s\tremaining: 794ms\n",
      "9972:\ttotal: 4m 53s\tremaining: 765ms\n",
      "9973:\ttotal: 4m 53s\tremaining: 736ms\n",
      "9974:\ttotal: 4m 53s\tremaining: 706ms\n",
      "9975:\ttotal: 4m 53s\tremaining: 677ms\n",
      "9976:\ttotal: 4m 53s\tremaining: 647ms\n",
      "9977:\ttotal: 4m 53s\tremaining: 618ms\n",
      "9978:\ttotal: 4m 53s\tremaining: 588ms\n",
      "9979:\ttotal: 4m 53s\tremaining: 559ms\n",
      "9980:\ttotal: 4m 53s\tremaining: 530ms\n",
      "9981:\ttotal: 4m 53s\tremaining: 500ms\n",
      "9982:\ttotal: 4m 53s\tremaining: 471ms\n",
      "9983:\ttotal: 4m 53s\tremaining: 441ms\n",
      "9984:\ttotal: 4m 53s\tremaining: 412ms\n",
      "9985:\ttotal: 4m 53s\tremaining: 382ms\n",
      "9986:\ttotal: 4m 53s\tremaining: 353ms\n",
      "9987:\ttotal: 4m 53s\tremaining: 324ms\n",
      "9988:\ttotal: 4m 53s\tremaining: 294ms\n",
      "9989:\ttotal: 4m 53s\tremaining: 265ms\n",
      "9990:\ttotal: 4m 53s\tremaining: 235ms\n",
      "9991:\ttotal: 4m 53s\tremaining: 206ms\n",
      "9992:\ttotal: 4m 53s\tremaining: 176ms\n",
      "9993:\ttotal: 4m 53s\tremaining: 147ms\n",
      "9994:\ttotal: 4m 53s\tremaining: 118ms\n",
      "9995:\ttotal: 4m 54s\tremaining: 88.2ms\n",
      "9996:\ttotal: 4m 54s\tremaining: 58.8ms\n",
      "9997:\ttotal: 4m 54s\tremaining: 29.4ms\n",
      "9998:\ttotal: 4m 54s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0026\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_139\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0025\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_81\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 83.92%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0019\n",
       "                \n",
       "                    &plusmn; 0.0009\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_26\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 84.37%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0018\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_12\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 85.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0017\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_110\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 85.09%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0017\n",
       "                \n",
       "                    &plusmn; 0.0006\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_21\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 85.16%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0017\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_109\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0016\n",
       "                \n",
       "                    &plusmn; 0.0007\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_80\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.31%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0015\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_6\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.44%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0015\n",
       "                \n",
       "                    &plusmn; 0.0008\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_22\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.50%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0015\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_53\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.85%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0014\n",
       "                \n",
       "                    &plusmn; 0.0007\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_146\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.02%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0014\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_179\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.34%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0013\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_2\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.51%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0013\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_166\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.71%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0013\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_78\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.41%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0012\n",
       "                \n",
       "                    &plusmn; 0.0006\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_148\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0012\n",
       "                \n",
       "                    &plusmn; 0.0008\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_174\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.79%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0011\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_99\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.07%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0011\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_0\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.07%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 180 more &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X1, y, test_size = 0.2, random_state=42)\n",
    "model = CatBoostClassifier(iterations=9999,\n",
    "                                  max_depth=2,\n",
    "                                  learning_rate=0.02,\n",
    "                                  colsample_bylevel=0.03,\n",
    "                                  eval_metric='AUC',\n",
    "                                  objective=\"Logloss\").fit(X_train, y_train)\n",
    "base_features = list(X1.columns)\n",
    "perm = PermutationImportance(model, random_state=1).fit(X_val, y_val)\n",
    "eli5.show_weights(perm, feature_names = base_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0026\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_139\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0025\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_81\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 83.92%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0019\n",
       "                \n",
       "                    &plusmn; 0.0009\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_26\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 84.37%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0018\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_12\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 85.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0017\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_110\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 85.09%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0017\n",
       "                \n",
       "                    &plusmn; 0.0006\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_21\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 85.16%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0017\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_109\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0016\n",
       "                \n",
       "                    &plusmn; 0.0007\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_80\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.31%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0015\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_6\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.44%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0015\n",
       "                \n",
       "                    &plusmn; 0.0008\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_22\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.50%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0015\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_53\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.85%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0014\n",
       "                \n",
       "                    &plusmn; 0.0007\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_146\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.02%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0014\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_179\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.34%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0013\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_2\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.51%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0013\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_166\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.71%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0013\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_78\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.41%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0012\n",
       "                \n",
       "                    &plusmn; 0.0006\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_148\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0012\n",
       "                \n",
       "                    &plusmn; 0.0008\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_174\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.79%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0011\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_99\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.07%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0011\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_0\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.24%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0011\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_165\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.67%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0010\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_13\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.70%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0010\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_44\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.25%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0009\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_76\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.47%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0009\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_164\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.58%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0009\n",
       "                \n",
       "                    &plusmn; 0.0007\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_1\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.62%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0009\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_154\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.69%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0009\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_133\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.73%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0009\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_149\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.92%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0008\n",
       "                \n",
       "                    &plusmn; 0.0007\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_170\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.99%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0008\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_190\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.07%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0008\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_34\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.11%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0008\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_91\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.30%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0008\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_191\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.46%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0008\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_198\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.57%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0008\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_122\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0007\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_9\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.85%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0007\n",
       "                \n",
       "                    &plusmn; 0.0006\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_173\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.93%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0007\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_94\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.97%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0007\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_123\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.09%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0007\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_169\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.29%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0007\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_40\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.29%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0007\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_33\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.29%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0007\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_147\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.62%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0006\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_18\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.66%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0006\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_192\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.12%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0006\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_86\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.25%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_36\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.29%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_188\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.42%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_5\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.55%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_131\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.60%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_35\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.68%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_172\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.91%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_121\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_184\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.04%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_75\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.27%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_108\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.32%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0007\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_118\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.41%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_106\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.50%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_87\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.60%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_167\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.60%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0006\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_92\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.60%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_162\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_89\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_48\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.69%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0006\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_177\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.79%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_112\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.79%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_115\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.79%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_163\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.84%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_49\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.93%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_155\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0004\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_107\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.13%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_95\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.23%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_196\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.28%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_130\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.28%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_150\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.28%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_180\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.48%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_125\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.48%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_128\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.58%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_145\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.74%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_102\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.74%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_56\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.90%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_119\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.95%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_83\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_132\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_82\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.11%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_90\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.17%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_114\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.28%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_11\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.33%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_71\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.50%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_127\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.50%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_28\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.62%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_141\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.73%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_195\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.73%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_151\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.73%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_157\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.73%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_137\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.91%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_194\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_197\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.22%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_67\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.22%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_23\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.34%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_186\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.41%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_199\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.41%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_8\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.47%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_19\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.61%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_55\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.74%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_24\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.81%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0006\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_93\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.88%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_62\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.95%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_116\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_77\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_31\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.17%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_25\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.17%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_74\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.17%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_52\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.25%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_138\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.33%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_15\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.41%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_140\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.41%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_97\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.57%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_73\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.57%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 80 more &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_weights(perm, feature_names = base_features, top=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb\n",
      "0  0.010545  0.010837\n",
      "1  0.455996  0.423705\n",
      "2  0.004485  0.003918\n",
      "3  0.243523  0.253600\n",
      "4  0.097492  0.088431\n",
      "0.89951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12a.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9254\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12b.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Mar  5 01:48:53 2019\n",
      "[0]\tvalidation_0-auc:0.570315\n",
      "Will train until validation_0-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.854037\n",
      "[2000]\tvalidation_0-auc:0.876939\n",
      "[3000]\tvalidation_0-auc:0.885982\n",
      "[4000]\tvalidation_0-auc:0.890766\n",
      "[5000]\tvalidation_0-auc:0.893753\n",
      "[6000]\tvalidation_0-auc:0.895637\n",
      "[7000]\tvalidation_0-auc:0.896913\n",
      "[8000]\tvalidation_0-auc:0.897829\n",
      "[9000]\tvalidation_0-auc:0.898302\n",
      "[10000]\tvalidation_0-auc:0.898613\n",
      "[11000]\tvalidation_0-auc:0.898773\n",
      "[12000]\tvalidation_0-auc:0.898846\n",
      "Stopping. Best iteration:\n",
      "[11784]\tvalidation_0-auc:0.89891\n",
      "\n",
      "Fold 1 started at Tue Mar  5 02:13:25 2019\n",
      "[0]\tvalidation_0-auc:0.572324\n",
      "Will train until validation_0-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.848814\n",
      "[2000]\tvalidation_0-auc:0.873248\n",
      "[3000]\tvalidation_0-auc:0.883325\n",
      "[4000]\tvalidation_0-auc:0.888573\n",
      "[5000]\tvalidation_0-auc:0.891781\n",
      "[6000]\tvalidation_0-auc:0.893966\n",
      "[7000]\tvalidation_0-auc:0.895187\n",
      "[8000]\tvalidation_0-auc:0.896024\n",
      "[9000]\tvalidation_0-auc:0.896513\n",
      "[10000]\tvalidation_0-auc:0.896834\n",
      "[11000]\tvalidation_0-auc:0.897092\n",
      "[12000]\tvalidation_0-auc:0.897229\n",
      "[13000]\tvalidation_0-auc:0.897205\n",
      "Stopping. Best iteration:\n",
      "[12597]\tvalidation_0-auc:0.897274\n",
      "\n",
      "Fold 2 started at Tue Mar  5 02:43:56 2019\n",
      "[0]\tvalidation_0-auc:0.576043\n",
      "Will train until validation_0-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.854841\n",
      "[2000]\tvalidation_0-auc:0.877912\n",
      "[3000]\tvalidation_0-auc:0.886829\n",
      "[4000]\tvalidation_0-auc:0.891362\n",
      "[5000]\tvalidation_0-auc:0.894155\n",
      "[6000]\tvalidation_0-auc:0.895861\n",
      "[7000]\tvalidation_0-auc:0.897059\n",
      "[8000]\tvalidation_0-auc:0.897839\n",
      "[9000]\tvalidation_0-auc:0.898436\n",
      "[10000]\tvalidation_0-auc:0.898758\n",
      "[11000]\tvalidation_0-auc:0.899178\n",
      "[12000]\tvalidation_0-auc:0.899355\n",
      "[13000]\tvalidation_0-auc:0.899538\n",
      "[14000]\tvalidation_0-auc:0.899527\n",
      "Stopping. Best iteration:\n",
      "[13113]\tvalidation_0-auc:0.899575\n",
      "\n",
      "Fold 3 started at Tue Mar  5 03:16:02 2019\n",
      "[0]\tvalidation_0-auc:0.579766\n",
      "Will train until validation_0-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.854285\n",
      "[2000]\tvalidation_0-auc:0.878054\n",
      "[3000]\tvalidation_0-auc:0.887235\n",
      "[4000]\tvalidation_0-auc:0.892751\n",
      "[5000]\tvalidation_0-auc:0.89571\n",
      "[6000]\tvalidation_0-auc:0.897501\n",
      "[7000]\tvalidation_0-auc:0.898669\n",
      "[8000]\tvalidation_0-auc:0.899391\n",
      "[9000]\tvalidation_0-auc:0.900061\n",
      "[10000]\tvalidation_0-auc:0.900387\n",
      "[11000]\tvalidation_0-auc:0.900523\n",
      "[12000]\tvalidation_0-auc:0.900685\n",
      "[13000]\tvalidation_0-auc:0.900777\n",
      "[14000]\tvalidation_0-auc:0.900786\n",
      "Stopping. Best iteration:\n",
      "[13062]\tvalidation_0-auc:0.900807\n",
      "\n",
      "Fold 4 started at Tue Mar  5 03:48:37 2019\n",
      "[0]\tvalidation_0-auc:0.566121\n",
      "Will train until validation_0-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.855609\n",
      "[2000]\tvalidation_0-auc:0.881251\n",
      "[3000]\tvalidation_0-auc:0.891594\n",
      "[4000]\tvalidation_0-auc:0.89727\n",
      "[5000]\tvalidation_0-auc:0.901156\n",
      "[6000]\tvalidation_0-auc:0.903565\n",
      "[7000]\tvalidation_0-auc:0.904874\n",
      "[8000]\tvalidation_0-auc:0.905982\n",
      "[9000]\tvalidation_0-auc:0.906547\n",
      "[10000]\tvalidation_0-auc:0.907053\n",
      "[11000]\tvalidation_0-auc:0.907389\n",
      "[12000]\tvalidation_0-auc:0.907559\n",
      "[13000]\tvalidation_0-auc:0.907634\n",
      "Stopping. Best iteration:\n",
      "[12930]\tvalidation_0-auc:0.907643\n",
      "\n",
      "Fold 5 started at Tue Mar  5 04:21:23 2019\n",
      "[0]\tvalidation_0-auc:0.577991\n",
      "Will train until validation_0-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.853284\n",
      "[2000]\tvalidation_0-auc:0.877285\n",
      "[3000]\tvalidation_0-auc:0.887403\n",
      "[4000]\tvalidation_0-auc:0.892827\n",
      "[5000]\tvalidation_0-auc:0.896125\n",
      "[6000]\tvalidation_0-auc:0.897997\n",
      "[7000]\tvalidation_0-auc:0.899449\n",
      "[8000]\tvalidation_0-auc:0.900303\n",
      "[9000]\tvalidation_0-auc:0.900798\n",
      "[10000]\tvalidation_0-auc:0.901097\n",
      "[11000]\tvalidation_0-auc:0.901261\n",
      "[12000]\tvalidation_0-auc:0.901456\n",
      "[13000]\tvalidation_0-auc:0.90156\n",
      "[14000]\tvalidation_0-auc:0.901567\n",
      "[15000]\tvalidation_0-auc:0.901575\n",
      "Stopping. Best iteration:\n",
      "[14409]\tvalidation_0-auc:0.901625\n",
      "\n",
      "Fold 6 started at Tue Mar  5 04:57:55 2019\n",
      "[0]\tvalidation_0-auc:0.572469\n",
      "Will train until validation_0-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.851194\n",
      "[2000]\tvalidation_0-auc:0.875831\n",
      "[3000]\tvalidation_0-auc:0.885952\n",
      "[4000]\tvalidation_0-auc:0.891412\n",
      "[5000]\tvalidation_0-auc:0.894974\n",
      "[6000]\tvalidation_0-auc:0.897199\n",
      "[7000]\tvalidation_0-auc:0.898736\n",
      "[8000]\tvalidation_0-auc:0.89975\n",
      "[9000]\tvalidation_0-auc:0.900472\n",
      "[10000]\tvalidation_0-auc:0.900983\n",
      "[11000]\tvalidation_0-auc:0.901242\n",
      "[12000]\tvalidation_0-auc:0.901523\n",
      "[13000]\tvalidation_0-auc:0.901634\n",
      "[14000]\tvalidation_0-auc:0.901762\n",
      "[15000]\tvalidation_0-auc:0.901769\n",
      "[16000]\tvalidation_0-auc:0.901781\n",
      "[17000]\tvalidation_0-auc:0.901793\n",
      "Stopping. Best iteration:\n",
      "[16489]\tvalidation_0-auc:0.901841\n",
      "\n",
      "Fold 7 started at Tue Mar  5 05:38:13 2019\n",
      "[0]\tvalidation_0-auc:0.574775\n",
      "Will train until validation_0-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.844687\n",
      "[2000]\tvalidation_0-auc:0.87002\n",
      "[3000]\tvalidation_0-auc:0.87936\n",
      "[4000]\tvalidation_0-auc:0.88416\n",
      "[5000]\tvalidation_0-auc:0.886948\n",
      "[6000]\tvalidation_0-auc:0.888903\n",
      "[7000]\tvalidation_0-auc:0.890265\n",
      "[8000]\tvalidation_0-auc:0.891172\n",
      "[9000]\tvalidation_0-auc:0.891816\n",
      "[10000]\tvalidation_0-auc:0.892205\n",
      "[11000]\tvalidation_0-auc:0.892537\n",
      "[12000]\tvalidation_0-auc:0.892721\n",
      "[13000]\tvalidation_0-auc:0.892891\n",
      "[14000]\tvalidation_0-auc:0.893003\n",
      "[15000]\tvalidation_0-auc:0.893071\n",
      "[16000]\tvalidation_0-auc:0.893144\n",
      "[17000]\tvalidation_0-auc:0.893181\n",
      "Stopping. Best iteration:\n",
      "[16502]\tvalidation_0-auc:0.893213\n",
      "\n",
      "Fold 8 started at Tue Mar  5 06:17:36 2019\n",
      "[0]\tvalidation_0-auc:0.569739\n",
      "Will train until validation_0-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.842715\n",
      "[2000]\tvalidation_0-auc:0.866193\n",
      "[3000]\tvalidation_0-auc:0.87714\n",
      "[4000]\tvalidation_0-auc:0.883444\n",
      "[5000]\tvalidation_0-auc:0.887444\n",
      "[6000]\tvalidation_0-auc:0.88971\n",
      "[7000]\tvalidation_0-auc:0.891187\n",
      "[8000]\tvalidation_0-auc:0.892272\n",
      "[9000]\tvalidation_0-auc:0.89305\n",
      "[10000]\tvalidation_0-auc:0.893552\n",
      "[11000]\tvalidation_0-auc:0.893995\n",
      "[12000]\tvalidation_0-auc:0.89418\n",
      "[13000]\tvalidation_0-auc:0.894349\n",
      "[14000]\tvalidation_0-auc:0.894465\n",
      "[15000]\tvalidation_0-auc:0.894554\n",
      "[16000]\tvalidation_0-auc:0.894659\n",
      "[17000]\tvalidation_0-auc:0.894687\n",
      "[18000]\tvalidation_0-auc:0.894659\n",
      "Stopping. Best iteration:\n",
      "[17232]\tvalidation_0-auc:0.894728\n",
      "\n",
      "Fold 9 started at Tue Mar  5 06:58:39 2019\n",
      "[0]\tvalidation_0-auc:0.562701\n",
      "Will train until validation_0-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.850456\n",
      "[2000]\tvalidation_0-auc:0.872689\n",
      "[3000]\tvalidation_0-auc:0.882468\n",
      "[4000]\tvalidation_0-auc:0.887416\n",
      "[5000]\tvalidation_0-auc:0.890504\n",
      "[6000]\tvalidation_0-auc:0.892565\n",
      "[7000]\tvalidation_0-auc:0.894066\n",
      "[8000]\tvalidation_0-auc:0.894927\n",
      "[9000]\tvalidation_0-auc:0.895387\n",
      "[10000]\tvalidation_0-auc:0.895632\n",
      "[11000]\tvalidation_0-auc:0.895823\n",
      "[12000]\tvalidation_0-auc:0.895872\n",
      "Stopping. Best iteration:\n",
      "[11535]\tvalidation_0-auc:0.895948\n",
      "\n",
      "CV mean score: 0.8992, std: 0.0040.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_xgb, prediction_xgb, _ = train_model(X1, X2, y, params=np.random.randint(1,101), folds=folds, model_type='xgb', plot_feature_importance=False)\n",
    "oof.append(oof_xgb)\n",
    "preds.append(prediction_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_xgb_quant_1_10_1', oof)\n",
    "np.save('../cache/preds_xgb_quant_1_10_1', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb\n",
      "0  0.010545  0.010837  0.008313\n",
      "1  0.455996  0.423705  0.392034\n",
      "2  0.004485  0.003918  0.004919\n",
      "3  0.243523  0.253600  0.340744\n",
      "4  0.097492  0.088431  0.110311\n",
      "0.925455\n"
     ]
    }
   ],
   "source": [
    "stage2['xgb'] = oof_xgb\n",
    "stage2_test['xgb'] = prediction_xgb\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12c.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_12 9559\n",
      "var_68 459\n",
      "var_91 7959\n",
      "var_103 9376\n",
      "var_108 8524\n"
     ]
    }
   ],
   "source": [
    "for col in list(X1.columns):\n",
    "    if X1[col].nunique() <= 10000:\n",
    "        print(col, X1[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1a384093c8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1a34704128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(X1['var_12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.563783\tvalid_1's auc: 0.562899\n",
      "Early stopping, best iteration is:\n",
      "[793]\ttraining's auc: 0.56358\tvalid_1's auc: 0.562954\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.561006\tvalid_1's auc: 0.563367\n",
      "Early stopping, best iteration is:\n",
      "[924]\ttraining's auc: 0.560917\tvalid_1's auc: 0.563415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.3,\n",
       "        importance_type='split', learning_rate=0.02, max_depth=-1,\n",
       "        metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
       "        min_split_gain=0.0, n_estimators=999999, n_jobs=-1, num_leaves=2,\n",
       "        objective='binary', random_state=None, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# var_12 9559 (yes)\n",
    "# var_68 459 (no)\n",
    "# var_91 7959 (yes)\n",
    "# var_103 9376 (no)\n",
    "# var_108 8524 (no)\n",
    "\n",
    "bins = np.linspace(-6.0,6.0,40)\n",
    "X = pd.get_dummies(pd.cut(X1['var_12'].values, bins))\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "                 max_depth=-1,\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate=0.02,\n",
    "                 colsample_bytree=0.3,\n",
    "                 num_leaves=2,\n",
    "                 metric='auc',\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "col = 'var_12'\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1[col].reshape(-1,1), y, test_size=0.2, stratify=y)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1a347072e8>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1a347075f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(X1['var_68'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517942\tvalid_1's auc: 0.505162\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's auc: 0.509768\tvalid_1's auc: 0.508296\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515845\tvalid_1's auc: 0.505296\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's auc: 0.509246\tvalid_1's auc: 0.507739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.3,\n",
       "        importance_type='split', learning_rate=0.02, max_depth=-1,\n",
       "        metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
       "        min_split_gain=0.0, n_estimators=999999, n_jobs=-1, num_leaves=2,\n",
       "        objective='binary', random_state=None, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# var_12 9559\n",
    "# var_68 459 (no)\n",
    "# var_91 7959 (yes)\n",
    "# var_103 9376 (no)\n",
    "# var_108 8524 (no)\n",
    "bins = np.linspace(-6.0,6.0,50)\n",
    "X = pd.get_dummies(pd.cut(X1['var_68'].values, bins))\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "                 max_depth=-1,\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate=0.02,\n",
    "                 colsample_bytree=0.3,\n",
    "                 num_leaves=2,\n",
    "                 metric='auc',\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "col = 'var_68'\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1[col].reshape(-1,1), y, test_size=0.2, stratify=y)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1a94cd32b0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1a14064470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(X1['var_91'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.539451\tvalid_1's auc: 0.534348\n",
      "Early stopping, best iteration is:\n",
      "[434]\ttraining's auc: 0.537981\tvalid_1's auc: 0.535603\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535931\tvalid_1's auc: 0.534446\n",
      "[2000]\ttraining's auc: 0.536851\tvalid_1's auc: 0.534873\n",
      "Early stopping, best iteration is:\n",
      "[1677]\ttraining's auc: 0.536578\tvalid_1's auc: 0.536109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.3,\n",
       "        importance_type='split', learning_rate=0.02, max_depth=-1,\n",
       "        metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
       "        min_split_gain=0.0, n_estimators=999999, n_jobs=-1, num_leaves=2,\n",
       "        objective='binary', random_state=None, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# var_12 9559\n",
    "# var_68 459\n",
    "# var_91 7959 (yes)\n",
    "# var_103 9376 (no)\n",
    "# var_108 8524 (no)\n",
    "bins = np.linspace(-6.0,6.0,50)\n",
    "X = pd.get_dummies(pd.cut(X1['var_91'].values, bins))\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "                 max_depth=-1,\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate=0.02,\n",
    "                 colsample_bytree=0.3,\n",
    "                 num_leaves=2,\n",
    "                 metric='auc',\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "col = 'var_91'\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1[col].reshape(-1,1), y, test_size=0.2, stratify=y)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512506\tvalid_1's auc: 0.506379\n",
      "Early stopping, best iteration is:\n",
      "[186]\ttraining's auc: 0.509507\tvalid_1's auc: 0.507907\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.506929\tvalid_1's auc: 0.501271\n",
      "Early stopping, best iteration is:\n",
      "[236]\ttraining's auc: 0.506269\tvalid_1's auc: 0.502114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.3,\n",
       "        importance_type='split', learning_rate=0.02, max_depth=-1,\n",
       "        metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
       "        min_split_gain=0.0, n_estimators=999999, n_jobs=-1, num_leaves=2,\n",
       "        objective='binary', random_state=None, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# var_12 9559\n",
    "# var_68 459\n",
    "# var_91 7959\n",
    "# var_103 9376 (no)\n",
    "# var_108 8524 (no)\n",
    "bins = np.linspace(-6.0,6.0,30)\n",
    "X = pd.get_dummies(pd.cut(X1['var_103'].values, bins))\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "                 max_depth=-1,\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate=0.02,\n",
    "                 colsample_bytree=0.3,\n",
    "                 num_leaves=2,\n",
    "                 metric='auc',\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "col = 'var_103'\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1[col].reshape(-1,1), y, test_size=0.2, stratify=y)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546544\tvalid_1's auc: 0.541101\n",
      "[2000]\ttraining's auc: 0.547374\tvalid_1's auc: 0.541089\n",
      "[3000]\ttraining's auc: 0.548477\tvalid_1's auc: 0.542933\n",
      "[4000]\ttraining's auc: 0.550013\tvalid_1's auc: 0.542877\n",
      "Early stopping, best iteration is:\n",
      "[3220]\ttraining's auc: 0.548511\tvalid_1's auc: 0.543037\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.541797\tvalid_1's auc: 0.538098\n",
      "[2000]\ttraining's auc: 0.542078\tvalid_1's auc: 0.538022\n",
      "Early stopping, best iteration is:\n",
      "[1712]\ttraining's auc: 0.542045\tvalid_1's auc: 0.538155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.3,\n",
       "        importance_type='split', learning_rate=0.02, max_depth=-1,\n",
       "        metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
       "        min_split_gain=0.0, n_estimators=999999, n_jobs=-1, num_leaves=2,\n",
       "        objective='binary', random_state=None, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# var_12 9559\n",
    "# var_68 459\n",
    "# var_91 7959\n",
    "# var_103 9376\n",
    "# var_108 8524 (no)\n",
    "bins = np.linspace(-6.0,6.0,50)\n",
    "X = pd.get_dummies(pd.cut(X1['var_108'].values, bins))\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "                 max_depth=-1,\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate=0.02,\n",
    "                 colsample_bytree=0.3,\n",
    "                 num_leaves=2,\n",
    "                 metric='auc',\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "col = 'var_108'\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1[col].reshape(-1,1), y, test_size=0.2, stratify=y)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oof = np.load('../cache/oof_cat_top30_inter_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_top30_inter_1_10_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb\n",
      "0  0.010545  0.010837  0.008313\n",
      "1  0.455996  0.423705  0.392034\n",
      "2  0.004485  0.003918  0.004919\n",
      "3  0.243523  0.253600  0.340744\n",
      "4  0.097492  0.088431  0.110311\n"
     ]
    }
   ],
   "source": [
    "print(stage2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb      cat2\n",
      "0  0.010545  0.010837  0.008313  0.012104\n",
      "1  0.455996  0.423705  0.392034  0.495291\n",
      "2  0.004485  0.003918  0.004919  0.006758\n",
      "3  0.243523  0.253600  0.340744  0.283394\n",
      "4  0.097492  0.088431  0.110311  0.069909\n",
      "0.92538\n"
     ]
    }
   ],
   "source": [
    "stage2['cat2'] = oof[0]\n",
    "stage2_test['cat2'] = preds[0]\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12d.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "\n",
    "bins = np.linspace(-6.0,6.0,40)\n",
    "X3_12_bins = pd.get_dummies(pd.cut(X3['var_12'].values, bins))\n",
    "X4_12_bins = pd.get_dummies(pd.cut(X4['var_12'].values, bins))\n",
    "\n",
    "bins = np.linspace(-6.0,6.0,50)\n",
    "X3_91_bins = pd.get_dummies(pd.cut(X3['var_91'].values, bins))\n",
    "X4_91_bins = pd.get_dummies(pd.cut(X4['var_91'].values, bins))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 39), (200000, 49))"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3_12_bins.shape, X3_91_bins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X3_12_bins = pd.DataFrame(X3_12_bins)\n",
    "cols = ['var_12_' + str(i) for i in range(X3_12_bins.shape[1])]\n",
    "X3_12_bins.columns = cols\n",
    "\n",
    "X4_12_bins = pd.DataFrame(X4_12_bins)\n",
    "X4_12_bins.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X3_91_bins = pd.DataFrame(X3_91_bins)\n",
    "cols = ['var_91_' + str(i) for i in range(X3_91_bins.shape[1])]\n",
    "X3_91_bins.columns = cols\n",
    "\n",
    "X4_91_bins = pd.DataFrame(X4_91_bins)\n",
    "X4_91_bins.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 286), (200000, 286))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = pd.concat([X3, X3_12_bins], axis=1)\n",
    "X3 = pd.concat([X3, X3_91_bins], axis=1)\n",
    "\n",
    "X4 = pd.concat([X4, X4_12_bins], axis=1)\n",
    "X4 = pd.concat([X4, X4_91_bins], axis=1)\n",
    "\n",
    "X3 = X3.drop(['var_12', 'var_91'], axis=1)\n",
    "X4 = X4.drop(['var_12', 'var_91'], axis=1)\n",
    "\n",
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Mar  5 13:41:42 2019\n",
      "0:\ttest: 0.5134372\tbest: 0.5134372 (0)\ttotal: 54.9ms\tremaining: 15h 14m 34s\n",
      "1000:\ttest: 0.8531031\tbest: 0.8531315 (998)\ttotal: 26.7s\tremaining: 7h 23m 53s\n",
      "2000:\ttest: 0.8707125\tbest: 0.8707125 (2000)\ttotal: 55.2s\tremaining: 7h 39m 3s\n",
      "3000:\ttest: 0.8806626\tbest: 0.8806626 (3000)\ttotal: 1m 21s\tremaining: 7h 32m 52s\n",
      "4000:\ttest: 0.8865557\tbest: 0.8865557 (4000)\ttotal: 1m 47s\tremaining: 7h 27m 37s\n",
      "5000:\ttest: 0.8904276\tbest: 0.8904292 (4998)\ttotal: 2m 14s\tremaining: 7h 26m 19s\n",
      "6000:\ttest: 0.8929733\tbest: 0.8929733 (6000)\ttotal: 2m 43s\tremaining: 7h 30m 45s\n",
      "7000:\ttest: 0.8949140\tbest: 0.8949174 (6996)\ttotal: 3m 10s\tremaining: 7h 30m 21s\n",
      "8000:\ttest: 0.8962513\tbest: 0.8962513 (8000)\ttotal: 3m 36s\tremaining: 7h 28m 5s\n",
      "9000:\ttest: 0.8970759\tbest: 0.8970849 (8989)\ttotal: 4m 4s\tremaining: 7h 28m 34s\n",
      "10000:\ttest: 0.8978386\tbest: 0.8978417 (9999)\ttotal: 4m 33s\tremaining: 7h 30m 56s\n",
      "11000:\ttest: 0.8985648\tbest: 0.8985897 (10965)\ttotal: 5m\tremaining: 7h 29m 32s\n",
      "12000:\ttest: 0.8990742\tbest: 0.8990742 (12000)\ttotal: 5m 27s\tremaining: 7h 29m\n",
      "13000:\ttest: 0.8993934\tbest: 0.8993938 (12998)\ttotal: 5m 54s\tremaining: 7h 28m 32s\n",
      "14000:\ttest: 0.8996231\tbest: 0.8996339 (13874)\ttotal: 6m 22s\tremaining: 7h 28m 45s\n",
      "15000:\ttest: 0.8999522\tbest: 0.8999547 (14996)\ttotal: 6m 51s\tremaining: 7h 30m 6s\n",
      "16000:\ttest: 0.9001123\tbest: 0.9001227 (15972)\ttotal: 7m 20s\tremaining: 7h 31m 21s\n",
      "17000:\ttest: 0.9002317\tbest: 0.9002325 (16973)\ttotal: 7m 49s\tremaining: 7h 32m 17s\n",
      "18000:\ttest: 0.9004049\tbest: 0.9004100 (17985)\ttotal: 8m 19s\tremaining: 7h 34m 5s\n",
      "19000:\ttest: 0.9005462\tbest: 0.9005470 (18999)\ttotal: 8m 48s\tremaining: 7h 34m 51s\n",
      "20000:\ttest: 0.9005835\tbest: 0.9006007 (19916)\ttotal: 9m 18s\tremaining: 7h 36m 21s\n",
      "21000:\ttest: 0.9006522\tbest: 0.9006627 (20962)\ttotal: 9m 48s\tremaining: 7h 37m 36s\n",
      "22000:\ttest: 0.9007448\tbest: 0.9007646 (21926)\ttotal: 10m 18s\tremaining: 7h 37m 55s\n",
      "23000:\ttest: 0.9008006\tbest: 0.9008075 (22987)\ttotal: 10m 49s\tremaining: 7h 39m 58s\n",
      "24000:\ttest: 0.9008625\tbest: 0.9008749 (23921)\ttotal: 11m 19s\tremaining: 7h 40m 48s\n",
      "25000:\ttest: 0.9008359\tbest: 0.9008764 (24623)\ttotal: 11m 50s\tremaining: 7h 41m 54s\n",
      "26000:\ttest: 0.9009216\tbest: 0.9009244 (25988)\ttotal: 12m 21s\tremaining: 7h 43m 10s\n",
      "27000:\ttest: 0.9009573\tbest: 0.9009573 (27000)\ttotal: 12m 53s\tremaining: 7h 44m 32s\n",
      "28000:\ttest: 0.9009499\tbest: 0.9009766 (27468)\ttotal: 13m 26s\tremaining: 7h 46m 21s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9009765801\n",
      "bestIteration = 27468\n",
      "\n",
      "Shrink model to first 27469 iterations.\n",
      "Fold 1 started at Tue Mar  5 13:56:30 2019\n",
      "0:\ttest: 0.5235765\tbest: 0.5235765 (0)\ttotal: 41.8ms\tremaining: 11h 37m 20s\n",
      "1000:\ttest: 0.8480064\tbest: 0.8481312 (984)\ttotal: 30.4s\tremaining: 8h 26m 8s\n",
      "2000:\ttest: 0.8670775\tbest: 0.8670775 (2000)\ttotal: 1m\tremaining: 8h 22m 22s\n",
      "3000:\ttest: 0.8772631\tbest: 0.8772633 (2998)\ttotal: 1m 31s\tremaining: 8h 28m 19s\n",
      "4000:\ttest: 0.8834415\tbest: 0.8834415 (4000)\ttotal: 2m 3s\tremaining: 8h 32m 46s\n",
      "5000:\ttest: 0.8875117\tbest: 0.8875198 (4998)\ttotal: 2m 35s\tremaining: 8h 34m\n",
      "6000:\ttest: 0.8900577\tbest: 0.8900676 (5998)\ttotal: 3m 5s\tremaining: 8h 31m 15s\n",
      "7000:\ttest: 0.8921226\tbest: 0.8921227 (6995)\ttotal: 3m 35s\tremaining: 8h 29m 47s\n",
      "8000:\ttest: 0.8936130\tbest: 0.8936234 (7988)\ttotal: 4m 6s\tremaining: 8h 30m\n",
      "9000:\ttest: 0.8946900\tbest: 0.8946997 (8997)\ttotal: 4m 37s\tremaining: 8h 28m 57s\n",
      "10000:\ttest: 0.8955811\tbest: 0.8955811 (10000)\ttotal: 5m 9s\tremaining: 8h 30m 5s\n",
      "11000:\ttest: 0.8961918\tbest: 0.8961918 (11000)\ttotal: 5m 40s\tremaining: 8h 30m 9s\n",
      "12000:\ttest: 0.8966537\tbest: 0.8966548 (11999)\ttotal: 6m 11s\tremaining: 8h 29m 57s\n",
      "13000:\ttest: 0.8969638\tbest: 0.8969649 (12993)\ttotal: 6m 42s\tremaining: 8h 29m 17s\n",
      "14000:\ttest: 0.8972505\tbest: 0.8972601 (13977)\ttotal: 7m 14s\tremaining: 8h 29m 42s\n",
      "15000:\ttest: 0.8974248\tbest: 0.8974256 (14999)\ttotal: 7m 45s\tremaining: 8h 29m 58s\n",
      "16000:\ttest: 0.8976448\tbest: 0.8976531 (15993)\ttotal: 8m 16s\tremaining: 8h 29m 11s\n",
      "17000:\ttest: 0.8978217\tbest: 0.8978249 (16966)\ttotal: 8m 47s\tremaining: 8h 28m 35s\n",
      "18000:\ttest: 0.8978964\tbest: 0.8979197 (17951)\ttotal: 9m 19s\tremaining: 8h 28m 57s\n",
      "19000:\ttest: 0.8980208\tbest: 0.8980228 (18972)\ttotal: 9m 50s\tremaining: 8h 28m 20s\n",
      "20000:\ttest: 0.8980358\tbest: 0.8980441 (19977)\ttotal: 10m 23s\tremaining: 8h 29m 18s\n",
      "21000:\ttest: 0.8980362\tbest: 0.8980465 (20175)\ttotal: 10m 56s\tremaining: 8h 30m 2s\n",
      "22000:\ttest: 0.8980700\tbest: 0.8980855 (21275)\ttotal: 11m 28s\tremaining: 8h 29m 56s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8980854994\n",
      "bestIteration = 21275\n",
      "\n",
      "Shrink model to first 21276 iterations.\n",
      "Fold 2 started at Tue Mar  5 14:08:59 2019\n",
      "0:\ttest: 0.5123115\tbest: 0.5123115 (0)\ttotal: 48.7ms\tremaining: 13h 31m 35s\n",
      "1000:\ttest: 0.8518889\tbest: 0.8519214 (997)\ttotal: 29.8s\tremaining: 8h 15m 40s\n",
      "2000:\ttest: 0.8708871\tbest: 0.8708871 (2000)\ttotal: 1m\tremaining: 8h 22m 53s\n",
      "3000:\ttest: 0.8809791\tbest: 0.8809791 (3000)\ttotal: 1m 31s\tremaining: 8h 27m 20s\n",
      "4000:\ttest: 0.8869019\tbest: 0.8869019 (4000)\ttotal: 2m 2s\tremaining: 8h 29m 42s\n",
      "5000:\ttest: 0.8904707\tbest: 0.8904707 (5000)\ttotal: 2m 35s\tremaining: 8h 34m 25s\n",
      "6000:\ttest: 0.8928735\tbest: 0.8928778 (5985)\ttotal: 3m 6s\tremaining: 8h 33m 53s\n",
      "7000:\ttest: 0.8946295\tbest: 0.8946295 (7000)\ttotal: 3m 38s\tremaining: 8h 35m 30s\n",
      "8000:\ttest: 0.8957433\tbest: 0.8957529 (7985)\ttotal: 4m 9s\tremaining: 8h 35m 38s\n",
      "9000:\ttest: 0.8966805\tbest: 0.8966805 (9000)\ttotal: 4m 40s\tremaining: 8h 34m 18s\n",
      "10000:\ttest: 0.8973132\tbest: 0.8973132 (10000)\ttotal: 5m 12s\tremaining: 8h 36m 20s\n",
      "11000:\ttest: 0.8977974\tbest: 0.8977974 (11000)\ttotal: 5m 46s\tremaining: 8h 39m 2s\n",
      "12000:\ttest: 0.8981201\tbest: 0.8981269 (11991)\ttotal: 6m 20s\tremaining: 8h 41m 49s\n",
      "13000:\ttest: 0.8983815\tbest: 0.8983815 (13000)\ttotal: 6m 53s\tremaining: 8h 43m 41s\n",
      "14000:\ttest: 0.8986217\tbest: 0.8986255 (13976)\ttotal: 7m 27s\tremaining: 8h 44m 53s\n",
      "15000:\ttest: 0.8987868\tbest: 0.8987955 (14869)\ttotal: 8m\tremaining: 8h 46m 14s\n",
      "16000:\ttest: 0.8989379\tbest: 0.8989563 (15903)\ttotal: 8m 33s\tremaining: 8h 46m 11s\n",
      "17000:\ttest: 0.8991019\tbest: 0.8991036 (16994)\ttotal: 9m 4s\tremaining: 8h 45m 6s\n",
      "18000:\ttest: 0.8991465\tbest: 0.8991579 (17192)\ttotal: 9m 35s\tremaining: 8h 43m 29s\n",
      "19000:\ttest: 0.8991929\tbest: 0.8991998 (18986)\ttotal: 10m 7s\tremaining: 8h 42m 31s\n",
      "20000:\ttest: 0.8992857\tbest: 0.8993104 (19884)\ttotal: 10m 38s\tremaining: 8h 41m 36s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8993103687\n",
      "bestIteration = 19884\n",
      "\n",
      "Shrink model to first 19885 iterations.\n",
      "Fold 3 started at Tue Mar  5 14:20:49 2019\n",
      "0:\ttest: 0.5156240\tbest: 0.5156240 (0)\ttotal: 40.6ms\tremaining: 11h 15m 53s\n",
      "1000:\ttest: 0.8536250\tbest: 0.8536311 (999)\ttotal: 29.9s\tremaining: 8h 17m 54s\n",
      "2000:\ttest: 0.8713883\tbest: 0.8713972 (1999)\ttotal: 1m 1s\tremaining: 8h 31m 31s\n",
      "3000:\ttest: 0.8814214\tbest: 0.8814214 (3000)\ttotal: 1m 34s\tremaining: 8h 42m 9s\n",
      "4000:\ttest: 0.8880357\tbest: 0.8880406 (3998)\ttotal: 2m 6s\tremaining: 8h 43m 48s\n",
      "5000:\ttest: 0.8919798\tbest: 0.8919798 (5000)\ttotal: 2m 38s\tremaining: 8h 46m 5s\n",
      "6000:\ttest: 0.8946763\tbest: 0.8946763 (6000)\ttotal: 3m 12s\tremaining: 8h 50m 31s\n",
      "7000:\ttest: 0.8964930\tbest: 0.8964982 (6995)\ttotal: 3m 43s\tremaining: 8h 48m 21s\n",
      "8000:\ttest: 0.8979130\tbest: 0.8979154 (7999)\ttotal: 4m 16s\tremaining: 8h 49m 18s\n",
      "9000:\ttest: 0.8988131\tbest: 0.8988162 (8999)\ttotal: 4m 48s\tremaining: 8h 49m 32s\n",
      "10000:\ttest: 0.8996058\tbest: 0.8996112 (9994)\ttotal: 5m 21s\tremaining: 8h 51m 11s\n",
      "11000:\ttest: 0.9000943\tbest: 0.9000943 (11000)\ttotal: 5m 54s\tremaining: 8h 51m 30s\n",
      "12000:\ttest: 0.9005742\tbest: 0.9005742 (12000)\ttotal: 6m 26s\tremaining: 8h 50m 47s\n",
      "13000:\ttest: 0.9009851\tbest: 0.9009851 (13000)\ttotal: 6m 59s\tremaining: 8h 50m 28s\n",
      "14000:\ttest: 0.9012466\tbest: 0.9012504 (13994)\ttotal: 7m 31s\tremaining: 8h 49m 40s\n",
      "15000:\ttest: 0.9014227\tbest: 0.9014242 (14983)\ttotal: 8m 3s\tremaining: 8h 48m 53s\n",
      "16000:\ttest: 0.9015737\tbest: 0.9016060 (15783)\ttotal: 8m 34s\tremaining: 8h 47m 42s\n",
      "17000:\ttest: 0.9016267\tbest: 0.9016304 (16912)\ttotal: 9m 7s\tremaining: 8h 47m 27s\n",
      "18000:\ttest: 0.9016337\tbest: 0.9016622 (17723)\ttotal: 9m 39s\tremaining: 8h 46m 46s\n",
      "19000:\ttest: 0.9017652\tbest: 0.9017689 (18989)\ttotal: 10m 11s\tremaining: 8h 45m 48s\n",
      "20000:\ttest: 0.9017961\tbest: 0.9018115 (19582)\ttotal: 10m 42s\tremaining: 8h 44m 56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000:\ttest: 0.9017826\tbest: 0.9018271 (20551)\ttotal: 11m 15s\tremaining: 8h 44m 30s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9018270792\n",
      "bestIteration = 20551\n",
      "\n",
      "Shrink model to first 20552 iterations.\n",
      "Fold 4 started at Tue Mar  5 14:33:11 2019\n",
      "0:\ttest: 0.5175121\tbest: 0.5175121 (0)\ttotal: 47.4ms\tremaining: 13h 10m 32s\n",
      "1000:\ttest: 0.8529257\tbest: 0.8529257 (1000)\ttotal: 31.3s\tremaining: 8h 40m 46s\n",
      "2000:\ttest: 0.8734117\tbest: 0.8734117 (2000)\ttotal: 1m 2s\tremaining: 8h 38m 17s\n",
      "3000:\ttest: 0.8855334\tbest: 0.8855334 (3000)\ttotal: 1m 33s\tremaining: 8h 39m 33s\n",
      "4000:\ttest: 0.8925587\tbest: 0.8925648 (3999)\ttotal: 2m 6s\tremaining: 8h 44m 19s\n",
      "5000:\ttest: 0.8971872\tbest: 0.8971872 (5000)\ttotal: 2m 37s\tremaining: 8h 42m 41s\n",
      "6000:\ttest: 0.9003432\tbest: 0.9003432 (6000)\ttotal: 3m 10s\tremaining: 8h 44m 32s\n",
      "7000:\ttest: 0.9024043\tbest: 0.9024043 (7000)\ttotal: 3m 42s\tremaining: 8h 46m 41s\n",
      "8000:\ttest: 0.9039765\tbest: 0.9039861 (7995)\ttotal: 4m 13s\tremaining: 8h 44m 42s\n",
      "9000:\ttest: 0.9052030\tbest: 0.9052030 (9000)\ttotal: 4m 45s\tremaining: 8h 43m 6s\n",
      "10000:\ttest: 0.9061370\tbest: 0.9061448 (9937)\ttotal: 5m 17s\tremaining: 8h 44m 24s\n",
      "11000:\ttest: 0.9067437\tbest: 0.9067437 (11000)\ttotal: 5m 50s\tremaining: 8h 44m 33s\n",
      "12000:\ttest: 0.9073576\tbest: 0.9073592 (11998)\ttotal: 6m 22s\tremaining: 8h 44m 49s\n",
      "13000:\ttest: 0.9077637\tbest: 0.9077642 (12997)\ttotal: 6m 55s\tremaining: 8h 45m 38s\n",
      "14000:\ttest: 0.9080917\tbest: 0.9080943 (13998)\ttotal: 7m 27s\tremaining: 8h 45m 48s\n",
      "15000:\ttest: 0.9083282\tbest: 0.9083318 (14988)\ttotal: 8m 2s\tremaining: 8h 47m 41s\n",
      "16000:\ttest: 0.9085357\tbest: 0.9085439 (15951)\ttotal: 8m 36s\tremaining: 8h 49m 3s\n",
      "17000:\ttest: 0.9087528\tbest: 0.9087555 (16998)\ttotal: 9m 9s\tremaining: 8h 49m 43s\n",
      "18000:\ttest: 0.9088849\tbest: 0.9088980 (17881)\ttotal: 9m 41s\tremaining: 8h 49m 4s\n",
      "19000:\ttest: 0.9089083\tbest: 0.9089514 (18561)\ttotal: 10m 13s\tremaining: 8h 48m 11s\n",
      "20000:\ttest: 0.9089411\tbest: 0.9089560 (19383)\ttotal: 10m 46s\tremaining: 8h 47m 37s\n",
      "21000:\ttest: 0.9090015\tbest: 0.9090022 (20998)\ttotal: 11m 18s\tremaining: 8h 47m 8s\n",
      "22000:\ttest: 0.9090096\tbest: 0.9090291 (21588)\ttotal: 11m 51s\tremaining: 8h 47m 29s\n",
      "23000:\ttest: 0.9090918\tbest: 0.9090962 (22957)\ttotal: 12m 24s\tremaining: 8h 47m 23s\n",
      "24000:\ttest: 0.9091620\tbest: 0.9091632 (23998)\ttotal: 12m 58s\tremaining: 8h 47m 17s\n",
      "25000:\ttest: 0.9091900\tbest: 0.9091921 (24809)\ttotal: 13m 31s\tremaining: 8h 47m 8s\n",
      "26000:\ttest: 0.9091492\tbest: 0.9092051 (25387)\ttotal: 14m 2s\tremaining: 8h 45m 58s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9092050863\n",
      "bestIteration = 25387\n",
      "\n",
      "Shrink model to first 25388 iterations.\n",
      "Fold 5 started at Tue Mar  5 14:48:26 2019\n",
      "0:\ttest: 0.5087885\tbest: 0.5087885 (0)\ttotal: 68.6ms\tremaining: 19h 3m 38s\n",
      "1000:\ttest: 0.8529919\tbest: 0.8530167 (999)\ttotal: 30.8s\tremaining: 8h 32m 22s\n",
      "2000:\ttest: 0.8705007\tbest: 0.8705007 (2000)\ttotal: 1m 2s\tremaining: 8h 37m 43s\n",
      "3000:\ttest: 0.8819278\tbest: 0.8819278 (3000)\ttotal: 1m 33s\tremaining: 8h 37m 47s\n",
      "4000:\ttest: 0.8887103\tbest: 0.8887108 (3997)\ttotal: 2m 4s\tremaining: 8h 36m 45s\n",
      "5000:\ttest: 0.8928098\tbest: 0.8928098 (5000)\ttotal: 2m 37s\tremaining: 8h 43m 51s\n",
      "6000:\ttest: 0.8954512\tbest: 0.8954512 (6000)\ttotal: 3m 11s\tremaining: 8h 48m 14s\n",
      "7000:\ttest: 0.8972940\tbest: 0.8972984 (6996)\ttotal: 3m 45s\tremaining: 8h 53m 17s\n",
      "8000:\ttest: 0.8989262\tbest: 0.8989262 (8000)\ttotal: 4m 17s\tremaining: 8h 51m 47s\n",
      "9000:\ttest: 0.9001056\tbest: 0.9001133 (8996)\ttotal: 4m 50s\tremaining: 8h 53m 47s\n",
      "10000:\ttest: 0.9009428\tbest: 0.9009428 (10000)\ttotal: 5m 23s\tremaining: 8h 54m 16s\n",
      "11000:\ttest: 0.9014598\tbest: 0.9014632 (10995)\ttotal: 5m 56s\tremaining: 8h 53m 55s\n",
      "12000:\ttest: 0.9018588\tbest: 0.9018621 (11994)\ttotal: 6m 29s\tremaining: 8h 53m 51s\n",
      "13000:\ttest: 0.9021629\tbest: 0.9021674 (12996)\ttotal: 7m 3s\tremaining: 8h 55m 45s\n",
      "14000:\ttest: 0.9024943\tbest: 0.9024969 (13994)\ttotal: 7m 37s\tremaining: 8h 56m 33s\n",
      "15000:\ttest: 0.9027380\tbest: 0.9027380 (15000)\ttotal: 8m 9s\tremaining: 8h 55m 45s\n",
      "16000:\ttest: 0.9028663\tbest: 0.9028805 (15824)\ttotal: 8m 42s\tremaining: 8h 55m 45s\n",
      "17000:\ttest: 0.9029263\tbest: 0.9029400 (16832)\ttotal: 9m 14s\tremaining: 8h 54m 11s\n",
      "18000:\ttest: 0.9030003\tbest: 0.9030050 (17830)\ttotal: 9m 47s\tremaining: 8h 54m 7s\n",
      "19000:\ttest: 0.9030957\tbest: 0.9031033 (18943)\ttotal: 10m 21s\tremaining: 8h 54m 25s\n",
      "20000:\ttest: 0.9031928\tbest: 0.9031951 (19996)\ttotal: 10m 53s\tremaining: 8h 53m 40s\n",
      "21000:\ttest: 0.9033095\tbest: 0.9033183 (20989)\ttotal: 11m 26s\tremaining: 8h 53m 43s\n",
      "22000:\ttest: 0.9033267\tbest: 0.9033638 (21742)\ttotal: 11m 59s\tremaining: 8h 52m 45s\n",
      "23000:\ttest: 0.9033815\tbest: 0.9033966 (22946)\ttotal: 12m 31s\tremaining: 8h 52m 8s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9033965802\n",
      "bestIteration = 22946\n",
      "\n",
      "Shrink model to first 22947 iterations.\n",
      "Fold 6 started at Tue Mar  5 15:02:23 2019\n",
      "0:\ttest: 0.5198000\tbest: 0.5198000 (0)\ttotal: 57ms\tremaining: 15h 49m 28s\n",
      "1000:\ttest: 0.8494733\tbest: 0.8494978 (999)\ttotal: 32.2s\tremaining: 8h 55m 54s\n",
      "2000:\ttest: 0.8691694\tbest: 0.8691876 (1999)\ttotal: 1m 4s\tremaining: 8h 58m 14s\n",
      "3000:\ttest: 0.8811460\tbest: 0.8811460 (3000)\ttotal: 1m 36s\tremaining: 8h 52m 54s\n",
      "4000:\ttest: 0.8882330\tbest: 0.8882348 (3999)\ttotal: 2m 8s\tremaining: 8h 52m 20s\n",
      "5000:\ttest: 0.8927578\tbest: 0.8927624 (4988)\ttotal: 2m 40s\tremaining: 8h 50m 40s\n",
      "6000:\ttest: 0.8958534\tbest: 0.8958534 (6000)\ttotal: 3m 13s\tremaining: 8h 52m 59s\n",
      "7000:\ttest: 0.8980476\tbest: 0.8980476 (7000)\ttotal: 3m 45s\tremaining: 8h 53m 58s\n",
      "8000:\ttest: 0.8995653\tbest: 0.8995662 (7996)\ttotal: 4m 19s\tremaining: 8h 55m 35s\n",
      "9000:\ttest: 0.9006253\tbest: 0.9006275 (8996)\ttotal: 4m 54s\tremaining: 8h 59m 37s\n",
      "10000:\ttest: 0.9014763\tbest: 0.9014785 (9996)\ttotal: 5m 27s\tremaining: 9h 33s\n",
      "11000:\ttest: 0.9022810\tbest: 0.9022833 (10997)\ttotal: 6m 1s\tremaining: 9h 2m 19s\n",
      "12000:\ttest: 0.9026506\tbest: 0.9026534 (11997)\ttotal: 6m 35s\tremaining: 9h 2m 30s\n",
      "13000:\ttest: 0.9030751\tbest: 0.9030774 (12998)\ttotal: 7m 8s\tremaining: 9h 2m 18s\n",
      "14000:\ttest: 0.9034232\tbest: 0.9034343 (13967)\ttotal: 7m 41s\tremaining: 9h 1m 12s\n",
      "15000:\ttest: 0.9037142\tbest: 0.9037185 (14996)\ttotal: 8m 12s\tremaining: 8h 58m 55s\n",
      "16000:\ttest: 0.9039167\tbest: 0.9039196 (15996)\ttotal: 8m 45s\tremaining: 8h 58m 12s\n",
      "17000:\ttest: 0.9040991\tbest: 0.9041157 (16951)\ttotal: 9m 16s\tremaining: 8h 56m 18s\n",
      "18000:\ttest: 0.9042364\tbest: 0.9042379 (17970)\ttotal: 9m 49s\tremaining: 8h 56m\n",
      "19000:\ttest: 0.9043513\tbest: 0.9043740 (18814)\ttotal: 10m 22s\tremaining: 8h 55m 39s\n",
      "20000:\ttest: 0.9044087\tbest: 0.9044257 (19907)\ttotal: 10m 54s\tremaining: 8h 54m 36s\n",
      "21000:\ttest: 0.9044952\tbest: 0.9044980 (20993)\ttotal: 11m 26s\tremaining: 8h 53m 30s\n",
      "22000:\ttest: 0.9046195\tbest: 0.9046201 (21997)\ttotal: 11m 58s\tremaining: 8h 52m 28s\n",
      "23000:\ttest: 0.9046213\tbest: 0.9046318 (22831)\ttotal: 12m 32s\tremaining: 8h 52m 38s\n",
      "24000:\ttest: 0.9047190\tbest: 0.9047342 (23925)\ttotal: 13m 6s\tremaining: 8h 53m 11s\n",
      "25000:\ttest: 0.9047492\tbest: 0.9047641 (24524)\ttotal: 13m 40s\tremaining: 8h 53m 18s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9047640895\n",
      "bestIteration = 24524\n",
      "\n",
      "Shrink model to first 24525 iterations.\n",
      "Fold 7 started at Tue Mar  5 15:17:20 2019\n",
      "0:\ttest: 0.5136953\tbest: 0.5136953 (0)\ttotal: 53.2ms\tremaining: 14h 46m 10s\n",
      "1000:\ttest: 0.8420769\tbest: 0.8421094 (999)\ttotal: 32.3s\tremaining: 8h 57m 32s\n",
      "2000:\ttest: 0.8610961\tbest: 0.8611052 (1999)\ttotal: 1m 4s\tremaining: 8h 54m 4s\n",
      "3000:\ttest: 0.8726119\tbest: 0.8726119 (3000)\ttotal: 1m 36s\tremaining: 8h 55m 2s\n",
      "4000:\ttest: 0.8791707\tbest: 0.8791707 (4000)\ttotal: 2m 9s\tremaining: 8h 56m 3s\n",
      "5000:\ttest: 0.8834119\tbest: 0.8834119 (5000)\ttotal: 2m 41s\tremaining: 8h 53m 54s\n",
      "6000:\ttest: 0.8859451\tbest: 0.8859451 (6000)\ttotal: 3m 13s\tremaining: 8h 54m 19s\n",
      "7000:\ttest: 0.8880623\tbest: 0.8880667 (6989)\ttotal: 3m 45s\tremaining: 8h 53m 43s\n",
      "8000:\ttest: 0.8895954\tbest: 0.8895954 (8000)\ttotal: 4m 18s\tremaining: 8h 54m 14s\n",
      "9000:\ttest: 0.8905663\tbest: 0.8905663 (9000)\ttotal: 4m 50s\tremaining: 8h 52m 47s\n",
      "10000:\ttest: 0.8915050\tbest: 0.8915050 (10000)\ttotal: 5m 23s\tremaining: 8h 53m 14s\n",
      "11000:\ttest: 0.8922456\tbest: 0.8922456 (11000)\ttotal: 5m 56s\tremaining: 8h 53m 39s\n",
      "12000:\ttest: 0.8926819\tbest: 0.8926819 (12000)\ttotal: 6m 28s\tremaining: 8h 52m 56s\n",
      "13000:\ttest: 0.8931147\tbest: 0.8931147 (13000)\ttotal: 7m\tremaining: 8h 51m 31s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000:\ttest: 0.8935601\tbest: 0.8935723 (13961)\ttotal: 7m 32s\tremaining: 8h 51m 1s\n",
      "15000:\ttest: 0.8938397\tbest: 0.8938397 (15000)\ttotal: 8m 4s\tremaining: 8h 50m 42s\n",
      "16000:\ttest: 0.8940916\tbest: 0.8941010 (15984)\ttotal: 8m 37s\tremaining: 8h 50m 11s\n",
      "17000:\ttest: 0.8942629\tbest: 0.8942745 (16959)\ttotal: 9m 9s\tremaining: 8h 49m 52s\n",
      "18000:\ttest: 0.8944891\tbest: 0.8944904 (17993)\ttotal: 9m 43s\tremaining: 8h 50m 13s\n",
      "19000:\ttest: 0.8946104\tbest: 0.8946115 (18990)\ttotal: 10m 16s\tremaining: 8h 50m 10s\n",
      "20000:\ttest: 0.8946624\tbest: 0.8946661 (19968)\ttotal: 10m 48s\tremaining: 8h 49m 44s\n",
      "21000:\ttest: 0.8947203\tbest: 0.8947293 (20923)\ttotal: 11m 20s\tremaining: 8h 48m 59s\n",
      "22000:\ttest: 0.8948299\tbest: 0.8948299 (22000)\ttotal: 11m 53s\tremaining: 8h 48m 31s\n",
      "23000:\ttest: 0.8948745\tbest: 0.8948745 (23000)\ttotal: 12m 26s\tremaining: 8h 48m 34s\n",
      "24000:\ttest: 0.8949810\tbest: 0.8949941 (23805)\ttotal: 12m 57s\tremaining: 8h 47m 15s\n",
      "25000:\ttest: 0.8950051\tbest: 0.8950117 (24977)\ttotal: 13m 29s\tremaining: 8h 46m 9s\n",
      "26000:\ttest: 0.8950675\tbest: 0.8950745 (25557)\ttotal: 14m 1s\tremaining: 8h 45m 19s\n",
      "27000:\ttest: 0.8951156\tbest: 0.8951189 (26887)\ttotal: 14m 35s\tremaining: 8h 45m 40s\n",
      "28000:\ttest: 0.8951240\tbest: 0.8951364 (27859)\ttotal: 15m 7s\tremaining: 8h 45m 6s\n",
      "29000:\ttest: 0.8951805\tbest: 0.8952062 (28715)\ttotal: 15m 39s\tremaining: 8h 44m 14s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8952062367\n",
      "bestIteration = 28715\n",
      "\n",
      "Shrink model to first 28716 iterations.\n",
      "Fold 8 started at Tue Mar  5 15:34:33 2019\n",
      "0:\ttest: 0.5363029\tbest: 0.5363029 (0)\ttotal: 81.4ms\tremaining: 22h 36m 35s\n",
      "1000:\ttest: 0.8436515\tbest: 0.8436515 (1000)\ttotal: 30.6s\tremaining: 8h 29m 32s\n",
      "2000:\ttest: 0.8618607\tbest: 0.8618607 (2000)\ttotal: 1m 2s\tremaining: 8h 38m 2s\n",
      "3000:\ttest: 0.8727187\tbest: 0.8727234 (2999)\ttotal: 1m 32s\tremaining: 8h 34m 18s\n",
      "4000:\ttest: 0.8791551\tbest: 0.8791551 (4000)\ttotal: 2m 5s\tremaining: 8h 42m 26s\n",
      "5000:\ttest: 0.8839897\tbest: 0.8839961 (4998)\ttotal: 2m 38s\tremaining: 8h 45m 13s\n",
      "6000:\ttest: 0.8874289\tbest: 0.8874322 (5997)\ttotal: 3m 10s\tremaining: 8h 45m 58s\n",
      "7000:\ttest: 0.8894905\tbest: 0.8894905 (7000)\ttotal: 3m 43s\tremaining: 8h 48m 1s\n",
      "8000:\ttest: 0.8911587\tbest: 0.8911587 (8000)\ttotal: 4m 15s\tremaining: 8h 47m 53s\n",
      "9000:\ttest: 0.8924013\tbest: 0.8924013 (9000)\ttotal: 4m 48s\tremaining: 8h 49m 24s\n",
      "10000:\ttest: 0.8935288\tbest: 0.8935288 (10000)\ttotal: 5m 21s\tremaining: 8h 49m 51s\n",
      "11000:\ttest: 0.8942823\tbest: 0.8942850 (10977)\ttotal: 5m 53s\tremaining: 8h 49m 18s\n",
      "12000:\ttest: 0.8948738\tbest: 0.8948738 (12000)\ttotal: 6m 24s\tremaining: 8h 48m 3s\n",
      "13000:\ttest: 0.8953216\tbest: 0.8953248 (12983)\ttotal: 6m 55s\tremaining: 8h 46m 2s\n",
      "14000:\ttest: 0.8957857\tbest: 0.8957885 (13998)\ttotal: 7m 28s\tremaining: 8h 46m 57s\n",
      "15000:\ttest: 0.8960265\tbest: 0.8960265 (15000)\ttotal: 8m 1s\tremaining: 8h 46m 44s\n",
      "16000:\ttest: 0.8962732\tbest: 0.8962866 (15967)\ttotal: 8m 33s\tremaining: 8h 46m 16s\n",
      "17000:\ttest: 0.8964107\tbest: 0.8964121 (16983)\ttotal: 9m 6s\tremaining: 8h 46m 35s\n",
      "18000:\ttest: 0.8965191\tbest: 0.8965206 (17999)\ttotal: 9m 38s\tremaining: 8h 46m 19s\n",
      "19000:\ttest: 0.8966706\tbest: 0.8966856 (18913)\ttotal: 10m 11s\tremaining: 8h 45m 53s\n",
      "20000:\ttest: 0.8967421\tbest: 0.8967425 (19880)\ttotal: 10m 43s\tremaining: 8h 45m 10s\n",
      "21000:\ttest: 0.8968462\tbest: 0.8968463 (20947)\ttotal: 11m 14s\tremaining: 8h 44m 10s\n",
      "22000:\ttest: 0.8969042\tbest: 0.8969094 (21992)\ttotal: 11m 46s\tremaining: 8h 43m 13s\n",
      "23000:\ttest: 0.8969712\tbest: 0.8969808 (22809)\ttotal: 12m 18s\tremaining: 8h 42m 41s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8969807628\n",
      "bestIteration = 22809\n",
      "\n",
      "Shrink model to first 22810 iterations.\n",
      "Fold 9 started at Tue Mar  5 15:48:11 2019\n",
      "0:\ttest: 0.5384201\tbest: 0.5384201 (0)\ttotal: 62ms\tremaining: 17h 12m 36s\n",
      "1000:\ttest: 0.8477364\tbest: 0.8477577 (999)\ttotal: 31.1s\tremaining: 8h 36m 30s\n",
      "2000:\ttest: 0.8652246\tbest: 0.8652246 (2000)\ttotal: 1m 2s\tremaining: 8h 38m 25s\n",
      "3000:\ttest: 0.8758026\tbest: 0.8758053 (2999)\ttotal: 1m 33s\tremaining: 8h 37m 19s\n",
      "4000:\ttest: 0.8827778\tbest: 0.8827778 (4000)\ttotal: 2m 5s\tremaining: 8h 39m 1s\n",
      "5000:\ttest: 0.8866018\tbest: 0.8866018 (5000)\ttotal: 2m 36s\tremaining: 8h 38m 28s\n",
      "6000:\ttest: 0.8894657\tbest: 0.8894657 (6000)\ttotal: 3m 8s\tremaining: 8h 40m 9s\n",
      "7000:\ttest: 0.8914640\tbest: 0.8914681 (6993)\ttotal: 3m 41s\tremaining: 8h 42m 54s\n",
      "8000:\ttest: 0.8930340\tbest: 0.8930340 (8000)\ttotal: 4m 12s\tremaining: 8h 42m 5s\n",
      "9000:\ttest: 0.8941864\tbest: 0.8941864 (9000)\ttotal: 4m 45s\tremaining: 8h 43m 34s\n",
      "10000:\ttest: 0.8950113\tbest: 0.8950155 (9997)\ttotal: 5m 17s\tremaining: 8h 43m 48s\n",
      "11000:\ttest: 0.8955887\tbest: 0.8955887 (11000)\ttotal: 5m 49s\tremaining: 8h 43m 26s\n",
      "12000:\ttest: 0.8961217\tbest: 0.8961232 (11954)\ttotal: 6m 21s\tremaining: 8h 43m 29s\n",
      "13000:\ttest: 0.8965026\tbest: 0.8965072 (12993)\ttotal: 6m 52s\tremaining: 8h 42m 8s\n",
      "14000:\ttest: 0.8966953\tbest: 0.8967096 (13942)\ttotal: 7m 24s\tremaining: 8h 41m 25s\n",
      "15000:\ttest: 0.8968870\tbest: 0.8968953 (14925)\ttotal: 7m 56s\tremaining: 8h 41m 6s\n",
      "16000:\ttest: 0.8970698\tbest: 0.8970735 (15990)\ttotal: 8m 27s\tremaining: 8h 40m 12s\n",
      "17000:\ttest: 0.8972056\tbest: 0.8972124 (16983)\ttotal: 9m 1s\tremaining: 8h 41m 25s\n",
      "18000:\ttest: 0.8973136\tbest: 0.8973437 (17696)\ttotal: 9m 32s\tremaining: 8h 40m 41s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8973437209\n",
      "bestIteration = 17696\n",
      "\n",
      "Shrink model to first 17697 iterations.\n",
      "CV mean score: 0.9007, std: 0.0040.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_cat, prediction_cat, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, model_type='cat', plot_feature_importance=False)\n",
    "oof.append(oof_cat)\n",
    "preds.append(prediction_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_cat_quant_bin_1_10_1', oof)\n",
    "np.save('../cache/preds_cat_quant_bin_1_10_1', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb      cat2\n",
      "0  0.010545  0.010837  0.008313  0.011734\n",
      "1  0.455996  0.423705  0.392034  0.468508\n",
      "2  0.004485  0.003918  0.004919  0.004418\n",
      "3  0.243523  0.253600  0.340744  0.263807\n",
      "4  0.097492  0.088431  0.110311  0.092751\n",
      "0.925455\n"
     ]
    }
   ],
   "source": [
    "stage2['cat2'] = oof[0]\n",
    "stage2_test['cat2'] = preds[0]\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12e.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb\n",
      "0  0.011734  0.010837  0.008313\n",
      "1  0.468508  0.423705  0.392034\n",
      "2  0.004418  0.003918  0.004919\n",
      "3  0.263807  0.253600  0.340744\n",
      "4  0.092751  0.088431  0.110311\n",
      "0.925285\n"
     ]
    }
   ],
   "source": [
    "stage22 = stage2.drop('cat2', axis=1)\n",
    "stage22_test = stage2_test.drop('cat2', axis=1)\n",
    "stage22['cat'] = oof[0]\n",
    "stage22_test['cat'] = preds[0]\n",
    "print(stage22.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage22, y)\n",
    "predictions = lr.predict_proba(stage22_test)[:, 1]\n",
    "\n",
    "# sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "# sub['target'] = predictions\n",
    "# sub.to_csv('../submissions/sub12e.csv', index=False)\n",
    "print(lr.score(stage22, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8806457913323875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "# clf = SVC(gamma='auto')\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1, y, test_size=0.2, random_state=51, stratify=y)\n",
    "\n",
    "n_estimators=20\n",
    "clf = OneVsRestClassifier(BaggingClassifier(SVC(gamma='auto', probability=True, class_weight='balanced'), \n",
    "                                            max_samples=1.0 / n_estimators, n_estimators=n_estimators, n_jobs=-1), n_jobs=-1)\n",
    "# clf = OneVsRestClassifier(SVC(gamma='auto', probability=True, class_weight='balanced'), n_jobs=-1)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_valid = clf.predict_proba(X_valid)[:,1].reshape(-1,1)\n",
    "score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.879372989152224\n"
     ]
    }
   ],
   "source": [
    "y_pred_valid = clf.predict_proba(X_valid)[:,1].reshape(-1,) # without stratify=y\n",
    "score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.829992\tvalid_1's auc: 0.824131\n",
      "[2000]\ttraining's auc: 0.860055\tvalid_1's auc: 0.854852\n",
      "[3000]\ttraining's auc: 0.874626\tvalid_1's auc: 0.868969\n",
      "[4000]\ttraining's auc: 0.8835\tvalid_1's auc: 0.87717\n",
      "[5000]\ttraining's auc: 0.88936\tvalid_1's auc: 0.88251\n",
      "[6000]\ttraining's auc: 0.893626\tvalid_1's auc: 0.886321\n",
      "[7000]\ttraining's auc: 0.896871\tvalid_1's auc: 0.889163\n",
      "[8000]\ttraining's auc: 0.899431\tvalid_1's auc: 0.891409\n",
      "[9000]\ttraining's auc: 0.901506\tvalid_1's auc: 0.893196\n",
      "[10000]\ttraining's auc: 0.903209\tvalid_1's auc: 0.894601\n",
      "[11000]\ttraining's auc: 0.904574\tvalid_1's auc: 0.895745\n",
      "[12000]\ttraining's auc: 0.905747\tvalid_1's auc: 0.896644\n",
      "[13000]\ttraining's auc: 0.906739\tvalid_1's auc: 0.897375\n",
      "[14000]\ttraining's auc: 0.907591\tvalid_1's auc: 0.897987\n",
      "[15000]\ttraining's auc: 0.908306\tvalid_1's auc: 0.898482\n",
      "[16000]\ttraining's auc: 0.908943\tvalid_1's auc: 0.898863\n",
      "[17000]\ttraining's auc: 0.909501\tvalid_1's auc: 0.899179\n",
      "[18000]\ttraining's auc: 0.909999\tvalid_1's auc: 0.899468\n",
      "[19000]\ttraining's auc: 0.910424\tvalid_1's auc: 0.89969\n",
      "[20000]\ttraining's auc: 0.910814\tvalid_1's auc: 0.899856\n",
      "[21000]\ttraining's auc: 0.911176\tvalid_1's auc: 0.90003\n",
      "[22000]\ttraining's auc: 0.9115\tvalid_1's auc: 0.900157\n",
      "[23000]\ttraining's auc: 0.91181\tvalid_1's auc: 0.900261\n",
      "[24000]\ttraining's auc: 0.912083\tvalid_1's auc: 0.90034\n",
      "[25000]\ttraining's auc: 0.912338\tvalid_1's auc: 0.90041\n",
      "[26000]\ttraining's auc: 0.912578\tvalid_1's auc: 0.900454\n",
      "[27000]\ttraining's auc: 0.912805\tvalid_1's auc: 0.900472\n",
      "[28000]\ttraining's auc: 0.913034\tvalid_1's auc: 0.900522\n",
      "[29000]\ttraining's auc: 0.913255\tvalid_1's auc: 0.900548\n",
      "[30000]\ttraining's auc: 0.913474\tvalid_1's auc: 0.900558\n",
      "[31000]\ttraining's auc: 0.913677\tvalid_1's auc: 0.900564\n",
      "[32000]\ttraining's auc: 0.913873\tvalid_1's auc: 0.900575\n",
      "Early stopping, best iteration is:\n",
      "[31302]\ttraining's auc: 0.913737\tvalid_1's auc: 0.900584\n",
      "0.9005837059837002\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier(\n",
    "                 max_depth=-1,\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate=0.02,\n",
    "                 colsample_bytree=0.3,\n",
    "                 num_leaves=2,\n",
    "                 metric='auc',\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1, y, test_size=0.2, stratify=y)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "y_pred_valid = model.predict_proba(X_valid)[:,1].reshape(-1,1)\n",
    "score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8784370877684948\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "n_estimators=20\n",
    "clf = OneVsRestClassifier(BaggingClassifier(SVC(gamma='auto', probability=True, class_weight='balanced'), \n",
    "                                            max_samples=1.0 / n_estimators, n_estimators=n_estimators, n_jobs=-1), n_jobs=-1)\n",
    "# clf = OneVsRestClassifier(SVC(gamma='auto', probability=True, class_weight='balanced'), n_jobs=-1)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_valid = clf.predict_proba(X_valid)[:,1].reshape(-1,1)\n",
    "score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8786467122420141\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "n_estimators=20\n",
    "clf = OneVsRestClassifier(BaggingClassifier(SVC(gamma='auto', probability=True, class_weight='balanced', C=0.01), \n",
    "                                            max_samples=1.0 / n_estimators, n_estimators=n_estimators, n_jobs=-1), n_jobs=-1)\n",
    "# clf = OneVsRestClassifier(SVC(gamma='auto', probability=True, class_weight='balanced'), n_jobs=-1)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_valid = clf.predict_proba(X_valid)[:,1].reshape(-1,1)\n",
    "score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8787070138468303\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "n_estimators=20\n",
    "clf = OneVsRestClassifier(BaggingClassifier(SVC(gamma='scale', probability=True, class_weight='balanced', C=0.01), \n",
    "                                            max_samples=1.0 / n_estimators, n_estimators=n_estimators, n_jobs=-1), n_jobs=-1)\n",
    "# clf = OneVsRestClassifier(SVC(gamma='auto', probability=True, class_weight='balanced'), n_jobs=-1)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_valid = clf.predict_proba(X_valid)[:,1].reshape(-1,1)\n",
    "score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875586865561022\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "n_estimators=20\n",
    "clf = OneVsRestClassifier(BaggingClassifier(SVC(gamma='scale', probability=True, C=0.001), \n",
    "                                            max_samples=1.0 / n_estimators, n_estimators=n_estimators, n_jobs=-1), n_jobs=-1)\n",
    "# clf = OneVsRestClassifier(SVC(gamma='auto', probability=True, class_weight='balanced'), n_jobs=-1)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_valid = clf.predict_proba(X_valid)[:,1].reshape(-1,1)\n",
    "score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8610643074234167\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "n_estimators=20\n",
    "clf = OneVsRestClassifier(BaggingClassifier(SVC(kernel='poly', gamma='scale', probability=True, class_weight='balanced', C=0.01), \n",
    "                                            max_samples=1.0 / n_estimators, n_estimators=n_estimators, n_jobs=-1), n_jobs=-1)\n",
    "# clf = OneVsRestClassifier(SVC(gamma='auto', probability=True, class_weight='balanced'), n_jobs=-1)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_valid = clf.predict_proba(X_valid)[:,1].reshape(-1,1)\n",
    "score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8e5d4ca0b8496abf4e52831ea745ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider\n",
    "IntSlider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [02:43<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "\n",
    "cols = list(X3.columns)\n",
    "for col in tqdm(cols):\n",
    "    pc = len(X3.loc[X3[col] >= 0])\n",
    "    nc = len(X3.loc[X3[col] < 0])\n",
    "    X3[col+'_pnc'] = X3[col].map(lambda x: pc if x >= 0 else nc)\n",
    "    \n",
    "    pc = len(X4.loc[X4[col] >= 0])\n",
    "    nc = len(X4.loc[X4[col] < 0])\n",
    "    X4[col+'_pnc'] = X4[col].map(lambda x: pc if x >= 0 else nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 400)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>var_99</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "      <th>var_0_pnc</th>\n",
       "      <th>var_1_pnc</th>\n",
       "      <th>var_2_pnc</th>\n",
       "      <th>var_3_pnc</th>\n",
       "      <th>var_4_pnc</th>\n",
       "      <th>var_5_pnc</th>\n",
       "      <th>var_6_pnc</th>\n",
       "      <th>var_7_pnc</th>\n",
       "      <th>var_8_pnc</th>\n",
       "      <th>var_9_pnc</th>\n",
       "      <th>var_10_pnc</th>\n",
       "      <th>var_11_pnc</th>\n",
       "      <th>var_12_pnc</th>\n",
       "      <th>var_13_pnc</th>\n",
       "      <th>var_14_pnc</th>\n",
       "      <th>var_15_pnc</th>\n",
       "      <th>var_16_pnc</th>\n",
       "      <th>var_17_pnc</th>\n",
       "      <th>var_18_pnc</th>\n",
       "      <th>var_19_pnc</th>\n",
       "      <th>var_20_pnc</th>\n",
       "      <th>var_21_pnc</th>\n",
       "      <th>var_22_pnc</th>\n",
       "      <th>var_23_pnc</th>\n",
       "      <th>var_24_pnc</th>\n",
       "      <th>var_25_pnc</th>\n",
       "      <th>var_26_pnc</th>\n",
       "      <th>var_27_pnc</th>\n",
       "      <th>var_28_pnc</th>\n",
       "      <th>var_29_pnc</th>\n",
       "      <th>var_30_pnc</th>\n",
       "      <th>var_31_pnc</th>\n",
       "      <th>var_32_pnc</th>\n",
       "      <th>var_33_pnc</th>\n",
       "      <th>var_34_pnc</th>\n",
       "      <th>var_35_pnc</th>\n",
       "      <th>var_36_pnc</th>\n",
       "      <th>var_37_pnc</th>\n",
       "      <th>var_38_pnc</th>\n",
       "      <th>var_39_pnc</th>\n",
       "      <th>var_40_pnc</th>\n",
       "      <th>var_41_pnc</th>\n",
       "      <th>var_42_pnc</th>\n",
       "      <th>var_43_pnc</th>\n",
       "      <th>var_44_pnc</th>\n",
       "      <th>var_45_pnc</th>\n",
       "      <th>var_46_pnc</th>\n",
       "      <th>var_47_pnc</th>\n",
       "      <th>var_48_pnc</th>\n",
       "      <th>var_49_pnc</th>\n",
       "      <th>var_50_pnc</th>\n",
       "      <th>var_51_pnc</th>\n",
       "      <th>var_52_pnc</th>\n",
       "      <th>var_53_pnc</th>\n",
       "      <th>var_54_pnc</th>\n",
       "      <th>var_55_pnc</th>\n",
       "      <th>var_56_pnc</th>\n",
       "      <th>var_57_pnc</th>\n",
       "      <th>var_58_pnc</th>\n",
       "      <th>var_59_pnc</th>\n",
       "      <th>var_60_pnc</th>\n",
       "      <th>var_61_pnc</th>\n",
       "      <th>var_62_pnc</th>\n",
       "      <th>var_63_pnc</th>\n",
       "      <th>var_64_pnc</th>\n",
       "      <th>var_65_pnc</th>\n",
       "      <th>var_66_pnc</th>\n",
       "      <th>var_67_pnc</th>\n",
       "      <th>var_68_pnc</th>\n",
       "      <th>var_69_pnc</th>\n",
       "      <th>var_70_pnc</th>\n",
       "      <th>var_71_pnc</th>\n",
       "      <th>var_72_pnc</th>\n",
       "      <th>var_73_pnc</th>\n",
       "      <th>var_74_pnc</th>\n",
       "      <th>var_75_pnc</th>\n",
       "      <th>var_76_pnc</th>\n",
       "      <th>var_77_pnc</th>\n",
       "      <th>var_78_pnc</th>\n",
       "      <th>var_79_pnc</th>\n",
       "      <th>var_80_pnc</th>\n",
       "      <th>var_81_pnc</th>\n",
       "      <th>var_82_pnc</th>\n",
       "      <th>var_83_pnc</th>\n",
       "      <th>var_84_pnc</th>\n",
       "      <th>var_85_pnc</th>\n",
       "      <th>var_86_pnc</th>\n",
       "      <th>var_87_pnc</th>\n",
       "      <th>var_88_pnc</th>\n",
       "      <th>var_89_pnc</th>\n",
       "      <th>var_90_pnc</th>\n",
       "      <th>var_91_pnc</th>\n",
       "      <th>var_92_pnc</th>\n",
       "      <th>var_93_pnc</th>\n",
       "      <th>var_94_pnc</th>\n",
       "      <th>var_95_pnc</th>\n",
       "      <th>var_96_pnc</th>\n",
       "      <th>var_97_pnc</th>\n",
       "      <th>var_98_pnc</th>\n",
       "      <th>var_99_pnc</th>\n",
       "      <th>var_100_pnc</th>\n",
       "      <th>var_101_pnc</th>\n",
       "      <th>var_102_pnc</th>\n",
       "      <th>var_103_pnc</th>\n",
       "      <th>var_104_pnc</th>\n",
       "      <th>var_105_pnc</th>\n",
       "      <th>var_106_pnc</th>\n",
       "      <th>var_107_pnc</th>\n",
       "      <th>var_108_pnc</th>\n",
       "      <th>var_109_pnc</th>\n",
       "      <th>var_110_pnc</th>\n",
       "      <th>var_111_pnc</th>\n",
       "      <th>var_112_pnc</th>\n",
       "      <th>var_113_pnc</th>\n",
       "      <th>var_114_pnc</th>\n",
       "      <th>var_115_pnc</th>\n",
       "      <th>var_116_pnc</th>\n",
       "      <th>var_117_pnc</th>\n",
       "      <th>var_118_pnc</th>\n",
       "      <th>var_119_pnc</th>\n",
       "      <th>var_120_pnc</th>\n",
       "      <th>var_121_pnc</th>\n",
       "      <th>var_122_pnc</th>\n",
       "      <th>var_123_pnc</th>\n",
       "      <th>var_124_pnc</th>\n",
       "      <th>var_125_pnc</th>\n",
       "      <th>var_126_pnc</th>\n",
       "      <th>var_127_pnc</th>\n",
       "      <th>var_128_pnc</th>\n",
       "      <th>var_129_pnc</th>\n",
       "      <th>var_130_pnc</th>\n",
       "      <th>var_131_pnc</th>\n",
       "      <th>var_132_pnc</th>\n",
       "      <th>var_133_pnc</th>\n",
       "      <th>var_134_pnc</th>\n",
       "      <th>var_135_pnc</th>\n",
       "      <th>var_136_pnc</th>\n",
       "      <th>var_137_pnc</th>\n",
       "      <th>var_138_pnc</th>\n",
       "      <th>var_139_pnc</th>\n",
       "      <th>var_140_pnc</th>\n",
       "      <th>var_141_pnc</th>\n",
       "      <th>var_142_pnc</th>\n",
       "      <th>var_143_pnc</th>\n",
       "      <th>var_144_pnc</th>\n",
       "      <th>var_145_pnc</th>\n",
       "      <th>var_146_pnc</th>\n",
       "      <th>var_147_pnc</th>\n",
       "      <th>var_148_pnc</th>\n",
       "      <th>var_149_pnc</th>\n",
       "      <th>var_150_pnc</th>\n",
       "      <th>var_151_pnc</th>\n",
       "      <th>var_152_pnc</th>\n",
       "      <th>var_153_pnc</th>\n",
       "      <th>var_154_pnc</th>\n",
       "      <th>var_155_pnc</th>\n",
       "      <th>var_156_pnc</th>\n",
       "      <th>var_157_pnc</th>\n",
       "      <th>var_158_pnc</th>\n",
       "      <th>var_159_pnc</th>\n",
       "      <th>var_160_pnc</th>\n",
       "      <th>var_161_pnc</th>\n",
       "      <th>var_162_pnc</th>\n",
       "      <th>var_163_pnc</th>\n",
       "      <th>var_164_pnc</th>\n",
       "      <th>var_165_pnc</th>\n",
       "      <th>var_166_pnc</th>\n",
       "      <th>var_167_pnc</th>\n",
       "      <th>var_168_pnc</th>\n",
       "      <th>var_169_pnc</th>\n",
       "      <th>var_170_pnc</th>\n",
       "      <th>var_171_pnc</th>\n",
       "      <th>var_172_pnc</th>\n",
       "      <th>var_173_pnc</th>\n",
       "      <th>var_174_pnc</th>\n",
       "      <th>var_175_pnc</th>\n",
       "      <th>var_176_pnc</th>\n",
       "      <th>var_177_pnc</th>\n",
       "      <th>var_178_pnc</th>\n",
       "      <th>var_179_pnc</th>\n",
       "      <th>var_180_pnc</th>\n",
       "      <th>var_181_pnc</th>\n",
       "      <th>var_182_pnc</th>\n",
       "      <th>var_183_pnc</th>\n",
       "      <th>var_184_pnc</th>\n",
       "      <th>var_185_pnc</th>\n",
       "      <th>var_186_pnc</th>\n",
       "      <th>var_187_pnc</th>\n",
       "      <th>var_188_pnc</th>\n",
       "      <th>var_189_pnc</th>\n",
       "      <th>var_190_pnc</th>\n",
       "      <th>var_191_pnc</th>\n",
       "      <th>var_192_pnc</th>\n",
       "      <th>var_193_pnc</th>\n",
       "      <th>var_194_pnc</th>\n",
       "      <th>var_195_pnc</th>\n",
       "      <th>var_196_pnc</th>\n",
       "      <th>var_197_pnc</th>\n",
       "      <th>var_198_pnc</th>\n",
       "      <th>var_199_pnc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.508872</td>\n",
       "      <td>-1.224011</td>\n",
       "      <td>0.424961</td>\n",
       "      <td>-0.744061</td>\n",
       "      <td>0.197456</td>\n",
       "      <td>-0.462353</td>\n",
       "      <td>-0.275825</td>\n",
       "      <td>0.547453</td>\n",
       "      <td>-1.537319</td>\n",
       "      <td>-1.430491</td>\n",
       "      <td>0.417258</td>\n",
       "      <td>1.061130</td>\n",
       "      <td>-0.053554</td>\n",
       "      <td>-1.701265</td>\n",
       "      <td>0.478528</td>\n",
       "      <td>-0.009846</td>\n",
       "      <td>-1.439048</td>\n",
       "      <td>-0.218093</td>\n",
       "      <td>-1.389731</td>\n",
       "      <td>2.437745</td>\n",
       "      <td>-0.376164</td>\n",
       "      <td>-0.120907</td>\n",
       "      <td>-0.539969</td>\n",
       "      <td>-0.996270</td>\n",
       "      <td>0.896423</td>\n",
       "      <td>-0.758004</td>\n",
       "      <td>-0.152229</td>\n",
       "      <td>0.405968</td>\n",
       "      <td>-0.756842</td>\n",
       "      <td>0.371556</td>\n",
       "      <td>0.832625</td>\n",
       "      <td>1.126437</td>\n",
       "      <td>-1.228115</td>\n",
       "      <td>0.436273</td>\n",
       "      <td>-0.391536</td>\n",
       "      <td>1.275088</td>\n",
       "      <td>-0.471711</td>\n",
       "      <td>0.837338</td>\n",
       "      <td>-1.378347</td>\n",
       "      <td>0.734911</td>\n",
       "      <td>0.156171</td>\n",
       "      <td>-0.262772</td>\n",
       "      <td>0.432555</td>\n",
       "      <td>1.536329</td>\n",
       "      <td>0.402899</td>\n",
       "      <td>0.215349</td>\n",
       "      <td>-1.998225</td>\n",
       "      <td>-0.133493</td>\n",
       "      <td>0.117516</td>\n",
       "      <td>-1.445268</td>\n",
       "      <td>0.235074</td>\n",
       "      <td>1.924706</td>\n",
       "      <td>0.345646</td>\n",
       "      <td>-1.018154</td>\n",
       "      <td>-0.198386</td>\n",
       "      <td>0.321070</td>\n",
       "      <td>-1.503508</td>\n",
       "      <td>-1.077196</td>\n",
       "      <td>-0.256230</td>\n",
       "      <td>1.325150</td>\n",
       "      <td>0.924402</td>\n",
       "      <td>1.472348</td>\n",
       "      <td>0.605144</td>\n",
       "      <td>-0.253106</td>\n",
       "      <td>1.593805</td>\n",
       "      <td>2.625341</td>\n",
       "      <td>0.016916</td>\n",
       "      <td>1.386769</td>\n",
       "      <td>-1.104747</td>\n",
       "      <td>-0.295027</td>\n",
       "      <td>-0.193527</td>\n",
       "      <td>-0.344534</td>\n",
       "      <td>1.110739</td>\n",
       "      <td>-1.461900</td>\n",
       "      <td>1.682793</td>\n",
       "      <td>0.223397</td>\n",
       "      <td>-1.042293</td>\n",
       "      <td>1.045428</td>\n",
       "      <td>0.489750</td>\n",
       "      <td>-1.821784</td>\n",
       "      <td>1.012049</td>\n",
       "      <td>-0.429499</td>\n",
       "      <td>0.463789</td>\n",
       "      <td>0.194024</td>\n",
       "      <td>-0.285872</td>\n",
       "      <td>0.697372</td>\n",
       "      <td>0.428779</td>\n",
       "      <td>0.843065</td>\n",
       "      <td>-0.232475</td>\n",
       "      <td>-1.571557</td>\n",
       "      <td>-0.315599</td>\n",
       "      <td>-1.336105</td>\n",
       "      <td>-0.346051</td>\n",
       "      <td>-1.036536</td>\n",
       "      <td>1.373301</td>\n",
       "      <td>0.438034</td>\n",
       "      <td>-0.548828</td>\n",
       "      <td>-0.144918</td>\n",
       "      <td>0.551146</td>\n",
       "      <td>-1.449367</td>\n",
       "      <td>1.847650</td>\n",
       "      <td>-0.014313</td>\n",
       "      <td>0.442737</td>\n",
       "      <td>-0.614159</td>\n",
       "      <td>1.611917</td>\n",
       "      <td>2.003843</td>\n",
       "      <td>0.469827</td>\n",
       "      <td>-0.049574</td>\n",
       "      <td>-0.610181</td>\n",
       "      <td>1.277076</td>\n",
       "      <td>-0.880408</td>\n",
       "      <td>0.369523</td>\n",
       "      <td>0.268365</td>\n",
       "      <td>-1.968934</td>\n",
       "      <td>-0.532551</td>\n",
       "      <td>-0.262303</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>-0.026808</td>\n",
       "      <td>-0.875037</td>\n",
       "      <td>0.967843</td>\n",
       "      <td>-0.651054</td>\n",
       "      <td>-0.853578</td>\n",
       "      <td>-0.049838</td>\n",
       "      <td>-0.997225</td>\n",
       "      <td>-0.023484</td>\n",
       "      <td>-1.579968</td>\n",
       "      <td>0.721599</td>\n",
       "      <td>-0.445202</td>\n",
       "      <td>-0.207919</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.619072</td>\n",
       "      <td>-0.371550</td>\n",
       "      <td>1.226177</td>\n",
       "      <td>-0.612904</td>\n",
       "      <td>1.665347</td>\n",
       "      <td>0.958218</td>\n",
       "      <td>0.530219</td>\n",
       "      <td>1.509807</td>\n",
       "      <td>0.345895</td>\n",
       "      <td>1.007441</td>\n",
       "      <td>0.934049</td>\n",
       "      <td>-1.225500</td>\n",
       "      <td>1.193279</td>\n",
       "      <td>-0.347329</td>\n",
       "      <td>0.040059</td>\n",
       "      <td>0.864729</td>\n",
       "      <td>0.456535</td>\n",
       "      <td>-1.820471</td>\n",
       "      <td>0.164587</td>\n",
       "      <td>1.194714</td>\n",
       "      <td>0.623941</td>\n",
       "      <td>0.146003</td>\n",
       "      <td>0.408562</td>\n",
       "      <td>0.019368</td>\n",
       "      <td>0.620921</td>\n",
       "      <td>0.636454</td>\n",
       "      <td>-1.017515</td>\n",
       "      <td>-1.532977</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.310293</td>\n",
       "      <td>-0.695170</td>\n",
       "      <td>-1.458345</td>\n",
       "      <td>-1.188871</td>\n",
       "      <td>-1.118597</td>\n",
       "      <td>1.820037</td>\n",
       "      <td>-1.292122</td>\n",
       "      <td>-0.672492</td>\n",
       "      <td>0.415463</td>\n",
       "      <td>1.427898</td>\n",
       "      <td>-0.377262</td>\n",
       "      <td>-1.015148</td>\n",
       "      <td>-1.391223</td>\n",
       "      <td>0.127603</td>\n",
       "      <td>0.598396</td>\n",
       "      <td>-0.212151</td>\n",
       "      <td>-1.345542</td>\n",
       "      <td>-0.884819</td>\n",
       "      <td>0.608339</td>\n",
       "      <td>-0.264803</td>\n",
       "      <td>-0.276711</td>\n",
       "      <td>-1.007248</td>\n",
       "      <td>-0.724973</td>\n",
       "      <td>0.224407</td>\n",
       "      <td>1.167918</td>\n",
       "      <td>1.344013</td>\n",
       "      <td>2.243360</td>\n",
       "      <td>0.866955</td>\n",
       "      <td>-0.718892</td>\n",
       "      <td>0.508070</td>\n",
       "      <td>-0.139913</td>\n",
       "      <td>0.255350</td>\n",
       "      <td>-1.133055</td>\n",
       "      <td>0.796166</td>\n",
       "      <td>-0.407116</td>\n",
       "      <td>0.153824</td>\n",
       "      <td>-1.630450</td>\n",
       "      <td>0.953034</td>\n",
       "      <td>-0.328012</td>\n",
       "      <td>-1.022937</td>\n",
       "      <td>0.134878</td>\n",
       "      <td>99667</td>\n",
       "      <td>100023</td>\n",
       "      <td>99823</td>\n",
       "      <td>99886</td>\n",
       "      <td>99821</td>\n",
       "      <td>99493</td>\n",
       "      <td>99950</td>\n",
       "      <td>100049</td>\n",
       "      <td>99730</td>\n",
       "      <td>99840</td>\n",
       "      <td>100039</td>\n",
       "      <td>99921</td>\n",
       "      <td>99723</td>\n",
       "      <td>99952</td>\n",
       "      <td>99893</td>\n",
       "      <td>99889</td>\n",
       "      <td>100032</td>\n",
       "      <td>100253</td>\n",
       "      <td>100356</td>\n",
       "      <td>99766</td>\n",
       "      <td>100079</td>\n",
       "      <td>100002</td>\n",
       "      <td>100081</td>\n",
       "      <td>99983</td>\n",
       "      <td>99820</td>\n",
       "      <td>99792</td>\n",
       "      <td>100276</td>\n",
       "      <td>99634</td>\n",
       "      <td>100206</td>\n",
       "      <td>100170</td>\n",
       "      <td>99880</td>\n",
       "      <td>100018</td>\n",
       "      <td>99783</td>\n",
       "      <td>100345</td>\n",
       "      <td>99961</td>\n",
       "      <td>100271</td>\n",
       "      <td>100151</td>\n",
       "      <td>100239</td>\n",
       "      <td>99773</td>\n",
       "      <td>100244</td>\n",
       "      <td>99906</td>\n",
       "      <td>100145</td>\n",
       "      <td>100285</td>\n",
       "      <td>99984</td>\n",
       "      <td>99727</td>\n",
       "      <td>99473</td>\n",
       "      <td>100010</td>\n",
       "      <td>99712</td>\n",
       "      <td>100042</td>\n",
       "      <td>99900</td>\n",
       "      <td>100114</td>\n",
       "      <td>100048</td>\n",
       "      <td>99498</td>\n",
       "      <td>100110</td>\n",
       "      <td>99477</td>\n",
       "      <td>99810</td>\n",
       "      <td>99764</td>\n",
       "      <td>99641</td>\n",
       "      <td>99854</td>\n",
       "      <td>99937</td>\n",
       "      <td>99919</td>\n",
       "      <td>100049</td>\n",
       "      <td>100252</td>\n",
       "      <td>99841</td>\n",
       "      <td>99671</td>\n",
       "      <td>100087</td>\n",
       "      <td>99671</td>\n",
       "      <td>100137</td>\n",
       "      <td>99560</td>\n",
       "      <td>99872</td>\n",
       "      <td>99928</td>\n",
       "      <td>99607</td>\n",
       "      <td>100037</td>\n",
       "      <td>100034</td>\n",
       "      <td>99984</td>\n",
       "      <td>99589</td>\n",
       "      <td>99947</td>\n",
       "      <td>100077</td>\n",
       "      <td>100208</td>\n",
       "      <td>99811</td>\n",
       "      <td>99998</td>\n",
       "      <td>99913</td>\n",
       "      <td>100027</td>\n",
       "      <td>99630</td>\n",
       "      <td>100183</td>\n",
       "      <td>99982</td>\n",
       "      <td>99824</td>\n",
       "      <td>100048</td>\n",
       "      <td>100148</td>\n",
       "      <td>99772</td>\n",
       "      <td>100398</td>\n",
       "      <td>100015</td>\n",
       "      <td>99887</td>\n",
       "      <td>99901</td>\n",
       "      <td>100182</td>\n",
       "      <td>100178</td>\n",
       "      <td>100075</td>\n",
       "      <td>99780</td>\n",
       "      <td>100128</td>\n",
       "      <td>99726</td>\n",
       "      <td>100337</td>\n",
       "      <td>100217</td>\n",
       "      <td>100107</td>\n",
       "      <td>99998</td>\n",
       "      <td>100127</td>\n",
       "      <td>99709</td>\n",
       "      <td>99886</td>\n",
       "      <td>100085</td>\n",
       "      <td>99829</td>\n",
       "      <td>99826</td>\n",
       "      <td>99711</td>\n",
       "      <td>100256</td>\n",
       "      <td>99986</td>\n",
       "      <td>100204</td>\n",
       "      <td>99520</td>\n",
       "      <td>100041</td>\n",
       "      <td>99828</td>\n",
       "      <td>100126</td>\n",
       "      <td>100198</td>\n",
       "      <td>99789</td>\n",
       "      <td>99695</td>\n",
       "      <td>100016</td>\n",
       "      <td>99773</td>\n",
       "      <td>99879</td>\n",
       "      <td>99559</td>\n",
       "      <td>100432</td>\n",
       "      <td>100046</td>\n",
       "      <td>99855</td>\n",
       "      <td>99678</td>\n",
       "      <td>99987</td>\n",
       "      <td>100299</td>\n",
       "      <td>99394</td>\n",
       "      <td>99778</td>\n",
       "      <td>100192</td>\n",
       "      <td>100130</td>\n",
       "      <td>100069</td>\n",
       "      <td>99499</td>\n",
       "      <td>99792</td>\n",
       "      <td>100006</td>\n",
       "      <td>99910</td>\n",
       "      <td>99822</td>\n",
       "      <td>99591</td>\n",
       "      <td>100656</td>\n",
       "      <td>99791</td>\n",
       "      <td>99519</td>\n",
       "      <td>99998</td>\n",
       "      <td>100356</td>\n",
       "      <td>99987</td>\n",
       "      <td>100471</td>\n",
       "      <td>99849</td>\n",
       "      <td>100454</td>\n",
       "      <td>100130</td>\n",
       "      <td>99816</td>\n",
       "      <td>99949</td>\n",
       "      <td>100128</td>\n",
       "      <td>100156</td>\n",
       "      <td>99664</td>\n",
       "      <td>99898</td>\n",
       "      <td>100232</td>\n",
       "      <td>100257</td>\n",
       "      <td>100061</td>\n",
       "      <td>100169</td>\n",
       "      <td>100035</td>\n",
       "      <td>100126</td>\n",
       "      <td>100031</td>\n",
       "      <td>99805</td>\n",
       "      <td>100152</td>\n",
       "      <td>100344</td>\n",
       "      <td>99832</td>\n",
       "      <td>99543</td>\n",
       "      <td>100005</td>\n",
       "      <td>100138</td>\n",
       "      <td>100300</td>\n",
       "      <td>100240</td>\n",
       "      <td>100302</td>\n",
       "      <td>99805</td>\n",
       "      <td>100067</td>\n",
       "      <td>100023</td>\n",
       "      <td>100019</td>\n",
       "      <td>99970</td>\n",
       "      <td>100331</td>\n",
       "      <td>99980</td>\n",
       "      <td>100030</td>\n",
       "      <td>99709</td>\n",
       "      <td>99829</td>\n",
       "      <td>100415</td>\n",
       "      <td>100028</td>\n",
       "      <td>99407</td>\n",
       "      <td>100385</td>\n",
       "      <td>99847</td>\n",
       "      <td>100013</td>\n",
       "      <td>100018</td>\n",
       "      <td>99861</td>\n",
       "      <td>99885</td>\n",
       "      <td>100252</td>\n",
       "      <td>100007</td>\n",
       "      <td>100320</td>\n",
       "      <td>100051</td>\n",
       "      <td>100093</td>\n",
       "      <td>99989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.297270</td>\n",
       "      <td>-0.544298</td>\n",
       "      <td>1.172628</td>\n",
       "      <td>-0.607792</td>\n",
       "      <td>0.734201</td>\n",
       "      <td>1.614287</td>\n",
       "      <td>0.241267</td>\n",
       "      <td>0.018579</td>\n",
       "      <td>0.744474</td>\n",
       "      <td>0.318751</td>\n",
       "      <td>-0.147742</td>\n",
       "      <td>1.855147</td>\n",
       "      <td>-0.005018</td>\n",
       "      <td>-0.040022</td>\n",
       "      <td>-0.830584</td>\n",
       "      <td>-2.378103</td>\n",
       "      <td>1.654614</td>\n",
       "      <td>-1.448337</td>\n",
       "      <td>-0.855770</td>\n",
       "      <td>2.067098</td>\n",
       "      <td>-1.780175</td>\n",
       "      <td>-1.735751</td>\n",
       "      <td>1.471335</td>\n",
       "      <td>0.618493</td>\n",
       "      <td>-0.857791</td>\n",
       "      <td>0.770085</td>\n",
       "      <td>-1.304933</td>\n",
       "      <td>-0.839379</td>\n",
       "      <td>-0.612092</td>\n",
       "      <td>-1.839963</td>\n",
       "      <td>-0.192078</td>\n",
       "      <td>-1.131588</td>\n",
       "      <td>0.606855</td>\n",
       "      <td>0.952798</td>\n",
       "      <td>-0.102877</td>\n",
       "      <td>-0.560898</td>\n",
       "      <td>0.177043</td>\n",
       "      <td>-0.018274</td>\n",
       "      <td>0.060267</td>\n",
       "      <td>0.687294</td>\n",
       "      <td>-0.477138</td>\n",
       "      <td>-1.199335</td>\n",
       "      <td>-1.918378</td>\n",
       "      <td>0.613762</td>\n",
       "      <td>-1.230809</td>\n",
       "      <td>-1.634408</td>\n",
       "      <td>-1.377250</td>\n",
       "      <td>1.120979</td>\n",
       "      <td>1.726299</td>\n",
       "      <td>1.098832</td>\n",
       "      <td>-1.281720</td>\n",
       "      <td>-0.022211</td>\n",
       "      <td>-0.302553</td>\n",
       "      <td>0.880001</td>\n",
       "      <td>-1.017145</td>\n",
       "      <td>0.943759</td>\n",
       "      <td>0.684960</td>\n",
       "      <td>0.878579</td>\n",
       "      <td>-0.278480</td>\n",
       "      <td>-0.035784</td>\n",
       "      <td>-0.914147</td>\n",
       "      <td>0.346486</td>\n",
       "      <td>0.471670</td>\n",
       "      <td>-0.585352</td>\n",
       "      <td>1.111624</td>\n",
       "      <td>0.128322</td>\n",
       "      <td>-0.319592</td>\n",
       "      <td>-0.515512</td>\n",
       "      <td>0.387078</td>\n",
       "      <td>1.394067</td>\n",
       "      <td>1.306603</td>\n",
       "      <td>-0.528980</td>\n",
       "      <td>0.596881</td>\n",
       "      <td>0.093272</td>\n",
       "      <td>-0.798098</td>\n",
       "      <td>-1.607583</td>\n",
       "      <td>-0.348275</td>\n",
       "      <td>1.066162</td>\n",
       "      <td>0.029902</td>\n",
       "      <td>-0.402509</td>\n",
       "      <td>-0.399567</td>\n",
       "      <td>1.586062</td>\n",
       "      <td>0.358464</td>\n",
       "      <td>-0.643884</td>\n",
       "      <td>-0.421958</td>\n",
       "      <td>-1.119128</td>\n",
       "      <td>-2.427165</td>\n",
       "      <td>-0.030128</td>\n",
       "      <td>-0.269634</td>\n",
       "      <td>2.256102</td>\n",
       "      <td>1.241995</td>\n",
       "      <td>-0.582009</td>\n",
       "      <td>-1.075128</td>\n",
       "      <td>0.706666</td>\n",
       "      <td>0.240256</td>\n",
       "      <td>-2.034121</td>\n",
       "      <td>1.167620</td>\n",
       "      <td>0.564559</td>\n",
       "      <td>0.167518</td>\n",
       "      <td>0.764519</td>\n",
       "      <td>-0.723209</td>\n",
       "      <td>-0.947558</td>\n",
       "      <td>1.536535</td>\n",
       "      <td>0.688999</td>\n",
       "      <td>1.881550</td>\n",
       "      <td>-2.002999</td>\n",
       "      <td>-0.641066</td>\n",
       "      <td>1.918169</td>\n",
       "      <td>-1.532596</td>\n",
       "      <td>-1.256374</td>\n",
       "      <td>0.313218</td>\n",
       "      <td>0.699958</td>\n",
       "      <td>1.252457</td>\n",
       "      <td>0.071572</td>\n",
       "      <td>0.488486</td>\n",
       "      <td>0.693152</td>\n",
       "      <td>0.328724</td>\n",
       "      <td>-0.567134</td>\n",
       "      <td>-0.667435</td>\n",
       "      <td>1.029607</td>\n",
       "      <td>0.555876</td>\n",
       "      <td>-1.010936</td>\n",
       "      <td>1.343949</td>\n",
       "      <td>0.418425</td>\n",
       "      <td>-1.497856</td>\n",
       "      <td>0.302147</td>\n",
       "      <td>1.846011</td>\n",
       "      <td>0.432920</td>\n",
       "      <td>0.518568</td>\n",
       "      <td>0.251070</td>\n",
       "      <td>0.167631</td>\n",
       "      <td>-1.531463</td>\n",
       "      <td>-1.379899</td>\n",
       "      <td>0.620230</td>\n",
       "      <td>1.068799</td>\n",
       "      <td>-0.560273</td>\n",
       "      <td>-1.209419</td>\n",
       "      <td>0.079714</td>\n",
       "      <td>-0.665734</td>\n",
       "      <td>1.079765</td>\n",
       "      <td>0.022897</td>\n",
       "      <td>0.474761</td>\n",
       "      <td>-2.380235</td>\n",
       "      <td>-0.020845</td>\n",
       "      <td>-0.048759</td>\n",
       "      <td>1.612866</td>\n",
       "      <td>-0.573041</td>\n",
       "      <td>2.068104</td>\n",
       "      <td>1.350506</td>\n",
       "      <td>-0.903608</td>\n",
       "      <td>1.520615</td>\n",
       "      <td>1.163130</td>\n",
       "      <td>0.545001</td>\n",
       "      <td>0.031187</td>\n",
       "      <td>-1.498223</td>\n",
       "      <td>0.378988</td>\n",
       "      <td>-0.934742</td>\n",
       "      <td>-0.815222</td>\n",
       "      <td>0.514175</td>\n",
       "      <td>0.097214</td>\n",
       "      <td>0.416364</td>\n",
       "      <td>1.031509</td>\n",
       "      <td>-1.034575</td>\n",
       "      <td>0.901728</td>\n",
       "      <td>-0.350887</td>\n",
       "      <td>-0.788782</td>\n",
       "      <td>0.566720</td>\n",
       "      <td>1.715981</td>\n",
       "      <td>-0.280807</td>\n",
       "      <td>0.415510</td>\n",
       "      <td>1.254368</td>\n",
       "      <td>1.066374</td>\n",
       "      <td>0.253110</td>\n",
       "      <td>1.002574</td>\n",
       "      <td>1.399291</td>\n",
       "      <td>-0.328477</td>\n",
       "      <td>-0.542583</td>\n",
       "      <td>1.846540</td>\n",
       "      <td>-0.667170</td>\n",
       "      <td>2.926470</td>\n",
       "      <td>-0.483336</td>\n",
       "      <td>-0.375052</td>\n",
       "      <td>-1.779155</td>\n",
       "      <td>0.155339</td>\n",
       "      <td>0.989980</td>\n",
       "      <td>-0.176510</td>\n",
       "      <td>-0.384679</td>\n",
       "      <td>-0.421209</td>\n",
       "      <td>-0.457714</td>\n",
       "      <td>-1.054505</td>\n",
       "      <td>0.935328</td>\n",
       "      <td>0.104340</td>\n",
       "      <td>0.438242</td>\n",
       "      <td>2.040567</td>\n",
       "      <td>-0.739803</td>\n",
       "      <td>1.468769</td>\n",
       "      <td>1.003240</td>\n",
       "      <td>-0.097438</td>\n",
       "      <td>0.773022</td>\n",
       "      <td>0.391499</td>\n",
       "      <td>100333</td>\n",
       "      <td>100023</td>\n",
       "      <td>99823</td>\n",
       "      <td>99886</td>\n",
       "      <td>99821</td>\n",
       "      <td>100507</td>\n",
       "      <td>100050</td>\n",
       "      <td>100049</td>\n",
       "      <td>100270</td>\n",
       "      <td>100160</td>\n",
       "      <td>99961</td>\n",
       "      <td>99921</td>\n",
       "      <td>99723</td>\n",
       "      <td>99952</td>\n",
       "      <td>100107</td>\n",
       "      <td>99889</td>\n",
       "      <td>99968</td>\n",
       "      <td>100253</td>\n",
       "      <td>100356</td>\n",
       "      <td>99766</td>\n",
       "      <td>100079</td>\n",
       "      <td>100002</td>\n",
       "      <td>99919</td>\n",
       "      <td>100017</td>\n",
       "      <td>100180</td>\n",
       "      <td>100208</td>\n",
       "      <td>100276</td>\n",
       "      <td>100366</td>\n",
       "      <td>100206</td>\n",
       "      <td>99830</td>\n",
       "      <td>100120</td>\n",
       "      <td>99982</td>\n",
       "      <td>100217</td>\n",
       "      <td>100345</td>\n",
       "      <td>99961</td>\n",
       "      <td>99729</td>\n",
       "      <td>99849</td>\n",
       "      <td>99761</td>\n",
       "      <td>100227</td>\n",
       "      <td>100244</td>\n",
       "      <td>100094</td>\n",
       "      <td>100145</td>\n",
       "      <td>99715</td>\n",
       "      <td>99984</td>\n",
       "      <td>100273</td>\n",
       "      <td>100527</td>\n",
       "      <td>100010</td>\n",
       "      <td>100288</td>\n",
       "      <td>100042</td>\n",
       "      <td>100100</td>\n",
       "      <td>99886</td>\n",
       "      <td>99952</td>\n",
       "      <td>100502</td>\n",
       "      <td>99890</td>\n",
       "      <td>99477</td>\n",
       "      <td>99810</td>\n",
       "      <td>100236</td>\n",
       "      <td>100359</td>\n",
       "      <td>99854</td>\n",
       "      <td>100063</td>\n",
       "      <td>100081</td>\n",
       "      <td>100049</td>\n",
       "      <td>100252</td>\n",
       "      <td>99841</td>\n",
       "      <td>99671</td>\n",
       "      <td>100087</td>\n",
       "      <td>100329</td>\n",
       "      <td>99863</td>\n",
       "      <td>100440</td>\n",
       "      <td>100128</td>\n",
       "      <td>100072</td>\n",
       "      <td>99607</td>\n",
       "      <td>100037</td>\n",
       "      <td>99966</td>\n",
       "      <td>100016</td>\n",
       "      <td>100411</td>\n",
       "      <td>99947</td>\n",
       "      <td>100077</td>\n",
       "      <td>100208</td>\n",
       "      <td>99811</td>\n",
       "      <td>100002</td>\n",
       "      <td>100087</td>\n",
       "      <td>100027</td>\n",
       "      <td>100370</td>\n",
       "      <td>100183</td>\n",
       "      <td>100018</td>\n",
       "      <td>100176</td>\n",
       "      <td>99952</td>\n",
       "      <td>100148</td>\n",
       "      <td>100228</td>\n",
       "      <td>99602</td>\n",
       "      <td>100015</td>\n",
       "      <td>99887</td>\n",
       "      <td>100099</td>\n",
       "      <td>100182</td>\n",
       "      <td>99822</td>\n",
       "      <td>99925</td>\n",
       "      <td>100220</td>\n",
       "      <td>100128</td>\n",
       "      <td>100274</td>\n",
       "      <td>99663</td>\n",
       "      <td>100217</td>\n",
       "      <td>100107</td>\n",
       "      <td>100002</td>\n",
       "      <td>100127</td>\n",
       "      <td>100291</td>\n",
       "      <td>100114</td>\n",
       "      <td>99915</td>\n",
       "      <td>99829</td>\n",
       "      <td>100174</td>\n",
       "      <td>100289</td>\n",
       "      <td>100256</td>\n",
       "      <td>99986</td>\n",
       "      <td>99796</td>\n",
       "      <td>100480</td>\n",
       "      <td>99959</td>\n",
       "      <td>99828</td>\n",
       "      <td>100126</td>\n",
       "      <td>100198</td>\n",
       "      <td>99789</td>\n",
       "      <td>100305</td>\n",
       "      <td>100016</td>\n",
       "      <td>100227</td>\n",
       "      <td>100121</td>\n",
       "      <td>99559</td>\n",
       "      <td>99568</td>\n",
       "      <td>100046</td>\n",
       "      <td>100145</td>\n",
       "      <td>100322</td>\n",
       "      <td>99987</td>\n",
       "      <td>100299</td>\n",
       "      <td>99394</td>\n",
       "      <td>100222</td>\n",
       "      <td>99808</td>\n",
       "      <td>100130</td>\n",
       "      <td>99931</td>\n",
       "      <td>100501</td>\n",
       "      <td>99792</td>\n",
       "      <td>99994</td>\n",
       "      <td>99910</td>\n",
       "      <td>99822</td>\n",
       "      <td>100409</td>\n",
       "      <td>99344</td>\n",
       "      <td>99791</td>\n",
       "      <td>100481</td>\n",
       "      <td>99998</td>\n",
       "      <td>99644</td>\n",
       "      <td>100013</td>\n",
       "      <td>100471</td>\n",
       "      <td>100151</td>\n",
       "      <td>100454</td>\n",
       "      <td>100130</td>\n",
       "      <td>99816</td>\n",
       "      <td>99949</td>\n",
       "      <td>99872</td>\n",
       "      <td>100156</td>\n",
       "      <td>99664</td>\n",
       "      <td>99898</td>\n",
       "      <td>100232</td>\n",
       "      <td>100257</td>\n",
       "      <td>99939</td>\n",
       "      <td>99831</td>\n",
       "      <td>100035</td>\n",
       "      <td>99874</td>\n",
       "      <td>99969</td>\n",
       "      <td>99805</td>\n",
       "      <td>99848</td>\n",
       "      <td>100344</td>\n",
       "      <td>100168</td>\n",
       "      <td>100457</td>\n",
       "      <td>99995</td>\n",
       "      <td>99862</td>\n",
       "      <td>100300</td>\n",
       "      <td>100240</td>\n",
       "      <td>99698</td>\n",
       "      <td>99805</td>\n",
       "      <td>100067</td>\n",
       "      <td>100023</td>\n",
       "      <td>100019</td>\n",
       "      <td>100030</td>\n",
       "      <td>100331</td>\n",
       "      <td>99980</td>\n",
       "      <td>99970</td>\n",
       "      <td>99709</td>\n",
       "      <td>99829</td>\n",
       "      <td>99585</td>\n",
       "      <td>99972</td>\n",
       "      <td>99407</td>\n",
       "      <td>99615</td>\n",
       "      <td>99847</td>\n",
       "      <td>100013</td>\n",
       "      <td>99982</td>\n",
       "      <td>99861</td>\n",
       "      <td>100115</td>\n",
       "      <td>99748</td>\n",
       "      <td>99993</td>\n",
       "      <td>100320</td>\n",
       "      <td>100051</td>\n",
       "      <td>99907</td>\n",
       "      <td>99989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.618451</td>\n",
       "      <td>-0.245573</td>\n",
       "      <td>0.480749</td>\n",
       "      <td>0.467453</td>\n",
       "      <td>-0.293870</td>\n",
       "      <td>-0.441126</td>\n",
       "      <td>1.677984</td>\n",
       "      <td>-0.475440</td>\n",
       "      <td>-1.536994</td>\n",
       "      <td>-1.234482</td>\n",
       "      <td>-0.134566</td>\n",
       "      <td>-1.334812</td>\n",
       "      <td>0.833189</td>\n",
       "      <td>-0.248027</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.161770</td>\n",
       "      <td>-0.584461</td>\n",
       "      <td>0.539358</td>\n",
       "      <td>-1.319420</td>\n",
       "      <td>0.939151</td>\n",
       "      <td>0.653720</td>\n",
       "      <td>0.104864</td>\n",
       "      <td>-1.038240</td>\n",
       "      <td>0.873865</td>\n",
       "      <td>-1.300904</td>\n",
       "      <td>-1.643380</td>\n",
       "      <td>-0.618555</td>\n",
       "      <td>-1.077166</td>\n",
       "      <td>0.368799</td>\n",
       "      <td>0.425365</td>\n",
       "      <td>-0.376045</td>\n",
       "      <td>0.943547</td>\n",
       "      <td>-0.592219</td>\n",
       "      <td>0.877742</td>\n",
       "      <td>-0.648724</td>\n",
       "      <td>0.039195</td>\n",
       "      <td>0.753857</td>\n",
       "      <td>0.404276</td>\n",
       "      <td>0.076792</td>\n",
       "      <td>0.054415</td>\n",
       "      <td>-0.751531</td>\n",
       "      <td>-1.116606</td>\n",
       "      <td>0.369634</td>\n",
       "      <td>1.518869</td>\n",
       "      <td>-0.847484</td>\n",
       "      <td>0.180038</td>\n",
       "      <td>-0.055466</td>\n",
       "      <td>0.539323</td>\n",
       "      <td>0.489698</td>\n",
       "      <td>-1.186818</td>\n",
       "      <td>0.227993</td>\n",
       "      <td>0.699977</td>\n",
       "      <td>0.627710</td>\n",
       "      <td>0.484729</td>\n",
       "      <td>0.901436</td>\n",
       "      <td>-1.051980</td>\n",
       "      <td>-0.253840</td>\n",
       "      <td>-0.510835</td>\n",
       "      <td>-0.464773</td>\n",
       "      <td>0.288369</td>\n",
       "      <td>0.099469</td>\n",
       "      <td>0.109420</td>\n",
       "      <td>0.040736</td>\n",
       "      <td>1.734531</td>\n",
       "      <td>-1.774418</td>\n",
       "      <td>-0.524056</td>\n",
       "      <td>1.353284</td>\n",
       "      <td>-0.265841</td>\n",
       "      <td>0.494971</td>\n",
       "      <td>-0.535918</td>\n",
       "      <td>-1.978672</td>\n",
       "      <td>0.630636</td>\n",
       "      <td>1.437780</td>\n",
       "      <td>-0.965224</td>\n",
       "      <td>0.012912</td>\n",
       "      <td>0.376530</td>\n",
       "      <td>-0.206770</td>\n",
       "      <td>0.994052</td>\n",
       "      <td>0.390825</td>\n",
       "      <td>-1.158125</td>\n",
       "      <td>0.161875</td>\n",
       "      <td>0.419260</td>\n",
       "      <td>2.150092</td>\n",
       "      <td>1.013412</td>\n",
       "      <td>1.270152</td>\n",
       "      <td>0.163425</td>\n",
       "      <td>-1.258190</td>\n",
       "      <td>-0.285558</td>\n",
       "      <td>1.226880</td>\n",
       "      <td>-0.394776</td>\n",
       "      <td>-0.379486</td>\n",
       "      <td>0.595137</td>\n",
       "      <td>0.316112</td>\n",
       "      <td>-0.130305</td>\n",
       "      <td>1.135716</td>\n",
       "      <td>0.570475</td>\n",
       "      <td>0.621755</td>\n",
       "      <td>-0.093896</td>\n",
       "      <td>-0.091112</td>\n",
       "      <td>0.377371</td>\n",
       "      <td>0.623128</td>\n",
       "      <td>-0.129664</td>\n",
       "      <td>-0.809111</td>\n",
       "      <td>-0.918498</td>\n",
       "      <td>1.794271</td>\n",
       "      <td>-0.259363</td>\n",
       "      <td>-1.737203</td>\n",
       "      <td>-1.177718</td>\n",
       "      <td>-0.179867</td>\n",
       "      <td>0.386238</td>\n",
       "      <td>-0.556903</td>\n",
       "      <td>-0.508250</td>\n",
       "      <td>0.639902</td>\n",
       "      <td>-1.308650</td>\n",
       "      <td>0.045270</td>\n",
       "      <td>0.227395</td>\n",
       "      <td>-0.677826</td>\n",
       "      <td>0.887164</td>\n",
       "      <td>-1.331599</td>\n",
       "      <td>-0.194194</td>\n",
       "      <td>-0.245200</td>\n",
       "      <td>-1.397905</td>\n",
       "      <td>0.270739</td>\n",
       "      <td>-1.465552</td>\n",
       "      <td>-0.207618</td>\n",
       "      <td>-0.062015</td>\n",
       "      <td>1.182201</td>\n",
       "      <td>-1.002538</td>\n",
       "      <td>0.042932</td>\n",
       "      <td>0.370518</td>\n",
       "      <td>-0.760395</td>\n",
       "      <td>-0.898428</td>\n",
       "      <td>0.613838</td>\n",
       "      <td>-0.467036</td>\n",
       "      <td>1.452315</td>\n",
       "      <td>-0.903757</td>\n",
       "      <td>0.119156</td>\n",
       "      <td>-0.159544</td>\n",
       "      <td>0.557135</td>\n",
       "      <td>0.072352</td>\n",
       "      <td>0.739237</td>\n",
       "      <td>-0.710614</td>\n",
       "      <td>-0.263203</td>\n",
       "      <td>-1.600036</td>\n",
       "      <td>0.454048</td>\n",
       "      <td>-0.122892</td>\n",
       "      <td>0.429289</td>\n",
       "      <td>0.640164</td>\n",
       "      <td>0.376283</td>\n",
       "      <td>-0.615873</td>\n",
       "      <td>-0.954431</td>\n",
       "      <td>-0.463432</td>\n",
       "      <td>-1.055587</td>\n",
       "      <td>-1.290920</td>\n",
       "      <td>-2.745608</td>\n",
       "      <td>0.447071</td>\n",
       "      <td>1.327196</td>\n",
       "      <td>0.538040</td>\n",
       "      <td>2.181733</td>\n",
       "      <td>0.323881</td>\n",
       "      <td>-0.927167</td>\n",
       "      <td>0.915526</td>\n",
       "      <td>-0.419830</td>\n",
       "      <td>-0.964374</td>\n",
       "      <td>0.245508</td>\n",
       "      <td>0.225399</td>\n",
       "      <td>0.768412</td>\n",
       "      <td>-0.185962</td>\n",
       "      <td>0.771519</td>\n",
       "      <td>0.340265</td>\n",
       "      <td>-1.632533</td>\n",
       "      <td>-0.530541</td>\n",
       "      <td>-1.421305</td>\n",
       "      <td>-0.800601</td>\n",
       "      <td>0.673423</td>\n",
       "      <td>-0.205923</td>\n",
       "      <td>0.269172</td>\n",
       "      <td>-1.128605</td>\n",
       "      <td>-0.942777</td>\n",
       "      <td>-0.038613</td>\n",
       "      <td>-0.268657</td>\n",
       "      <td>-0.441243</td>\n",
       "      <td>-1.557885</td>\n",
       "      <td>-1.249777</td>\n",
       "      <td>1.046953</td>\n",
       "      <td>-0.264548</td>\n",
       "      <td>-1.088363</td>\n",
       "      <td>0.292334</td>\n",
       "      <td>-1.325225</td>\n",
       "      <td>-1.650219</td>\n",
       "      <td>-0.060696</td>\n",
       "      <td>0.763213</td>\n",
       "      <td>-0.154204</td>\n",
       "      <td>-0.408408</td>\n",
       "      <td>1.064675</td>\n",
       "      <td>2.188146</td>\n",
       "      <td>-1.635374</td>\n",
       "      <td>-0.657751</td>\n",
       "      <td>-0.374011</td>\n",
       "      <td>0.255679</td>\n",
       "      <td>99667</td>\n",
       "      <td>100023</td>\n",
       "      <td>99823</td>\n",
       "      <td>100114</td>\n",
       "      <td>100179</td>\n",
       "      <td>99493</td>\n",
       "      <td>100050</td>\n",
       "      <td>99951</td>\n",
       "      <td>99730</td>\n",
       "      <td>99840</td>\n",
       "      <td>99961</td>\n",
       "      <td>100079</td>\n",
       "      <td>100277</td>\n",
       "      <td>99952</td>\n",
       "      <td>99893</td>\n",
       "      <td>100111</td>\n",
       "      <td>100032</td>\n",
       "      <td>99747</td>\n",
       "      <td>100356</td>\n",
       "      <td>99766</td>\n",
       "      <td>99921</td>\n",
       "      <td>99998</td>\n",
       "      <td>100081</td>\n",
       "      <td>100017</td>\n",
       "      <td>100180</td>\n",
       "      <td>99792</td>\n",
       "      <td>100276</td>\n",
       "      <td>100366</td>\n",
       "      <td>99794</td>\n",
       "      <td>100170</td>\n",
       "      <td>100120</td>\n",
       "      <td>100018</td>\n",
       "      <td>99783</td>\n",
       "      <td>100345</td>\n",
       "      <td>99961</td>\n",
       "      <td>100271</td>\n",
       "      <td>99849</td>\n",
       "      <td>100239</td>\n",
       "      <td>100227</td>\n",
       "      <td>100244</td>\n",
       "      <td>100094</td>\n",
       "      <td>100145</td>\n",
       "      <td>100285</td>\n",
       "      <td>99984</td>\n",
       "      <td>100273</td>\n",
       "      <td>99473</td>\n",
       "      <td>100010</td>\n",
       "      <td>100288</td>\n",
       "      <td>100042</td>\n",
       "      <td>99900</td>\n",
       "      <td>100114</td>\n",
       "      <td>100048</td>\n",
       "      <td>99498</td>\n",
       "      <td>99890</td>\n",
       "      <td>100523</td>\n",
       "      <td>100190</td>\n",
       "      <td>99764</td>\n",
       "      <td>99641</td>\n",
       "      <td>99854</td>\n",
       "      <td>99937</td>\n",
       "      <td>99919</td>\n",
       "      <td>100049</td>\n",
       "      <td>100252</td>\n",
       "      <td>100159</td>\n",
       "      <td>100329</td>\n",
       "      <td>99913</td>\n",
       "      <td>99671</td>\n",
       "      <td>99863</td>\n",
       "      <td>100440</td>\n",
       "      <td>99872</td>\n",
       "      <td>99928</td>\n",
       "      <td>100393</td>\n",
       "      <td>100037</td>\n",
       "      <td>100034</td>\n",
       "      <td>99984</td>\n",
       "      <td>99589</td>\n",
       "      <td>99947</td>\n",
       "      <td>100077</td>\n",
       "      <td>100208</td>\n",
       "      <td>99811</td>\n",
       "      <td>99998</td>\n",
       "      <td>100087</td>\n",
       "      <td>100027</td>\n",
       "      <td>99630</td>\n",
       "      <td>99817</td>\n",
       "      <td>99982</td>\n",
       "      <td>100176</td>\n",
       "      <td>99952</td>\n",
       "      <td>99852</td>\n",
       "      <td>99772</td>\n",
       "      <td>100398</td>\n",
       "      <td>99985</td>\n",
       "      <td>100113</td>\n",
       "      <td>99901</td>\n",
       "      <td>100182</td>\n",
       "      <td>100178</td>\n",
       "      <td>99925</td>\n",
       "      <td>99780</td>\n",
       "      <td>99872</td>\n",
       "      <td>100274</td>\n",
       "      <td>100337</td>\n",
       "      <td>100217</td>\n",
       "      <td>99893</td>\n",
       "      <td>99998</td>\n",
       "      <td>100127</td>\n",
       "      <td>100291</td>\n",
       "      <td>100114</td>\n",
       "      <td>100085</td>\n",
       "      <td>99829</td>\n",
       "      <td>99826</td>\n",
       "      <td>99711</td>\n",
       "      <td>99744</td>\n",
       "      <td>99986</td>\n",
       "      <td>100204</td>\n",
       "      <td>100480</td>\n",
       "      <td>99959</td>\n",
       "      <td>100172</td>\n",
       "      <td>99874</td>\n",
       "      <td>100198</td>\n",
       "      <td>100211</td>\n",
       "      <td>99695</td>\n",
       "      <td>100016</td>\n",
       "      <td>100227</td>\n",
       "      <td>99879</td>\n",
       "      <td>99559</td>\n",
       "      <td>100432</td>\n",
       "      <td>100046</td>\n",
       "      <td>99855</td>\n",
       "      <td>100322</td>\n",
       "      <td>99987</td>\n",
       "      <td>99701</td>\n",
       "      <td>99394</td>\n",
       "      <td>99778</td>\n",
       "      <td>100192</td>\n",
       "      <td>100130</td>\n",
       "      <td>99931</td>\n",
       "      <td>99499</td>\n",
       "      <td>100208</td>\n",
       "      <td>100006</td>\n",
       "      <td>99910</td>\n",
       "      <td>99822</td>\n",
       "      <td>99591</td>\n",
       "      <td>99344</td>\n",
       "      <td>99791</td>\n",
       "      <td>99519</td>\n",
       "      <td>100002</td>\n",
       "      <td>100356</td>\n",
       "      <td>100013</td>\n",
       "      <td>100471</td>\n",
       "      <td>100151</td>\n",
       "      <td>99546</td>\n",
       "      <td>99870</td>\n",
       "      <td>100184</td>\n",
       "      <td>100051</td>\n",
       "      <td>99872</td>\n",
       "      <td>100156</td>\n",
       "      <td>100336</td>\n",
       "      <td>100102</td>\n",
       "      <td>100232</td>\n",
       "      <td>100257</td>\n",
       "      <td>100061</td>\n",
       "      <td>99831</td>\n",
       "      <td>100035</td>\n",
       "      <td>100126</td>\n",
       "      <td>100031</td>\n",
       "      <td>100195</td>\n",
       "      <td>99848</td>\n",
       "      <td>99656</td>\n",
       "      <td>99832</td>\n",
       "      <td>100457</td>\n",
       "      <td>100005</td>\n",
       "      <td>100138</td>\n",
       "      <td>99700</td>\n",
       "      <td>99760</td>\n",
       "      <td>99698</td>\n",
       "      <td>99805</td>\n",
       "      <td>99933</td>\n",
       "      <td>99977</td>\n",
       "      <td>100019</td>\n",
       "      <td>99970</td>\n",
       "      <td>100331</td>\n",
       "      <td>99980</td>\n",
       "      <td>99970</td>\n",
       "      <td>100291</td>\n",
       "      <td>99829</td>\n",
       "      <td>99585</td>\n",
       "      <td>99972</td>\n",
       "      <td>100593</td>\n",
       "      <td>99615</td>\n",
       "      <td>99847</td>\n",
       "      <td>99987</td>\n",
       "      <td>99982</td>\n",
       "      <td>100139</td>\n",
       "      <td>99885</td>\n",
       "      <td>100252</td>\n",
       "      <td>99993</td>\n",
       "      <td>99680</td>\n",
       "      <td>100051</td>\n",
       "      <td>100093</td>\n",
       "      <td>99989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.164890</td>\n",
       "      <td>-0.117078</td>\n",
       "      <td>-0.585200</td>\n",
       "      <td>0.157349</td>\n",
       "      <td>0.872216</td>\n",
       "      <td>0.329188</td>\n",
       "      <td>0.487350</td>\n",
       "      <td>-0.389125</td>\n",
       "      <td>-1.921131</td>\n",
       "      <td>0.419160</td>\n",
       "      <td>0.307435</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>-0.931003</td>\n",
       "      <td>0.608538</td>\n",
       "      <td>-0.396133</td>\n",
       "      <td>0.586940</td>\n",
       "      <td>0.526417</td>\n",
       "      <td>0.733822</td>\n",
       "      <td>0.080706</td>\n",
       "      <td>0.135256</td>\n",
       "      <td>1.101799</td>\n",
       "      <td>-0.553052</td>\n",
       "      <td>0.859466</td>\n",
       "      <td>-0.507061</td>\n",
       "      <td>0.364480</td>\n",
       "      <td>0.239023</td>\n",
       "      <td>0.826839</td>\n",
       "      <td>0.119632</td>\n",
       "      <td>0.441525</td>\n",
       "      <td>-0.805016</td>\n",
       "      <td>-0.845457</td>\n",
       "      <td>0.034382</td>\n",
       "      <td>-0.695561</td>\n",
       "      <td>-2.189135</td>\n",
       "      <td>-1.516525</td>\n",
       "      <td>-1.932500</td>\n",
       "      <td>2.026980</td>\n",
       "      <td>-1.287160</td>\n",
       "      <td>-0.310039</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.321017</td>\n",
       "      <td>-1.985113</td>\n",
       "      <td>0.710987</td>\n",
       "      <td>-0.887689</td>\n",
       "      <td>-0.238625</td>\n",
       "      <td>-0.505049</td>\n",
       "      <td>0.476698</td>\n",
       "      <td>-2.490257</td>\n",
       "      <td>-1.183433</td>\n",
       "      <td>-0.966592</td>\n",
       "      <td>-0.084303</td>\n",
       "      <td>-0.416061</td>\n",
       "      <td>0.833559</td>\n",
       "      <td>-0.966700</td>\n",
       "      <td>0.755139</td>\n",
       "      <td>-0.032799</td>\n",
       "      <td>-0.570372</td>\n",
       "      <td>0.374488</td>\n",
       "      <td>0.032898</td>\n",
       "      <td>0.566703</td>\n",
       "      <td>0.829222</td>\n",
       "      <td>-1.037884</td>\n",
       "      <td>1.242952</td>\n",
       "      <td>-1.622311</td>\n",
       "      <td>1.567230</td>\n",
       "      <td>1.380963</td>\n",
       "      <td>-0.136638</td>\n",
       "      <td>1.547047</td>\n",
       "      <td>0.387078</td>\n",
       "      <td>-0.148498</td>\n",
       "      <td>-1.779969</td>\n",
       "      <td>-1.594785</td>\n",
       "      <td>0.142535</td>\n",
       "      <td>0.585380</td>\n",
       "      <td>-1.019246</td>\n",
       "      <td>-0.216774</td>\n",
       "      <td>0.656501</td>\n",
       "      <td>1.107313</td>\n",
       "      <td>-0.553129</td>\n",
       "      <td>0.594906</td>\n",
       "      <td>-0.662863</td>\n",
       "      <td>-1.632401</td>\n",
       "      <td>0.492959</td>\n",
       "      <td>-0.699263</td>\n",
       "      <td>0.615482</td>\n",
       "      <td>-0.067478</td>\n",
       "      <td>-1.009606</td>\n",
       "      <td>0.705633</td>\n",
       "      <td>-1.662203</td>\n",
       "      <td>1.069324</td>\n",
       "      <td>0.585484</td>\n",
       "      <td>0.283522</td>\n",
       "      <td>0.366869</td>\n",
       "      <td>0.356381</td>\n",
       "      <td>-1.303319</td>\n",
       "      <td>-1.686633</td>\n",
       "      <td>-0.299951</td>\n",
       "      <td>-0.020712</td>\n",
       "      <td>1.547316</td>\n",
       "      <td>1.423637</td>\n",
       "      <td>-1.392710</td>\n",
       "      <td>2.013377</td>\n",
       "      <td>-0.387876</td>\n",
       "      <td>-1.119885</td>\n",
       "      <td>-1.065794</td>\n",
       "      <td>0.063046</td>\n",
       "      <td>0.909662</td>\n",
       "      <td>-1.405054</td>\n",
       "      <td>0.467736</td>\n",
       "      <td>-0.859922</td>\n",
       "      <td>-0.280208</td>\n",
       "      <td>0.573904</td>\n",
       "      <td>-0.943593</td>\n",
       "      <td>0.694045</td>\n",
       "      <td>-0.482200</td>\n",
       "      <td>-1.766656</td>\n",
       "      <td>1.937721</td>\n",
       "      <td>1.636874</td>\n",
       "      <td>0.975198</td>\n",
       "      <td>0.440983</td>\n",
       "      <td>0.474520</td>\n",
       "      <td>0.029304</td>\n",
       "      <td>0.471976</td>\n",
       "      <td>-2.476439</td>\n",
       "      <td>-0.553328</td>\n",
       "      <td>-0.392191</td>\n",
       "      <td>-0.774560</td>\n",
       "      <td>-0.629027</td>\n",
       "      <td>1.445556</td>\n",
       "      <td>-0.331451</td>\n",
       "      <td>2.675090</td>\n",
       "      <td>1.045683</td>\n",
       "      <td>0.694758</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>0.506397</td>\n",
       "      <td>0.790789</td>\n",
       "      <td>0.368310</td>\n",
       "      <td>0.689426</td>\n",
       "      <td>-0.857501</td>\n",
       "      <td>0.109790</td>\n",
       "      <td>0.059002</td>\n",
       "      <td>2.306857</td>\n",
       "      <td>-0.297730</td>\n",
       "      <td>-1.493999</td>\n",
       "      <td>1.403607</td>\n",
       "      <td>-0.434232</td>\n",
       "      <td>0.044184</td>\n",
       "      <td>0.981833</td>\n",
       "      <td>-1.123022</td>\n",
       "      <td>-0.271991</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>1.784289</td>\n",
       "      <td>-0.009344</td>\n",
       "      <td>0.667803</td>\n",
       "      <td>2.287839</td>\n",
       "      <td>-1.557644</td>\n",
       "      <td>-1.175974</td>\n",
       "      <td>0.637988</td>\n",
       "      <td>0.255583</td>\n",
       "      <td>0.511670</td>\n",
       "      <td>0.613640</td>\n",
       "      <td>1.527798</td>\n",
       "      <td>1.280436</td>\n",
       "      <td>-1.402649</td>\n",
       "      <td>-1.459517</td>\n",
       "      <td>-0.376527</td>\n",
       "      <td>-0.983600</td>\n",
       "      <td>0.167928</td>\n",
       "      <td>-1.475845</td>\n",
       "      <td>-0.750309</td>\n",
       "      <td>-1.651426</td>\n",
       "      <td>-0.948260</td>\n",
       "      <td>-0.165418</td>\n",
       "      <td>2.301551</td>\n",
       "      <td>-0.722535</td>\n",
       "      <td>-1.177467</td>\n",
       "      <td>2.007137</td>\n",
       "      <td>0.684408</td>\n",
       "      <td>-1.032636</td>\n",
       "      <td>1.125581</td>\n",
       "      <td>-0.181224</td>\n",
       "      <td>-1.216340</td>\n",
       "      <td>0.114589</td>\n",
       "      <td>-0.269345</td>\n",
       "      <td>-1.454077</td>\n",
       "      <td>0.794270</td>\n",
       "      <td>-0.297652</td>\n",
       "      <td>-0.187792</td>\n",
       "      <td>-0.696291</td>\n",
       "      <td>1.060400</td>\n",
       "      <td>0.262148</td>\n",
       "      <td>-0.827281</td>\n",
       "      <td>-0.789504</td>\n",
       "      <td>-0.471215</td>\n",
       "      <td>1.642264</td>\n",
       "      <td>-0.750986</td>\n",
       "      <td>-0.835896</td>\n",
       "      <td>1.492051</td>\n",
       "      <td>0.643598</td>\n",
       "      <td>-0.488734</td>\n",
       "      <td>100333</td>\n",
       "      <td>100023</td>\n",
       "      <td>100177</td>\n",
       "      <td>100114</td>\n",
       "      <td>99821</td>\n",
       "      <td>100507</td>\n",
       "      <td>100050</td>\n",
       "      <td>99951</td>\n",
       "      <td>99730</td>\n",
       "      <td>100160</td>\n",
       "      <td>100039</td>\n",
       "      <td>99921</td>\n",
       "      <td>99723</td>\n",
       "      <td>100048</td>\n",
       "      <td>100107</td>\n",
       "      <td>100111</td>\n",
       "      <td>99968</td>\n",
       "      <td>99747</td>\n",
       "      <td>99644</td>\n",
       "      <td>99766</td>\n",
       "      <td>99921</td>\n",
       "      <td>100002</td>\n",
       "      <td>99919</td>\n",
       "      <td>99983</td>\n",
       "      <td>99820</td>\n",
       "      <td>100208</td>\n",
       "      <td>99724</td>\n",
       "      <td>99634</td>\n",
       "      <td>99794</td>\n",
       "      <td>99830</td>\n",
       "      <td>100120</td>\n",
       "      <td>100018</td>\n",
       "      <td>99783</td>\n",
       "      <td>99655</td>\n",
       "      <td>99961</td>\n",
       "      <td>99729</td>\n",
       "      <td>99849</td>\n",
       "      <td>99761</td>\n",
       "      <td>99773</td>\n",
       "      <td>100244</td>\n",
       "      <td>99906</td>\n",
       "      <td>100145</td>\n",
       "      <td>100285</td>\n",
       "      <td>100016</td>\n",
       "      <td>100273</td>\n",
       "      <td>100527</td>\n",
       "      <td>99990</td>\n",
       "      <td>99712</td>\n",
       "      <td>99958</td>\n",
       "      <td>99900</td>\n",
       "      <td>99886</td>\n",
       "      <td>99952</td>\n",
       "      <td>99498</td>\n",
       "      <td>100110</td>\n",
       "      <td>100523</td>\n",
       "      <td>100190</td>\n",
       "      <td>99764</td>\n",
       "      <td>100359</td>\n",
       "      <td>100146</td>\n",
       "      <td>99937</td>\n",
       "      <td>99919</td>\n",
       "      <td>99951</td>\n",
       "      <td>100252</td>\n",
       "      <td>99841</td>\n",
       "      <td>99671</td>\n",
       "      <td>100087</td>\n",
       "      <td>100329</td>\n",
       "      <td>100137</td>\n",
       "      <td>100440</td>\n",
       "      <td>99872</td>\n",
       "      <td>99928</td>\n",
       "      <td>99607</td>\n",
       "      <td>100037</td>\n",
       "      <td>99966</td>\n",
       "      <td>100016</td>\n",
       "      <td>100411</td>\n",
       "      <td>100053</td>\n",
       "      <td>100077</td>\n",
       "      <td>99792</td>\n",
       "      <td>100189</td>\n",
       "      <td>100002</td>\n",
       "      <td>99913</td>\n",
       "      <td>100027</td>\n",
       "      <td>100370</td>\n",
       "      <td>99817</td>\n",
       "      <td>100018</td>\n",
       "      <td>100176</td>\n",
       "      <td>100048</td>\n",
       "      <td>100148</td>\n",
       "      <td>100228</td>\n",
       "      <td>99602</td>\n",
       "      <td>99985</td>\n",
       "      <td>100113</td>\n",
       "      <td>100099</td>\n",
       "      <td>99818</td>\n",
       "      <td>99822</td>\n",
       "      <td>100075</td>\n",
       "      <td>99780</td>\n",
       "      <td>100128</td>\n",
       "      <td>100274</td>\n",
       "      <td>99663</td>\n",
       "      <td>99783</td>\n",
       "      <td>99893</td>\n",
       "      <td>99998</td>\n",
       "      <td>99873</td>\n",
       "      <td>99709</td>\n",
       "      <td>99886</td>\n",
       "      <td>100085</td>\n",
       "      <td>100171</td>\n",
       "      <td>100174</td>\n",
       "      <td>99711</td>\n",
       "      <td>100256</td>\n",
       "      <td>100014</td>\n",
       "      <td>99796</td>\n",
       "      <td>99520</td>\n",
       "      <td>100041</td>\n",
       "      <td>99828</td>\n",
       "      <td>99874</td>\n",
       "      <td>99802</td>\n",
       "      <td>99789</td>\n",
       "      <td>100305</td>\n",
       "      <td>99984</td>\n",
       "      <td>100227</td>\n",
       "      <td>99879</td>\n",
       "      <td>99559</td>\n",
       "      <td>100432</td>\n",
       "      <td>99954</td>\n",
       "      <td>99855</td>\n",
       "      <td>100322</td>\n",
       "      <td>100013</td>\n",
       "      <td>100299</td>\n",
       "      <td>100606</td>\n",
       "      <td>99778</td>\n",
       "      <td>99808</td>\n",
       "      <td>100130</td>\n",
       "      <td>100069</td>\n",
       "      <td>99499</td>\n",
       "      <td>99792</td>\n",
       "      <td>99994</td>\n",
       "      <td>99910</td>\n",
       "      <td>99822</td>\n",
       "      <td>100409</td>\n",
       "      <td>99344</td>\n",
       "      <td>99791</td>\n",
       "      <td>99519</td>\n",
       "      <td>100002</td>\n",
       "      <td>100356</td>\n",
       "      <td>100013</td>\n",
       "      <td>99529</td>\n",
       "      <td>100151</td>\n",
       "      <td>100454</td>\n",
       "      <td>100130</td>\n",
       "      <td>100184</td>\n",
       "      <td>99949</td>\n",
       "      <td>100128</td>\n",
       "      <td>99844</td>\n",
       "      <td>99664</td>\n",
       "      <td>100102</td>\n",
       "      <td>100232</td>\n",
       "      <td>100257</td>\n",
       "      <td>99939</td>\n",
       "      <td>99831</td>\n",
       "      <td>99965</td>\n",
       "      <td>100126</td>\n",
       "      <td>99969</td>\n",
       "      <td>99805</td>\n",
       "      <td>100152</td>\n",
       "      <td>100344</td>\n",
       "      <td>100168</td>\n",
       "      <td>99543</td>\n",
       "      <td>100005</td>\n",
       "      <td>100138</td>\n",
       "      <td>99700</td>\n",
       "      <td>100240</td>\n",
       "      <td>100302</td>\n",
       "      <td>99805</td>\n",
       "      <td>99933</td>\n",
       "      <td>100023</td>\n",
       "      <td>100019</td>\n",
       "      <td>100030</td>\n",
       "      <td>100331</td>\n",
       "      <td>99980</td>\n",
       "      <td>100030</td>\n",
       "      <td>100291</td>\n",
       "      <td>100171</td>\n",
       "      <td>100415</td>\n",
       "      <td>99972</td>\n",
       "      <td>99407</td>\n",
       "      <td>99615</td>\n",
       "      <td>100153</td>\n",
       "      <td>100013</td>\n",
       "      <td>100018</td>\n",
       "      <td>100139</td>\n",
       "      <td>99885</td>\n",
       "      <td>100252</td>\n",
       "      <td>100007</td>\n",
       "      <td>99680</td>\n",
       "      <td>99949</td>\n",
       "      <td>99907</td>\n",
       "      <td>100011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.211808</td>\n",
       "      <td>0.026459</td>\n",
       "      <td>0.810557</td>\n",
       "      <td>-0.078550</td>\n",
       "      <td>0.678320</td>\n",
       "      <td>0.903906</td>\n",
       "      <td>0.600080</td>\n",
       "      <td>0.718477</td>\n",
       "      <td>2.080059</td>\n",
       "      <td>0.039485</td>\n",
       "      <td>-1.810503</td>\n",
       "      <td>-1.500196</td>\n",
       "      <td>-0.921100</td>\n",
       "      <td>-0.142960</td>\n",
       "      <td>0.095549</td>\n",
       "      <td>1.129267</td>\n",
       "      <td>-0.273694</td>\n",
       "      <td>0.354230</td>\n",
       "      <td>-1.035619</td>\n",
       "      <td>-0.124821</td>\n",
       "      <td>1.322313</td>\n",
       "      <td>0.197623</td>\n",
       "      <td>1.947101</td>\n",
       "      <td>-0.522810</td>\n",
       "      <td>0.848551</td>\n",
       "      <td>-0.373927</td>\n",
       "      <td>1.215579</td>\n",
       "      <td>-1.423534</td>\n",
       "      <td>0.434855</td>\n",
       "      <td>1.102985</td>\n",
       "      <td>0.462110</td>\n",
       "      <td>-0.407015</td>\n",
       "      <td>0.161091</td>\n",
       "      <td>0.942374</td>\n",
       "      <td>1.897077</td>\n",
       "      <td>-0.580938</td>\n",
       "      <td>0.174982</td>\n",
       "      <td>-0.027314</td>\n",
       "      <td>2.016599</td>\n",
       "      <td>0.174612</td>\n",
       "      <td>0.113982</td>\n",
       "      <td>0.726296</td>\n",
       "      <td>0.930890</td>\n",
       "      <td>-0.156744</td>\n",
       "      <td>-0.428221</td>\n",
       "      <td>-2.952938</td>\n",
       "      <td>0.865036</td>\n",
       "      <td>1.104484</td>\n",
       "      <td>-1.272008</td>\n",
       "      <td>-0.248106</td>\n",
       "      <td>0.807656</td>\n",
       "      <td>-0.315787</td>\n",
       "      <td>0.199608</td>\n",
       "      <td>-0.301347</td>\n",
       "      <td>-1.073173</td>\n",
       "      <td>0.221583</td>\n",
       "      <td>0.084137</td>\n",
       "      <td>-0.088323</td>\n",
       "      <td>-0.271010</td>\n",
       "      <td>0.308106</td>\n",
       "      <td>-0.740974</td>\n",
       "      <td>-1.323349</td>\n",
       "      <td>-0.633256</td>\n",
       "      <td>-0.738366</td>\n",
       "      <td>-0.697693</td>\n",
       "      <td>-1.360630</td>\n",
       "      <td>1.016607</td>\n",
       "      <td>-1.394650</td>\n",
       "      <td>1.602210</td>\n",
       "      <td>0.019471</td>\n",
       "      <td>1.258486</td>\n",
       "      <td>0.359168</td>\n",
       "      <td>-0.315953</td>\n",
       "      <td>-2.084248</td>\n",
       "      <td>-1.695007</td>\n",
       "      <td>0.994619</td>\n",
       "      <td>-0.625458</td>\n",
       "      <td>0.039450</td>\n",
       "      <td>0.853863</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>-0.346869</td>\n",
       "      <td>-0.630328</td>\n",
       "      <td>-0.596506</td>\n",
       "      <td>-0.872785</td>\n",
       "      <td>1.701726</td>\n",
       "      <td>-0.899515</td>\n",
       "      <td>0.130964</td>\n",
       "      <td>1.526536</td>\n",
       "      <td>-0.229793</td>\n",
       "      <td>0.042084</td>\n",
       "      <td>-1.870504</td>\n",
       "      <td>0.115046</td>\n",
       "      <td>-0.752552</td>\n",
       "      <td>-0.261078</td>\n",
       "      <td>1.047878</td>\n",
       "      <td>0.189055</td>\n",
       "      <td>0.033327</td>\n",
       "      <td>1.283528</td>\n",
       "      <td>-0.109116</td>\n",
       "      <td>0.752971</td>\n",
       "      <td>-1.772310</td>\n",
       "      <td>-0.178384</td>\n",
       "      <td>-0.526223</td>\n",
       "      <td>0.611996</td>\n",
       "      <td>-2.457333</td>\n",
       "      <td>1.089462</td>\n",
       "      <td>0.107907</td>\n",
       "      <td>-0.888125</td>\n",
       "      <td>-0.758004</td>\n",
       "      <td>2.707881</td>\n",
       "      <td>-1.517573</td>\n",
       "      <td>-0.300406</td>\n",
       "      <td>-1.329435</td>\n",
       "      <td>-0.032582</td>\n",
       "      <td>-0.155980</td>\n",
       "      <td>-2.250268</td>\n",
       "      <td>-0.512999</td>\n",
       "      <td>0.664848</td>\n",
       "      <td>2.367604</td>\n",
       "      <td>0.362486</td>\n",
       "      <td>1.325056</td>\n",
       "      <td>0.723817</td>\n",
       "      <td>0.711948</td>\n",
       "      <td>0.350970</td>\n",
       "      <td>-0.122835</td>\n",
       "      <td>-1.058520</td>\n",
       "      <td>-0.715063</td>\n",
       "      <td>2.267362</td>\n",
       "      <td>2.460381</td>\n",
       "      <td>-1.062081</td>\n",
       "      <td>-0.906078</td>\n",
       "      <td>0.782836</td>\n",
       "      <td>-0.036337</td>\n",
       "      <td>-0.482013</td>\n",
       "      <td>-0.288017</td>\n",
       "      <td>1.415189</td>\n",
       "      <td>-0.467899</td>\n",
       "      <td>0.818329</td>\n",
       "      <td>0.314132</td>\n",
       "      <td>0.769602</td>\n",
       "      <td>0.585749</td>\n",
       "      <td>-1.022495</td>\n",
       "      <td>-0.015473</td>\n",
       "      <td>1.279296</td>\n",
       "      <td>-0.985721</td>\n",
       "      <td>1.086086</td>\n",
       "      <td>0.456960</td>\n",
       "      <td>-1.213096</td>\n",
       "      <td>-1.140077</td>\n",
       "      <td>0.417696</td>\n",
       "      <td>1.282586</td>\n",
       "      <td>-2.713985</td>\n",
       "      <td>-0.581736</td>\n",
       "      <td>1.515462</td>\n",
       "      <td>-0.410844</td>\n",
       "      <td>2.150333</td>\n",
       "      <td>-0.157288</td>\n",
       "      <td>-1.413970</td>\n",
       "      <td>1.071768</td>\n",
       "      <td>-0.470122</td>\n",
       "      <td>0.754880</td>\n",
       "      <td>0.254470</td>\n",
       "      <td>0.069214</td>\n",
       "      <td>1.340414</td>\n",
       "      <td>1.803853</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>0.643057</td>\n",
       "      <td>0.280899</td>\n",
       "      <td>-0.529132</td>\n",
       "      <td>-0.209787</td>\n",
       "      <td>0.303584</td>\n",
       "      <td>-0.318079</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.473040</td>\n",
       "      <td>-0.224194</td>\n",
       "      <td>1.594588</td>\n",
       "      <td>-0.096306</td>\n",
       "      <td>0.541834</td>\n",
       "      <td>0.276986</td>\n",
       "      <td>0.946520</td>\n",
       "      <td>-0.861178</td>\n",
       "      <td>-0.178980</td>\n",
       "      <td>0.406928</td>\n",
       "      <td>0.692215</td>\n",
       "      <td>-0.064579</td>\n",
       "      <td>-0.783722</td>\n",
       "      <td>-0.763558</td>\n",
       "      <td>1.408359</td>\n",
       "      <td>0.178520</td>\n",
       "      <td>-0.976163</td>\n",
       "      <td>-0.992715</td>\n",
       "      <td>0.675456</td>\n",
       "      <td>-1.392943</td>\n",
       "      <td>1.493446</td>\n",
       "      <td>-1.511752</td>\n",
       "      <td>-0.929213</td>\n",
       "      <td>0.227504</td>\n",
       "      <td>0.581454</td>\n",
       "      <td>0.653283</td>\n",
       "      <td>-0.473020</td>\n",
       "      <td>99667</td>\n",
       "      <td>99977</td>\n",
       "      <td>99823</td>\n",
       "      <td>99886</td>\n",
       "      <td>99821</td>\n",
       "      <td>100507</td>\n",
       "      <td>100050</td>\n",
       "      <td>100049</td>\n",
       "      <td>100270</td>\n",
       "      <td>100160</td>\n",
       "      <td>99961</td>\n",
       "      <td>100079</td>\n",
       "      <td>99723</td>\n",
       "      <td>99952</td>\n",
       "      <td>99893</td>\n",
       "      <td>100111</td>\n",
       "      <td>100032</td>\n",
       "      <td>99747</td>\n",
       "      <td>100356</td>\n",
       "      <td>100234</td>\n",
       "      <td>99921</td>\n",
       "      <td>99998</td>\n",
       "      <td>99919</td>\n",
       "      <td>99983</td>\n",
       "      <td>99820</td>\n",
       "      <td>99792</td>\n",
       "      <td>99724</td>\n",
       "      <td>100366</td>\n",
       "      <td>99794</td>\n",
       "      <td>100170</td>\n",
       "      <td>99880</td>\n",
       "      <td>99982</td>\n",
       "      <td>100217</td>\n",
       "      <td>100345</td>\n",
       "      <td>100039</td>\n",
       "      <td>99729</td>\n",
       "      <td>99849</td>\n",
       "      <td>99761</td>\n",
       "      <td>100227</td>\n",
       "      <td>100244</td>\n",
       "      <td>99906</td>\n",
       "      <td>99855</td>\n",
       "      <td>100285</td>\n",
       "      <td>100016</td>\n",
       "      <td>100273</td>\n",
       "      <td>100527</td>\n",
       "      <td>99990</td>\n",
       "      <td>100288</td>\n",
       "      <td>99958</td>\n",
       "      <td>99900</td>\n",
       "      <td>100114</td>\n",
       "      <td>99952</td>\n",
       "      <td>99498</td>\n",
       "      <td>100110</td>\n",
       "      <td>99477</td>\n",
       "      <td>99810</td>\n",
       "      <td>100236</td>\n",
       "      <td>99641</td>\n",
       "      <td>99854</td>\n",
       "      <td>99937</td>\n",
       "      <td>100081</td>\n",
       "      <td>99951</td>\n",
       "      <td>99748</td>\n",
       "      <td>99841</td>\n",
       "      <td>100329</td>\n",
       "      <td>99913</td>\n",
       "      <td>99671</td>\n",
       "      <td>99863</td>\n",
       "      <td>100440</td>\n",
       "      <td>100128</td>\n",
       "      <td>100072</td>\n",
       "      <td>100393</td>\n",
       "      <td>99963</td>\n",
       "      <td>100034</td>\n",
       "      <td>100016</td>\n",
       "      <td>99589</td>\n",
       "      <td>99947</td>\n",
       "      <td>100077</td>\n",
       "      <td>100208</td>\n",
       "      <td>100189</td>\n",
       "      <td>100002</td>\n",
       "      <td>99913</td>\n",
       "      <td>99973</td>\n",
       "      <td>100370</td>\n",
       "      <td>99817</td>\n",
       "      <td>100018</td>\n",
       "      <td>99824</td>\n",
       "      <td>100048</td>\n",
       "      <td>100148</td>\n",
       "      <td>100228</td>\n",
       "      <td>100398</td>\n",
       "      <td>99985</td>\n",
       "      <td>99887</td>\n",
       "      <td>99901</td>\n",
       "      <td>100182</td>\n",
       "      <td>100178</td>\n",
       "      <td>99925</td>\n",
       "      <td>100220</td>\n",
       "      <td>99872</td>\n",
       "      <td>100274</td>\n",
       "      <td>99663</td>\n",
       "      <td>100217</td>\n",
       "      <td>99893</td>\n",
       "      <td>100002</td>\n",
       "      <td>99873</td>\n",
       "      <td>99709</td>\n",
       "      <td>99886</td>\n",
       "      <td>100085</td>\n",
       "      <td>99829</td>\n",
       "      <td>99826</td>\n",
       "      <td>99711</td>\n",
       "      <td>99744</td>\n",
       "      <td>100014</td>\n",
       "      <td>100204</td>\n",
       "      <td>99520</td>\n",
       "      <td>100041</td>\n",
       "      <td>100172</td>\n",
       "      <td>99874</td>\n",
       "      <td>99802</td>\n",
       "      <td>99789</td>\n",
       "      <td>100305</td>\n",
       "      <td>99984</td>\n",
       "      <td>100227</td>\n",
       "      <td>100121</td>\n",
       "      <td>99559</td>\n",
       "      <td>100432</td>\n",
       "      <td>99954</td>\n",
       "      <td>100145</td>\n",
       "      <td>100322</td>\n",
       "      <td>100013</td>\n",
       "      <td>99701</td>\n",
       "      <td>100606</td>\n",
       "      <td>100222</td>\n",
       "      <td>100192</td>\n",
       "      <td>99870</td>\n",
       "      <td>100069</td>\n",
       "      <td>100501</td>\n",
       "      <td>99792</td>\n",
       "      <td>100006</td>\n",
       "      <td>99910</td>\n",
       "      <td>99822</td>\n",
       "      <td>99591</td>\n",
       "      <td>99344</td>\n",
       "      <td>100209</td>\n",
       "      <td>100481</td>\n",
       "      <td>99998</td>\n",
       "      <td>100356</td>\n",
       "      <td>99987</td>\n",
       "      <td>99529</td>\n",
       "      <td>99849</td>\n",
       "      <td>100454</td>\n",
       "      <td>99870</td>\n",
       "      <td>100184</td>\n",
       "      <td>99949</td>\n",
       "      <td>99872</td>\n",
       "      <td>100156</td>\n",
       "      <td>99664</td>\n",
       "      <td>99898</td>\n",
       "      <td>100232</td>\n",
       "      <td>99743</td>\n",
       "      <td>99939</td>\n",
       "      <td>99831</td>\n",
       "      <td>99965</td>\n",
       "      <td>99874</td>\n",
       "      <td>100031</td>\n",
       "      <td>100195</td>\n",
       "      <td>99848</td>\n",
       "      <td>100344</td>\n",
       "      <td>100168</td>\n",
       "      <td>99543</td>\n",
       "      <td>99995</td>\n",
       "      <td>100138</td>\n",
       "      <td>100300</td>\n",
       "      <td>100240</td>\n",
       "      <td>100302</td>\n",
       "      <td>100195</td>\n",
       "      <td>100067</td>\n",
       "      <td>100023</td>\n",
       "      <td>99981</td>\n",
       "      <td>100030</td>\n",
       "      <td>100331</td>\n",
       "      <td>99980</td>\n",
       "      <td>100030</td>\n",
       "      <td>99709</td>\n",
       "      <td>100171</td>\n",
       "      <td>99585</td>\n",
       "      <td>99972</td>\n",
       "      <td>100593</td>\n",
       "      <td>100385</td>\n",
       "      <td>99847</td>\n",
       "      <td>99987</td>\n",
       "      <td>99982</td>\n",
       "      <td>100139</td>\n",
       "      <td>100115</td>\n",
       "      <td>99748</td>\n",
       "      <td>100007</td>\n",
       "      <td>100320</td>\n",
       "      <td>99949</td>\n",
       "      <td>99907</td>\n",
       "      <td>100011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      var_0     var_1     var_2     var_3     var_4     var_5     var_6  \\\n",
       "0 -0.508872 -1.224011  0.424961 -0.744061  0.197456 -0.462353 -0.275825   \n",
       "1  0.297270 -0.544298  1.172628 -0.607792  0.734201  1.614287  0.241267   \n",
       "2 -0.618451 -0.245573  0.480749  0.467453 -0.293870 -0.441126  1.677984   \n",
       "3  0.164890 -0.117078 -0.585200  0.157349  0.872216  0.329188  0.487350   \n",
       "4 -0.211808  0.026459  0.810557 -0.078550  0.678320  0.903906  0.600080   \n",
       "\n",
       "      var_7     var_8     var_9    var_10    var_11    var_12    var_13  \\\n",
       "0  0.547453 -1.537319 -1.430491  0.417258  1.061130 -0.053554 -1.701265   \n",
       "1  0.018579  0.744474  0.318751 -0.147742  1.855147 -0.005018 -0.040022   \n",
       "2 -0.475440 -1.536994 -1.234482 -0.134566 -1.334812  0.833189 -0.248027   \n",
       "3 -0.389125 -1.921131  0.419160  0.307435  0.999677 -0.931003  0.608538   \n",
       "4  0.718477  2.080059  0.039485 -1.810503 -1.500196 -0.921100 -0.142960   \n",
       "\n",
       "     var_14    var_15    var_16    var_17    var_18    var_19    var_20  \\\n",
       "0  0.478528 -0.009846 -1.439048 -0.218093 -1.389731  2.437745 -0.376164   \n",
       "1 -0.830584 -2.378103  1.654614 -1.448337 -0.855770  2.067098 -1.780175   \n",
       "2  0.000127  0.161770 -0.584461  0.539358 -1.319420  0.939151  0.653720   \n",
       "3 -0.396133  0.586940  0.526417  0.733822  0.080706  0.135256  1.101799   \n",
       "4  0.095549  1.129267 -0.273694  0.354230 -1.035619 -0.124821  1.322313   \n",
       "\n",
       "     var_21    var_22    var_23    var_24    var_25    var_26    var_27  \\\n",
       "0 -0.120907 -0.539969 -0.996270  0.896423 -0.758004 -0.152229  0.405968   \n",
       "1 -1.735751  1.471335  0.618493 -0.857791  0.770085 -1.304933 -0.839379   \n",
       "2  0.104864 -1.038240  0.873865 -1.300904 -1.643380 -0.618555 -1.077166   \n",
       "3 -0.553052  0.859466 -0.507061  0.364480  0.239023  0.826839  0.119632   \n",
       "4  0.197623  1.947101 -0.522810  0.848551 -0.373927  1.215579 -1.423534   \n",
       "\n",
       "     var_28    var_29    var_30    var_31    var_32    var_33    var_34  \\\n",
       "0 -0.756842  0.371556  0.832625  1.126437 -1.228115  0.436273 -0.391536   \n",
       "1 -0.612092 -1.839963 -0.192078 -1.131588  0.606855  0.952798 -0.102877   \n",
       "2  0.368799  0.425365 -0.376045  0.943547 -0.592219  0.877742 -0.648724   \n",
       "3  0.441525 -0.805016 -0.845457  0.034382 -0.695561 -2.189135 -1.516525   \n",
       "4  0.434855  1.102985  0.462110 -0.407015  0.161091  0.942374  1.897077   \n",
       "\n",
       "     var_35    var_36    var_37    var_38    var_39    var_40    var_41  \\\n",
       "0  1.275088 -0.471711  0.837338 -1.378347  0.734911  0.156171 -0.262772   \n",
       "1 -0.560898  0.177043 -0.018274  0.060267  0.687294 -0.477138 -1.199335   \n",
       "2  0.039195  0.753857  0.404276  0.076792  0.054415 -0.751531 -1.116606   \n",
       "3 -1.932500  2.026980 -1.287160 -0.310039  0.001734  0.321017 -1.985113   \n",
       "4 -0.580938  0.174982 -0.027314  2.016599  0.174612  0.113982  0.726296   \n",
       "\n",
       "     var_42    var_43    var_44    var_45    var_46    var_47    var_48  \\\n",
       "0  0.432555  1.536329  0.402899  0.215349 -1.998225 -0.133493  0.117516   \n",
       "1 -1.918378  0.613762 -1.230809 -1.634408 -1.377250  1.120979  1.726299   \n",
       "2  0.369634  1.518869 -0.847484  0.180038 -0.055466  0.539323  0.489698   \n",
       "3  0.710987 -0.887689 -0.238625 -0.505049  0.476698 -2.490257 -1.183433   \n",
       "4  0.930890 -0.156744 -0.428221 -2.952938  0.865036  1.104484 -1.272008   \n",
       "\n",
       "     var_49    var_50    var_51    var_52    var_53    var_54    var_55  \\\n",
       "0 -1.445268  0.235074  1.924706  0.345646 -1.018154 -0.198386  0.321070   \n",
       "1  1.098832 -1.281720 -0.022211 -0.302553  0.880001 -1.017145  0.943759   \n",
       "2 -1.186818  0.227993  0.699977  0.627710  0.484729  0.901436 -1.051980   \n",
       "3 -0.966592 -0.084303 -0.416061  0.833559 -0.966700  0.755139 -0.032799   \n",
       "4 -0.248106  0.807656 -0.315787  0.199608 -0.301347 -1.073173  0.221583   \n",
       "\n",
       "     var_56    var_57    var_58    var_59    var_60    var_61    var_62  \\\n",
       "0 -1.503508 -1.077196 -0.256230  1.325150  0.924402  1.472348  0.605144   \n",
       "1  0.684960  0.878579 -0.278480 -0.035784 -0.914147  0.346486  0.471670   \n",
       "2 -0.253840 -0.510835 -0.464773  0.288369  0.099469  0.109420  0.040736   \n",
       "3 -0.570372  0.374488  0.032898  0.566703  0.829222 -1.037884  1.242952   \n",
       "4  0.084137 -0.088323 -0.271010  0.308106 -0.740974 -1.323349 -0.633256   \n",
       "\n",
       "     var_63    var_64    var_65    var_66    var_67    var_68    var_69  \\\n",
       "0 -0.253106  1.593805  2.625341  0.016916  1.386769 -1.104747 -0.295027   \n",
       "1 -0.585352  1.111624  0.128322 -0.319592 -0.515512  0.387078  1.394067   \n",
       "2  1.734531 -1.774418 -0.524056  1.353284 -0.265841  0.494971 -0.535918   \n",
       "3 -1.622311  1.567230  1.380963 -0.136638  1.547047  0.387078 -0.148498   \n",
       "4 -0.738366 -0.697693 -1.360630  1.016607 -1.394650  1.602210  0.019471   \n",
       "\n",
       "     var_70    var_71    var_72    var_73    var_74    var_75    var_76  \\\n",
       "0 -0.193527 -0.344534  1.110739 -1.461900  1.682793  0.223397 -1.042293   \n",
       "1  1.306603 -0.528980  0.596881  0.093272 -0.798098 -1.607583 -0.348275   \n",
       "2 -1.978672  0.630636  1.437780 -0.965224  0.012912  0.376530 -0.206770   \n",
       "3 -1.779969 -1.594785  0.142535  0.585380 -1.019246 -0.216774  0.656501   \n",
       "4  1.258486  0.359168 -0.315953 -2.084248 -1.695007  0.994619 -0.625458   \n",
       "\n",
       "     var_77    var_78    var_79    var_80    var_81    var_82    var_83  \\\n",
       "0  1.045428  0.489750 -1.821784  1.012049 -0.429499  0.463789  0.194024   \n",
       "1  1.066162  0.029902 -0.402509 -0.399567  1.586062  0.358464 -0.643884   \n",
       "2  0.994052  0.390825 -1.158125  0.161875  0.419260  2.150092  1.013412   \n",
       "3  1.107313 -0.553129  0.594906 -0.662863 -1.632401  0.492959 -0.699263   \n",
       "4  0.039450  0.853863  0.005216 -0.346869 -0.630328 -0.596506 -0.872785   \n",
       "\n",
       "     var_84    var_85    var_86    var_87    var_88    var_89    var_90  \\\n",
       "0 -0.285872  0.697372  0.428779  0.843065 -0.232475 -1.571557 -0.315599   \n",
       "1 -0.421958 -1.119128 -2.427165 -0.030128 -0.269634  2.256102  1.241995   \n",
       "2  1.270152  0.163425 -1.258190 -0.285558  1.226880 -0.394776 -0.379486   \n",
       "3  0.615482 -0.067478 -1.009606  0.705633 -1.662203  1.069324  0.585484   \n",
       "4  1.701726 -0.899515  0.130964  1.526536 -0.229793  0.042084 -1.870504   \n",
       "\n",
       "     var_91    var_92    var_93    var_94    var_95    var_96    var_97  \\\n",
       "0 -1.336105 -0.346051 -1.036536  1.373301  0.438034 -0.548828 -0.144918   \n",
       "1 -0.582009 -1.075128  0.706666  0.240256 -2.034121  1.167620  0.564559   \n",
       "2  0.595137  0.316112 -0.130305  1.135716  0.570475  0.621755 -0.093896   \n",
       "3  0.283522  0.366869  0.356381 -1.303319 -1.686633 -0.299951 -0.020712   \n",
       "4  0.115046 -0.752552 -0.261078  1.047878  0.189055  0.033327  1.283528   \n",
       "\n",
       "     var_98    var_99   var_100   var_101   var_102   var_103   var_104  \\\n",
       "0  0.551146 -1.449367  1.847650 -0.014313  0.442737 -0.614159  1.611917   \n",
       "1  0.167518  0.764519 -0.723209 -0.947558  1.536535  0.688999  1.881550   \n",
       "2 -0.091112  0.377371  0.623128 -0.129664 -0.809111 -0.918498  1.794271   \n",
       "3  1.547316  1.423637 -1.392710  2.013377 -0.387876 -1.119885 -1.065794   \n",
       "4 -0.109116  0.752971 -1.772310 -0.178384 -0.526223  0.611996 -2.457333   \n",
       "\n",
       "    var_105   var_106   var_107   var_108   var_109   var_110   var_111  \\\n",
       "0  2.003843  0.469827 -0.049574 -0.610181  1.277076 -0.880408  0.369523   \n",
       "1 -2.002999 -0.641066  1.918169 -1.532596 -1.256374  0.313218  0.699958   \n",
       "2 -0.259363 -1.737203 -1.177718 -0.179867  0.386238 -0.556903 -0.508250   \n",
       "3  0.063046  0.909662 -1.405054  0.467736 -0.859922 -0.280208  0.573904   \n",
       "4  1.089462  0.107907 -0.888125 -0.758004  2.707881 -1.517573 -0.300406   \n",
       "\n",
       "    var_112   var_113   var_114   var_115   var_116   var_117   var_118  \\\n",
       "0  0.268365 -1.968934 -0.532551 -0.262303  0.048077 -0.026808 -0.875037   \n",
       "1  1.252457  0.071572  0.488486  0.693152  0.328724 -0.567134 -0.667435   \n",
       "2  0.639902 -1.308650  0.045270  0.227395 -0.677826  0.887164 -1.331599   \n",
       "3 -0.943593  0.694045 -0.482200 -1.766656  1.937721  1.636874  0.975198   \n",
       "4 -1.329435 -0.032582 -0.155980 -2.250268 -0.512999  0.664848  2.367604   \n",
       "\n",
       "    var_119   var_120   var_121   var_122   var_123   var_124   var_125  \\\n",
       "0  0.967843 -0.651054 -0.853578 -0.049838 -0.997225 -0.023484 -1.579968   \n",
       "1  1.029607  0.555876 -1.010936  1.343949  0.418425 -1.497856  0.302147   \n",
       "2 -0.194194 -0.245200 -1.397905  0.270739 -1.465552 -0.207618 -0.062015   \n",
       "3  0.440983  0.474520  0.029304  0.471976 -2.476439 -0.553328 -0.392191   \n",
       "4  0.362486  1.325056  0.723817  0.711948  0.350970 -0.122835 -1.058520   \n",
       "\n",
       "    var_126   var_127   var_128   var_129   var_130   var_131   var_132  \\\n",
       "0  0.721599 -0.445202 -0.207919  0.004237  0.619072 -0.371550  1.226177   \n",
       "1  1.846011  0.432920  0.518568  0.251070  0.167631 -1.531463 -1.379899   \n",
       "2  1.182201 -1.002538  0.042932  0.370518 -0.760395 -0.898428  0.613838   \n",
       "3 -0.774560 -0.629027  1.445556 -0.331451  2.675090  1.045683  0.694758   \n",
       "4 -0.715063  2.267362  2.460381 -1.062081 -0.906078  0.782836 -0.036337   \n",
       "\n",
       "    var_133   var_134   var_135   var_136   var_137   var_138   var_139  \\\n",
       "0 -0.612904  1.665347  0.958218  0.530219  1.509807  0.345895  1.007441   \n",
       "1  0.620230  1.068799 -0.560273 -1.209419  0.079714 -0.665734  1.079765   \n",
       "2 -0.467036  1.452315 -0.903757  0.119156 -0.159544  0.557135  0.072352   \n",
       "3  0.994800  0.506397  0.790789  0.368310  0.689426 -0.857501  0.109790   \n",
       "4 -0.482013 -0.288017  1.415189 -0.467899  0.818329  0.314132  0.769602   \n",
       "\n",
       "    var_140   var_141   var_142   var_143   var_144   var_145   var_146  \\\n",
       "0  0.934049 -1.225500  1.193279 -0.347329  0.040059  0.864729  0.456535   \n",
       "1  0.022897  0.474761 -2.380235 -0.020845 -0.048759  1.612866 -0.573041   \n",
       "2  0.739237 -0.710614 -0.263203 -1.600036  0.454048 -0.122892  0.429289   \n",
       "3  0.059002  2.306857 -0.297730 -1.493999  1.403607 -0.434232  0.044184   \n",
       "4  0.585749 -1.022495 -0.015473  1.279296 -0.985721  1.086086  0.456960   \n",
       "\n",
       "    var_147   var_148   var_149   var_150   var_151   var_152   var_153  \\\n",
       "0 -1.820471  0.164587  1.194714  0.623941  0.146003  0.408562  0.019368   \n",
       "1  2.068104  1.350506 -0.903608  1.520615  1.163130  0.545001  0.031187   \n",
       "2  0.640164  0.376283 -0.615873 -0.954431 -0.463432 -1.055587 -1.290920   \n",
       "3  0.981833 -1.123022 -0.271991  0.480400  1.784289 -0.009344  0.667803   \n",
       "4 -1.213096 -1.140077  0.417696  1.282586 -2.713985 -0.581736  1.515462   \n",
       "\n",
       "    var_154   var_155   var_156   var_157   var_158   var_159   var_160  \\\n",
       "0  0.620921  0.636454 -1.017515 -1.532977  0.000039  0.310293 -0.695170   \n",
       "1 -1.498223  0.378988 -0.934742 -0.815222  0.514175  0.097214  0.416364   \n",
       "2 -2.745608  0.447071  1.327196  0.538040  2.181733  0.323881 -0.927167   \n",
       "3  2.287839 -1.557644 -1.175974  0.637988  0.255583  0.511670  0.613640   \n",
       "4 -0.410844  2.150333 -0.157288 -1.413970  1.071768 -0.470122  0.754880   \n",
       "\n",
       "    var_161   var_162   var_163   var_164   var_165   var_166   var_167  \\\n",
       "0 -1.458345 -1.188871 -1.118597  1.820037 -1.292122 -0.672492  0.415463   \n",
       "1  1.031509 -1.034575  0.901728 -0.350887 -0.788782  0.566720  1.715981   \n",
       "2  0.915526 -0.419830 -0.964374  0.245508  0.225399  0.768412 -0.185962   \n",
       "3  1.527798  1.280436 -1.402649 -1.459517 -0.376527 -0.983600  0.167928   \n",
       "4  0.254470  0.069214  1.340414  1.803853  0.356000  0.643057  0.280899   \n",
       "\n",
       "    var_168   var_169   var_170   var_171   var_172   var_173   var_174  \\\n",
       "0  1.427898 -0.377262 -1.015148 -1.391223  0.127603  0.598396 -0.212151   \n",
       "1 -0.280807  0.415510  1.254368  1.066374  0.253110  1.002574  1.399291   \n",
       "2  0.771519  0.340265 -1.632533 -0.530541 -1.421305 -0.800601  0.673423   \n",
       "3 -1.475845 -0.750309 -1.651426 -0.948260 -0.165418  2.301551 -0.722535   \n",
       "4 -0.529132 -0.209787  0.303584 -0.318079  0.009688  0.473040 -0.224194   \n",
       "\n",
       "    var_175   var_176   var_177   var_178   var_179   var_180   var_181  \\\n",
       "0 -1.345542 -0.884819  0.608339 -0.264803 -0.276711 -1.007248 -0.724973   \n",
       "1 -0.328477 -0.542583  1.846540 -0.667170  2.926470 -0.483336 -0.375052   \n",
       "2 -0.205923  0.269172 -1.128605 -0.942777 -0.038613 -0.268657 -0.441243   \n",
       "3 -1.177467  2.007137  0.684408 -1.032636  1.125581 -0.181224 -1.216340   \n",
       "4  1.594588 -0.096306  0.541834  0.276986  0.946520 -0.861178 -0.178980   \n",
       "\n",
       "    var_182   var_183   var_184   var_185   var_186   var_187   var_188  \\\n",
       "0  0.224407  1.167918  1.344013  2.243360  0.866955 -0.718892  0.508070   \n",
       "1 -1.779155  0.155339  0.989980 -0.176510 -0.384679 -0.421209 -0.457714   \n",
       "2 -1.557885 -1.249777  1.046953 -0.264548 -1.088363  0.292334 -1.325225   \n",
       "3  0.114589 -0.269345 -1.454077  0.794270 -0.297652 -0.187792 -0.696291   \n",
       "4  0.406928  0.692215 -0.064579 -0.783722 -0.763558  1.408359  0.178520   \n",
       "\n",
       "    var_189   var_190   var_191   var_192   var_193   var_194   var_195  \\\n",
       "0 -0.139913  0.255350 -1.133055  0.796166 -0.407116  0.153824 -1.630450   \n",
       "1 -1.054505  0.935328  0.104340  0.438242  2.040567 -0.739803  1.468769   \n",
       "2 -1.650219 -0.060696  0.763213 -0.154204 -0.408408  1.064675  2.188146   \n",
       "3  1.060400  0.262148 -0.827281 -0.789504 -0.471215  1.642264 -0.750986   \n",
       "4 -0.976163 -0.992715  0.675456 -1.392943  1.493446 -1.511752 -0.929213   \n",
       "\n",
       "    var_196   var_197   var_198   var_199  var_0_pnc  var_1_pnc  var_2_pnc  \\\n",
       "0  0.953034 -0.328012 -1.022937  0.134878      99667     100023      99823   \n",
       "1  1.003240 -0.097438  0.773022  0.391499     100333     100023      99823   \n",
       "2 -1.635374 -0.657751 -0.374011  0.255679      99667     100023      99823   \n",
       "3 -0.835896  1.492051  0.643598 -0.488734     100333     100023     100177   \n",
       "4  0.227504  0.581454  0.653283 -0.473020      99667      99977      99823   \n",
       "\n",
       "   var_3_pnc  var_4_pnc  var_5_pnc  var_6_pnc  var_7_pnc  var_8_pnc  \\\n",
       "0      99886      99821      99493      99950     100049      99730   \n",
       "1      99886      99821     100507     100050     100049     100270   \n",
       "2     100114     100179      99493     100050      99951      99730   \n",
       "3     100114      99821     100507     100050      99951      99730   \n",
       "4      99886      99821     100507     100050     100049     100270   \n",
       "\n",
       "   var_9_pnc  var_10_pnc  var_11_pnc  var_12_pnc  var_13_pnc  var_14_pnc  \\\n",
       "0      99840      100039       99921       99723       99952       99893   \n",
       "1     100160       99961       99921       99723       99952      100107   \n",
       "2      99840       99961      100079      100277       99952       99893   \n",
       "3     100160      100039       99921       99723      100048      100107   \n",
       "4     100160       99961      100079       99723       99952       99893   \n",
       "\n",
       "   var_15_pnc  var_16_pnc  var_17_pnc  var_18_pnc  var_19_pnc  var_20_pnc  \\\n",
       "0       99889      100032      100253      100356       99766      100079   \n",
       "1       99889       99968      100253      100356       99766      100079   \n",
       "2      100111      100032       99747      100356       99766       99921   \n",
       "3      100111       99968       99747       99644       99766       99921   \n",
       "4      100111      100032       99747      100356      100234       99921   \n",
       "\n",
       "   var_21_pnc  var_22_pnc  var_23_pnc  var_24_pnc  var_25_pnc  var_26_pnc  \\\n",
       "0      100002      100081       99983       99820       99792      100276   \n",
       "1      100002       99919      100017      100180      100208      100276   \n",
       "2       99998      100081      100017      100180       99792      100276   \n",
       "3      100002       99919       99983       99820      100208       99724   \n",
       "4       99998       99919       99983       99820       99792       99724   \n",
       "\n",
       "   var_27_pnc  var_28_pnc  var_29_pnc  var_30_pnc  var_31_pnc  var_32_pnc  \\\n",
       "0       99634      100206      100170       99880      100018       99783   \n",
       "1      100366      100206       99830      100120       99982      100217   \n",
       "2      100366       99794      100170      100120      100018       99783   \n",
       "3       99634       99794       99830      100120      100018       99783   \n",
       "4      100366       99794      100170       99880       99982      100217   \n",
       "\n",
       "   var_33_pnc  var_34_pnc  var_35_pnc  var_36_pnc  var_37_pnc  var_38_pnc  \\\n",
       "0      100345       99961      100271      100151      100239       99773   \n",
       "1      100345       99961       99729       99849       99761      100227   \n",
       "2      100345       99961      100271       99849      100239      100227   \n",
       "3       99655       99961       99729       99849       99761       99773   \n",
       "4      100345      100039       99729       99849       99761      100227   \n",
       "\n",
       "   var_39_pnc  var_40_pnc  var_41_pnc  var_42_pnc  var_43_pnc  var_44_pnc  \\\n",
       "0      100244       99906      100145      100285       99984       99727   \n",
       "1      100244      100094      100145       99715       99984      100273   \n",
       "2      100244      100094      100145      100285       99984      100273   \n",
       "3      100244       99906      100145      100285      100016      100273   \n",
       "4      100244       99906       99855      100285      100016      100273   \n",
       "\n",
       "   var_45_pnc  var_46_pnc  var_47_pnc  var_48_pnc  var_49_pnc  var_50_pnc  \\\n",
       "0       99473      100010       99712      100042       99900      100114   \n",
       "1      100527      100010      100288      100042      100100       99886   \n",
       "2       99473      100010      100288      100042       99900      100114   \n",
       "3      100527       99990       99712       99958       99900       99886   \n",
       "4      100527       99990      100288       99958       99900      100114   \n",
       "\n",
       "   var_51_pnc  var_52_pnc  var_53_pnc  var_54_pnc  var_55_pnc  var_56_pnc  \\\n",
       "0      100048       99498      100110       99477       99810       99764   \n",
       "1       99952      100502       99890       99477       99810      100236   \n",
       "2      100048       99498       99890      100523      100190       99764   \n",
       "3       99952       99498      100110      100523      100190       99764   \n",
       "4       99952       99498      100110       99477       99810      100236   \n",
       "\n",
       "   var_57_pnc  var_58_pnc  var_59_pnc  var_60_pnc  var_61_pnc  var_62_pnc  \\\n",
       "0       99641       99854       99937       99919      100049      100252   \n",
       "1      100359       99854      100063      100081      100049      100252   \n",
       "2       99641       99854       99937       99919      100049      100252   \n",
       "3      100359      100146       99937       99919       99951      100252   \n",
       "4       99641       99854       99937      100081       99951       99748   \n",
       "\n",
       "   var_63_pnc  var_64_pnc  var_65_pnc  var_66_pnc  var_67_pnc  var_68_pnc  \\\n",
       "0       99841       99671      100087       99671      100137       99560   \n",
       "1       99841       99671      100087      100329       99863      100440   \n",
       "2      100159      100329       99913       99671       99863      100440   \n",
       "3       99841       99671      100087      100329      100137      100440   \n",
       "4       99841      100329       99913       99671       99863      100440   \n",
       "\n",
       "   var_69_pnc  var_70_pnc  var_71_pnc  var_72_pnc  var_73_pnc  var_74_pnc  \\\n",
       "0       99872       99928       99607      100037      100034       99984   \n",
       "1      100128      100072       99607      100037       99966      100016   \n",
       "2       99872       99928      100393      100037      100034       99984   \n",
       "3       99872       99928       99607      100037       99966      100016   \n",
       "4      100128      100072      100393       99963      100034      100016   \n",
       "\n",
       "   var_75_pnc  var_76_pnc  var_77_pnc  var_78_pnc  var_79_pnc  var_80_pnc  \\\n",
       "0       99589       99947      100077      100208       99811       99998   \n",
       "1      100411       99947      100077      100208       99811      100002   \n",
       "2       99589       99947      100077      100208       99811       99998   \n",
       "3      100411      100053      100077       99792      100189      100002   \n",
       "4       99589       99947      100077      100208      100189      100002   \n",
       "\n",
       "   var_81_pnc  var_82_pnc  var_83_pnc  var_84_pnc  var_85_pnc  var_86_pnc  \\\n",
       "0       99913      100027       99630      100183       99982       99824   \n",
       "1      100087      100027      100370      100183      100018      100176   \n",
       "2      100087      100027       99630       99817       99982      100176   \n",
       "3       99913      100027      100370       99817      100018      100176   \n",
       "4       99913       99973      100370       99817      100018       99824   \n",
       "\n",
       "   var_87_pnc  var_88_pnc  var_89_pnc  var_90_pnc  var_91_pnc  var_92_pnc  \\\n",
       "0      100048      100148       99772      100398      100015       99887   \n",
       "1       99952      100148      100228       99602      100015       99887   \n",
       "2       99952       99852       99772      100398       99985      100113   \n",
       "3      100048      100148      100228       99602       99985      100113   \n",
       "4      100048      100148      100228      100398       99985       99887   \n",
       "\n",
       "   var_93_pnc  var_94_pnc  var_95_pnc  var_96_pnc  var_97_pnc  var_98_pnc  \\\n",
       "0       99901      100182      100178      100075       99780      100128   \n",
       "1      100099      100182       99822       99925      100220      100128   \n",
       "2       99901      100182      100178       99925       99780       99872   \n",
       "3      100099       99818       99822      100075       99780      100128   \n",
       "4       99901      100182      100178       99925      100220       99872   \n",
       "\n",
       "   var_99_pnc  var_100_pnc  var_101_pnc  var_102_pnc  var_103_pnc  \\\n",
       "0       99726       100337       100217       100107        99998   \n",
       "1      100274        99663       100217       100107       100002   \n",
       "2      100274       100337       100217        99893        99998   \n",
       "3      100274        99663        99783        99893        99998   \n",
       "4      100274        99663       100217        99893       100002   \n",
       "\n",
       "   var_104_pnc  var_105_pnc  var_106_pnc  var_107_pnc  var_108_pnc  \\\n",
       "0       100127        99709        99886       100085        99829   \n",
       "1       100127       100291       100114        99915        99829   \n",
       "2       100127       100291       100114       100085        99829   \n",
       "3        99873        99709        99886       100085       100171   \n",
       "4        99873        99709        99886       100085        99829   \n",
       "\n",
       "   var_109_pnc  var_110_pnc  var_111_pnc  var_112_pnc  var_113_pnc  \\\n",
       "0        99826        99711       100256        99986       100204   \n",
       "1       100174       100289       100256        99986        99796   \n",
       "2        99826        99711        99744        99986       100204   \n",
       "3       100174        99711       100256       100014        99796   \n",
       "4        99826        99711        99744       100014       100204   \n",
       "\n",
       "   var_114_pnc  var_115_pnc  var_116_pnc  var_117_pnc  var_118_pnc  \\\n",
       "0        99520       100041        99828       100126       100198   \n",
       "1       100480        99959        99828       100126       100198   \n",
       "2       100480        99959       100172        99874       100198   \n",
       "3        99520       100041        99828        99874        99802   \n",
       "4        99520       100041       100172        99874        99802   \n",
       "\n",
       "   var_119_pnc  var_120_pnc  var_121_pnc  var_122_pnc  var_123_pnc  \\\n",
       "0        99789        99695       100016        99773        99879   \n",
       "1        99789       100305       100016       100227       100121   \n",
       "2       100211        99695       100016       100227        99879   \n",
       "3        99789       100305        99984       100227        99879   \n",
       "4        99789       100305        99984       100227       100121   \n",
       "\n",
       "   var_124_pnc  var_125_pnc  var_126_pnc  var_127_pnc  var_128_pnc  \\\n",
       "0        99559       100432       100046        99855        99678   \n",
       "1        99559        99568       100046       100145       100322   \n",
       "2        99559       100432       100046        99855       100322   \n",
       "3        99559       100432        99954        99855       100322   \n",
       "4        99559       100432        99954       100145       100322   \n",
       "\n",
       "   var_129_pnc  var_130_pnc  var_131_pnc  var_132_pnc  var_133_pnc  \\\n",
       "0        99987       100299        99394        99778       100192   \n",
       "1        99987       100299        99394       100222        99808   \n",
       "2        99987        99701        99394        99778       100192   \n",
       "3       100013       100299       100606        99778        99808   \n",
       "4       100013        99701       100606       100222       100192   \n",
       "\n",
       "   var_134_pnc  var_135_pnc  var_136_pnc  var_137_pnc  var_138_pnc  \\\n",
       "0       100130       100069        99499        99792       100006   \n",
       "1       100130        99931       100501        99792        99994   \n",
       "2       100130        99931        99499       100208       100006   \n",
       "3       100130       100069        99499        99792        99994   \n",
       "4        99870       100069       100501        99792       100006   \n",
       "\n",
       "   var_139_pnc  var_140_pnc  var_141_pnc  var_142_pnc  var_143_pnc  \\\n",
       "0        99910        99822        99591       100656        99791   \n",
       "1        99910        99822       100409        99344        99791   \n",
       "2        99910        99822        99591        99344        99791   \n",
       "3        99910        99822       100409        99344        99791   \n",
       "4        99910        99822        99591        99344       100209   \n",
       "\n",
       "   var_144_pnc  var_145_pnc  var_146_pnc  var_147_pnc  var_148_pnc  \\\n",
       "0        99519        99998       100356        99987       100471   \n",
       "1       100481        99998        99644       100013       100471   \n",
       "2        99519       100002       100356       100013       100471   \n",
       "3        99519       100002       100356       100013        99529   \n",
       "4       100481        99998       100356        99987        99529   \n",
       "\n",
       "   var_149_pnc  var_150_pnc  var_151_pnc  var_152_pnc  var_153_pnc  \\\n",
       "0        99849       100454       100130        99816        99949   \n",
       "1       100151       100454       100130        99816        99949   \n",
       "2       100151        99546        99870       100184       100051   \n",
       "3       100151       100454       100130       100184        99949   \n",
       "4        99849       100454        99870       100184        99949   \n",
       "\n",
       "   var_154_pnc  var_155_pnc  var_156_pnc  var_157_pnc  var_158_pnc  \\\n",
       "0       100128       100156        99664        99898       100232   \n",
       "1        99872       100156        99664        99898       100232   \n",
       "2        99872       100156       100336       100102       100232   \n",
       "3       100128        99844        99664       100102       100232   \n",
       "4        99872       100156        99664        99898       100232   \n",
       "\n",
       "   var_159_pnc  var_160_pnc  var_161_pnc  var_162_pnc  var_163_pnc  \\\n",
       "0       100257       100061       100169       100035       100126   \n",
       "1       100257        99939        99831       100035        99874   \n",
       "2       100257       100061        99831       100035       100126   \n",
       "3       100257        99939        99831        99965       100126   \n",
       "4        99743        99939        99831        99965        99874   \n",
       "\n",
       "   var_164_pnc  var_165_pnc  var_166_pnc  var_167_pnc  var_168_pnc  \\\n",
       "0       100031        99805       100152       100344        99832   \n",
       "1        99969        99805        99848       100344       100168   \n",
       "2       100031       100195        99848        99656        99832   \n",
       "3        99969        99805       100152       100344       100168   \n",
       "4       100031       100195        99848       100344       100168   \n",
       "\n",
       "   var_169_pnc  var_170_pnc  var_171_pnc  var_172_pnc  var_173_pnc  \\\n",
       "0        99543       100005       100138       100300       100240   \n",
       "1       100457        99995        99862       100300       100240   \n",
       "2       100457       100005       100138        99700        99760   \n",
       "3        99543       100005       100138        99700       100240   \n",
       "4        99543        99995       100138       100300       100240   \n",
       "\n",
       "   var_174_pnc  var_175_pnc  var_176_pnc  var_177_pnc  var_178_pnc  \\\n",
       "0       100302        99805       100067       100023       100019   \n",
       "1        99698        99805       100067       100023       100019   \n",
       "2        99698        99805        99933        99977       100019   \n",
       "3       100302        99805        99933       100023       100019   \n",
       "4       100302       100195       100067       100023        99981   \n",
       "\n",
       "   var_179_pnc  var_180_pnc  var_181_pnc  var_182_pnc  var_183_pnc  \\\n",
       "0        99970       100331        99980       100030        99709   \n",
       "1       100030       100331        99980        99970        99709   \n",
       "2        99970       100331        99980        99970       100291   \n",
       "3       100030       100331        99980       100030       100291   \n",
       "4       100030       100331        99980       100030        99709   \n",
       "\n",
       "   var_184_pnc  var_185_pnc  var_186_pnc  var_187_pnc  var_188_pnc  \\\n",
       "0        99829       100415       100028        99407       100385   \n",
       "1        99829        99585        99972        99407        99615   \n",
       "2        99829        99585        99972       100593        99615   \n",
       "3       100171       100415        99972        99407        99615   \n",
       "4       100171        99585        99972       100593       100385   \n",
       "\n",
       "   var_189_pnc  var_190_pnc  var_191_pnc  var_192_pnc  var_193_pnc  \\\n",
       "0        99847       100013       100018        99861        99885   \n",
       "1        99847       100013        99982        99861       100115   \n",
       "2        99847        99987        99982       100139        99885   \n",
       "3       100153       100013       100018       100139        99885   \n",
       "4        99847        99987        99982       100139       100115   \n",
       "\n",
       "   var_194_pnc  var_195_pnc  var_196_pnc  var_197_pnc  var_198_pnc  \\\n",
       "0       100252       100007       100320       100051       100093   \n",
       "1        99748        99993       100320       100051        99907   \n",
       "2       100252        99993        99680       100051       100093   \n",
       "3       100252       100007        99680        99949        99907   \n",
       "4        99748       100007       100320        99949        99907   \n",
       "\n",
       "   var_199_pnc  \n",
       "0        99989  \n",
       "1        99989  \n",
       "2        99989  \n",
       "3       100011  \n",
       "4       100011  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fold = 10\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Wed Mar  6 13:34:37 2019\n",
      "0:\ttest: 0.5340388\tbest: 0.5340388 (0)\ttotal: 118ms\tremaining: 1d 8h 39m 21s\n",
      "1000:\ttest: 0.8524085\tbest: 0.8524085 (1000)\ttotal: 28.4s\tremaining: 7h 52m 4s\n",
      "2000:\ttest: 0.8711751\tbest: 0.8711751 (2000)\ttotal: 59.7s\tremaining: 8h 16m 12s\n",
      "3000:\ttest: 0.8817056\tbest: 0.8817056 (3000)\ttotal: 1m 28s\tremaining: 8h 10m 11s\n",
      "4000:\ttest: 0.8877080\tbest: 0.8877080 (4000)\ttotal: 1m 57s\tremaining: 8h 7m 58s\n",
      "5000:\ttest: 0.8914568\tbest: 0.8914568 (5000)\ttotal: 2m 27s\tremaining: 8h 7m 36s\n",
      "6000:\ttest: 0.8938049\tbest: 0.8938078 (5999)\ttotal: 2m 56s\tremaining: 8h 7m 5s\n",
      "7000:\ttest: 0.8955610\tbest: 0.8955674 (6997)\ttotal: 3m 24s\tremaining: 8h 4m 12s\n",
      "8000:\ttest: 0.8970216\tbest: 0.8970245 (7999)\ttotal: 3m 52s\tremaining: 8h 1m 17s\n",
      "9000:\ttest: 0.8979392\tbest: 0.8979392 (9000)\ttotal: 4m 20s\tremaining: 7h 58m 52s\n",
      "10000:\ttest: 0.8986429\tbest: 0.8986530 (9992)\ttotal: 4m 49s\tremaining: 7h 57m 4s\n",
      "11000:\ttest: 0.8991063\tbest: 0.8991148 (10983)\ttotal: 5m 18s\tremaining: 7h 56m 48s\n",
      "12000:\ttest: 0.8994986\tbest: 0.8995007 (11998)\ttotal: 5m 48s\tremaining: 7h 57m 46s\n",
      "13000:\ttest: 0.8997856\tbest: 0.8997940 (12968)\ttotal: 6m 20s\tremaining: 8h 1m 36s\n",
      "14000:\ttest: 0.9000284\tbest: 0.9000349 (13962)\ttotal: 6m 53s\tremaining: 8h 5m 33s\n",
      "15000:\ttest: 0.9001355\tbest: 0.9001361 (14997)\ttotal: 7m 27s\tremaining: 8h 9m 32s\n",
      "16000:\ttest: 0.9003187\tbest: 0.9003219 (15997)\ttotal: 7m 59s\tremaining: 8h 11m 29s\n",
      "17000:\ttest: 0.9004541\tbest: 0.9004544 (16997)\ttotal: 8m 31s\tremaining: 8h 12m 56s\n",
      "18000:\ttest: 0.9006065\tbest: 0.9006139 (17986)\ttotal: 9m 3s\tremaining: 8h 13m 56s\n",
      "19000:\ttest: 0.9007352\tbest: 0.9007666 (18605)\ttotal: 9m 35s\tremaining: 8h 15m 4s\n",
      "20000:\ttest: 0.9008127\tbest: 0.9008268 (19928)\ttotal: 10m 8s\tremaining: 8h 17m 12s\n",
      "21000:\ttest: 0.9009400\tbest: 0.9009518 (20960)\ttotal: 10m 41s\tremaining: 8h 18m 40s\n",
      "22000:\ttest: 0.9009665\tbest: 0.9009829 (21777)\ttotal: 11m 14s\tremaining: 8h 20m 2s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.900982885\n",
      "bestIteration = 21777\n",
      "\n",
      "Shrink model to first 21778 iterations.\n",
      "Fold 1 started at Wed Mar  6 13:47:15 2019\n",
      "0:\ttest: 0.5134034\tbest: 0.5134034 (0)\ttotal: 54.7ms\tremaining: 15h 12m 27s\n",
      "1000:\ttest: 0.8476892\tbest: 0.8476892 (1000)\ttotal: 30.1s\tremaining: 8h 20m 59s\n",
      "2000:\ttest: 0.8672619\tbest: 0.8672619 (2000)\ttotal: 1m 2s\tremaining: 8h 43m 19s\n",
      "3000:\ttest: 0.8777044\tbest: 0.8777044 (3000)\ttotal: 1m 35s\tremaining: 8h 49m 47s\n",
      "4000:\ttest: 0.8838713\tbest: 0.8838713 (4000)\ttotal: 2m 9s\tremaining: 8h 55m 21s\n",
      "5000:\ttest: 0.8882254\tbest: 0.8882254 (5000)\ttotal: 2m 42s\tremaining: 8h 58m 58s\n",
      "6000:\ttest: 0.8908758\tbest: 0.8908758 (6000)\ttotal: 3m 16s\tremaining: 9h 2m 26s\n",
      "7000:\ttest: 0.8928295\tbest: 0.8928306 (6986)\ttotal: 3m 50s\tremaining: 9h 5m 2s\n",
      "8000:\ttest: 0.8943179\tbest: 0.8943242 (7996)\ttotal: 4m 23s\tremaining: 9h 4m 43s\n",
      "9000:\ttest: 0.8953666\tbest: 0.8953850 (8984)\ttotal: 4m 57s\tremaining: 9h 6m 36s\n",
      "10000:\ttest: 0.8960260\tbest: 0.8960297 (9996)\ttotal: 5m 31s\tremaining: 9h 7m 21s\n",
      "11000:\ttest: 0.8966256\tbest: 0.8966313 (10993)\ttotal: 6m 6s\tremaining: 9h 9m\n",
      "12000:\ttest: 0.8970857\tbest: 0.8970949 (11976)\ttotal: 6m 40s\tremaining: 9h 9m 36s\n",
      "13000:\ttest: 0.8973700\tbest: 0.8973700 (13000)\ttotal: 7m 15s\tremaining: 9h 10m 45s\n",
      "14000:\ttest: 0.8975781\tbest: 0.8975789 (13997)\ttotal: 7m 49s\tremaining: 9h 10m 36s\n",
      "15000:\ttest: 0.8977141\tbest: 0.8977141 (15000)\ttotal: 8m 23s\tremaining: 9h 10m 57s\n",
      "16000:\ttest: 0.8977797\tbest: 0.8977876 (15935)\ttotal: 8m 57s\tremaining: 9h 11m\n",
      "17000:\ttest: 0.8978792\tbest: 0.8978923 (16807)\ttotal: 9m 32s\tremaining: 9h 11m 25s\n",
      "18000:\ttest: 0.8979439\tbest: 0.8979467 (17990)\ttotal: 10m 6s\tremaining: 9h 11m 21s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8979466793\n",
      "bestIteration = 17990\n",
      "\n",
      "Shrink model to first 17991 iterations.\n",
      "Fold 2 started at Wed Mar  6 13:58:45 2019\n",
      "0:\ttest: 0.5528762\tbest: 0.5528762 (0)\ttotal: 47.2ms\tremaining: 13h 6m 13s\n",
      "1000:\ttest: 0.8552459\tbest: 0.8554237 (989)\ttotal: 32.2s\tremaining: 8h 55m\n",
      "2000:\ttest: 0.8726752\tbest: 0.8726858 (1999)\ttotal: 1m 5s\tremaining: 9h 7m 34s\n",
      "3000:\ttest: 0.8826412\tbest: 0.8826412 (3000)\ttotal: 1m 39s\tremaining: 9h 9m 13s\n",
      "4000:\ttest: 0.8881167\tbest: 0.8881167 (4000)\ttotal: 2m 14s\tremaining: 9h 19m 58s\n",
      "5000:\ttest: 0.8917775\tbest: 0.8917775 (5000)\ttotal: 2m 50s\tremaining: 9h 23m 45s\n",
      "6000:\ttest: 0.8940206\tbest: 0.8940234 (5999)\ttotal: 3m 25s\tremaining: 9h 27m 39s\n",
      "7000:\ttest: 0.8955822\tbest: 0.8955822 (7000)\ttotal: 4m\tremaining: 9h 28m 53s\n",
      "8000:\ttest: 0.8967432\tbest: 0.8967463 (7995)\ttotal: 4m 35s\tremaining: 9h 28m 54s\n",
      "9000:\ttest: 0.8974663\tbest: 0.8974663 (8999)\ttotal: 5m 9s\tremaining: 9h 27m 5s\n",
      "10000:\ttest: 0.8980486\tbest: 0.8980508 (9997)\ttotal: 5m 42s\tremaining: 9h 24m 49s\n",
      "11000:\ttest: 0.8984227\tbest: 0.8984235 (10999)\ttotal: 6m 16s\tremaining: 9h 23m 39s\n",
      "12000:\ttest: 0.8988015\tbest: 0.8988015 (12000)\ttotal: 6m 49s\tremaining: 9h 21m 25s\n",
      "13000:\ttest: 0.8989944\tbest: 0.8990068 (12969)\ttotal: 7m 22s\tremaining: 9h 20m 18s\n",
      "14000:\ttest: 0.8991500\tbest: 0.8991675 (13907)\ttotal: 7m 56s\tremaining: 9h 19m 50s\n",
      "15000:\ttest: 0.8993238\tbest: 0.8993286 (14997)\ttotal: 8m 32s\tremaining: 9h 21m\n",
      "16000:\ttest: 0.8993661\tbest: 0.8993804 (15894)\ttotal: 9m 7s\tremaining: 9h 21m 2s\n",
      "17000:\ttest: 0.8994230\tbest: 0.8994237 (16998)\ttotal: 9m 42s\tremaining: 9h 21m 9s\n",
      "18000:\ttest: 0.8995106\tbest: 0.8995161 (17815)\ttotal: 10m 16s\tremaining: 9h 20m 57s\n",
      "19000:\ttest: 0.8995376\tbest: 0.8995594 (18576)\ttotal: 10m 51s\tremaining: 9h 20m 22s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8995593738\n",
      "bestIteration = 18576\n",
      "\n",
      "Shrink model to first 18577 iterations.\n",
      "Fold 3 started at Wed Mar  6 14:10:47 2019\n",
      "0:\ttest: 0.5362426\tbest: 0.5362426 (0)\ttotal: 60.5ms\tremaining: 16h 48m 25s\n",
      "1000:\ttest: 0.8538747\tbest: 0.8538747 (1000)\ttotal: 32.5s\tremaining: 9h 22s\n",
      "2000:\ttest: 0.8729433\tbest: 0.8729499 (1998)\ttotal: 1m 6s\tremaining: 9h 10m 5s\n",
      "3000:\ttest: 0.8831729\tbest: 0.8831729 (3000)\ttotal: 1m 39s\tremaining: 9h 11m 26s\n",
      "4000:\ttest: 0.8888958\tbest: 0.8888958 (4000)\ttotal: 2m 14s\tremaining: 9h 16m 9s\n",
      "5000:\ttest: 0.8928442\tbest: 0.8928583 (4994)\ttotal: 2m 48s\tremaining: 9h 20m 1s\n",
      "6000:\ttest: 0.8955201\tbest: 0.8955218 (5999)\ttotal: 3m 22s\tremaining: 9h 20m 5s\n",
      "7000:\ttest: 0.8975444\tbest: 0.8975444 (7000)\ttotal: 3m 57s\tremaining: 9h 20m 49s\n",
      "8000:\ttest: 0.8988991\tbest: 0.8988991 (8000)\ttotal: 4m 30s\tremaining: 9h 18m 33s\n",
      "9000:\ttest: 0.8997642\tbest: 0.8997642 (9000)\ttotal: 5m 4s\tremaining: 9h 18m 38s\n",
      "10000:\ttest: 0.9004208\tbest: 0.9004217 (9988)\ttotal: 5m 38s\tremaining: 9h 19m 14s\n",
      "11000:\ttest: 0.9007881\tbest: 0.9007898 (10988)\ttotal: 6m 13s\tremaining: 9h 19m 50s\n",
      "12000:\ttest: 0.9012038\tbest: 0.9012064 (11979)\ttotal: 6m 48s\tremaining: 9h 20m 57s\n",
      "13000:\ttest: 0.9015453\tbest: 0.9015453 (13000)\ttotal: 7m 23s\tremaining: 9h 20m 46s\n",
      "14000:\ttest: 0.9017964\tbest: 0.9018056 (13967)\ttotal: 7m 58s\tremaining: 9h 21m 17s\n",
      "15000:\ttest: 0.9019039\tbest: 0.9019117 (14992)\ttotal: 8m 33s\tremaining: 9h 21m 34s\n",
      "16000:\ttest: 0.9020530\tbest: 0.9020588 (15915)\ttotal: 9m 7s\tremaining: 9h 21m 21s\n",
      "17000:\ttest: 0.9020900\tbest: 0.9021226 (16823)\ttotal: 9m 44s\tremaining: 9h 23m\n",
      "18000:\ttest: 0.9021902\tbest: 0.9021902 (18000)\ttotal: 10m 19s\tremaining: 9h 22m 59s\n",
      "19000:\ttest: 0.9022642\tbest: 0.9022768 (18979)\ttotal: 10m 55s\tremaining: 9h 23m 56s\n",
      "20000:\ttest: 0.9022961\tbest: 0.9023054 (19634)\ttotal: 11m 32s\tremaining: 9h 25m 27s\n",
      "21000:\ttest: 0.9023131\tbest: 0.9023511 (20635)\ttotal: 12m 6s\tremaining: 9h 24m 42s\n",
      "22000:\ttest: 0.9023271\tbest: 0.9023701 (21577)\ttotal: 12m 42s\tremaining: 9h 25m 11s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.902370084\n",
      "bestIteration = 21577\n",
      "\n",
      "Shrink model to first 21578 iterations.\n",
      "Fold 4 started at Wed Mar  6 14:24:49 2019\n",
      "0:\ttest: 0.5508904\tbest: 0.5508904 (0)\ttotal: 42.3ms\tremaining: 11h 44m 24s\n",
      "1000:\ttest: 0.8555755\tbest: 0.8555755 (1000)\ttotal: 32.4s\tremaining: 8h 59m 21s\n",
      "2000:\ttest: 0.8749490\tbest: 0.8749490 (2000)\ttotal: 1m 7s\tremaining: 9h 19m 25s\n",
      "3000:\ttest: 0.8871827\tbest: 0.8871827 (3000)\ttotal: 1m 42s\tremaining: 9h 29m 5s\n",
      "4000:\ttest: 0.8941588\tbest: 0.8941588 (4000)\ttotal: 2m 18s\tremaining: 9h 34m 27s\n",
      "5000:\ttest: 0.8983858\tbest: 0.8983985 (4998)\ttotal: 2m 54s\tremaining: 9h 38m 36s\n",
      "6000:\ttest: 0.9014377\tbest: 0.9014387 (5995)\ttotal: 3m 29s\tremaining: 9h 38m 38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000:\ttest: 0.9034742\tbest: 0.9034765 (6997)\ttotal: 4m 5s\tremaining: 9h 40m 48s\n",
      "8000:\ttest: 0.9049099\tbest: 0.9049099 (8000)\ttotal: 4m 41s\tremaining: 9h 41m 3s\n",
      "9000:\ttest: 0.9058854\tbest: 0.9058901 (8994)\ttotal: 5m 16s\tremaining: 9h 39m 55s\n",
      "10000:\ttest: 0.9067599\tbest: 0.9067599 (10000)\ttotal: 5m 51s\tremaining: 9h 39m 50s\n",
      "11000:\ttest: 0.9073394\tbest: 0.9073394 (11000)\ttotal: 6m 25s\tremaining: 9h 37m 50s\n",
      "12000:\ttest: 0.9077551\tbest: 0.9077642 (11969)\ttotal: 7m\tremaining: 9h 36m 52s\n",
      "13000:\ttest: 0.9081031\tbest: 0.9081074 (12997)\ttotal: 7m 35s\tremaining: 9h 36m 30s\n",
      "14000:\ttest: 0.9083382\tbest: 0.9083397 (13990)\ttotal: 8m 10s\tremaining: 9h 36m 11s\n",
      "15000:\ttest: 0.9085270\tbest: 0.9085299 (14962)\ttotal: 8m 46s\tremaining: 9h 35m 58s\n",
      "16000:\ttest: 0.9087176\tbest: 0.9087178 (15991)\ttotal: 9m 21s\tremaining: 9h 35m 33s\n",
      "17000:\ttest: 0.9088208\tbest: 0.9088378 (16901)\ttotal: 9m 56s\tremaining: 9h 35m 10s\n",
      "18000:\ttest: 0.9089504\tbest: 0.9089511 (17884)\ttotal: 10m 31s\tremaining: 9h 34m 30s\n",
      "19000:\ttest: 0.9090763\tbest: 0.9090880 (18977)\ttotal: 11m 7s\tremaining: 9h 34m 6s\n",
      "20000:\ttest: 0.9091374\tbest: 0.9091560 (19918)\ttotal: 11m 41s\tremaining: 9h 33m 7s\n",
      "21000:\ttest: 0.9091731\tbest: 0.9091733 (20996)\ttotal: 12m 17s\tremaining: 9h 32m 46s\n",
      "22000:\ttest: 0.9091929\tbest: 0.9092165 (21878)\ttotal: 12m 52s\tremaining: 9h 32m 6s\n",
      "23000:\ttest: 0.9091687\tbest: 0.9092249 (22124)\ttotal: 13m 25s\tremaining: 9h 30m 18s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9092249149\n",
      "bestIteration = 22124\n",
      "\n",
      "Shrink model to first 22125 iterations.\n",
      "Fold 5 started at Wed Mar  6 14:39:19 2019\n",
      "0:\ttest: 0.5567425\tbest: 0.5567425 (0)\ttotal: 43.3ms\tremaining: 12h 51s\n",
      "1000:\ttest: 0.8541136\tbest: 0.8541136 (1000)\ttotal: 32.3s\tremaining: 8h 58m 5s\n",
      "2000:\ttest: 0.8727720\tbest: 0.8727720 (2000)\ttotal: 1m 5s\tremaining: 9h 6m 32s\n",
      "3000:\ttest: 0.8833646\tbest: 0.8833646 (3000)\ttotal: 1m 39s\tremaining: 9h 12m 58s\n",
      "4000:\ttest: 0.8894740\tbest: 0.8894926 (3998)\ttotal: 2m 15s\tremaining: 9h 20m 19s\n",
      "5000:\ttest: 0.8935219\tbest: 0.8935219 (5000)\ttotal: 2m 50s\tremaining: 9h 25m 3s\n",
      "6000:\ttest: 0.8963821\tbest: 0.8963821 (6000)\ttotal: 3m 24s\tremaining: 9h 25m 22s\n",
      "7000:\ttest: 0.8982283\tbest: 0.8982327 (6997)\ttotal: 3m 59s\tremaining: 9h 26m 33s\n",
      "8000:\ttest: 0.8994428\tbest: 0.8994465 (7990)\ttotal: 4m 36s\tremaining: 9h 31m 40s\n",
      "9000:\ttest: 0.9003566\tbest: 0.9003605 (8999)\ttotal: 5m 12s\tremaining: 9h 34m 19s\n",
      "10000:\ttest: 0.9011236\tbest: 0.9011276 (9994)\ttotal: 5m 49s\tremaining: 9h 36m 37s\n",
      "11000:\ttest: 0.9015212\tbest: 0.9015219 (10991)\ttotal: 6m 24s\tremaining: 9h 36m 20s\n",
      "12000:\ttest: 0.9018293\tbest: 0.9018321 (11999)\ttotal: 7m 1s\tremaining: 9h 38m 22s\n",
      "13000:\ttest: 0.9021815\tbest: 0.9021833 (12999)\ttotal: 7m 36s\tremaining: 9h 38m\n",
      "14000:\ttest: 0.9024507\tbest: 0.9024568 (13885)\ttotal: 8m 12s\tremaining: 9h 38m 22s\n",
      "15000:\ttest: 0.9026397\tbest: 0.9026405 (14990)\ttotal: 8m 48s\tremaining: 9h 38m 1s\n",
      "16000:\ttest: 0.9027803\tbest: 0.9027803 (16000)\ttotal: 9m 23s\tremaining: 9h 37m 7s\n",
      "17000:\ttest: 0.9028466\tbest: 0.9028509 (16919)\ttotal: 9m 58s\tremaining: 9h 36m 49s\n",
      "18000:\ttest: 0.9029136\tbest: 0.9029181 (17983)\ttotal: 10m 34s\tremaining: 9h 36m 55s\n",
      "19000:\ttest: 0.9030210\tbest: 0.9030263 (18922)\ttotal: 11m 9s\tremaining: 9h 35m 48s\n",
      "20000:\ttest: 0.9031067\tbest: 0.9031187 (19969)\ttotal: 11m 44s\tremaining: 9h 35m 15s\n",
      "21000:\ttest: 0.9031459\tbest: 0.9031630 (20931)\ttotal: 12m 18s\tremaining: 9h 34m 4s\n",
      "22000:\ttest: 0.9031391\tbest: 0.9031665 (21118)\ttotal: 12m 55s\tremaining: 9h 34m 21s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9031664634\n",
      "bestIteration = 21118\n",
      "\n",
      "Shrink model to first 21119 iterations.\n",
      "Fold 6 started at Wed Mar  6 14:53:18 2019\n",
      "0:\ttest: 0.5495244\tbest: 0.5495244 (0)\ttotal: 54.1ms\tremaining: 15h 1m 16s\n",
      "1000:\ttest: 0.8510585\tbest: 0.8510647 (995)\ttotal: 32.3s\tremaining: 8h 57m 14s\n",
      "2000:\ttest: 0.8709028\tbest: 0.8709028 (2000)\ttotal: 1m 8s\tremaining: 9h 25m 46s\n",
      "3000:\ttest: 0.8822281\tbest: 0.8822281 (3000)\ttotal: 1m 43s\tremaining: 9h 35m 30s\n",
      "4000:\ttest: 0.8889168\tbest: 0.8889267 (3995)\ttotal: 2m 19s\tremaining: 9h 39m 52s\n",
      "5000:\ttest: 0.8934711\tbest: 0.8934731 (4998)\ttotal: 2m 55s\tremaining: 9h 41m 59s\n",
      "6000:\ttest: 0.8963160\tbest: 0.8963186 (5998)\ttotal: 3m 31s\tremaining: 9h 44m 31s\n",
      "7000:\ttest: 0.8982939\tbest: 0.8982939 (7000)\ttotal: 4m 9s\tremaining: 9h 48m 49s\n",
      "8000:\ttest: 0.8999029\tbest: 0.8999029 (8000)\ttotal: 4m 43s\tremaining: 9h 45m 35s\n",
      "9000:\ttest: 0.9009950\tbest: 0.9009952 (8999)\ttotal: 5m 19s\tremaining: 9h 45m 22s\n",
      "10000:\ttest: 0.9018168\tbest: 0.9018170 (9998)\ttotal: 5m 54s\tremaining: 9h 44m 24s\n",
      "11000:\ttest: 0.9024158\tbest: 0.9024168 (10999)\ttotal: 6m 29s\tremaining: 9h 44m 20s\n",
      "12000:\ttest: 0.9029119\tbest: 0.9029153 (11998)\ttotal: 7m 4s\tremaining: 9h 42m 57s\n",
      "13000:\ttest: 0.9032943\tbest: 0.9032943 (13000)\ttotal: 7m 39s\tremaining: 9h 41m 24s\n",
      "14000:\ttest: 0.9036355\tbest: 0.9036385 (13986)\ttotal: 8m 16s\tremaining: 9h 42m 13s\n",
      "15000:\ttest: 0.9037803\tbest: 0.9037880 (14986)\ttotal: 8m 52s\tremaining: 9h 42m 38s\n",
      "16000:\ttest: 0.9039983\tbest: 0.9039984 (15996)\ttotal: 9m 27s\tremaining: 9h 42m 5s\n",
      "17000:\ttest: 0.9041484\tbest: 0.9041523 (16972)\ttotal: 10m 3s\tremaining: 9h 41m 53s\n",
      "18000:\ttest: 0.9042812\tbest: 0.9042906 (17887)\ttotal: 10m 40s\tremaining: 9h 41m 57s\n",
      "19000:\ttest: 0.9044696\tbest: 0.9044733 (18965)\ttotal: 11m 16s\tremaining: 9h 42m 28s\n",
      "20000:\ttest: 0.9045584\tbest: 0.9045647 (19983)\ttotal: 11m 53s\tremaining: 9h 42m 36s\n",
      "21000:\ttest: 0.9045883\tbest: 0.9045939 (20932)\ttotal: 12m 31s\tremaining: 9h 43m 50s\n",
      "22000:\ttest: 0.9046777\tbest: 0.9046847 (21943)\ttotal: 13m 8s\tremaining: 9h 43m 59s\n",
      "23000:\ttest: 0.9047032\tbest: 0.9047231 (22214)\ttotal: 13m 44s\tremaining: 9h 43m 31s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9047230772\n",
      "bestIteration = 22214\n",
      "\n",
      "Shrink model to first 22215 iterations.\n",
      "Fold 7 started at Wed Mar  6 15:08:12 2019\n",
      "0:\ttest: 0.5452810\tbest: 0.5452810 (0)\ttotal: 65.4ms\tremaining: 18h 10m 29s\n",
      "1000:\ttest: 0.8458951\tbest: 0.8459904 (992)\ttotal: 34s\tremaining: 9h 25m 30s\n",
      "2000:\ttest: 0.8642625\tbest: 0.8642625 (2000)\ttotal: 1m 10s\tremaining: 9h 41m 56s\n",
      "3000:\ttest: 0.8752581\tbest: 0.8752581 (3000)\ttotal: 1m 44s\tremaining: 9h 39m 59s\n",
      "4000:\ttest: 0.8813287\tbest: 0.8813287 (4000)\ttotal: 2m 20s\tremaining: 9h 43m 45s\n",
      "5000:\ttest: 0.8850658\tbest: 0.8850661 (4998)\ttotal: 2m 54s\tremaining: 9h 40m 8s\n",
      "6000:\ttest: 0.8874803\tbest: 0.8874944 (5993)\ttotal: 3m 29s\tremaining: 9h 39m 3s\n",
      "7000:\ttest: 0.8892151\tbest: 0.8892151 (7000)\ttotal: 4m 6s\tremaining: 9h 43m 8s\n",
      "8000:\ttest: 0.8905621\tbest: 0.8905646 (7998)\ttotal: 4m 44s\tremaining: 9h 47m 51s\n",
      "9000:\ttest: 0.8915118\tbest: 0.8915118 (9000)\ttotal: 5m 20s\tremaining: 9h 47m 35s\n",
      "10000:\ttest: 0.8923160\tbest: 0.8923175 (9995)\ttotal: 5m 54s\tremaining: 9h 45m 40s\n",
      "11000:\ttest: 0.8929052\tbest: 0.8929078 (10997)\ttotal: 6m 29s\tremaining: 9h 43m 46s\n",
      "12000:\ttest: 0.8933131\tbest: 0.8933209 (11915)\ttotal: 7m 6s\tremaining: 9h 45m 38s\n",
      "13000:\ttest: 0.8938041\tbest: 0.8938041 (13000)\ttotal: 7m 41s\tremaining: 9h 44m 30s\n",
      "14000:\ttest: 0.8940704\tbest: 0.8940749 (13963)\ttotal: 8m 15s\tremaining: 9h 42m 7s\n",
      "15000:\ttest: 0.8943248\tbest: 0.8943248 (15000)\ttotal: 8m 50s\tremaining: 9h 40m 59s\n",
      "16000:\ttest: 0.8945241\tbest: 0.8945266 (15990)\ttotal: 9m 25s\tremaining: 9h 39m 11s\n",
      "17000:\ttest: 0.8947247\tbest: 0.8947247 (17000)\ttotal: 10m\tremaining: 9h 38m 43s\n",
      "18000:\ttest: 0.8948576\tbest: 0.8948625 (17992)\ttotal: 10m 35s\tremaining: 9h 37m 51s\n",
      "19000:\ttest: 0.8949940\tbest: 0.8949951 (18997)\ttotal: 11m 10s\tremaining: 9h 36m 41s\n",
      "20000:\ttest: 0.8951252\tbest: 0.8951308 (19925)\ttotal: 11m 44s\tremaining: 9h 35m 6s\n",
      "21000:\ttest: 0.8951002\tbest: 0.8951442 (20095)\ttotal: 12m 19s\tremaining: 9h 34m 16s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8951442067\n",
      "bestIteration = 20095\n",
      "\n",
      "Shrink model to first 20096 iterations.\n",
      "Fold 8 started at Wed Mar  6 15:21:33 2019\n",
      "0:\ttest: 0.5282931\tbest: 0.5282931 (0)\ttotal: 64.3ms\tremaining: 17h 51m 43s\n",
      "1000:\ttest: 0.8434781\tbest: 0.8434781 (1000)\ttotal: 35.1s\tremaining: 9h 43m 26s\n",
      "2000:\ttest: 0.8634539\tbest: 0.8634539 (2000)\ttotal: 1m 11s\tremaining: 9h 54m 12s\n",
      "3000:\ttest: 0.8745528\tbest: 0.8745528 (3000)\ttotal: 1m 46s\tremaining: 9h 52m 1s\n",
      "4000:\ttest: 0.8809203\tbest: 0.8809203 (4000)\ttotal: 2m 24s\tremaining: 10h 27s\n",
      "5000:\ttest: 0.8851789\tbest: 0.8851819 (4994)\ttotal: 3m 1s\tremaining: 10h 24s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000:\ttest: 0.8882843\tbest: 0.8882862 (5995)\ttotal: 3m 37s\tremaining: 10h 51s\n",
      "7000:\ttest: 0.8904369\tbest: 0.8904369 (7000)\ttotal: 4m 14s\tremaining: 10h 28s\n",
      "8000:\ttest: 0.8921197\tbest: 0.8921237 (7996)\ttotal: 4m 51s\tremaining: 10h 2m 41s\n",
      "9000:\ttest: 0.8932533\tbest: 0.8932533 (9000)\ttotal: 5m 29s\tremaining: 10h 4m 30s\n",
      "10000:\ttest: 0.8941915\tbest: 0.8941915 (10000)\ttotal: 6m 9s\tremaining: 10h 9m 30s\n",
      "11000:\ttest: 0.8947179\tbest: 0.8947179 (11000)\ttotal: 6m 47s\tremaining: 10h 10m 22s\n",
      "12000:\ttest: 0.8952858\tbest: 0.8952858 (12000)\ttotal: 7m 25s\tremaining: 10h 10m 46s\n",
      "13000:\ttest: 0.8957308\tbest: 0.8957319 (12999)\ttotal: 8m 2s\tremaining: 10h 9m 52s\n",
      "14000:\ttest: 0.8960639\tbest: 0.8960656 (13986)\ttotal: 8m 40s\tremaining: 10h 10m 30s\n",
      "15000:\ttest: 0.8963648\tbest: 0.8963709 (14997)\ttotal: 9m 20s\tremaining: 10h 13m 23s\n",
      "16000:\ttest: 0.8964858\tbest: 0.8964858 (16000)\ttotal: 9m 57s\tremaining: 10h 12m 43s\n",
      "17000:\ttest: 0.8966433\tbest: 0.8966465 (16793)\ttotal: 10m 34s\tremaining: 10h 10m 59s\n",
      "18000:\ttest: 0.8967516\tbest: 0.8967568 (17865)\ttotal: 11m 10s\tremaining: 10h 9m 18s\n",
      "19000:\ttest: 0.8968172\tbest: 0.8968258 (18905)\ttotal: 11m 46s\tremaining: 10h 7m 50s\n",
      "20000:\ttest: 0.8968892\tbest: 0.8968902 (19981)\ttotal: 12m 22s\tremaining: 10h 6m 25s\n",
      "21000:\ttest: 0.8969785\tbest: 0.8969835 (20874)\ttotal: 12m 59s\tremaining: 10h 5m 15s\n",
      "22000:\ttest: 0.8970784\tbest: 0.8970785 (21997)\ttotal: 13m 34s\tremaining: 10h 3m 30s\n",
      "23000:\ttest: 0.8971182\tbest: 0.8971285 (22946)\ttotal: 14m 10s\tremaining: 10h 2m 7s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8971285137\n",
      "bestIteration = 22946\n",
      "\n",
      "Shrink model to first 22947 iterations.\n",
      "Fold 9 started at Wed Mar  6 15:37:20 2019\n",
      "0:\ttest: 0.5346238\tbest: 0.5346238 (0)\ttotal: 67.4ms\tremaining: 18h 44m 9s\n",
      "1000:\ttest: 0.8512595\tbest: 0.8512595 (1000)\ttotal: 33.6s\tremaining: 9h 19m 33s\n",
      "2000:\ttest: 0.8680175\tbest: 0.8680175 (2000)\ttotal: 1m 8s\tremaining: 9h 27m 1s\n",
      "3000:\ttest: 0.8788252\tbest: 0.8788252 (3000)\ttotal: 1m 43s\tremaining: 9h 32m 59s\n",
      "4000:\ttest: 0.8846929\tbest: 0.8846929 (4000)\ttotal: 2m 18s\tremaining: 9h 33m 9s\n",
      "5000:\ttest: 0.8884344\tbest: 0.8884344 (5000)\ttotal: 2m 53s\tremaining: 9h 34m 50s\n",
      "6000:\ttest: 0.8911393\tbest: 0.8911393 (6000)\ttotal: 3m 29s\tremaining: 9h 37m 43s\n",
      "7000:\ttest: 0.8930628\tbest: 0.8930628 (7000)\ttotal: 4m 4s\tremaining: 9h 38m 27s\n",
      "8000:\ttest: 0.8944215\tbest: 0.8944215 (8000)\ttotal: 4m 40s\tremaining: 9h 38m 36s\n",
      "9000:\ttest: 0.8953177\tbest: 0.8953177 (9000)\ttotal: 5m 17s\tremaining: 9h 43m 4s\n",
      "10000:\ttest: 0.8960022\tbest: 0.8960094 (9986)\ttotal: 5m 53s\tremaining: 9h 43m 33s\n",
      "11000:\ttest: 0.8965815\tbest: 0.8965930 (10987)\ttotal: 6m 30s\tremaining: 9h 44m 32s\n",
      "12000:\ttest: 0.8969540\tbest: 0.8969619 (11974)\ttotal: 7m 5s\tremaining: 9h 43m 32s\n",
      "13000:\ttest: 0.8972051\tbest: 0.8972110 (12981)\ttotal: 7m 39s\tremaining: 9h 41m 20s\n",
      "14000:\ttest: 0.8974197\tbest: 0.8974240 (13982)\ttotal: 8m 13s\tremaining: 9h 39m 9s\n",
      "15000:\ttest: 0.8975664\tbest: 0.8975821 (14972)\ttotal: 8m 49s\tremaining: 9h 39m 27s\n",
      "16000:\ttest: 0.8976962\tbest: 0.8977309 (15753)\ttotal: 9m 25s\tremaining: 9h 39m 25s\n",
      "17000:\ttest: 0.8978322\tbest: 0.8978488 (16962)\ttotal: 10m\tremaining: 9h 38m 57s\n",
      "18000:\ttest: 0.8979202\tbest: 0.8979235 (17998)\ttotal: 10m 36s\tremaining: 9h 38m 17s\n",
      "19000:\ttest: 0.8979524\tbest: 0.8979528 (18994)\ttotal: 11m 11s\tremaining: 9h 37m 54s\n",
      "20000:\ttest: 0.8979924\tbest: 0.8979978 (19197)\ttotal: 11m 47s\tremaining: 9h 37m 43s\n",
      "21000:\ttest: 0.8980498\tbest: 0.8980498 (20998)\ttotal: 12m 22s\tremaining: 9h 37m 5s\n",
      "22000:\ttest: 0.8980545\tbest: 0.8980637 (21316)\ttotal: 12m 58s\tremaining: 9h 36m 33s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8980636884\n",
      "bestIteration = 21316\n",
      "\n",
      "Shrink model to first 21317 iterations.\n",
      "CV mean score: 0.9008, std: 0.0040.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_cat, prediction_cat, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, model_type='cat', plot_feature_importance=False)\n",
    "oof.append(oof_cat)\n",
    "preds.append(prediction_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb\n",
      "0  0.010545  0.010837  0.008313\n",
      "1  0.455996  0.423705  0.392034\n",
      "2  0.004485  0.003918  0.004919\n",
      "3  0.243523  0.253600  0.340744\n",
      "4  0.097492  0.088431  0.110311\n",
      "0.925455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "# sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "# sub['target'] = predictions\n",
    "# sub.to_csv('../submissions/sub12c.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb\n",
      "0  0.010545  0.010837  0.008313\n",
      "1  0.455996  0.423705  0.392034\n",
      "2  0.004485  0.003918  0.004919\n",
      "3  0.243523  0.253600  0.340744\n",
      "4  0.097492  0.088431  0.110311\n",
      "0.857165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "print(stage2.head())\n",
    "\n",
    "lr = OneVsRestClassifier(BaggingClassifier(SVC(gamma='auto', probability=True, class_weight='balanced', C=0.001), \n",
    "                                            max_samples=1.0 / n_estimators, n_estimators=n_estimators, n_jobs=-1), n_jobs=-1)\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12f.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb\n",
      "0  0.010545  0.010837  0.008313\n",
      "1  0.455996  0.423705  0.392034\n",
      "2  0.004485  0.003918  0.004919\n",
      "3  0.243523  0.253600  0.340744\n",
      "4  0.097492  0.088431  0.110311\n",
      "0.9227\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "print(stage2.head())\n",
    "\n",
    "lr = OneVsRestClassifier(BaggingClassifier(SVC(kernel='poly',gamma='auto', probability=True, class_weight='balanced'), \n",
    "                                            max_samples=1.0 / n_estimators, n_estimators=n_estimators, n_jobs=-1), n_jobs=-1)\n",
    "lr.fit(stage2, y)\n",
    "# predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "# sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "# sub['target'] = predictions\n",
    "# sub.to_csv('../submissions/sub12c.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb\n",
      "0  0.010545  0.010837  0.008313\n",
      "1  0.455996  0.423705  0.392034\n",
      "2  0.004485  0.003918  0.004919\n",
      "3  0.243523  0.253600  0.340744\n",
      "4  0.097492  0.088431  0.110311\n",
      "0.9126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "print(stage2.head())\n",
    "\n",
    "lr = OneVsRestClassifier(BaggingClassifier(SVC(kernel='poly',gamma='auto', probability=True, class_weight='balanced', C=0.001), \n",
    "                                            max_samples=1.0 / n_estimators, n_estimators=n_estimators, n_jobs=-1), n_jobs=-1)\n",
    "lr.fit(stage2, y)\n",
    "# predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "# sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "# sub['target'] = predictions\n",
    "# sub.to_csv('../submissions/sub12c.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb\n",
      "0  0.010545  0.010837  0.008313\n",
      "1  0.455996  0.423705  0.392034\n",
      "2  0.004485  0.003918  0.004919\n",
      "3  0.243523  0.253600  0.340744\n",
      "4  0.097492  0.088431  0.110311\n",
      "0.919625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "print(stage2.head())\n",
    "\n",
    "lr = OneVsRestClassifier(BaggingClassifier(SVC(kernel='poly',gamma='auto', probability=True, class_weight='balanced', C=10.0), \n",
    "                                            max_samples=1.0 / n_estimators, n_estimators=n_estimators, n_jobs=-1), n_jobs=-1)\n",
    "lr.fit(stage2, y)\n",
    "# predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "# sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "# sub['target'] = predictions\n",
    "# sub.to_csv('../submissions/sub12c.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb\n",
      "0  0.010545  0.010837  0.008313\n",
      "1  0.455996  0.423705  0.392034\n",
      "2  0.004485  0.003918  0.004919\n",
      "3  0.243523  0.253600  0.340744\n",
      "4  0.097492  0.088431  0.110311\n",
      "0.9193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "print(stage2.head())\n",
    "\n",
    "lr = OneVsRestClassifier(BaggingClassifier(SVC(kernel='poly',gamma='scale', probability=True, class_weight='balanced'), \n",
    "                                            max_samples=1.0 / n_estimators, n_estimators=n_estimators, n_jobs=-1), n_jobs=-1)\n",
    "lr.fit(stage2, y)\n",
    "# predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "# sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "# sub['target'] = predictions\n",
    "# sub.to_csv('../submissions/sub12c.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb\n",
      "0  0.010545  0.010837  0.008313\n",
      "1  0.455996  0.423705  0.392034\n",
      "2  0.004485  0.003918  0.004919\n",
      "3  0.243523  0.253600  0.340744\n",
      "4  0.097492  0.088431  0.110311\n",
      "0.87875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "print(stage2.head())\n",
    "n_estimators = 20\n",
    "lr = OneVsRestClassifier(BaggingClassifier(SVC(kernel='linear',gamma='scale', probability=True, class_weight='balanced'), \n",
    "                                            max_samples=1.0 / n_estimators, n_estimators=n_estimators, n_jobs=-1), n_jobs=-1)\n",
    "lr.fit(stage2, y)\n",
    "# predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "# sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "# sub['target'] = predictions\n",
    "# sub.to_csv('../submissions/sub12c.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb\n",
      "0  0.010545  0.010837  0.008313\n",
      "1  0.455996  0.423705  0.392034\n",
      "2  0.004485  0.003918  0.004919\n",
      "3  0.243523  0.253600  0.340744\n",
      "4  0.097492  0.088431  0.110311\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "print(stage2.head())\n",
    "\n",
    "lr = RandomForestClassifier(n_estimators=500,n_jobs=-1)\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12f.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb\n",
      "0  0.010545  0.010837  0.008313\n",
      "1  0.455996  0.423705  0.392034\n",
      "2  0.004485  0.003918  0.004919\n",
      "3  0.243523  0.253600  0.340744\n",
      "4  0.097492  0.088431  0.110311\n",
      "0.925455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "\n",
    "# stage2['cat2'] = oof_cat\n",
    "# stage2_test['cat2'] = prediction_cat\n",
    "\n",
    "print(stage2.head())\n",
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "# sub.to_csv('../submissions/sub12g.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.826045\n",
      "[2000]\tvalid_0's auc: 0.853949\n",
      "[3000]\tvalid_0's auc: 0.867037\n",
      "[4000]\tvalid_0's auc: 0.874934\n",
      "[5000]\tvalid_0's auc: 0.88029\n",
      "[6000]\tvalid_0's auc: 0.884068\n",
      "[7000]\tvalid_0's auc: 0.886903\n",
      "[8000]\tvalid_0's auc: 0.889136\n",
      "[9000]\tvalid_0's auc: 0.890871\n",
      "[10000]\tvalid_0's auc: 0.892171\n",
      "[11000]\tvalid_0's auc: 0.893202\n",
      "[12000]\tvalid_0's auc: 0.894073\n",
      "[13000]\tvalid_0's auc: 0.89481\n",
      "[14000]\tvalid_0's auc: 0.895409\n",
      "[15000]\tvalid_0's auc: 0.895924\n",
      "[16000]\tvalid_0's auc: 0.896362\n",
      "[17000]\tvalid_0's auc: 0.896711\n",
      "[18000]\tvalid_0's auc: 0.896999\n",
      "[19000]\tvalid_0's auc: 0.89729\n",
      "[20000]\tvalid_0's auc: 0.897507\n",
      "[21000]\tvalid_0's auc: 0.897715\n",
      "[22000]\tvalid_0's auc: 0.897883\n",
      "[23000]\tvalid_0's auc: 0.898022\n",
      "[24000]\tvalid_0's auc: 0.898173\n",
      "[25000]\tvalid_0's auc: 0.898285\n",
      "[26000]\tvalid_0's auc: 0.898394\n",
      "[27000]\tvalid_0's auc: 0.898471\n",
      "[28000]\tvalid_0's auc: 0.898569\n",
      "[29000]\tvalid_0's auc: 0.898655\n",
      "[30000]\tvalid_0's auc: 0.898724\n",
      "[31000]\tvalid_0's auc: 0.898777\n",
      "[32000]\tvalid_0's auc: 0.898811\n",
      "[33000]\tvalid_0's auc: 0.898843\n",
      "[34000]\tvalid_0's auc: 0.898863\n",
      "[35000]\tvalid_0's auc: 0.898883\n",
      "[36000]\tvalid_0's auc: 0.898902\n",
      "[37000]\tvalid_0's auc: 0.898909\n",
      "[38000]\tvalid_0's auc: 0.898923\n",
      "[39000]\tvalid_0's auc: 0.898923\n",
      "Early stopping, best iteration is:\n",
      "[38204]\tvalid_0's auc: 0.898934\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0334\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_81\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 84.33%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0236\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_139\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 84.82%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0225\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_12\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.04%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0200\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_6\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.33%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0194\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_53\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.01%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0180\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_110\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0180\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_26\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.29%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0175\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_146\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.58%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0169\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_174\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.79%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0165\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_22\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0160\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_166\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.22%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0157\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_80\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.37%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0154\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_99\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.43%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0153\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_76\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.89%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0144\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_21\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.03%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0142\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_133\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.19%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0139\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_109\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.31%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0136\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_2\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.56%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0132\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_198\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.57%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0132\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                var_165\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 89.57%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 180 more &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(123)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X1, y, test_size = 0.2, random_state=42)\n",
    "model = lgb.LGBMClassifier(\n",
    "                 max_depth=-1,\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate=0.02,\n",
    "                 colsample_bytree=0.3,\n",
    "                 num_leaves=2,\n",
    "                 metric='auc',\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "            \n",
    "model.fit(X_train, y_train, \n",
    "    eval_set=[(X_val, y_val\n",
    "    verbose=1000, \n",
    "    early_stopping_rounds=1000)\n",
    "base_features = list(X1.columns)\n",
    "# perm = PermutationImportance(model, random_state=1).fit(X_val, y_val)\n",
    "eli5.show_weights(model, feature_names = base_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=0\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.966</b>, score <b>-3.343</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.264\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.22%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.195\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_34\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.57%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.161\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_21\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.60%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.158\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_6\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.69%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.149\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_192\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.73%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.146\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_107\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.80%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.139\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_76\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.84%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.135\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_139\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.86%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.134\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_146\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.93%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.127\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_166\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.95%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.126\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_1\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.99%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.123\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_174\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.122\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_94\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.122\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_9\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.01%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.121\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_22\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.01%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.120\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_190\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.05%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.117\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_173\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.06%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.117\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_49\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.07%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.115\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_115\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.09%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.114\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_110\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.11%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.112\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_191\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.12%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.111\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_128\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.13%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.111\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_80\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.17%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.107\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_104\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.19%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.106\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_26\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.22%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.103\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_164\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.27%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.099\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_32\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.27%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.099\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_108\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.29%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.097\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_35\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.31%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.096\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_90\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.32%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.095\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_170\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.37%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.091\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_147\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.38%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.090\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_23\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.39%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.090\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_2\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.41%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.087\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_123\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.42%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.087\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_131\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.49%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.082\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_85\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.53%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.078\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_162\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.53%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.078\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_106\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.57%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.076\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_196\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.63%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.071\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_44\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.66%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.068\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_93\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.67%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.068\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_178\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.67%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.068\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_48\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.67%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.068\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_197\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.71%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.065\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_179\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.73%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.064\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_134\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.73%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.064\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_198\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.74%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.063\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_51\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.76%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.062\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_58\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.77%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.061\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_52\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.80%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.059\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_187\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.80%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.058\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_175\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.84%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.056\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_180\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.85%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.055\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_91\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.86%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.055\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_111\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.90%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.052\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_57\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.92%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.051\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_15\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.95%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.048\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_97\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.98%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.046\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_195\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.045\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_84\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.01%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.044\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_167\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.02%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.044\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_28\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.03%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.043\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_157\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.05%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.042\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_130\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.05%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.042\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_193\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.06%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.041\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_88\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.06%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.041\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_63\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.08%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.040\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_177\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.18%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.034\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_151\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.18%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.034\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_133\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.18%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.034\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_17\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.24%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.031\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_95\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.24%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.030\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_120\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.24%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.030\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_143\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.25%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.030\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_47\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.31%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.027\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_70\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.35%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.025\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_3\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.35%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.025\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_105\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.41%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.021\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_188\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.42%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.021\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_16\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.42%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.021\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_149\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.43%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.020\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_112\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.49%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.017\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_75\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.50%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.017\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_46\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.51%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.016\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_83\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.53%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.015\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_181\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.54%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.015\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_163\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.57%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.014\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_136\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.59%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.013\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_126\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.59%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.013\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_12\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.65%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.010\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_59\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.66%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.010\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_68\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.69%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.009\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_42\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.69%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.008\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_29\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.71%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.008\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_176\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.71%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.008\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_183\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.72%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.007\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_77\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.72%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.007\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_153\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.74%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.007\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_60\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.75%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.006\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_4\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.76%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.006\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_103\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.76%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.006\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_152\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.78%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.005\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_182\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.78%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.005\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_41\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_145\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.84%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_129\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.91%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.002\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_37\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.92%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.001\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_168\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.93%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.001\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_171\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.93%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.001\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_98\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.94%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.001\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_121\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.95%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.001\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_185\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.96%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.000\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_150\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.96%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.000\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_160\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.98%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.000\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_113\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.99%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.000\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_33\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.99%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.000\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_100\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.98%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.000\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_30\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.97%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.000\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_158\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.96%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.000\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_39\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.95%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.001\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_61\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.94%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.001\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_14\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.93%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.001\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_78\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.92%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.001\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_7\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.92%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.001\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_96\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.90%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.002\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_43\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.90%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.002\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_138\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.88%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.002\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_25\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.86%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.003\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_8\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.82%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.004\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_20\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.82%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.004\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_142\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.81%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.004\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_69\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.78%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.005\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_144\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.78%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.005\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_19\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.77%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.005\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_45\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.77%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.006\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_73\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.74%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.006\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_189\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.74%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.006\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_64\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.72%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.007\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_31\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.72%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.007\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_135\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.72%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.007\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_102\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.70%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.008\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_118\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.70%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.008\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_65\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.70%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.008\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_117\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.70%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.008\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_40\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.69%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.009\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_199\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.65%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.010\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_172\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.64%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.010\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_156\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.63%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.011\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_56\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.62%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.011\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_154\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.60%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.012\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_101\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.59%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.013\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_140\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.54%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.015\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_125\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.54%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.015\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_161\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.53%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.015\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_127\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.51%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.016\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_38\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.47%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.018\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_54\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.47%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.018\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_10\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.46%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.019\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_114\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.44%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.020\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_79\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.43%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.020\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_62\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.40%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.022\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_66\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.38%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.023\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_141\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.38%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.023\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_55\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.34%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.025\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_24\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.33%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.026\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_72\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.33%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.026\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_124\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.33%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.026\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_116\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.30%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.027\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_82\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.10%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.039\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_159\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.07%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.041\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_36\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.97%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.047\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_89\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.97%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.047\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_74\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.97%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.047\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_87\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.87%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.054\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_148\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.86%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.054\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_67\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.85%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.055\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_13\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.80%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.059\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_11\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.64%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.070\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_27\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.63%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.071\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_119\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.58%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.075\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_71\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.41%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.088\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_165\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.24%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.102\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_86\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.23%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.102\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_194\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.03%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.119\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_132\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.122\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_50\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 97.92%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.128\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_0\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 97.79%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.141\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_169\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 97.68%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.151\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_99\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 97.66%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.152\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_109\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 97.56%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.162\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_155\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 97.44%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.173\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_137\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 96.61%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.259\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_186\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 96.31%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.292\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_18\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 95.95%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.333\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_184\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 95.48%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.390\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_92\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 94.79%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.477\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_5\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 93.77%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.617\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_122\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 93.42%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.667\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_81\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 91.99%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.884\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        var_53\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eli5 import show_prediction\n",
    "# ?? show_prediction\n",
    "eli5.show_prediction(model, X_val.iloc[1], targets=[0, 1], feature_names=list(X_train.columns))\n",
    "# eli5.show_prediction(model, X_val, feature_names = base_features, show_feature_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = eli5.formatters.as_dataframe.explain_prediction_df(model, X_val.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;BIAS&gt;</td>\n",
       "      <td>3.263848</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>var_34</td>\n",
       "      <td>0.194992</td>\n",
       "      <td>2.237072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>var_21</td>\n",
       "      <td>0.160823</td>\n",
       "      <td>2.816402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>var_6</td>\n",
       "      <td>0.157545</td>\n",
       "      <td>-1.121865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>var_192</td>\n",
       "      <td>0.149123</td>\n",
       "      <td>1.838830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  feature    weight     value\n",
       "0       0   <BIAS>  3.263848  1.000000\n",
       "1       0   var_34  0.194992  2.237072\n",
       "2       0   var_21  0.160823  2.816402\n",
       "3       0    var_6  0.157545 -1.121865\n",
       "4       0  var_192  0.149123  1.838830"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_neg = df.loc[df.weight < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0</td>\n",
       "      <td>var_100</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-2.463950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "      <td>var_30</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>0.087428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>var_158</td>\n",
       "      <td>-0.000250</td>\n",
       "      <td>1.797573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>var_39</td>\n",
       "      <td>-0.000461</td>\n",
       "      <td>-1.260696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>var_61</td>\n",
       "      <td>-0.000635</td>\n",
       "      <td>-0.112805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target  feature    weight     value\n",
       "117       0  var_100 -0.000056 -2.463950\n",
       "118       0   var_30 -0.000136  0.087428\n",
       "119       0  var_158 -0.000250  1.797573\n",
       "120       0   var_39 -0.000461 -1.260696\n",
       "121       0   var_61 -0.000635 -0.112805"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pos = df.loc[df.weight > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;BIAS&gt;</td>\n",
       "      <td>3.263848</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>var_34</td>\n",
       "      <td>0.194992</td>\n",
       "      <td>2.237072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>var_21</td>\n",
       "      <td>0.160823</td>\n",
       "      <td>2.816402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>var_6</td>\n",
       "      <td>0.157545</td>\n",
       "      <td>-1.121865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>var_192</td>\n",
       "      <td>0.149123</td>\n",
       "      <td>1.838830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  feature    weight     value\n",
       "0       0   <BIAS>  3.263848  1.000000\n",
       "1       0   var_34  0.194992  2.237072\n",
       "2       0   var_21  0.160823  2.816402\n",
       "3       0    var_6  0.157545 -1.121865\n",
       "4       0  var_192  0.149123  1.838830"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0</td>\n",
       "      <td>var_100</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-2.46395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target  feature    weight    value\n",
       "117       0  var_100 -0.000056 -2.46395"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.feature == 'var_100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X3 = X1.copy()\n",
    "for idx, row in df_neg.iterrows():\n",
    "#     print(row)\n",
    "    target = row['target']\n",
    "    feature = row['feature']\n",
    "    value = row['value']\n",
    "    X3.loc[(X3[feature] == value)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X3 = X1.copy()\n",
    "# for idx, row in df_neg.iterrows():\n",
    "# #     print(row)\n",
    "#     target = row['target']\n",
    "#     feature = row['feature']\n",
    "#     value = row['value']\n",
    "#     X3.loc[(X3[feature] == value)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.822715\n",
      "[2000]\tvalid_0's auc: 0.853177\n",
      "[3000]\tvalid_0's auc: 0.867385\n",
      "[4000]\tvalid_0's auc: 0.87541\n",
      "[5000]\tvalid_0's auc: 0.880898\n",
      "[6000]\tvalid_0's auc: 0.884749\n",
      "[7000]\tvalid_0's auc: 0.88763\n",
      "[8000]\tvalid_0's auc: 0.889902\n",
      "[9000]\tvalid_0's auc: 0.891642\n",
      "[10000]\tvalid_0's auc: 0.893106\n",
      "[11000]\tvalid_0's auc: 0.894196\n",
      "[12000]\tvalid_0's auc: 0.895119\n",
      "[13000]\tvalid_0's auc: 0.895819\n",
      "[14000]\tvalid_0's auc: 0.896432\n",
      "[15000]\tvalid_0's auc: 0.896941\n",
      "[16000]\tvalid_0's auc: 0.897332\n",
      "[17000]\tvalid_0's auc: 0.897702\n",
      "[18000]\tvalid_0's auc: 0.897999\n",
      "[19000]\tvalid_0's auc: 0.898264\n",
      "[20000]\tvalid_0's auc: 0.898461\n",
      "[21000]\tvalid_0's auc: 0.89862\n",
      "[22000]\tvalid_0's auc: 0.898772\n",
      "[23000]\tvalid_0's auc: 0.898882\n",
      "[24000]\tvalid_0's auc: 0.898991\n",
      "[25000]\tvalid_0's auc: 0.899075\n",
      "[26000]\tvalid_0's auc: 0.899132\n",
      "[27000]\tvalid_0's auc: 0.899175\n",
      "[28000]\tvalid_0's auc: 0.899216\n",
      "[29000]\tvalid_0's auc: 0.899237\n",
      "[30000]\tvalid_0's auc: 0.899255\n",
      "[31000]\tvalid_0's auc: 0.899263\n",
      "[32000]\tvalid_0's auc: 0.899286\n",
      "Early stopping, best iteration is:\n",
      "[31882]\tvalid_0's auc: 0.899293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.3,\n",
       "        importance_type='split', learning_rate=0.02, max_depth=-1,\n",
       "        metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
       "        min_split_gain=0.0, n_estimators=999999, n_jobs=-1, num_leaves=2,\n",
       "        objective='binary', random_state=None, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X3, y, test_size=0.2, stratify=y)\n",
    "\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "                 max_depth=-1,\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate=0.02,\n",
    "                 colsample_bytree=0.3,\n",
    "                 num_leaves=2,\n",
    "                 metric='auc',\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "            \n",
    "model.fit(X_train, y_train, \n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=1000, \n",
    "    early_stopping_rounds=1000)\n",
    "\n",
    "# after first time changing negatives to zeros\n",
    "# Early stopping, best iteration is:\n",
    "# [31882]\tvalid_0's auc: 0.899293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Wed Mar  6 21:28:52 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.82432\n",
      "[2000]\tvalid_0's auc: 0.853677\n",
      "[3000]\tvalid_0's auc: 0.867463\n",
      "[4000]\tvalid_0's auc: 0.875517\n",
      "[5000]\tvalid_0's auc: 0.880662\n",
      "[6000]\tvalid_0's auc: 0.884318\n",
      "[7000]\tvalid_0's auc: 0.887061\n",
      "[8000]\tvalid_0's auc: 0.889053\n",
      "[9000]\tvalid_0's auc: 0.890727\n",
      "[10000]\tvalid_0's auc: 0.89209\n",
      "[11000]\tvalid_0's auc: 0.893171\n",
      "[12000]\tvalid_0's auc: 0.894003\n",
      "[13000]\tvalid_0's auc: 0.894702\n",
      "[14000]\tvalid_0's auc: 0.895284\n",
      "[15000]\tvalid_0's auc: 0.895831\n",
      "[16000]\tvalid_0's auc: 0.896253\n",
      "[17000]\tvalid_0's auc: 0.896644\n",
      "[18000]\tvalid_0's auc: 0.896942\n",
      "[19000]\tvalid_0's auc: 0.897229\n",
      "[20000]\tvalid_0's auc: 0.897439\n",
      "[21000]\tvalid_0's auc: 0.897634\n",
      "[22000]\tvalid_0's auc: 0.897808\n",
      "[23000]\tvalid_0's auc: 0.897964\n",
      "[24000]\tvalid_0's auc: 0.898118\n",
      "[25000]\tvalid_0's auc: 0.898232\n",
      "[26000]\tvalid_0's auc: 0.898351\n",
      "[27000]\tvalid_0's auc: 0.89845\n",
      "[28000]\tvalid_0's auc: 0.89851\n",
      "[29000]\tvalid_0's auc: 0.898581\n",
      "[30000]\tvalid_0's auc: 0.898653\n",
      "[31000]\tvalid_0's auc: 0.898729\n",
      "[32000]\tvalid_0's auc: 0.898781\n",
      "[33000]\tvalid_0's auc: 0.898835\n",
      "[34000]\tvalid_0's auc: 0.898875\n",
      "[35000]\tvalid_0's auc: 0.898924\n",
      "[36000]\tvalid_0's auc: 0.898963\n",
      "[37000]\tvalid_0's auc: 0.898994\n",
      "[38000]\tvalid_0's auc: 0.899034\n",
      "[39000]\tvalid_0's auc: 0.899048\n",
      "[40000]\tvalid_0's auc: 0.899053\n",
      "[41000]\tvalid_0's auc: 0.899069\n",
      "[42000]\tvalid_0's auc: 0.89908\n",
      "[43000]\tvalid_0's auc: 0.899101\n",
      "[44000]\tvalid_0's auc: 0.899113\n",
      "Early stopping, best iteration is:\n",
      "[43353]\tvalid_0's auc: 0.89912\n",
      "Fold 1 started at Wed Mar  6 21:54:50 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.820096\n",
      "[2000]\tvalid_0's auc: 0.849418\n",
      "[3000]\tvalid_0's auc: 0.863474\n",
      "[4000]\tvalid_0's auc: 0.871613\n",
      "[5000]\tvalid_0's auc: 0.877104\n",
      "[6000]\tvalid_0's auc: 0.881157\n",
      "[7000]\tvalid_0's auc: 0.884069\n",
      "[8000]\tvalid_0's auc: 0.886295\n",
      "[9000]\tvalid_0's auc: 0.888054\n",
      "[10000]\tvalid_0's auc: 0.889428\n",
      "[11000]\tvalid_0's auc: 0.890581\n",
      "[12000]\tvalid_0's auc: 0.891552\n",
      "[13000]\tvalid_0's auc: 0.892331\n",
      "[14000]\tvalid_0's auc: 0.892965\n",
      "[15000]\tvalid_0's auc: 0.893536\n",
      "[16000]\tvalid_0's auc: 0.89401\n",
      "[17000]\tvalid_0's auc: 0.894402\n",
      "[18000]\tvalid_0's auc: 0.894771\n",
      "[19000]\tvalid_0's auc: 0.895052\n",
      "[20000]\tvalid_0's auc: 0.895292\n",
      "[21000]\tvalid_0's auc: 0.895531\n",
      "[22000]\tvalid_0's auc: 0.895709\n",
      "[23000]\tvalid_0's auc: 0.895884\n",
      "[24000]\tvalid_0's auc: 0.896015\n",
      "[25000]\tvalid_0's auc: 0.896113\n",
      "[26000]\tvalid_0's auc: 0.896226\n",
      "[27000]\tvalid_0's auc: 0.896331\n",
      "[28000]\tvalid_0's auc: 0.896394\n",
      "[29000]\tvalid_0's auc: 0.896465\n",
      "[30000]\tvalid_0's auc: 0.896547\n",
      "[31000]\tvalid_0's auc: 0.896609\n",
      "[32000]\tvalid_0's auc: 0.896652\n",
      "[33000]\tvalid_0's auc: 0.896703\n",
      "[34000]\tvalid_0's auc: 0.896747\n",
      "[35000]\tvalid_0's auc: 0.896771\n",
      "[36000]\tvalid_0's auc: 0.89679\n",
      "[37000]\tvalid_0's auc: 0.896813\n",
      "Early stopping, best iteration is:\n",
      "[36631]\tvalid_0's auc: 0.896823\n",
      "Fold 2 started at Wed Mar  6 22:14:13 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.826421\n",
      "[2000]\tvalid_0's auc: 0.855013\n",
      "[3000]\tvalid_0's auc: 0.868406\n",
      "[4000]\tvalid_0's auc: 0.876183\n",
      "[5000]\tvalid_0's auc: 0.881324\n",
      "[6000]\tvalid_0's auc: 0.885014\n",
      "[7000]\tvalid_0's auc: 0.887653\n",
      "[8000]\tvalid_0's auc: 0.889648\n",
      "[9000]\tvalid_0's auc: 0.891268\n",
      "[10000]\tvalid_0's auc: 0.892495\n",
      "[11000]\tvalid_0's auc: 0.893487\n",
      "[12000]\tvalid_0's auc: 0.894388\n",
      "[13000]\tvalid_0's auc: 0.895064\n",
      "[14000]\tvalid_0's auc: 0.895632\n",
      "[15000]\tvalid_0's auc: 0.89608\n",
      "[16000]\tvalid_0's auc: 0.896496\n",
      "[17000]\tvalid_0's auc: 0.896803\n",
      "[18000]\tvalid_0's auc: 0.897052\n",
      "[19000]\tvalid_0's auc: 0.897251\n",
      "[20000]\tvalid_0's auc: 0.89743\n",
      "[21000]\tvalid_0's auc: 0.89759\n",
      "[22000]\tvalid_0's auc: 0.89771\n",
      "[23000]\tvalid_0's auc: 0.897826\n",
      "[24000]\tvalid_0's auc: 0.897924\n",
      "[25000]\tvalid_0's auc: 0.898033\n",
      "[26000]\tvalid_0's auc: 0.898112\n",
      "[27000]\tvalid_0's auc: 0.898189\n",
      "[28000]\tvalid_0's auc: 0.898253\n",
      "[29000]\tvalid_0's auc: 0.898321\n",
      "[30000]\tvalid_0's auc: 0.898385\n",
      "[31000]\tvalid_0's auc: 0.898447\n",
      "[32000]\tvalid_0's auc: 0.898476\n",
      "[33000]\tvalid_0's auc: 0.898508\n",
      "[34000]\tvalid_0's auc: 0.898529\n",
      "[35000]\tvalid_0's auc: 0.898537\n",
      "[36000]\tvalid_0's auc: 0.89854\n",
      "Early stopping, best iteration is:\n",
      "[35612]\tvalid_0's auc: 0.898557\n",
      "Fold 3 started at Wed Mar  6 22:36:24 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.823947\n",
      "[2000]\tvalid_0's auc: 0.854335\n",
      "[3000]\tvalid_0's auc: 0.868235\n",
      "[4000]\tvalid_0's auc: 0.876212\n",
      "[5000]\tvalid_0's auc: 0.881315\n",
      "[6000]\tvalid_0's auc: 0.884995\n",
      "[7000]\tvalid_0's auc: 0.888027\n",
      "[8000]\tvalid_0's auc: 0.890199\n",
      "[9000]\tvalid_0's auc: 0.891962\n",
      "[10000]\tvalid_0's auc: 0.893346\n",
      "[11000]\tvalid_0's auc: 0.894424\n",
      "[12000]\tvalid_0's auc: 0.895345\n",
      "[13000]\tvalid_0's auc: 0.896104\n",
      "[14000]\tvalid_0's auc: 0.896714\n",
      "[15000]\tvalid_0's auc: 0.897184\n",
      "[16000]\tvalid_0's auc: 0.897624\n",
      "[17000]\tvalid_0's auc: 0.897951\n",
      "[18000]\tvalid_0's auc: 0.898256\n",
      "[19000]\tvalid_0's auc: 0.89852\n",
      "[20000]\tvalid_0's auc: 0.898723\n",
      "[21000]\tvalid_0's auc: 0.898913\n",
      "[22000]\tvalid_0's auc: 0.899061\n",
      "[23000]\tvalid_0's auc: 0.899187\n",
      "[24000]\tvalid_0's auc: 0.899316\n",
      "[25000]\tvalid_0's auc: 0.899386\n",
      "[26000]\tvalid_0's auc: 0.899452\n",
      "[27000]\tvalid_0's auc: 0.899494\n",
      "[28000]\tvalid_0's auc: 0.899538\n",
      "[29000]\tvalid_0's auc: 0.899601\n",
      "[30000]\tvalid_0's auc: 0.899665\n",
      "[31000]\tvalid_0's auc: 0.899712\n",
      "[32000]\tvalid_0's auc: 0.899745\n",
      "[33000]\tvalid_0's auc: 0.899786\n",
      "[34000]\tvalid_0's auc: 0.899823\n",
      "[35000]\tvalid_0's auc: 0.899874\n",
      "[36000]\tvalid_0's auc: 0.89989\n",
      "[37000]\tvalid_0's auc: 0.899919\n",
      "[38000]\tvalid_0's auc: 0.899933\n",
      "[39000]\tvalid_0's auc: 0.899948\n",
      "[40000]\tvalid_0's auc: 0.899963\n",
      "[41000]\tvalid_0's auc: 0.899979\n",
      "[42000]\tvalid_0's auc: 0.899996\n",
      "[43000]\tvalid_0's auc: 0.900006\n",
      "[44000]\tvalid_0's auc: 0.900024\n",
      "[45000]\tvalid_0's auc: 0.90004\n",
      "[46000]\tvalid_0's auc: 0.900053\n",
      "[47000]\tvalid_0's auc: 0.900073\n",
      "[48000]\tvalid_0's auc: 0.900089\n",
      "[49000]\tvalid_0's auc: 0.900099\n",
      "Early stopping, best iteration is:\n",
      "[48868]\tvalid_0's auc: 0.9001\n",
      "Fold 4 started at Wed Mar  6 23:03:52 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.827089\n",
      "[2000]\tvalid_0's auc: 0.856889\n",
      "[3000]\tvalid_0's auc: 0.871791\n",
      "[4000]\tvalid_0's auc: 0.880527\n",
      "[5000]\tvalid_0's auc: 0.886309\n",
      "[6000]\tvalid_0's auc: 0.890654\n",
      "[7000]\tvalid_0's auc: 0.893755\n",
      "[8000]\tvalid_0's auc: 0.896144\n",
      "[9000]\tvalid_0's auc: 0.898207\n",
      "[10000]\tvalid_0's auc: 0.899852\n",
      "[11000]\tvalid_0's auc: 0.901196\n",
      "[12000]\tvalid_0's auc: 0.902297\n",
      "[13000]\tvalid_0's auc: 0.903117\n",
      "[14000]\tvalid_0's auc: 0.903809\n",
      "[15000]\tvalid_0's auc: 0.904356\n",
      "[16000]\tvalid_0's auc: 0.90493\n",
      "[17000]\tvalid_0's auc: 0.905362\n",
      "[18000]\tvalid_0's auc: 0.905716\n",
      "[19000]\tvalid_0's auc: 0.906028\n",
      "[20000]\tvalid_0's auc: 0.906303\n",
      "[21000]\tvalid_0's auc: 0.906513\n",
      "[22000]\tvalid_0's auc: 0.906718\n",
      "[23000]\tvalid_0's auc: 0.906865\n",
      "[24000]\tvalid_0's auc: 0.907024\n",
      "[25000]\tvalid_0's auc: 0.907149\n",
      "[26000]\tvalid_0's auc: 0.90726\n",
      "[27000]\tvalid_0's auc: 0.907335\n",
      "[28000]\tvalid_0's auc: 0.907401\n",
      "[29000]\tvalid_0's auc: 0.907479\n",
      "[30000]\tvalid_0's auc: 0.907536\n",
      "[31000]\tvalid_0's auc: 0.907561\n",
      "[32000]\tvalid_0's auc: 0.907609\n",
      "[33000]\tvalid_0's auc: 0.907636\n",
      "[34000]\tvalid_0's auc: 0.907668\n",
      "[35000]\tvalid_0's auc: 0.907694\n",
      "[36000]\tvalid_0's auc: 0.907692\n",
      "[37000]\tvalid_0's auc: 0.907723\n",
      "[38000]\tvalid_0's auc: 0.907737\n",
      "[39000]\tvalid_0's auc: 0.907735\n",
      "[40000]\tvalid_0's auc: 0.907742\n",
      "[41000]\tvalid_0's auc: 0.907747\n",
      "[42000]\tvalid_0's auc: 0.907761\n",
      "Early stopping, best iteration is:\n",
      "[41603]\tvalid_0's auc: 0.907771\n",
      "Fold 5 started at Wed Mar  6 23:28:21 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.824354\n",
      "[2000]\tvalid_0's auc: 0.853115\n",
      "[3000]\tvalid_0's auc: 0.867321\n",
      "[4000]\tvalid_0's auc: 0.876207\n",
      "[5000]\tvalid_0's auc: 0.881854\n",
      "[6000]\tvalid_0's auc: 0.885756\n",
      "[7000]\tvalid_0's auc: 0.888719\n",
      "[8000]\tvalid_0's auc: 0.891127\n",
      "[9000]\tvalid_0's auc: 0.892956\n",
      "[10000]\tvalid_0's auc: 0.894505\n",
      "[11000]\tvalid_0's auc: 0.895744\n",
      "[12000]\tvalid_0's auc: 0.89673\n",
      "[13000]\tvalid_0's auc: 0.897565\n",
      "[14000]\tvalid_0's auc: 0.898193\n",
      "[15000]\tvalid_0's auc: 0.898766\n",
      "[16000]\tvalid_0's auc: 0.899256\n",
      "[17000]\tvalid_0's auc: 0.899598\n",
      "[18000]\tvalid_0's auc: 0.89993\n",
      "[19000]\tvalid_0's auc: 0.90017\n",
      "[20000]\tvalid_0's auc: 0.900362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21000]\tvalid_0's auc: 0.900557\n",
      "[22000]\tvalid_0's auc: 0.900728\n",
      "[23000]\tvalid_0's auc: 0.900902\n",
      "[24000]\tvalid_0's auc: 0.90101\n",
      "[25000]\tvalid_0's auc: 0.901128\n",
      "[26000]\tvalid_0's auc: 0.901215\n",
      "[27000]\tvalid_0's auc: 0.901301\n",
      "[28000]\tvalid_0's auc: 0.901358\n",
      "[29000]\tvalid_0's auc: 0.901427\n",
      "[30000]\tvalid_0's auc: 0.901493\n",
      "[31000]\tvalid_0's auc: 0.901541\n",
      "[32000]\tvalid_0's auc: 0.901607\n",
      "[33000]\tvalid_0's auc: 0.901664\n",
      "[34000]\tvalid_0's auc: 0.901703\n",
      "[35000]\tvalid_0's auc: 0.901732\n",
      "[36000]\tvalid_0's auc: 0.901759\n",
      "[37000]\tvalid_0's auc: 0.901768\n",
      "[38000]\tvalid_0's auc: 0.901796\n",
      "Early stopping, best iteration is:\n",
      "[37811]\tvalid_0's auc: 0.901807\n",
      "Fold 6 started at Wed Mar  6 23:53:26 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.821597\n",
      "[2000]\tvalid_0's auc: 0.851761\n",
      "[3000]\tvalid_0's auc: 0.866603\n",
      "[4000]\tvalid_0's auc: 0.875606\n",
      "[5000]\tvalid_0's auc: 0.881461\n",
      "[6000]\tvalid_0's auc: 0.885577\n",
      "[7000]\tvalid_0's auc: 0.888611\n",
      "[8000]\tvalid_0's auc: 0.891203\n",
      "[9000]\tvalid_0's auc: 0.893161\n",
      "[10000]\tvalid_0's auc: 0.894724\n",
      "[11000]\tvalid_0's auc: 0.89593\n",
      "[12000]\tvalid_0's auc: 0.896939\n",
      "[13000]\tvalid_0's auc: 0.897736\n",
      "[14000]\tvalid_0's auc: 0.898408\n",
      "[15000]\tvalid_0's auc: 0.898997\n",
      "[16000]\tvalid_0's auc: 0.899502\n",
      "[17000]\tvalid_0's auc: 0.899966\n",
      "[18000]\tvalid_0's auc: 0.900325\n",
      "[19000]\tvalid_0's auc: 0.900665\n",
      "[20000]\tvalid_0's auc: 0.90093\n",
      "[21000]\tvalid_0's auc: 0.901209\n",
      "[22000]\tvalid_0's auc: 0.901409\n",
      "[23000]\tvalid_0's auc: 0.901567\n",
      "[24000]\tvalid_0's auc: 0.901709\n",
      "[25000]\tvalid_0's auc: 0.901827\n",
      "[26000]\tvalid_0's auc: 0.901947\n",
      "[27000]\tvalid_0's auc: 0.902049\n",
      "[28000]\tvalid_0's auc: 0.902158\n",
      "[29000]\tvalid_0's auc: 0.902238\n",
      "[30000]\tvalid_0's auc: 0.902321\n",
      "[31000]\tvalid_0's auc: 0.90242\n",
      "[32000]\tvalid_0's auc: 0.902477\n",
      "[33000]\tvalid_0's auc: 0.902531\n",
      "[34000]\tvalid_0's auc: 0.902593\n",
      "[35000]\tvalid_0's auc: 0.902635\n",
      "[36000]\tvalid_0's auc: 0.902661\n",
      "[37000]\tvalid_0's auc: 0.902697\n",
      "[38000]\tvalid_0's auc: 0.902709\n",
      "[39000]\tvalid_0's auc: 0.902718\n",
      "[40000]\tvalid_0's auc: 0.902728\n",
      "Early stopping, best iteration is:\n",
      "[39836]\tvalid_0's auc: 0.902749\n",
      "Fold 7 started at Thu Mar  7 00:20:20 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.813285\n",
      "[2000]\tvalid_0's auc: 0.844154\n",
      "[3000]\tvalid_0's auc: 0.858815\n",
      "[4000]\tvalid_0's auc: 0.867389\n",
      "[5000]\tvalid_0's auc: 0.872762\n",
      "[6000]\tvalid_0's auc: 0.876553\n",
      "[7000]\tvalid_0's auc: 0.879149\n",
      "[8000]\tvalid_0's auc: 0.881227\n",
      "[9000]\tvalid_0's auc: 0.882946\n",
      "[10000]\tvalid_0's auc: 0.884281\n",
      "[11000]\tvalid_0's auc: 0.885386\n",
      "[12000]\tvalid_0's auc: 0.886291\n",
      "[13000]\tvalid_0's auc: 0.887099\n",
      "[14000]\tvalid_0's auc: 0.88777\n",
      "[15000]\tvalid_0's auc: 0.888319\n",
      "[16000]\tvalid_0's auc: 0.888825\n",
      "[17000]\tvalid_0's auc: 0.889266\n",
      "[18000]\tvalid_0's auc: 0.889631\n",
      "[19000]\tvalid_0's auc: 0.889999\n",
      "[20000]\tvalid_0's auc: 0.890317\n",
      "[21000]\tvalid_0's auc: 0.890592\n",
      "[22000]\tvalid_0's auc: 0.890862\n",
      "[23000]\tvalid_0's auc: 0.891094\n",
      "[24000]\tvalid_0's auc: 0.891273\n",
      "[25000]\tvalid_0's auc: 0.891466\n",
      "[26000]\tvalid_0's auc: 0.89163\n",
      "[27000]\tvalid_0's auc: 0.891776\n",
      "[28000]\tvalid_0's auc: 0.89189\n",
      "[29000]\tvalid_0's auc: 0.891968\n",
      "[30000]\tvalid_0's auc: 0.892071\n",
      "[31000]\tvalid_0's auc: 0.892166\n",
      "[32000]\tvalid_0's auc: 0.892228\n",
      "[33000]\tvalid_0's auc: 0.892315\n",
      "[34000]\tvalid_0's auc: 0.8924\n",
      "[35000]\tvalid_0's auc: 0.892455\n",
      "[36000]\tvalid_0's auc: 0.892521\n",
      "[37000]\tvalid_0's auc: 0.892566\n",
      "[38000]\tvalid_0's auc: 0.892619\n",
      "[39000]\tvalid_0's auc: 0.892669\n",
      "[40000]\tvalid_0's auc: 0.892716\n",
      "[41000]\tvalid_0's auc: 0.892758\n",
      "[42000]\tvalid_0's auc: 0.892782\n",
      "[43000]\tvalid_0's auc: 0.892828\n",
      "[44000]\tvalid_0's auc: 0.892871\n",
      "[45000]\tvalid_0's auc: 0.892897\n",
      "[46000]\tvalid_0's auc: 0.89294\n",
      "[47000]\tvalid_0's auc: 0.892982\n",
      "[48000]\tvalid_0's auc: 0.893008\n",
      "[49000]\tvalid_0's auc: 0.893035\n",
      "[50000]\tvalid_0's auc: 0.89307\n",
      "[51000]\tvalid_0's auc: 0.893111\n",
      "[52000]\tvalid_0's auc: 0.893131\n",
      "[53000]\tvalid_0's auc: 0.893149\n",
      "[54000]\tvalid_0's auc: 0.893197\n",
      "[55000]\tvalid_0's auc: 0.893224\n",
      "[56000]\tvalid_0's auc: 0.893255\n",
      "[57000]\tvalid_0's auc: 0.893285\n",
      "[58000]\tvalid_0's auc: 0.893316\n",
      "[59000]\tvalid_0's auc: 0.893343\n",
      "[60000]\tvalid_0's auc: 0.893365\n",
      "[61000]\tvalid_0's auc: 0.893386\n",
      "[62000]\tvalid_0's auc: 0.893396\n",
      "[63000]\tvalid_0's auc: 0.893432\n",
      "[64000]\tvalid_0's auc: 0.893455\n",
      "[65000]\tvalid_0's auc: 0.893469\n",
      "[66000]\tvalid_0's auc: 0.893492\n",
      "[67000]\tvalid_0's auc: 0.893497\n",
      "[68000]\tvalid_0's auc: 0.893527\n",
      "[69000]\tvalid_0's auc: 0.893548\n",
      "[70000]\tvalid_0's auc: 0.893576\n",
      "[71000]\tvalid_0's auc: 0.89359\n",
      "[72000]\tvalid_0's auc: 0.893611\n",
      "[73000]\tvalid_0's auc: 0.893637\n",
      "[74000]\tvalid_0's auc: 0.893658\n",
      "[75000]\tvalid_0's auc: 0.893669\n",
      "[76000]\tvalid_0's auc: 0.893676\n",
      "[77000]\tvalid_0's auc: 0.893697\n",
      "[78000]\tvalid_0's auc: 0.893716\n",
      "[79000]\tvalid_0's auc: 0.893728\n",
      "[80000]\tvalid_0's auc: 0.893737\n",
      "[81000]\tvalid_0's auc: 0.893754\n",
      "[82000]\tvalid_0's auc: 0.893753\n",
      "[83000]\tvalid_0's auc: 0.893763\n",
      "[84000]\tvalid_0's auc: 0.893778\n",
      "[85000]\tvalid_0's auc: 0.893778\n",
      "Early stopping, best iteration is:\n",
      "[84057]\tvalid_0's auc: 0.893782\n",
      "Fold 8 started at Thu Mar  7 01:11:41 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.817082\n",
      "[2000]\tvalid_0's auc: 0.845211\n",
      "[3000]\tvalid_0's auc: 0.858671\n",
      "[4000]\tvalid_0's auc: 0.866796\n",
      "[5000]\tvalid_0's auc: 0.872571\n",
      "[6000]\tvalid_0's auc: 0.877033\n",
      "[7000]\tvalid_0's auc: 0.88049\n",
      "[8000]\tvalid_0's auc: 0.883148\n",
      "[9000]\tvalid_0's auc: 0.885373\n",
      "[10000]\tvalid_0's auc: 0.887064\n",
      "[11000]\tvalid_0's auc: 0.888425\n",
      "[12000]\tvalid_0's auc: 0.889566\n",
      "[13000]\tvalid_0's auc: 0.89046\n",
      "[14000]\tvalid_0's auc: 0.891213\n",
      "[15000]\tvalid_0's auc: 0.891798\n",
      "[16000]\tvalid_0's auc: 0.892313\n",
      "[17000]\tvalid_0's auc: 0.892689\n",
      "[18000]\tvalid_0's auc: 0.893066\n",
      "[19000]\tvalid_0's auc: 0.893349\n",
      "[20000]\tvalid_0's auc: 0.893639\n",
      "[21000]\tvalid_0's auc: 0.893811\n",
      "[22000]\tvalid_0's auc: 0.89403\n",
      "[23000]\tvalid_0's auc: 0.89419\n",
      "[24000]\tvalid_0's auc: 0.894329\n",
      "[25000]\tvalid_0's auc: 0.894441\n",
      "[26000]\tvalid_0's auc: 0.894568\n",
      "[27000]\tvalid_0's auc: 0.894668\n",
      "[28000]\tvalid_0's auc: 0.894758\n",
      "[29000]\tvalid_0's auc: 0.89485\n",
      "[30000]\tvalid_0's auc: 0.894948\n",
      "[31000]\tvalid_0's auc: 0.89502\n",
      "[32000]\tvalid_0's auc: 0.895104\n",
      "[33000]\tvalid_0's auc: 0.895153\n",
      "[34000]\tvalid_0's auc: 0.895201\n",
      "[35000]\tvalid_0's auc: 0.895236\n",
      "[36000]\tvalid_0's auc: 0.895281\n",
      "[37000]\tvalid_0's auc: 0.89528\n",
      "Early stopping, best iteration is:\n",
      "[36881]\tvalid_0's auc: 0.895291\n",
      "Fold 9 started at Thu Mar  7 01:37:26 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.824569\n",
      "[2000]\tvalid_0's auc: 0.850604\n",
      "[3000]\tvalid_0's auc: 0.863342\n",
      "[4000]\tvalid_0's auc: 0.870976\n",
      "[5000]\tvalid_0's auc: 0.876548\n",
      "[6000]\tvalid_0's auc: 0.880506\n",
      "[7000]\tvalid_0's auc: 0.88332\n",
      "[8000]\tvalid_0's auc: 0.885358\n",
      "[9000]\tvalid_0's auc: 0.887114\n",
      "[10000]\tvalid_0's auc: 0.888536\n",
      "[11000]\tvalid_0's auc: 0.889671\n",
      "[12000]\tvalid_0's auc: 0.890584\n",
      "[13000]\tvalid_0's auc: 0.891366\n",
      "[14000]\tvalid_0's auc: 0.892017\n",
      "[15000]\tvalid_0's auc: 0.892539\n",
      "[16000]\tvalid_0's auc: 0.893006\n",
      "[17000]\tvalid_0's auc: 0.893367\n",
      "[18000]\tvalid_0's auc: 0.893702\n",
      "[19000]\tvalid_0's auc: 0.893979\n",
      "[20000]\tvalid_0's auc: 0.89421\n",
      "[21000]\tvalid_0's auc: 0.894417\n",
      "[22000]\tvalid_0's auc: 0.894541\n",
      "[23000]\tvalid_0's auc: 0.894671\n",
      "[24000]\tvalid_0's auc: 0.894786\n",
      "[25000]\tvalid_0's auc: 0.894872\n",
      "[26000]\tvalid_0's auc: 0.894946\n",
      "[27000]\tvalid_0's auc: 0.895008\n",
      "[28000]\tvalid_0's auc: 0.895063\n",
      "[29000]\tvalid_0's auc: 0.89507\n",
      "[30000]\tvalid_0's auc: 0.895118\n",
      "[31000]\tvalid_0's auc: 0.895106\n",
      "Early stopping, best iteration is:\n",
      "[30770]\tvalid_0's auc: 0.895127\n",
      "CV mean score: 0.8991, std: 0.0040.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X2, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_lgb_eli5_showp_1_10_1', oof)\n",
    "np.save('../cache/preds_lgb_eli5_showp_1_10_1', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb      cat2      lgb2\n",
      "0  0.010545  0.010837  0.008313  0.011734  0.010545\n",
      "1  0.455996  0.423705  0.392034  0.468508  0.455996\n",
      "2  0.004485  0.003918  0.004919  0.004418  0.004485\n",
      "3  0.243523  0.253600  0.340744  0.263807  0.243523\n",
      "4  0.097492  0.088431  0.110311  0.092751  0.097492\n",
      "0.92555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "\n",
    "oof_cat2 = np.load('../cache/oof_cat_quant_bin_1_10_1.npy')\n",
    "preds_cat2= np.load('../cache/preds_cat_quant_bin_1_10_1.npy')\n",
    "stage2['cat2'] = oof_cat2[0]\n",
    "stage2_test['cat2'] = preds_cat2[0]\n",
    "\n",
    "stage2['lgb2'] = oof[0]\n",
    "stage2_test['lgb2'] = preds[0]\n",
    "\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12h.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.826045\n",
      "[2000]\tvalid_0's auc: 0.853949\n",
      "[3000]\tvalid_0's auc: 0.867037\n",
      "[4000]\tvalid_0's auc: 0.874934\n",
      "[5000]\tvalid_0's auc: 0.88029\n",
      "[6000]\tvalid_0's auc: 0.884068\n",
      "[7000]\tvalid_0's auc: 0.886903\n",
      "[8000]\tvalid_0's auc: 0.889136\n",
      "[9000]\tvalid_0's auc: 0.890871\n",
      "[10000]\tvalid_0's auc: 0.892171\n",
      "[11000]\tvalid_0's auc: 0.893202\n",
      "[12000]\tvalid_0's auc: 0.894073\n",
      "[13000]\tvalid_0's auc: 0.89481\n",
      "[14000]\tvalid_0's auc: 0.895409\n",
      "[15000]\tvalid_0's auc: 0.895924\n",
      "[16000]\tvalid_0's auc: 0.896362\n",
      "[17000]\tvalid_0's auc: 0.896711\n",
      "[18000]\tvalid_0's auc: 0.896999\n",
      "[19000]\tvalid_0's auc: 0.89729\n",
      "[20000]\tvalid_0's auc: 0.897507\n",
      "[21000]\tvalid_0's auc: 0.897715\n",
      "[22000]\tvalid_0's auc: 0.897883\n",
      "[23000]\tvalid_0's auc: 0.898022\n",
      "[24000]\tvalid_0's auc: 0.898173\n",
      "[25000]\tvalid_0's auc: 0.898285\n",
      "[26000]\tvalid_0's auc: 0.898394\n",
      "[27000]\tvalid_0's auc: 0.898471\n",
      "[28000]\tvalid_0's auc: 0.898569\n",
      "[29000]\tvalid_0's auc: 0.898655\n",
      "[30000]\tvalid_0's auc: 0.898724\n",
      "[31000]\tvalid_0's auc: 0.898777\n",
      "[32000]\tvalid_0's auc: 0.898811\n",
      "[33000]\tvalid_0's auc: 0.898843\n",
      "[34000]\tvalid_0's auc: 0.898863\n",
      "[35000]\tvalid_0's auc: 0.898883\n",
      "[36000]\tvalid_0's auc: 0.898902\n",
      "[37000]\tvalid_0's auc: 0.898909\n",
      "[38000]\tvalid_0's auc: 0.898923\n",
      "[39000]\tvalid_0's auc: 0.898923\n",
      "Early stopping, best iteration is:\n",
      "[38204]\tvalid_0's auc: 0.898934\n"
     ]
    }
   ],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from eli5 import show_prediction\n",
    "np.random.seed(123)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X1, y, test_size = 0.2, random_state=42)\n",
    "model = lgb.LGBMClassifier(\n",
    "                 max_depth=-1,\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate=0.02,\n",
    "                 colsample_bytree=0.3,\n",
    "                 num_leaves=2,\n",
    "                 metric='auc',\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "            \n",
    "model.fit(X_train, y_train, \n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=1000, \n",
    "    early_stopping_rounds=1000)\n",
    "base_features = list(X1.columns)\n",
    "# perm = PermutationImportance(model, random_state=1).fit(X_val, y_val)\n",
    "# ?? show_prediction\n",
    "eli5.show_prediction(model, X_val.iloc[1], targets=[0, 1], feature_names=list(X_train.columns))\n",
    "df = eli5.formatters.as_dataframe.explain_prediction_df(model, X_val.iloc[1])\n",
    "df_neg = df.loc[df.weight < 0]\n",
    "X3 = X1.copy()\n",
    "for idx, row in df_neg.iterrows():\n",
    "    target = row['target']\n",
    "    feature = row['feature']\n",
    "    value = row['value']\n",
    "    X3[feature] = X3[feature] - value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Thu Mar  7 07:48:00 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.826999\n",
      "[2000]\tvalid_0's auc: 0.855674\n",
      "[3000]\tvalid_0's auc: 0.869226\n",
      "[4000]\tvalid_0's auc: 0.87721\n",
      "[5000]\tvalid_0's auc: 0.882488\n",
      "[6000]\tvalid_0's auc: 0.885973\n",
      "[7000]\tvalid_0's auc: 0.888545\n",
      "[8000]\tvalid_0's auc: 0.890578\n",
      "[9000]\tvalid_0's auc: 0.892228\n",
      "[10000]\tvalid_0's auc: 0.893563\n",
      "[11000]\tvalid_0's auc: 0.894568\n",
      "[12000]\tvalid_0's auc: 0.895473\n",
      "[13000]\tvalid_0's auc: 0.896164\n",
      "[14000]\tvalid_0's auc: 0.896733\n",
      "[15000]\tvalid_0's auc: 0.89721\n",
      "[16000]\tvalid_0's auc: 0.897651\n",
      "[17000]\tvalid_0's auc: 0.898011\n",
      "[18000]\tvalid_0's auc: 0.898322\n",
      "[19000]\tvalid_0's auc: 0.898555\n",
      "[20000]\tvalid_0's auc: 0.898778\n",
      "[21000]\tvalid_0's auc: 0.898947\n",
      "[22000]\tvalid_0's auc: 0.899069\n",
      "[23000]\tvalid_0's auc: 0.899207\n",
      "[24000]\tvalid_0's auc: 0.899319\n",
      "[25000]\tvalid_0's auc: 0.899433\n",
      "[26000]\tvalid_0's auc: 0.899535\n",
      "[27000]\tvalid_0's auc: 0.89959\n",
      "[28000]\tvalid_0's auc: 0.89968\n",
      "[29000]\tvalid_0's auc: 0.899733\n",
      "[30000]\tvalid_0's auc: 0.899788\n",
      "[31000]\tvalid_0's auc: 0.899828\n",
      "[32000]\tvalid_0's auc: 0.899853\n",
      "[33000]\tvalid_0's auc: 0.899878\n",
      "[34000]\tvalid_0's auc: 0.899909\n",
      "[35000]\tvalid_0's auc: 0.899939\n",
      "[36000]\tvalid_0's auc: 0.899953\n",
      "[37000]\tvalid_0's auc: 0.899977\n",
      "[38000]\tvalid_0's auc: 0.899994\n",
      "Early stopping, best iteration is:\n",
      "[37870]\tvalid_0's auc: 0.900009\n",
      "Fold 1 started at Thu Mar  7 08:08:06 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.821791\n",
      "[2000]\tvalid_0's auc: 0.850906\n",
      "[3000]\tvalid_0's auc: 0.864832\n",
      "[4000]\tvalid_0's auc: 0.873247\n",
      "[5000]\tvalid_0's auc: 0.878486\n",
      "[6000]\tvalid_0's auc: 0.88251\n",
      "[7000]\tvalid_0's auc: 0.885432\n",
      "[8000]\tvalid_0's auc: 0.887792\n",
      "[9000]\tvalid_0's auc: 0.889595\n",
      "[10000]\tvalid_0's auc: 0.891036\n",
      "[11000]\tvalid_0's auc: 0.892152\n",
      "[12000]\tvalid_0's auc: 0.893175\n",
      "[13000]\tvalid_0's auc: 0.893957\n",
      "[14000]\tvalid_0's auc: 0.894531\n",
      "[15000]\tvalid_0's auc: 0.895087\n",
      "[16000]\tvalid_0's auc: 0.895533\n",
      "[17000]\tvalid_0's auc: 0.895924\n",
      "[18000]\tvalid_0's auc: 0.896275\n",
      "[19000]\tvalid_0's auc: 0.896571\n",
      "[20000]\tvalid_0's auc: 0.896805\n",
      "[21000]\tvalid_0's auc: 0.896985\n",
      "[22000]\tvalid_0's auc: 0.897188\n",
      "[23000]\tvalid_0's auc: 0.897322\n",
      "[24000]\tvalid_0's auc: 0.897419\n",
      "[25000]\tvalid_0's auc: 0.897553\n",
      "[26000]\tvalid_0's auc: 0.897628\n",
      "[27000]\tvalid_0's auc: 0.897729\n",
      "[28000]\tvalid_0's auc: 0.897806\n",
      "[29000]\tvalid_0's auc: 0.897895\n",
      "[30000]\tvalid_0's auc: 0.897958\n",
      "[31000]\tvalid_0's auc: 0.898017\n",
      "[32000]\tvalid_0's auc: 0.898061\n",
      "[33000]\tvalid_0's auc: 0.898091\n",
      "[34000]\tvalid_0's auc: 0.898122\n",
      "[35000]\tvalid_0's auc: 0.898144\n",
      "[36000]\tvalid_0's auc: 0.898154\n",
      "Early stopping, best iteration is:\n",
      "[35590]\tvalid_0's auc: 0.898165\n",
      "Fold 2 started at Thu Mar  7 08:33:04 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.827785\n",
      "[2000]\tvalid_0's auc: 0.855993\n",
      "[3000]\tvalid_0's auc: 0.869313\n",
      "[4000]\tvalid_0's auc: 0.877364\n",
      "[5000]\tvalid_0's auc: 0.88242\n",
      "[6000]\tvalid_0's auc: 0.886014\n",
      "[7000]\tvalid_0's auc: 0.888726\n",
      "[8000]\tvalid_0's auc: 0.890606\n",
      "[9000]\tvalid_0's auc: 0.892199\n",
      "[10000]\tvalid_0's auc: 0.893382\n",
      "[11000]\tvalid_0's auc: 0.894428\n",
      "[12000]\tvalid_0's auc: 0.895186\n",
      "[13000]\tvalid_0's auc: 0.895854\n",
      "[14000]\tvalid_0's auc: 0.896342\n",
      "[15000]\tvalid_0's auc: 0.8968\n",
      "[16000]\tvalid_0's auc: 0.897128\n",
      "[17000]\tvalid_0's auc: 0.897381\n",
      "[18000]\tvalid_0's auc: 0.897639\n",
      "[19000]\tvalid_0's auc: 0.897826\n",
      "[20000]\tvalid_0's auc: 0.897999\n",
      "[21000]\tvalid_0's auc: 0.898129\n",
      "[22000]\tvalid_0's auc: 0.898275\n",
      "[23000]\tvalid_0's auc: 0.89834\n",
      "[24000]\tvalid_0's auc: 0.898431\n",
      "[25000]\tvalid_0's auc: 0.898532\n",
      "[26000]\tvalid_0's auc: 0.898594\n",
      "[27000]\tvalid_0's auc: 0.898684\n",
      "[28000]\tvalid_0's auc: 0.898754\n",
      "[29000]\tvalid_0's auc: 0.898809\n",
      "[30000]\tvalid_0's auc: 0.898869\n",
      "[31000]\tvalid_0's auc: 0.898918\n",
      "[32000]\tvalid_0's auc: 0.898934\n",
      "[33000]\tvalid_0's auc: 0.898945\n",
      "[34000]\tvalid_0's auc: 0.898972\n",
      "[35000]\tvalid_0's auc: 0.898976\n",
      "[36000]\tvalid_0's auc: 0.898989\n",
      "Early stopping, best iteration is:\n",
      "[35797]\tvalid_0's auc: 0.898999\n",
      "Fold 3 started at Thu Mar  7 08:58:18 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.825319\n",
      "[2000]\tvalid_0's auc: 0.855545\n",
      "[3000]\tvalid_0's auc: 0.869655\n",
      "[4000]\tvalid_0's auc: 0.877623\n",
      "[5000]\tvalid_0's auc: 0.882803\n",
      "[6000]\tvalid_0's auc: 0.886443\n",
      "[7000]\tvalid_0's auc: 0.889472\n",
      "[8000]\tvalid_0's auc: 0.891693\n",
      "[9000]\tvalid_0's auc: 0.893573\n",
      "[10000]\tvalid_0's auc: 0.894934\n",
      "[11000]\tvalid_0's auc: 0.896045\n",
      "[12000]\tvalid_0's auc: 0.89695\n",
      "[13000]\tvalid_0's auc: 0.897722\n",
      "[14000]\tvalid_0's auc: 0.898391\n",
      "[15000]\tvalid_0's auc: 0.898947\n",
      "[16000]\tvalid_0's auc: 0.899334\n",
      "[17000]\tvalid_0's auc: 0.899695\n",
      "[18000]\tvalid_0's auc: 0.899968\n",
      "[19000]\tvalid_0's auc: 0.900202\n",
      "[20000]\tvalid_0's auc: 0.900425\n",
      "[21000]\tvalid_0's auc: 0.900582\n",
      "[22000]\tvalid_0's auc: 0.900746\n",
      "[23000]\tvalid_0's auc: 0.900834\n",
      "[24000]\tvalid_0's auc: 0.900928\n",
      "[25000]\tvalid_0's auc: 0.900995\n",
      "[26000]\tvalid_0's auc: 0.901073\n",
      "[27000]\tvalid_0's auc: 0.901113\n",
      "[28000]\tvalid_0's auc: 0.901155\n",
      "[29000]\tvalid_0's auc: 0.901205\n",
      "[30000]\tvalid_0's auc: 0.901275\n",
      "[31000]\tvalid_0's auc: 0.901304\n",
      "[32000]\tvalid_0's auc: 0.901342\n",
      "[33000]\tvalid_0's auc: 0.901385\n",
      "[34000]\tvalid_0's auc: 0.901395\n",
      "[35000]\tvalid_0's auc: 0.901436\n",
      "[36000]\tvalid_0's auc: 0.901467\n",
      "[37000]\tvalid_0's auc: 0.901501\n",
      "[38000]\tvalid_0's auc: 0.90149\n",
      "Early stopping, best iteration is:\n",
      "[37059]\tvalid_0's auc: 0.901505\n",
      "Fold 4 started at Thu Mar  7 09:24:14 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.827947\n",
      "[2000]\tvalid_0's auc: 0.857134\n",
      "[3000]\tvalid_0's auc: 0.871795\n",
      "[4000]\tvalid_0's auc: 0.880953\n",
      "[5000]\tvalid_0's auc: 0.886905\n",
      "[6000]\tvalid_0's auc: 0.891184\n",
      "[7000]\tvalid_0's auc: 0.89422\n",
      "[8000]\tvalid_0's auc: 0.896751\n",
      "[9000]\tvalid_0's auc: 0.898777\n",
      "[10000]\tvalid_0's auc: 0.900416\n",
      "[11000]\tvalid_0's auc: 0.901774\n",
      "[12000]\tvalid_0's auc: 0.902874\n",
      "[13000]\tvalid_0's auc: 0.903729\n",
      "[14000]\tvalid_0's auc: 0.904453\n",
      "[15000]\tvalid_0's auc: 0.90504\n",
      "[16000]\tvalid_0's auc: 0.905537\n",
      "[17000]\tvalid_0's auc: 0.905942\n",
      "[18000]\tvalid_0's auc: 0.906303\n",
      "[19000]\tvalid_0's auc: 0.906588\n",
      "[20000]\tvalid_0's auc: 0.906822\n",
      "[21000]\tvalid_0's auc: 0.907048\n",
      "[22000]\tvalid_0's auc: 0.907245\n",
      "[23000]\tvalid_0's auc: 0.907419\n",
      "[24000]\tvalid_0's auc: 0.907533\n",
      "[25000]\tvalid_0's auc: 0.907646\n",
      "[26000]\tvalid_0's auc: 0.907731\n",
      "[27000]\tvalid_0's auc: 0.907836\n",
      "[28000]\tvalid_0's auc: 0.907893\n",
      "[29000]\tvalid_0's auc: 0.907953\n",
      "[30000]\tvalid_0's auc: 0.908003\n",
      "[31000]\tvalid_0's auc: 0.908038\n",
      "[32000]\tvalid_0's auc: 0.908078\n",
      "[33000]\tvalid_0's auc: 0.908095\n",
      "[34000]\tvalid_0's auc: 0.908099\n",
      "[35000]\tvalid_0's auc: 0.908139\n",
      "[36000]\tvalid_0's auc: 0.90816\n",
      "[37000]\tvalid_0's auc: 0.908182\n",
      "[38000]\tvalid_0's auc: 0.908186\n",
      "[39000]\tvalid_0's auc: 0.908206\n",
      "[40000]\tvalid_0's auc: 0.908224\n",
      "[41000]\tvalid_0's auc: 0.90822\n",
      "[42000]\tvalid_0's auc: 0.90824\n",
      "[43000]\tvalid_0's auc: 0.908235\n",
      "[44000]\tvalid_0's auc: 0.908236\n",
      "Early stopping, best iteration is:\n",
      "[43038]\tvalid_0's auc: 0.908245\n",
      "Fold 5 started at Thu Mar  7 09:53:06 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.825979\n",
      "[2000]\tvalid_0's auc: 0.854772\n",
      "[3000]\tvalid_0's auc: 0.868916\n",
      "[4000]\tvalid_0's auc: 0.877982\n",
      "[5000]\tvalid_0's auc: 0.883581\n",
      "[6000]\tvalid_0's auc: 0.887465\n",
      "[7000]\tvalid_0's auc: 0.890307\n",
      "[8000]\tvalid_0's auc: 0.892715\n",
      "[9000]\tvalid_0's auc: 0.894543\n",
      "[10000]\tvalid_0's auc: 0.896021\n",
      "[11000]\tvalid_0's auc: 0.897215\n",
      "[12000]\tvalid_0's auc: 0.898138\n",
      "[13000]\tvalid_0's auc: 0.898961\n",
      "[14000]\tvalid_0's auc: 0.899613\n",
      "[15000]\tvalid_0's auc: 0.900116\n",
      "[16000]\tvalid_0's auc: 0.900591\n",
      "[17000]\tvalid_0's auc: 0.900931\n",
      "[18000]\tvalid_0's auc: 0.901241\n",
      "[19000]\tvalid_0's auc: 0.90152\n",
      "[20000]\tvalid_0's auc: 0.901726\n",
      "[21000]\tvalid_0's auc: 0.901903\n",
      "[22000]\tvalid_0's auc: 0.902014\n",
      "[23000]\tvalid_0's auc: 0.902163\n",
      "[24000]\tvalid_0's auc: 0.902286\n",
      "[25000]\tvalid_0's auc: 0.902392\n",
      "[26000]\tvalid_0's auc: 0.902473\n",
      "[27000]\tvalid_0's auc: 0.902548\n",
      "[28000]\tvalid_0's auc: 0.902599\n",
      "[29000]\tvalid_0's auc: 0.902657\n",
      "[30000]\tvalid_0's auc: 0.902731\n",
      "[31000]\tvalid_0's auc: 0.90279\n",
      "[32000]\tvalid_0's auc: 0.902827\n",
      "[33000]\tvalid_0's auc: 0.902875\n",
      "[34000]\tvalid_0's auc: 0.902908\n",
      "[35000]\tvalid_0's auc: 0.902923\n",
      "[36000]\tvalid_0's auc: 0.902941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[35833]\tvalid_0's auc: 0.902945\n",
      "Fold 6 started at Thu Mar  7 10:17:43 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.822454\n",
      "[2000]\tvalid_0's auc: 0.852226\n",
      "[3000]\tvalid_0's auc: 0.866618\n",
      "[4000]\tvalid_0's auc: 0.875635\n",
      "[5000]\tvalid_0's auc: 0.881388\n",
      "[6000]\tvalid_0's auc: 0.885595\n",
      "[7000]\tvalid_0's auc: 0.888777\n",
      "[8000]\tvalid_0's auc: 0.891246\n",
      "[9000]\tvalid_0's auc: 0.893245\n",
      "[10000]\tvalid_0's auc: 0.894835\n",
      "[11000]\tvalid_0's auc: 0.896104\n",
      "[12000]\tvalid_0's auc: 0.897123\n",
      "[13000]\tvalid_0's auc: 0.897863\n",
      "[14000]\tvalid_0's auc: 0.89855\n",
      "[15000]\tvalid_0's auc: 0.899155\n",
      "[16000]\tvalid_0's auc: 0.899622\n",
      "[17000]\tvalid_0's auc: 0.900078\n",
      "[18000]\tvalid_0's auc: 0.900473\n",
      "[19000]\tvalid_0's auc: 0.900826\n",
      "[20000]\tvalid_0's auc: 0.901104\n",
      "[21000]\tvalid_0's auc: 0.901346\n",
      "[22000]\tvalid_0's auc: 0.901555\n",
      "[23000]\tvalid_0's auc: 0.90174\n",
      "[24000]\tvalid_0's auc: 0.901874\n",
      "[25000]\tvalid_0's auc: 0.902004\n",
      "[26000]\tvalid_0's auc: 0.902116\n",
      "[27000]\tvalid_0's auc: 0.902219\n",
      "[28000]\tvalid_0's auc: 0.90234\n",
      "[29000]\tvalid_0's auc: 0.90242\n",
      "[30000]\tvalid_0's auc: 0.902507\n",
      "[31000]\tvalid_0's auc: 0.902607\n",
      "[32000]\tvalid_0's auc: 0.902685\n",
      "[33000]\tvalid_0's auc: 0.902723\n",
      "[34000]\tvalid_0's auc: 0.902773\n",
      "[35000]\tvalid_0's auc: 0.902809\n",
      "[36000]\tvalid_0's auc: 0.902838\n",
      "[37000]\tvalid_0's auc: 0.902836\n",
      "Early stopping, best iteration is:\n",
      "[36461]\tvalid_0's auc: 0.902851\n",
      "Fold 7 started at Thu Mar  7 10:41:42 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.81563\n",
      "[2000]\tvalid_0's auc: 0.846341\n",
      "[3000]\tvalid_0's auc: 0.861068\n",
      "[4000]\tvalid_0's auc: 0.869751\n",
      "[5000]\tvalid_0's auc: 0.875207\n",
      "[6000]\tvalid_0's auc: 0.878797\n",
      "[7000]\tvalid_0's auc: 0.881539\n",
      "[8000]\tvalid_0's auc: 0.883623\n",
      "[9000]\tvalid_0's auc: 0.885296\n",
      "[10000]\tvalid_0's auc: 0.886554\n",
      "[11000]\tvalid_0's auc: 0.887655\n",
      "[12000]\tvalid_0's auc: 0.888569\n",
      "[13000]\tvalid_0's auc: 0.889309\n",
      "[14000]\tvalid_0's auc: 0.889941\n",
      "[15000]\tvalid_0's auc: 0.890446\n",
      "[16000]\tvalid_0's auc: 0.890945\n",
      "[17000]\tvalid_0's auc: 0.891365\n",
      "[18000]\tvalid_0's auc: 0.891741\n",
      "[19000]\tvalid_0's auc: 0.89205\n",
      "[20000]\tvalid_0's auc: 0.89236\n",
      "[21000]\tvalid_0's auc: 0.892612\n",
      "[22000]\tvalid_0's auc: 0.892809\n",
      "[23000]\tvalid_0's auc: 0.893015\n",
      "[24000]\tvalid_0's auc: 0.893213\n",
      "[25000]\tvalid_0's auc: 0.893351\n",
      "[26000]\tvalid_0's auc: 0.893478\n",
      "[27000]\tvalid_0's auc: 0.893604\n",
      "[28000]\tvalid_0's auc: 0.893721\n",
      "[29000]\tvalid_0's auc: 0.893828\n",
      "[30000]\tvalid_0's auc: 0.893901\n",
      "[31000]\tvalid_0's auc: 0.893999\n",
      "[32000]\tvalid_0's auc: 0.894071\n",
      "[33000]\tvalid_0's auc: 0.89411\n",
      "[34000]\tvalid_0's auc: 0.894163\n",
      "[35000]\tvalid_0's auc: 0.89422\n",
      "[36000]\tvalid_0's auc: 0.894275\n",
      "[37000]\tvalid_0's auc: 0.894298\n",
      "[38000]\tvalid_0's auc: 0.894328\n",
      "[39000]\tvalid_0's auc: 0.894363\n",
      "[40000]\tvalid_0's auc: 0.89438\n",
      "[41000]\tvalid_0's auc: 0.894384\n",
      "[42000]\tvalid_0's auc: 0.894411\n",
      "[43000]\tvalid_0's auc: 0.894434\n",
      "[44000]\tvalid_0's auc: 0.894468\n",
      "[45000]\tvalid_0's auc: 0.89449\n",
      "[46000]\tvalid_0's auc: 0.89452\n",
      "[47000]\tvalid_0's auc: 0.894536\n",
      "[48000]\tvalid_0's auc: 0.894566\n",
      "[49000]\tvalid_0's auc: 0.894585\n",
      "[50000]\tvalid_0's auc: 0.89462\n",
      "[51000]\tvalid_0's auc: 0.89464\n",
      "[52000]\tvalid_0's auc: 0.894662\n",
      "[53000]\tvalid_0's auc: 0.894688\n",
      "[54000]\tvalid_0's auc: 0.894691\n",
      "[55000]\tvalid_0's auc: 0.894719\n",
      "[56000]\tvalid_0's auc: 0.894731\n",
      "[57000]\tvalid_0's auc: 0.894748\n",
      "[58000]\tvalid_0's auc: 0.894757\n",
      "[59000]\tvalid_0's auc: 0.894755\n",
      "Early stopping, best iteration is:\n",
      "[58331]\tvalid_0's auc: 0.894769\n",
      "Fold 8 started at Thu Mar  7 11:20:19 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.817579\n",
      "[2000]\tvalid_0's auc: 0.845556\n",
      "[3000]\tvalid_0's auc: 0.858938\n",
      "[4000]\tvalid_0's auc: 0.867123\n",
      "[5000]\tvalid_0's auc: 0.872874\n",
      "[6000]\tvalid_0's auc: 0.877241\n",
      "[7000]\tvalid_0's auc: 0.880657\n",
      "[8000]\tvalid_0's auc: 0.883373\n",
      "[9000]\tvalid_0's auc: 0.885611\n",
      "[10000]\tvalid_0's auc: 0.887324\n",
      "[11000]\tvalid_0's auc: 0.888697\n",
      "[12000]\tvalid_0's auc: 0.88985\n",
      "[13000]\tvalid_0's auc: 0.890741\n",
      "[14000]\tvalid_0's auc: 0.891503\n",
      "[15000]\tvalid_0's auc: 0.89208\n",
      "[16000]\tvalid_0's auc: 0.892564\n",
      "[17000]\tvalid_0's auc: 0.89301\n",
      "[18000]\tvalid_0's auc: 0.893377\n",
      "[19000]\tvalid_0's auc: 0.893748\n",
      "[20000]\tvalid_0's auc: 0.894022\n",
      "[21000]\tvalid_0's auc: 0.89424\n",
      "[22000]\tvalid_0's auc: 0.89446\n",
      "[23000]\tvalid_0's auc: 0.894665\n",
      "[24000]\tvalid_0's auc: 0.894847\n",
      "[25000]\tvalid_0's auc: 0.894958\n",
      "[26000]\tvalid_0's auc: 0.895098\n",
      "[27000]\tvalid_0's auc: 0.895226\n",
      "[28000]\tvalid_0's auc: 0.895362\n",
      "[29000]\tvalid_0's auc: 0.895434\n",
      "[30000]\tvalid_0's auc: 0.895569\n",
      "[31000]\tvalid_0's auc: 0.895657\n",
      "[32000]\tvalid_0's auc: 0.895742\n",
      "[33000]\tvalid_0's auc: 0.895807\n",
      "[34000]\tvalid_0's auc: 0.89588\n",
      "[35000]\tvalid_0's auc: 0.895914\n",
      "[36000]\tvalid_0's auc: 0.895948\n",
      "[37000]\tvalid_0's auc: 0.895967\n",
      "[38000]\tvalid_0's auc: 0.895975\n",
      "[39000]\tvalid_0's auc: 0.895985\n",
      "[40000]\tvalid_0's auc: 0.89598\n",
      "Early stopping, best iteration is:\n",
      "[39674]\tvalid_0's auc: 0.895996\n",
      "Fold 9 started at Thu Mar  7 11:48:02 2019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.826871\n",
      "[2000]\tvalid_0's auc: 0.852722\n",
      "[3000]\tvalid_0's auc: 0.865493\n",
      "[4000]\tvalid_0's auc: 0.873262\n",
      "[5000]\tvalid_0's auc: 0.878708\n",
      "[6000]\tvalid_0's auc: 0.882632\n",
      "[7000]\tvalid_0's auc: 0.885494\n",
      "[8000]\tvalid_0's auc: 0.887658\n",
      "[9000]\tvalid_0's auc: 0.889316\n",
      "[10000]\tvalid_0's auc: 0.890735\n",
      "[11000]\tvalid_0's auc: 0.891866\n",
      "[12000]\tvalid_0's auc: 0.892779\n",
      "[13000]\tvalid_0's auc: 0.893515\n",
      "[14000]\tvalid_0's auc: 0.894156\n",
      "[15000]\tvalid_0's auc: 0.894642\n",
      "[16000]\tvalid_0's auc: 0.895087\n",
      "[17000]\tvalid_0's auc: 0.895425\n",
      "[18000]\tvalid_0's auc: 0.89576\n",
      "[19000]\tvalid_0's auc: 0.896035\n",
      "[20000]\tvalid_0's auc: 0.896246\n",
      "[21000]\tvalid_0's auc: 0.896416\n",
      "[22000]\tvalid_0's auc: 0.896573\n",
      "[23000]\tvalid_0's auc: 0.896675\n",
      "[24000]\tvalid_0's auc: 0.896769\n",
      "[25000]\tvalid_0's auc: 0.896838\n",
      "[26000]\tvalid_0's auc: 0.896896\n",
      "[27000]\tvalid_0's auc: 0.896928\n",
      "[28000]\tvalid_0's auc: 0.896961\n",
      "[29000]\tvalid_0's auc: 0.896998\n",
      "[30000]\tvalid_0's auc: 0.897033\n",
      "[31000]\tvalid_0's auc: 0.897026\n",
      "Early stopping, best iteration is:\n",
      "[30381]\tvalid_0's auc: 0.897044\n",
      "CV mean score: 0.9001, std: 0.0038.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X2, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_lgb_eli5_showp_1_10_2', oof)\n",
    "np.save('../cache/preds_lgb_eli5_showp_1_10_2', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb      cat2      lgb2\n",
      "0  0.010545  0.010837  0.008313  0.011734  0.010611\n",
      "1  0.455996  0.423705  0.392034  0.468508  0.403720\n",
      "2  0.004485  0.003918  0.004919  0.004418  0.003942\n",
      "3  0.243523  0.253600  0.340744  0.263807  0.259469\n",
      "4  0.097492  0.088431  0.110311  0.092751  0.085976\n",
      "0.925445\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "\n",
    "oof_cat2 = np.load('../cache/oof_cat_quant_bin_1_10_1.npy')\n",
    "preds_cat2= np.load('../cache/preds_cat_quant_bin_1_10_1.npy')\n",
    "stage2['cat2'] = oof_cat2[0]\n",
    "stage2_test['cat2'] = preds_cat2[0]\n",
    "\n",
    "oof = np.load('../cache/oof_lgb_eli5_showp_1_10_2.npy')\n",
    "preds = np.load('../cache/preds_lgb_eli5_showp_1_10_2.npy')\n",
    "\n",
    "\n",
    "stage2['lgb2'] = oof[0]\n",
    "stage2_test['lgb2'] = preds[0]\n",
    "\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12i.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Thu Mar  7 15:26:49 2019\n",
      "Fold 1 started at Thu Mar  7 16:16:55 2019\n",
      "Fold 2 started at Thu Mar  7 17:09:59 2019\n",
      "Fold 3 started at Thu Mar  7 18:03:59 2019\n",
      "Fold 4 started at Thu Mar  7 18:59:17 2019\n",
      "Fold 5 started at Thu Mar  7 19:51:05 2019\n",
      "Fold 6 started at Thu Mar  7 20:46:33 2019\n",
      "Fold 7 started at Thu Mar  7 21:37:19 2019\n",
      "Fold 8 started at Thu Mar  7 22:28:35 2019\n",
      "Fold 9 started at Thu Mar  7 23:17:02 2019\n",
      "CV mean score: 0.8794, std: 0.0044.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "n_estimators=20\n",
    "clf = OneVsRestClassifier(BaggingClassifier(SVC(gamma='scale', probability=True, class_weight='balanced', C=0.01), \n",
    "                                            max_samples=1.0 / n_estimators, n_estimators=n_estimators, n_jobs=-1), n_jobs=-1)\n",
    "oof_svc, prediction_svc, _ = train_model(X1, X2, y, params=np.random.randint(1,101), folds=folds,\n",
    "                                         model_type='sklearn', plot_feature_importance=False, model=clf)\n",
    "oof.append(oof_svc)\n",
    "preds.append(prediction_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_svm_1_10_1', oof)\n",
    "np.save('../cache/preds_svm_1_10_1', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb      cat2       svm\n",
      "0  0.010545  0.010837  0.008313  0.011734  0.044733\n",
      "1  0.455996  0.423705  0.392034  0.468508  0.267884\n",
      "2  0.004485  0.003918  0.004919  0.004418  0.043137\n",
      "3  0.243523  0.253600  0.340744  0.263807  0.226918\n",
      "4  0.097492  0.088431  0.110311  0.092751  0.142101\n",
      "0.92519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "\n",
    "oof_cat2 = np.load('../cache/oof_cat_quant_bin_1_10_1.npy')\n",
    "preds_cat2= np.load('../cache/preds_cat_quant_bin_1_10_1.npy')\n",
    "stage2['cat2'] = oof_cat2[0]\n",
    "stage2_test['cat2'] = preds_cat2[0]\n",
    "\n",
    "stage2['svm'] = oof_svc\n",
    "stage2_test['svm'] = prediction_svc\n",
    "\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12j.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Fri Mar  8 00:30:08 2019\n",
      "0:\ttest: 0.5418103\tbest: 0.5418103 (0)\ttotal: 50.6ms\tremaining: 14h 2m 57s\n",
      "1000:\ttest: 0.8544302\tbest: 0.8544382 (999)\ttotal: 28s\tremaining: 7h 45m 9s\n",
      "2000:\ttest: 0.8712567\tbest: 0.8712567 (2000)\ttotal: 55.2s\tremaining: 7h 39m 6s\n",
      "3000:\ttest: 0.8815427\tbest: 0.8815427 (3000)\ttotal: 1m 22s\tremaining: 7h 37m 29s\n",
      "4000:\ttest: 0.8873283\tbest: 0.8873283 (4000)\ttotal: 1m 50s\tremaining: 7h 38m 53s\n",
      "5000:\ttest: 0.8910732\tbest: 0.8910732 (5000)\ttotal: 2m 18s\tremaining: 7h 40m 18s\n",
      "6000:\ttest: 0.8936064\tbest: 0.8936064 (6000)\ttotal: 2m 47s\tremaining: 7h 41m 44s\n",
      "7000:\ttest: 0.8955560\tbest: 0.8955560 (7000)\ttotal: 3m 16s\tremaining: 7h 44m 16s\n",
      "8000:\ttest: 0.8967476\tbest: 0.8967551 (7996)\ttotal: 3m 46s\tremaining: 7h 48m 52s\n",
      "9000:\ttest: 0.8977432\tbest: 0.8977508 (8987)\ttotal: 4m 17s\tremaining: 7h 52m 52s\n",
      "10000:\ttest: 0.8985221\tbest: 0.8985221 (10000)\ttotal: 4m 50s\tremaining: 7h 59m 51s\n",
      "11000:\ttest: 0.8991185\tbest: 0.8991204 (10995)\ttotal: 5m 23s\tremaining: 8h 5m 13s\n",
      "12000:\ttest: 0.8995612\tbest: 0.8995614 (11993)\ttotal: 5m 56s\tremaining: 8h 9m 12s\n",
      "13000:\ttest: 0.8998448\tbest: 0.8998547 (12978)\ttotal: 6m 28s\tremaining: 8h 11m 14s\n",
      "14000:\ttest: 0.9000546\tbest: 0.9000593 (13997)\ttotal: 7m 1s\tremaining: 8h 14m 50s\n",
      "15000:\ttest: 0.9002405\tbest: 0.9002573 (14784)\ttotal: 7m 33s\tremaining: 8h 15m 53s\n",
      "16000:\ttest: 0.9004182\tbest: 0.9004224 (15992)\ttotal: 8m 6s\tremaining: 8h 18m 7s\n",
      "17000:\ttest: 0.9005420\tbest: 0.9005428 (16999)\ttotal: 8m 40s\tremaining: 8h 21m 10s\n",
      "18000:\ttest: 0.9006435\tbest: 0.9006787 (17837)\ttotal: 9m 12s\tremaining: 8h 22m 24s\n",
      "19000:\ttest: 0.9006889\tbest: 0.9007293 (18735)\ttotal: 9m 45s\tremaining: 8h 23m 24s\n",
      "20000:\ttest: 0.9008107\tbest: 0.9008118 (19996)\ttotal: 10m 16s\tremaining: 8h 23m 38s\n",
      "21000:\ttest: 0.9008371\tbest: 0.9008510 (20950)\ttotal: 10m 49s\tremaining: 8h 24m 26s\n",
      "22000:\ttest: 0.9009246\tbest: 0.9009346 (21976)\ttotal: 11m 21s\tremaining: 8h 24m 50s\n",
      "23000:\ttest: 0.9010015\tbest: 0.9010155 (22870)\ttotal: 11m 53s\tremaining: 8h 25m 13s\n",
      "24000:\ttest: 0.9010045\tbest: 0.9010327 (23618)\ttotal: 12m 25s\tremaining: 8h 25m 6s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9010326888\n",
      "bestIteration = 23618\n",
      "\n",
      "Shrink model to first 23619 iterations.\n",
      "Fold 1 started at Fri Mar  8 00:43:50 2019\n",
      "0:\ttest: 0.5003891\tbest: 0.5003891 (0)\ttotal: 50.2ms\tremaining: 13h 56m 1s\n",
      "1000:\ttest: 0.8475376\tbest: 0.8476772 (998)\ttotal: 30.3s\tremaining: 8h 24m\n",
      "2000:\ttest: 0.8668276\tbest: 0.8668276 (2000)\ttotal: 1m 1s\tremaining: 8h 31m 53s\n",
      "3000:\ttest: 0.8771447\tbest: 0.8771447 (3000)\ttotal: 1m 33s\tremaining: 8h 38m 40s\n",
      "4000:\ttest: 0.8836337\tbest: 0.8836439 (3998)\ttotal: 2m 6s\tremaining: 8h 44m 23s\n",
      "5000:\ttest: 0.8875125\tbest: 0.8875195 (4999)\ttotal: 2m 38s\tremaining: 8h 46m 4s\n",
      "6000:\ttest: 0.8903698\tbest: 0.8903698 (6000)\ttotal: 3m 10s\tremaining: 8h 46m 52s\n",
      "7000:\ttest: 0.8924015\tbest: 0.8924015 (7000)\ttotal: 3m 45s\tremaining: 8h 52m 6s\n",
      "8000:\ttest: 0.8939008\tbest: 0.8939046 (7996)\ttotal: 4m 18s\tremaining: 8h 53m 41s\n",
      "9000:\ttest: 0.8949176\tbest: 0.8949244 (8997)\ttotal: 4m 50s\tremaining: 8h 53m 36s\n",
      "10000:\ttest: 0.8957193\tbest: 0.8957251 (9996)\ttotal: 5m 22s\tremaining: 8h 51m 51s\n",
      "11000:\ttest: 0.8964218\tbest: 0.8964271 (10971)\ttotal: 5m 54s\tremaining: 8h 51m 7s\n",
      "12000:\ttest: 0.8969186\tbest: 0.8969208 (11995)\ttotal: 6m 26s\tremaining: 8h 50m 40s\n",
      "13000:\ttest: 0.8972495\tbest: 0.8972588 (12897)\ttotal: 6m 59s\tremaining: 8h 50m 12s\n",
      "14000:\ttest: 0.8974728\tbest: 0.8974731 (13999)\ttotal: 7m 31s\tremaining: 8h 50m 25s\n",
      "15000:\ttest: 0.8977223\tbest: 0.8977306 (14973)\ttotal: 8m 5s\tremaining: 8h 50m 46s\n",
      "16000:\ttest: 0.8978913\tbest: 0.8979001 (15957)\ttotal: 8m 37s\tremaining: 8h 50m 45s\n",
      "17000:\ttest: 0.8980563\tbest: 0.8980627 (16972)\ttotal: 9m 9s\tremaining: 8h 49m 38s\n",
      "18000:\ttest: 0.8981577\tbest: 0.8981594 (17989)\ttotal: 9m 42s\tremaining: 8h 49m 39s\n",
      "19000:\ttest: 0.8982418\tbest: 0.8982479 (18840)\ttotal: 10m 14s\tremaining: 8h 49m 5s\n",
      "20000:\ttest: 0.8982969\tbest: 0.8983434 (19586)\ttotal: 10m 48s\tremaining: 8h 49m 30s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8983434227\n",
      "bestIteration = 19586\n",
      "\n",
      "Shrink model to first 19587 iterations.\n",
      "Fold 2 started at Fri Mar  8 00:55:45 2019\n",
      "0:\ttest: 0.5243806\tbest: 0.5243806 (0)\ttotal: 63.1ms\tremaining: 17h 31m 13s\n",
      "1000:\ttest: 0.8530670\tbest: 0.8530968 (999)\ttotal: 32.8s\tremaining: 9h 5m 2s\n",
      "2000:\ttest: 0.8710605\tbest: 0.8710605 (2000)\ttotal: 1m 5s\tremaining: 9h 5m 14s\n",
      "3000:\ttest: 0.8815123\tbest: 0.8815123 (3000)\ttotal: 1m 38s\tremaining: 9h 3m 51s\n",
      "4000:\ttest: 0.8874923\tbest: 0.8874934 (3998)\ttotal: 2m 10s\tremaining: 9h 2m\n",
      "5000:\ttest: 0.8912347\tbest: 0.8912347 (5000)\ttotal: 2m 44s\tremaining: 9h 4m 42s\n",
      "6000:\ttest: 0.8934930\tbest: 0.8934930 (6000)\ttotal: 3m 17s\tremaining: 9h 5m 9s\n",
      "7000:\ttest: 0.8951724\tbest: 0.8951724 (7000)\ttotal: 3m 50s\tremaining: 9h 5m 23s\n",
      "8000:\ttest: 0.8962991\tbest: 0.8962991 (8000)\ttotal: 4m 24s\tremaining: 9h 6m 34s\n",
      "9000:\ttest: 0.8970440\tbest: 0.8970440 (9000)\ttotal: 4m 57s\tremaining: 9h 5m 56s\n",
      "10000:\ttest: 0.8976642\tbest: 0.8976786 (9966)\ttotal: 5m 30s\tremaining: 9h 5m 53s\n",
      "11000:\ttest: 0.8980826\tbest: 0.8980931 (10967)\ttotal: 6m 4s\tremaining: 9h 5m 46s\n",
      "12000:\ttest: 0.8982510\tbest: 0.8982541 (11996)\ttotal: 6m 37s\tremaining: 9h 5m 24s\n",
      "13000:\ttest: 0.8984974\tbest: 0.8985170 (12897)\ttotal: 7m 9s\tremaining: 9h 3m 57s\n",
      "14000:\ttest: 0.8987064\tbest: 0.8987106 (13998)\ttotal: 7m 43s\tremaining: 9h 3m 33s\n",
      "15000:\ttest: 0.8987525\tbest: 0.8987847 (14402)\ttotal: 8m 15s\tremaining: 9h 2m 25s\n",
      "16000:\ttest: 0.8988613\tbest: 0.8988750 (15967)\ttotal: 8m 48s\tremaining: 9h 1m 54s\n",
      "17000:\ttest: 0.8989703\tbest: 0.8989944 (16720)\ttotal: 9m 20s\tremaining: 9h 21s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8989944109\n",
      "bestIteration = 16720\n",
      "\n",
      "Shrink model to first 16721 iterations.\n",
      "Fold 3 started at Fri Mar  8 01:06:07 2019\n",
      "0:\ttest: 0.5377154\tbest: 0.5377154 (0)\ttotal: 51.7ms\tremaining: 14h 21m 3s\n",
      "1000:\ttest: 0.8536661\tbest: 0.8536661 (1000)\ttotal: 31.3s\tremaining: 8h 39m 47s\n",
      "2000:\ttest: 0.8725642\tbest: 0.8725642 (2000)\ttotal: 1m 3s\tremaining: 8h 49m 43s\n",
      "3000:\ttest: 0.8831417\tbest: 0.8831417 (3000)\ttotal: 1m 36s\tremaining: 8h 54m 25s\n",
      "4000:\ttest: 0.8890917\tbest: 0.8890935 (3998)\ttotal: 2m 8s\tremaining: 8h 55m\n",
      "5000:\ttest: 0.8930821\tbest: 0.8930821 (5000)\ttotal: 2m 43s\tremaining: 9h 2m 24s\n",
      "6000:\ttest: 0.8954112\tbest: 0.8954186 (5998)\ttotal: 3m 17s\tremaining: 9h 5m 4s\n",
      "7000:\ttest: 0.8973124\tbest: 0.8973124 (7000)\ttotal: 3m 52s\tremaining: 9h 8m 31s\n",
      "8000:\ttest: 0.8986568\tbest: 0.8986568 (8000)\ttotal: 4m 24s\tremaining: 9h 7m 28s\n",
      "9000:\ttest: 0.8996064\tbest: 0.8996064 (9000)\ttotal: 4m 59s\tremaining: 9h 9m 45s\n",
      "10000:\ttest: 0.9002439\tbest: 0.9002494 (9997)\ttotal: 5m 31s\tremaining: 9h 7m 34s\n",
      "11000:\ttest: 0.9008710\tbest: 0.9008729 (10998)\ttotal: 6m 5s\tremaining: 9h 7m 10s\n",
      "12000:\ttest: 0.9011962\tbest: 0.9012074 (11978)\ttotal: 6m 37s\tremaining: 9h 6m 2s\n",
      "13000:\ttest: 0.9015683\tbest: 0.9015719 (12997)\ttotal: 7m 11s\tremaining: 9h 5m 42s\n",
      "14000:\ttest: 0.9017719\tbest: 0.9017754 (13987)\ttotal: 7m 44s\tremaining: 9h 4m 57s\n",
      "15000:\ttest: 0.9019580\tbest: 0.9019803 (14952)\ttotal: 8m 16s\tremaining: 9h 3m 1s\n",
      "16000:\ttest: 0.9020951\tbest: 0.9020972 (15999)\ttotal: 8m 48s\tremaining: 9h 1m 29s\n",
      "17000:\ttest: 0.9022218\tbest: 0.9022362 (16920)\ttotal: 9m 21s\tremaining: 9h 42s\n",
      "18000:\ttest: 0.9023039\tbest: 0.9023262 (17925)\ttotal: 9m 53s\tremaining: 8h 59m 32s\n",
      "19000:\ttest: 0.9023927\tbest: 0.9023957 (18971)\ttotal: 10m 25s\tremaining: 8h 57m 58s\n",
      "20000:\ttest: 0.9024568\tbest: 0.9024568 (20000)\ttotal: 10m 57s\tremaining: 8h 57m 16s\n",
      "21000:\ttest: 0.9025004\tbest: 0.9025137 (20953)\ttotal: 11m 31s\tremaining: 8h 56m 52s\n",
      "22000:\ttest: 0.9025064\tbest: 0.9025206 (21418)\ttotal: 12m 2s\tremaining: 8h 55m 12s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9025206375\n",
      "bestIteration = 21418\n",
      "\n",
      "Shrink model to first 21419 iterations.\n",
      "Fold 4 started at Fri Mar  8 01:19:12 2019\n",
      "0:\ttest: 0.5268861\tbest: 0.5268861 (0)\ttotal: 53.3ms\tremaining: 14h 49m 8s\n",
      "1000:\ttest: 0.8541173\tbest: 0.8542741 (996)\ttotal: 30.2s\tremaining: 8h 22m 25s\n",
      "2000:\ttest: 0.8738079\tbest: 0.8738079 (2000)\ttotal: 1m 1s\tremaining: 8h 32m 15s\n",
      "3000:\ttest: 0.8859365\tbest: 0.8859365 (3000)\ttotal: 1m 33s\tremaining: 8h 39m 10s\n",
      "4000:\ttest: 0.8928415\tbest: 0.8928415 (4000)\ttotal: 2m 5s\tremaining: 8h 42m 15s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000:\ttest: 0.8973603\tbest: 0.8973603 (5000)\ttotal: 2m 39s\tremaining: 8h 48m 14s\n",
      "6000:\ttest: 0.9005813\tbest: 0.9005863 (5998)\ttotal: 3m 12s\tremaining: 8h 51m 6s\n",
      "7000:\ttest: 0.9026904\tbest: 0.9026904 (7000)\ttotal: 3m 44s\tremaining: 8h 49m 51s\n",
      "8000:\ttest: 0.9043908\tbest: 0.9043924 (7977)\ttotal: 4m 16s\tremaining: 8h 49m 55s\n",
      "9000:\ttest: 0.9055808\tbest: 0.9055843 (8999)\ttotal: 4m 49s\tremaining: 8h 50m 40s\n",
      "10000:\ttest: 0.9064250\tbest: 0.9064250 (10000)\ttotal: 5m 20s\tremaining: 8h 49m 1s\n",
      "11000:\ttest: 0.9070500\tbest: 0.9070577 (10905)\ttotal: 5m 53s\tremaining: 8h 49m 36s\n",
      "12000:\ttest: 0.9076150\tbest: 0.9076237 (11987)\ttotal: 6m 26s\tremaining: 8h 50m 44s\n",
      "13000:\ttest: 0.9078750\tbest: 0.9078750 (13000)\ttotal: 6m 58s\tremaining: 8h 49m 55s\n",
      "14000:\ttest: 0.9082084\tbest: 0.9082092 (13997)\ttotal: 7m 31s\tremaining: 8h 49m 58s\n",
      "15000:\ttest: 0.9084851\tbest: 0.9084981 (14991)\ttotal: 8m 7s\tremaining: 8h 53m 48s\n",
      "16000:\ttest: 0.9086814\tbest: 0.9086837 (15999)\ttotal: 8m 40s\tremaining: 8h 53m 39s\n",
      "17000:\ttest: 0.9088327\tbest: 0.9088364 (16948)\ttotal: 9m 14s\tremaining: 8h 53m 54s\n",
      "18000:\ttest: 0.9089188\tbest: 0.9089435 (17916)\ttotal: 9m 46s\tremaining: 8h 53m 1s\n",
      "19000:\ttest: 0.9090140\tbest: 0.9090304 (18931)\ttotal: 10m 17s\tremaining: 8h 51m 34s\n",
      "20000:\ttest: 0.9091049\tbest: 0.9091155 (19809)\ttotal: 10m 50s\tremaining: 8h 51m 4s\n",
      "21000:\ttest: 0.9091019\tbest: 0.9091351 (20712)\ttotal: 11m 21s\tremaining: 8h 49m 52s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9091350916\n",
      "bestIteration = 20712\n",
      "\n",
      "Shrink model to first 20713 iterations.\n",
      "Fold 5 started at Fri Mar  8 01:31:44 2019\n",
      "0:\ttest: 0.5424682\tbest: 0.5424682 (0)\ttotal: 41.7ms\tremaining: 11h 35m 36s\n",
      "1000:\ttest: 0.8524013\tbest: 0.8525358 (991)\ttotal: 32s\tremaining: 8h 51m 52s\n",
      "2000:\ttest: 0.8715192\tbest: 0.8715194 (1999)\ttotal: 1m 4s\tremaining: 8h 59m 53s\n",
      "3000:\ttest: 0.8825337\tbest: 0.8825337 (3000)\ttotal: 1m 36s\tremaining: 8h 53m 36s\n",
      "4000:\ttest: 0.8888676\tbest: 0.8888744 (3999)\ttotal: 2m 9s\tremaining: 8h 55m 22s\n",
      "5000:\ttest: 0.8929813\tbest: 0.8929813 (5000)\ttotal: 2m 42s\tremaining: 8h 59m 35s\n",
      "6000:\ttest: 0.8956261\tbest: 0.8956289 (5999)\ttotal: 3m 14s\tremaining: 8h 57m 44s\n",
      "7000:\ttest: 0.8976101\tbest: 0.8976156 (6998)\ttotal: 3m 48s\tremaining: 9h 12s\n",
      "8000:\ttest: 0.8992363\tbest: 0.8992363 (8000)\ttotal: 4m 21s\tremaining: 8h 59m 44s\n",
      "9000:\ttest: 0.9001274\tbest: 0.9001274 (9000)\ttotal: 4m 54s\tremaining: 9h 48s\n",
      "10000:\ttest: 0.9009830\tbest: 0.9009830 (10000)\ttotal: 5m 27s\tremaining: 9h 31s\n",
      "11000:\ttest: 0.9016199\tbest: 0.9016202 (10999)\ttotal: 6m\tremaining: 9h 42s\n",
      "12000:\ttest: 0.9020961\tbest: 0.9020978 (11960)\ttotal: 6m 34s\tremaining: 9h 1m 31s\n",
      "13000:\ttest: 0.9022776\tbest: 0.9022873 (12985)\ttotal: 7m 8s\tremaining: 9h 1m 53s\n",
      "14000:\ttest: 0.9025542\tbest: 0.9025568 (13989)\ttotal: 7m 40s\tremaining: 9h 27s\n",
      "15000:\ttest: 0.9027488\tbest: 0.9027535 (14951)\ttotal: 8m 12s\tremaining: 8h 59m 8s\n",
      "16000:\ttest: 0.9028784\tbest: 0.9028817 (15985)\ttotal: 8m 45s\tremaining: 8h 58m 29s\n",
      "17000:\ttest: 0.9029973\tbest: 0.9030166 (16948)\ttotal: 9m 18s\tremaining: 8h 58m 8s\n",
      "18000:\ttest: 0.9030526\tbest: 0.9030610 (17905)\ttotal: 9m 50s\tremaining: 8h 57m 11s\n",
      "19000:\ttest: 0.9031123\tbest: 0.9031220 (18935)\ttotal: 10m 24s\tremaining: 8h 57m 15s\n",
      "20000:\ttest: 0.9031497\tbest: 0.9031733 (19414)\ttotal: 10m 58s\tremaining: 8h 57m 38s\n",
      "21000:\ttest: 0.9031847\tbest: 0.9031902 (20756)\ttotal: 11m 30s\tremaining: 8h 56m 48s\n",
      "22000:\ttest: 0.9032007\tbest: 0.9032095 (21221)\ttotal: 12m 2s\tremaining: 8h 55m 37s\n",
      "23000:\ttest: 0.9031635\tbest: 0.9032168 (22345)\ttotal: 12m 35s\tremaining: 8h 55m 6s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9032168231\n",
      "bestIteration = 22345\n",
      "\n",
      "Shrink model to first 22346 iterations.\n",
      "Fold 6 started at Fri Mar  8 01:45:22 2019\n",
      "0:\ttest: 0.5324903\tbest: 0.5324903 (0)\ttotal: 54ms\tremaining: 14h 59m 20s\n",
      "1000:\ttest: 0.8496185\tbest: 0.8497180 (992)\ttotal: 30.6s\tremaining: 8h 29m 36s\n",
      "2000:\ttest: 0.8697702\tbest: 0.8697702 (2000)\ttotal: 1m 2s\tremaining: 8h 38m 28s\n",
      "3000:\ttest: 0.8818572\tbest: 0.8818572 (3000)\ttotal: 1m 34s\tremaining: 8h 45m 57s\n",
      "4000:\ttest: 0.8887872\tbest: 0.8887996 (3994)\ttotal: 2m 7s\tremaining: 8h 48m 16s\n",
      "5000:\ttest: 0.8934432\tbest: 0.8934599 (4994)\ttotal: 2m 40s\tremaining: 8h 52m 12s\n",
      "6000:\ttest: 0.8965071\tbest: 0.8965112 (5996)\ttotal: 3m 13s\tremaining: 8h 53m 49s\n",
      "7000:\ttest: 0.8986358\tbest: 0.8986397 (6999)\ttotal: 3m 47s\tremaining: 8h 56m 50s\n",
      "8000:\ttest: 0.9000434\tbest: 0.9000434 (8000)\ttotal: 4m 20s\tremaining: 8h 57m 38s\n",
      "9000:\ttest: 0.9011597\tbest: 0.9011597 (9000)\ttotal: 4m 53s\tremaining: 8h 58m 29s\n",
      "10000:\ttest: 0.9019985\tbest: 0.9020128 (9984)\ttotal: 5m 27s\tremaining: 9h 32s\n",
      "11000:\ttest: 0.9026215\tbest: 0.9026301 (10963)\ttotal: 6m\tremaining: 9h 17s\n",
      "12000:\ttest: 0.9031181\tbest: 0.9031217 (11989)\ttotal: 6m 32s\tremaining: 8h 58m 39s\n",
      "13000:\ttest: 0.9034922\tbest: 0.9034934 (12999)\ttotal: 7m 5s\tremaining: 8h 58m 44s\n",
      "14000:\ttest: 0.9038725\tbest: 0.9038774 (13947)\ttotal: 7m 39s\tremaining: 8h 59m 39s\n",
      "15000:\ttest: 0.9042442\tbest: 0.9042464 (14993)\ttotal: 8m 12s\tremaining: 8h 59m 25s\n",
      "16000:\ttest: 0.9044176\tbest: 0.9044191 (15983)\ttotal: 8m 45s\tremaining: 8h 58m 47s\n",
      "17000:\ttest: 0.9045792\tbest: 0.9045872 (16902)\ttotal: 9m 18s\tremaining: 8h 58m 40s\n",
      "18000:\ttest: 0.9046843\tbest: 0.9046843 (18000)\ttotal: 9m 52s\tremaining: 8h 58m 42s\n",
      "19000:\ttest: 0.9048041\tbest: 0.9048115 (18984)\ttotal: 10m 25s\tremaining: 8h 58m 11s\n",
      "20000:\ttest: 0.9048648\tbest: 0.9048701 (19967)\ttotal: 10m 58s\tremaining: 8h 57m 31s\n",
      "21000:\ttest: 0.9049203\tbest: 0.9049317 (20966)\ttotal: 11m 30s\tremaining: 8h 56m 47s\n",
      "22000:\ttest: 0.9049843\tbest: 0.9050079 (21899)\ttotal: 12m 4s\tremaining: 8h 56m 33s\n",
      "23000:\ttest: 0.9050680\tbest: 0.9050794 (22928)\ttotal: 12m 37s\tremaining: 8h 56m 2s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9050793559\n",
      "bestIteration = 22928\n",
      "\n",
      "Shrink model to first 22929 iterations.\n",
      "Fold 7 started at Fri Mar  8 01:59:23 2019\n",
      "0:\ttest: 0.5451235\tbest: 0.5451235 (0)\ttotal: 51.3ms\tremaining: 14h 15m 9s\n",
      "1000:\ttest: 0.8418097\tbest: 0.8418129 (999)\ttotal: 32.8s\tremaining: 9h 5m 18s\n",
      "2000:\ttest: 0.8616718\tbest: 0.8616837 (1995)\ttotal: 1m 6s\tremaining: 9h 8m 41s\n",
      "3000:\ttest: 0.8735117\tbest: 0.8735117 (3000)\ttotal: 1m 39s\tremaining: 9h 10m 7s\n",
      "4000:\ttest: 0.8799737\tbest: 0.8799737 (4000)\ttotal: 2m 12s\tremaining: 9h 7m 56s\n",
      "5000:\ttest: 0.8837697\tbest: 0.8837800 (4999)\ttotal: 2m 44s\tremaining: 9h 5m 58s\n",
      "6000:\ttest: 0.8863446\tbest: 0.8863511 (5999)\ttotal: 3m 17s\tremaining: 9h 4m 10s\n",
      "7000:\ttest: 0.8884407\tbest: 0.8884442 (6994)\ttotal: 3m 48s\tremaining: 8h 59m 20s\n",
      "8000:\ttest: 0.8898512\tbest: 0.8898512 (8000)\ttotal: 4m 20s\tremaining: 8h 58m 28s\n",
      "9000:\ttest: 0.8909136\tbest: 0.8909190 (8998)\ttotal: 4m 52s\tremaining: 8h 57m\n",
      "10000:\ttest: 0.8917293\tbest: 0.8917465 (9992)\ttotal: 5m 24s\tremaining: 8h 55m 52s\n",
      "11000:\ttest: 0.8924609\tbest: 0.8924724 (10984)\ttotal: 5m 56s\tremaining: 8h 54m 9s\n",
      "12000:\ttest: 0.8928989\tbest: 0.8929103 (11995)\ttotal: 6m 28s\tremaining: 8h 53m 39s\n",
      "13000:\ttest: 0.8932968\tbest: 0.8932998 (12998)\ttotal: 7m\tremaining: 8h 51m 25s\n",
      "14000:\ttest: 0.8936403\tbest: 0.8936441 (13999)\ttotal: 7m 31s\tremaining: 8h 49m 47s\n",
      "15000:\ttest: 0.8939360\tbest: 0.8939395 (14981)\ttotal: 8m 4s\tremaining: 8h 50m 3s\n",
      "16000:\ttest: 0.8941455\tbest: 0.8941463 (15998)\ttotal: 8m 36s\tremaining: 8h 49m 38s\n",
      "17000:\ttest: 0.8943504\tbest: 0.8943646 (16953)\ttotal: 9m 11s\tremaining: 8h 51m 29s\n",
      "18000:\ttest: 0.8944966\tbest: 0.8944972 (17999)\ttotal: 9m 43s\tremaining: 8h 50m 47s\n",
      "19000:\ttest: 0.8946979\tbest: 0.8947011 (18994)\ttotal: 10m 15s\tremaining: 8h 49m 59s\n",
      "20000:\ttest: 0.8948112\tbest: 0.8948164 (19990)\ttotal: 10m 48s\tremaining: 8h 49m 49s\n",
      "21000:\ttest: 0.8949688\tbest: 0.8949839 (20930)\ttotal: 11m 21s\tremaining: 8h 49m 38s\n",
      "22000:\ttest: 0.8950083\tbest: 0.8950129 (21994)\ttotal: 11m 53s\tremaining: 8h 48m 38s\n",
      "23000:\ttest: 0.8950908\tbest: 0.8950921 (22808)\ttotal: 12m 25s\tremaining: 8h 47m 46s\n",
      "24000:\ttest: 0.8951860\tbest: 0.8952003 (23950)\ttotal: 12m 57s\tremaining: 8h 47m 12s\n",
      "25000:\ttest: 0.8952441\tbest: 0.8952591 (24939)\ttotal: 13m 29s\tremaining: 8h 45m 57s\n",
      "26000:\ttest: 0.8952726\tbest: 0.8952918 (25333)\ttotal: 14m\tremaining: 8h 44m 32s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8952917735\n",
      "bestIteration = 25333\n",
      "\n",
      "Shrink model to first 25334 iterations.\n",
      "Fold 8 started at Fri Mar  8 02:14:34 2019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5263554\tbest: 0.5263554 (0)\ttotal: 54.9ms\tremaining: 15h 14m 49s\n",
      "1000:\ttest: 0.8447286\tbest: 0.8447981 (993)\ttotal: 30.8s\tremaining: 8h 32m 10s\n",
      "2000:\ttest: 0.8615747\tbest: 0.8615747 (2000)\ttotal: 1m 2s\tremaining: 8h 38m 5s\n",
      "3000:\ttest: 0.8731107\tbest: 0.8731107 (3000)\ttotal: 1m 34s\tremaining: 8h 44m 14s\n",
      "4000:\ttest: 0.8798758\tbest: 0.8798758 (4000)\ttotal: 2m 7s\tremaining: 8h 50m 33s\n",
      "5000:\ttest: 0.8842978\tbest: 0.8842978 (5000)\ttotal: 2m 40s\tremaining: 8h 53m 38s\n",
      "6000:\ttest: 0.8874861\tbest: 0.8874955 (5994)\ttotal: 3m 13s\tremaining: 8h 54m 16s\n",
      "7000:\ttest: 0.8900215\tbest: 0.8900215 (7000)\ttotal: 3m 46s\tremaining: 8h 54m 28s\n",
      "8000:\ttest: 0.8915842\tbest: 0.8915856 (7999)\ttotal: 4m 18s\tremaining: 8h 54m 16s\n",
      "9000:\ttest: 0.8926751\tbest: 0.8926758 (8998)\ttotal: 4m 51s\tremaining: 8h 54m 6s\n",
      "10000:\ttest: 0.8936522\tbest: 0.8936553 (9986)\ttotal: 5m 23s\tremaining: 8h 53m 58s\n",
      "11000:\ttest: 0.8944368\tbest: 0.8944393 (10999)\ttotal: 5m 56s\tremaining: 8h 53m 37s\n",
      "12000:\ttest: 0.8949920\tbest: 0.8949920 (12000)\ttotal: 6m 29s\tremaining: 8h 54m 38s\n",
      "13000:\ttest: 0.8953894\tbest: 0.8953964 (12952)\ttotal: 7m 2s\tremaining: 8h 54m 13s\n",
      "14000:\ttest: 0.8957425\tbest: 0.8957445 (13996)\ttotal: 7m 34s\tremaining: 8h 53m 23s\n",
      "15000:\ttest: 0.8960654\tbest: 0.8960665 (14990)\ttotal: 8m 7s\tremaining: 8h 53m 38s\n",
      "16000:\ttest: 0.8962510\tbest: 0.8962583 (15987)\ttotal: 8m 40s\tremaining: 8h 53m 54s\n",
      "17000:\ttest: 0.8964281\tbest: 0.8964325 (16998)\ttotal: 9m 13s\tremaining: 8h 53m 35s\n",
      "18000:\ttest: 0.8966290\tbest: 0.8966290 (18000)\ttotal: 9m 46s\tremaining: 8h 52m 47s\n",
      "19000:\ttest: 0.8967202\tbest: 0.8967215 (18998)\ttotal: 10m 18s\tremaining: 8h 52m 16s\n",
      "20000:\ttest: 0.8967334\tbest: 0.8967571 (19717)\ttotal: 10m 51s\tremaining: 8h 51m 37s\n",
      "21000:\ttest: 0.8968678\tbest: 0.8968698 (20996)\ttotal: 11m 23s\tremaining: 8h 51m 18s\n",
      "22000:\ttest: 0.8970001\tbest: 0.8970051 (21968)\ttotal: 11m 56s\tremaining: 8h 51m 3s\n",
      "23000:\ttest: 0.8971082\tbest: 0.8971116 (22991)\ttotal: 12m 30s\tremaining: 8h 50m 57s\n",
      "24000:\ttest: 0.8971725\tbest: 0.8971784 (23697)\ttotal: 13m 2s\tremaining: 8h 50m 27s\n",
      "25000:\ttest: 0.8971890\tbest: 0.8972143 (24864)\ttotal: 13m 35s\tremaining: 8h 50m 20s\n",
      "26000:\ttest: 0.8972057\tbest: 0.8972231 (25980)\ttotal: 14m 9s\tremaining: 8h 50m 4s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8972230577\n",
      "bestIteration = 25980\n",
      "\n",
      "Shrink model to first 25981 iterations.\n",
      "Fold 9 started at Fri Mar  8 02:30:17 2019\n",
      "0:\ttest: 0.5158260\tbest: 0.5158260 (0)\ttotal: 55.9ms\tremaining: 15h 31m 2s\n",
      "1000:\ttest: 0.8492705\tbest: 0.8493093 (999)\ttotal: 32s\tremaining: 8h 51m 57s\n",
      "2000:\ttest: 0.8659811\tbest: 0.8659884 (1995)\ttotal: 1m 2s\tremaining: 8h 42m 59s\n",
      "3000:\ttest: 0.8766634\tbest: 0.8766634 (3000)\ttotal: 1m 35s\tremaining: 8h 47m 54s\n",
      "4000:\ttest: 0.8835914\tbest: 0.8835940 (3998)\ttotal: 2m 7s\tremaining: 8h 47m 55s\n",
      "5000:\ttest: 0.8876674\tbest: 0.8876674 (5000)\ttotal: 2m 39s\tremaining: 8h 49m 48s\n",
      "6000:\ttest: 0.8903052\tbest: 0.8903052 (6000)\ttotal: 3m 11s\tremaining: 8h 48m 52s\n",
      "7000:\ttest: 0.8923503\tbest: 0.8923553 (6999)\ttotal: 3m 44s\tremaining: 8h 50m 14s\n",
      "8000:\ttest: 0.8937020\tbest: 0.8937020 (8000)\ttotal: 4m 16s\tremaining: 8h 50m 18s\n",
      "9000:\ttest: 0.8947820\tbest: 0.8947974 (8971)\ttotal: 4m 48s\tremaining: 8h 48m 50s\n",
      "10000:\ttest: 0.8954494\tbest: 0.8954499 (9998)\ttotal: 5m 21s\tremaining: 8h 49m 49s\n",
      "11000:\ttest: 0.8960102\tbest: 0.8960102 (11000)\ttotal: 5m 54s\tremaining: 8h 50m 30s\n",
      "12000:\ttest: 0.8964201\tbest: 0.8964229 (11997)\ttotal: 6m 26s\tremaining: 8h 50m 53s\n",
      "13000:\ttest: 0.8967627\tbest: 0.8967636 (12996)\ttotal: 6m 59s\tremaining: 8h 51m 4s\n",
      "14000:\ttest: 0.8969718\tbest: 0.8969763 (13990)\ttotal: 7m 33s\tremaining: 8h 52m 36s\n",
      "15000:\ttest: 0.8971575\tbest: 0.8971662 (14732)\ttotal: 8m 6s\tremaining: 8h 52m 49s\n",
      "16000:\ttest: 0.8972262\tbest: 0.8972688 (15425)\ttotal: 8m 40s\tremaining: 8h 53m 27s\n",
      "17000:\ttest: 0.8973660\tbest: 0.8973679 (16994)\ttotal: 9m 13s\tremaining: 8h 53m 12s\n",
      "18000:\ttest: 0.8973289\tbest: 0.8974059 (17355)\ttotal: 9m 47s\tremaining: 8h 53m 45s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8974058925\n",
      "bestIteration = 17355\n",
      "\n",
      "Shrink model to first 17356 iterations.\n",
      "CV mean score: 0.9008, std: 0.0040.\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Fold 0 started at Fri Mar  8 02:40:55 2019\n",
      "0:\ttest: 0.5258817\tbest: 0.5258817 (0)\ttotal: 56.6ms\tremaining: 15h 43m 50s\n",
      "1000:\ttest: 0.8540348\tbest: 0.8542591 (995)\ttotal: 29.8s\tremaining: 8h 16m 8s\n",
      "2000:\ttest: 0.8720693\tbest: 0.8720988 (1991)\ttotal: 1m 1s\tremaining: 8h 30m 9s\n",
      "3000:\ttest: 0.8821227\tbest: 0.8821227 (3000)\ttotal: 1m 34s\tremaining: 8h 41m 5s\n",
      "4000:\ttest: 0.8879631\tbest: 0.8879631 (4000)\ttotal: 2m 7s\tremaining: 8h 48m 48s\n",
      "5000:\ttest: 0.8915575\tbest: 0.8915575 (5000)\ttotal: 2m 38s\tremaining: 8h 46m 33s\n",
      "6000:\ttest: 0.8940003\tbest: 0.8940008 (5996)\ttotal: 3m 10s\tremaining: 8h 46m 20s\n",
      "7000:\ttest: 0.8958413\tbest: 0.8958465 (6994)\ttotal: 3m 42s\tremaining: 8h 45m 44s\n",
      "8000:\ttest: 0.8970440\tbest: 0.8970472 (7999)\ttotal: 4m 14s\tremaining: 8h 46m 12s\n",
      "9000:\ttest: 0.8979915\tbest: 0.8979915 (9000)\ttotal: 4m 46s\tremaining: 8h 45m 50s\n",
      "10000:\ttest: 0.8987659\tbest: 0.8987708 (9991)\ttotal: 5m 19s\tremaining: 8h 46m 39s\n",
      "11000:\ttest: 0.8993711\tbest: 0.8993711 (11000)\ttotal: 5m 51s\tremaining: 8h 46m 42s\n",
      "12000:\ttest: 0.8997913\tbest: 0.8997913 (12000)\ttotal: 6m 23s\tremaining: 8h 46m 3s\n",
      "13000:\ttest: 0.9001645\tbest: 0.9001697 (12994)\ttotal: 6m 55s\tremaining: 8h 45m 19s\n",
      "14000:\ttest: 0.9004142\tbest: 0.9004355 (13937)\ttotal: 7m 27s\tremaining: 8h 45m 48s\n",
      "15000:\ttest: 0.9005353\tbest: 0.9005627 (14893)\ttotal: 8m\tremaining: 8h 45m 18s\n",
      "16000:\ttest: 0.9007099\tbest: 0.9007099 (16000)\ttotal: 8m 32s\tremaining: 8h 44m 56s\n",
      "17000:\ttest: 0.9007859\tbest: 0.9007926 (16933)\ttotal: 9m 3s\tremaining: 8h 43m 50s\n",
      "18000:\ttest: 0.9009085\tbest: 0.9009101 (17965)\ttotal: 9m 36s\tremaining: 8h 44m 14s\n",
      "19000:\ttest: 0.9009669\tbest: 0.9009686 (18988)\ttotal: 10m 8s\tremaining: 8h 44m\n",
      "20000:\ttest: 0.9010024\tbest: 0.9010238 (19747)\ttotal: 10m 41s\tremaining: 8h 43m 37s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9010237844\n",
      "bestIteration = 19747\n",
      "\n",
      "Shrink model to first 19748 iterations.\n",
      "Fold 1 started at Fri Mar  8 02:52:45 2019\n",
      "0:\ttest: 0.5142161\tbest: 0.5142161 (0)\ttotal: 53ms\tremaining: 14h 43m 26s\n",
      "1000:\ttest: 0.8487022\tbest: 0.8489308 (993)\ttotal: 33.1s\tremaining: 9h 11m\n",
      "2000:\ttest: 0.8670757\tbest: 0.8670757 (2000)\ttotal: 1m 5s\tremaining: 9h 1m 45s\n",
      "3000:\ttest: 0.8779055\tbest: 0.8779055 (3000)\ttotal: 1m 37s\tremaining: 8h 57m 53s\n",
      "4000:\ttest: 0.8840299\tbest: 0.8840334 (3998)\ttotal: 2m 9s\tremaining: 8h 57m 38s\n",
      "5000:\ttest: 0.8881388\tbest: 0.8881521 (4994)\ttotal: 2m 41s\tremaining: 8h 54m 10s\n",
      "6000:\ttest: 0.8909946\tbest: 0.8909985 (5999)\ttotal: 3m 13s\tremaining: 8h 55m 16s\n",
      "7000:\ttest: 0.8929381\tbest: 0.8929440 (6999)\ttotal: 3m 47s\tremaining: 8h 57m 5s\n",
      "8000:\ttest: 0.8944133\tbest: 0.8944167 (7996)\ttotal: 4m 20s\tremaining: 8h 57m 26s\n",
      "9000:\ttest: 0.8952616\tbest: 0.8952626 (8996)\ttotal: 4m 52s\tremaining: 8h 57m 19s\n",
      "10000:\ttest: 0.8960042\tbest: 0.8960042 (10000)\ttotal: 5m 25s\tremaining: 8h 56m 19s\n",
      "11000:\ttest: 0.8964491\tbest: 0.8964532 (10997)\ttotal: 5m 58s\tremaining: 8h 56m 49s\n",
      "12000:\ttest: 0.8969174\tbest: 0.8969183 (11994)\ttotal: 6m 30s\tremaining: 8h 55m 34s\n",
      "13000:\ttest: 0.8972650\tbest: 0.8972664 (12996)\ttotal: 7m 2s\tremaining: 8h 54m 50s\n",
      "14000:\ttest: 0.8975864\tbest: 0.8975962 (13983)\ttotal: 7m 35s\tremaining: 8h 54m 36s\n",
      "15000:\ttest: 0.8978165\tbest: 0.8978302 (14940)\ttotal: 8m 7s\tremaining: 8h 53m 21s\n",
      "16000:\ttest: 0.8980351\tbest: 0.8980378 (15966)\ttotal: 8m 39s\tremaining: 8h 52m 28s\n",
      "17000:\ttest: 0.8981624\tbest: 0.8981716 (16918)\ttotal: 9m 11s\tremaining: 8h 51m 44s\n",
      "18000:\ttest: 0.8982660\tbest: 0.8982871 (17787)\ttotal: 9m 44s\tremaining: 8h 51m 39s\n",
      "19000:\ttest: 0.8983504\tbest: 0.8983522 (18990)\ttotal: 10m 17s\tremaining: 8h 51m 31s\n",
      "20000:\ttest: 0.8983767\tbest: 0.8983874 (19892)\ttotal: 10m 50s\tremaining: 8h 51m 4s\n",
      "21000:\ttest: 0.8983777\tbest: 0.8983943 (20466)\ttotal: 11m 22s\tremaining: 8h 50m 26s\n",
      "22000:\ttest: 0.8984508\tbest: 0.8984714 (21630)\ttotal: 11m 54s\tremaining: 8h 49m 41s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8984714303\n",
      "bestIteration = 21630\n",
      "\n",
      "Shrink model to first 21631 iterations.\n",
      "Fold 2 started at Fri Mar  8 03:05:51 2019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5081412\tbest: 0.5081412 (0)\ttotal: 77.8ms\tremaining: 21h 36m 36s\n",
      "1000:\ttest: 0.8548432\tbest: 0.8548432 (1000)\ttotal: 32.5s\tremaining: 9h 1m 7s\n",
      "2000:\ttest: 0.8727032\tbest: 0.8727211 (1996)\ttotal: 1m 5s\tremaining: 9h 4m 43s\n",
      "3000:\ttest: 0.8827831\tbest: 0.8827831 (3000)\ttotal: 1m 38s\tremaining: 9h 7m 41s\n",
      "4000:\ttest: 0.8881703\tbest: 0.8881724 (3999)\ttotal: 2m 12s\tremaining: 9h 8m 7s\n",
      "5000:\ttest: 0.8915078\tbest: 0.8915168 (4998)\ttotal: 2m 45s\tremaining: 9h 10m 4s\n",
      "6000:\ttest: 0.8938717\tbest: 0.8938717 (6000)\ttotal: 3m 18s\tremaining: 9h 8m 26s\n",
      "7000:\ttest: 0.8953224\tbest: 0.8953237 (6994)\ttotal: 3m 52s\tremaining: 9h 9m 15s\n",
      "8000:\ttest: 0.8963059\tbest: 0.8963059 (8000)\ttotal: 4m 25s\tremaining: 9h 8m 46s\n",
      "9000:\ttest: 0.8970360\tbest: 0.8970402 (8986)\ttotal: 4m 59s\tremaining: 9h 9m 44s\n",
      "10000:\ttest: 0.8976319\tbest: 0.8976388 (9993)\ttotal: 5m 32s\tremaining: 9h 7m 53s\n",
      "11000:\ttest: 0.8981039\tbest: 0.8981049 (10992)\ttotal: 6m 6s\tremaining: 9h 8m 24s\n",
      "12000:\ttest: 0.8983533\tbest: 0.8983549 (11978)\ttotal: 6m 39s\tremaining: 9h 7m 40s\n",
      "13000:\ttest: 0.8986077\tbest: 0.8986077 (13000)\ttotal: 7m 13s\tremaining: 9h 7m 57s\n",
      "14000:\ttest: 0.8988204\tbest: 0.8988204 (14000)\ttotal: 7m 47s\tremaining: 9h 8m 19s\n",
      "15000:\ttest: 0.8989004\tbest: 0.8989033 (14859)\ttotal: 8m 19s\tremaining: 9h 6m 33s\n",
      "16000:\ttest: 0.8989942\tbest: 0.8989963 (15998)\ttotal: 8m 52s\tremaining: 9h 5m 31s\n",
      "17000:\ttest: 0.8990567\tbest: 0.8990576 (16975)\ttotal: 9m 24s\tremaining: 9h 4m 26s\n",
      "18000:\ttest: 0.8991490\tbest: 0.8991532 (17971)\ttotal: 9m 58s\tremaining: 9h 4m 18s\n",
      "19000:\ttest: 0.8992520\tbest: 0.8992538 (18999)\ttotal: 10m 31s\tremaining: 9h 2m 59s\n",
      "20000:\ttest: 0.8992747\tbest: 0.8993063 (19710)\ttotal: 11m 3s\tremaining: 9h 1m 58s\n",
      "21000:\ttest: 0.8993437\tbest: 0.8993789 (20754)\ttotal: 11m 37s\tremaining: 9h 1m 50s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8993788976\n",
      "bestIteration = 20754\n",
      "\n",
      "Shrink model to first 20755 iterations.\n",
      "Fold 3 started at Fri Mar  8 03:18:42 2019\n",
      "0:\ttest: 0.5145267\tbest: 0.5145267 (0)\ttotal: 43.9ms\tremaining: 12h 11m 4s\n",
      "1000:\ttest: 0.8535361\tbest: 0.8535361 (1000)\ttotal: 30.1s\tremaining: 8h 20m 44s\n",
      "2000:\ttest: 0.8727660\tbest: 0.8727660 (2000)\ttotal: 1m\tremaining: 8h 24m 23s\n",
      "3000:\ttest: 0.8830705\tbest: 0.8830705 (3000)\ttotal: 1m 31s\tremaining: 8h 29m 19s\n",
      "4000:\ttest: 0.8891671\tbest: 0.8891699 (3999)\ttotal: 2m 2s\tremaining: 8h 29m 52s\n",
      "5000:\ttest: 0.8928285\tbest: 0.8928285 (5000)\ttotal: 2m 33s\tremaining: 8h 28m 48s\n",
      "6000:\ttest: 0.8954335\tbest: 0.8954346 (5999)\ttotal: 3m 4s\tremaining: 8h 29m 44s\n",
      "7000:\ttest: 0.8972808\tbest: 0.8972906 (6995)\ttotal: 3m 36s\tremaining: 8h 32m 21s\n",
      "8000:\ttest: 0.8985280\tbest: 0.8985294 (7997)\ttotal: 4m 8s\tremaining: 8h 32m 49s\n",
      "9000:\ttest: 0.8993916\tbest: 0.8993944 (8995)\ttotal: 4m 39s\tremaining: 8h 32m 6s\n",
      "10000:\ttest: 0.8999992\tbest: 0.9000092 (9990)\ttotal: 5m 8s\tremaining: 8h 29m 47s\n",
      "11000:\ttest: 0.9004717\tbest: 0.9004731 (10999)\ttotal: 5m 40s\tremaining: 8h 29m 47s\n",
      "12000:\ttest: 0.9008576\tbest: 0.9008583 (11999)\ttotal: 6m 10s\tremaining: 8h 28m 16s\n",
      "13000:\ttest: 0.9011795\tbest: 0.9012063 (12926)\ttotal: 6m 41s\tremaining: 8h 27m 28s\n",
      "14000:\ttest: 0.9013059\tbest: 0.9013080 (13998)\ttotal: 7m 12s\tremaining: 8h 27m 24s\n",
      "15000:\ttest: 0.9015146\tbest: 0.9015156 (14998)\ttotal: 7m 43s\tremaining: 8h 27m 12s\n",
      "16000:\ttest: 0.9017556\tbest: 0.9017594 (15891)\ttotal: 8m 15s\tremaining: 8h 28m 10s\n",
      "17000:\ttest: 0.9018300\tbest: 0.9018424 (16982)\ttotal: 8m 47s\tremaining: 8h 28m 16s\n",
      "18000:\ttest: 0.9018694\tbest: 0.9018930 (17857)\ttotal: 9m 18s\tremaining: 8h 27m 37s\n",
      "19000:\ttest: 0.9018986\tbest: 0.9019248 (18723)\ttotal: 9m 50s\tremaining: 8h 27m 43s\n",
      "20000:\ttest: 0.9019329\tbest: 0.9019975 (19622)\ttotal: 10m 21s\tremaining: 8h 27m 52s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9019974889\n",
      "bestIteration = 19622\n",
      "\n",
      "Shrink model to first 19623 iterations.\n",
      "Fold 4 started at Fri Mar  8 03:30:06 2019\n",
      "0:\ttest: 0.5216312\tbest: 0.5216312 (0)\ttotal: 58.4ms\tremaining: 16h 13m 25s\n",
      "1000:\ttest: 0.8550111\tbest: 0.8550111 (1000)\ttotal: 30.8s\tremaining: 8h 31m 39s\n",
      "2000:\ttest: 0.8752584\tbest: 0.8752584 (2000)\ttotal: 1m 2s\tremaining: 8h 40m 13s\n",
      "3000:\ttest: 0.8871840\tbest: 0.8871840 (3000)\ttotal: 1m 34s\tremaining: 8h 45m 3s\n",
      "4000:\ttest: 0.8940571\tbest: 0.8940640 (3999)\ttotal: 2m 8s\tremaining: 8h 51m 5s\n",
      "5000:\ttest: 0.8982327\tbest: 0.8982327 (5000)\ttotal: 2m 41s\tremaining: 8h 54m 2s\n",
      "6000:\ttest: 0.9013731\tbest: 0.9013731 (6000)\ttotal: 3m 14s\tremaining: 8h 57m 1s\n",
      "7000:\ttest: 0.9035292\tbest: 0.9035292 (7000)\ttotal: 3m 46s\tremaining: 8h 54m 54s\n",
      "8000:\ttest: 0.9050678\tbest: 0.9050678 (8000)\ttotal: 4m 18s\tremaining: 8h 54m 26s\n",
      "9000:\ttest: 0.9062132\tbest: 0.9062141 (8998)\ttotal: 4m 52s\tremaining: 8h 57m 8s\n",
      "10000:\ttest: 0.9069898\tbest: 0.9069904 (9998)\ttotal: 5m 25s\tremaining: 8h 56m 42s\n",
      "11000:\ttest: 0.9074984\tbest: 0.9075014 (10997)\ttotal: 5m 57s\tremaining: 8h 55m 34s\n",
      "12000:\ttest: 0.9079508\tbest: 0.9079562 (11949)\ttotal: 6m 30s\tremaining: 8h 56m 5s\n",
      "13000:\ttest: 0.9083220\tbest: 0.9083358 (12975)\ttotal: 7m 4s\tremaining: 8h 56m 48s\n",
      "14000:\ttest: 0.9085916\tbest: 0.9085967 (13994)\ttotal: 7m 37s\tremaining: 8h 57m 2s\n",
      "15000:\ttest: 0.9087574\tbest: 0.9087646 (14994)\ttotal: 8m 9s\tremaining: 8h 56m 13s\n",
      "16000:\ttest: 0.9088897\tbest: 0.9088979 (15991)\ttotal: 8m 43s\tremaining: 8h 56m 14s\n",
      "17000:\ttest: 0.9089577\tbest: 0.9089658 (16866)\ttotal: 9m 16s\tremaining: 8h 56m 11s\n",
      "18000:\ttest: 0.9090670\tbest: 0.9090853 (17923)\ttotal: 9m 49s\tremaining: 8h 55m 45s\n",
      "19000:\ttest: 0.9091842\tbest: 0.9091842 (19000)\ttotal: 10m 21s\tremaining: 8h 55m 5s\n",
      "20000:\ttest: 0.9091960\tbest: 0.9092020 (19910)\ttotal: 10m 55s\tremaining: 8h 55m 16s\n",
      "21000:\ttest: 0.9093015\tbest: 0.9093015 (20999)\ttotal: 11m 28s\tremaining: 8h 55m 14s\n",
      "22000:\ttest: 0.9093454\tbest: 0.9093484 (21999)\ttotal: 12m\tremaining: 8h 53m 53s\n",
      "23000:\ttest: 0.9094029\tbest: 0.9094082 (22966)\ttotal: 12m 34s\tremaining: 8h 53m 53s\n",
      "24000:\ttest: 0.9094420\tbest: 0.9094607 (23782)\ttotal: 13m 7s\tremaining: 8h 53m 31s\n",
      "25000:\ttest: 0.9094777\tbest: 0.9094777 (25000)\ttotal: 13m 39s\tremaining: 8h 52m 37s\n",
      "26000:\ttest: 0.9094757\tbest: 0.9095144 (25730)\ttotal: 14m 12s\tremaining: 8h 51m 57s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9095144345\n",
      "bestIteration = 25730\n",
      "\n",
      "Shrink model to first 25731 iterations.\n",
      "Fold 5 started at Fri Mar  8 03:45:43 2019\n",
      "0:\ttest: 0.5160757\tbest: 0.5160757 (0)\ttotal: 37.5ms\tremaining: 10h 25m 41s\n",
      "1000:\ttest: 0.8523659\tbest: 0.8523659 (1000)\ttotal: 31.1s\tremaining: 8h 37m 50s\n",
      "2000:\ttest: 0.8726028\tbest: 0.8726028 (2000)\ttotal: 1m 3s\tremaining: 8h 45m 4s\n",
      "3000:\ttest: 0.8830594\tbest: 0.8830594 (3000)\ttotal: 1m 34s\tremaining: 8h 45m 58s\n",
      "4000:\ttest: 0.8895150\tbest: 0.8895185 (3999)\ttotal: 2m 8s\tremaining: 8h 51m 54s\n",
      "5000:\ttest: 0.8933823\tbest: 0.8933872 (4998)\ttotal: 2m 41s\tremaining: 8h 54m 43s\n",
      "6000:\ttest: 0.8962375\tbest: 0.8962397 (5999)\ttotal: 3m 14s\tremaining: 8h 55m 46s\n",
      "7000:\ttest: 0.8982509\tbest: 0.8982509 (7000)\ttotal: 3m 46s\tremaining: 8h 54m 22s\n",
      "8000:\ttest: 0.8995442\tbest: 0.8995442 (8000)\ttotal: 4m 19s\tremaining: 8h 55m 56s\n",
      "9000:\ttest: 0.9003835\tbest: 0.9003842 (8999)\ttotal: 4m 51s\tremaining: 8h 55m 24s\n",
      "10000:\ttest: 0.9010806\tbest: 0.9010816 (9998)\ttotal: 5m 25s\tremaining: 8h 56m 51s\n",
      "11000:\ttest: 0.9015903\tbest: 0.9015903 (11000)\ttotal: 5m 58s\tremaining: 8h 57m\n",
      "12000:\ttest: 0.9020734\tbest: 0.9020738 (11996)\ttotal: 6m 31s\tremaining: 8h 57m 43s\n",
      "13000:\ttest: 0.9023115\tbest: 0.9023127 (12985)\ttotal: 7m 7s\tremaining: 9h 21s\n",
      "14000:\ttest: 0.9025482\tbest: 0.9025639 (13732)\ttotal: 7m 39s\tremaining: 8h 59m 41s\n",
      "15000:\ttest: 0.9028103\tbest: 0.9028136 (14995)\ttotal: 8m 11s\tremaining: 8h 57m 51s\n",
      "16000:\ttest: 0.9029054\tbest: 0.9029054 (16000)\ttotal: 8m 43s\tremaining: 8h 56m 23s\n",
      "17000:\ttest: 0.9030480\tbest: 0.9030560 (16938)\ttotal: 9m 15s\tremaining: 8h 55m 3s\n",
      "18000:\ttest: 0.9030536\tbest: 0.9030703 (17804)\ttotal: 9m 47s\tremaining: 8h 54m 36s\n",
      "19000:\ttest: 0.9031057\tbest: 0.9031158 (18860)\ttotal: 10m 20s\tremaining: 8h 54m 13s\n",
      "20000:\ttest: 0.9031819\tbest: 0.9031819 (20000)\ttotal: 10m 54s\tremaining: 8h 54m 5s\n",
      "21000:\ttest: 0.9031846\tbest: 0.9032020 (20566)\ttotal: 11m 27s\tremaining: 8h 53m 53s\n",
      "22000:\ttest: 0.9031892\tbest: 0.9032076 (21090)\ttotal: 11m 59s\tremaining: 8h 53m 12s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9032075586\n",
      "bestIteration = 21090\n",
      "\n",
      "Shrink model to first 21091 iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6 started at Fri Mar  8 03:58:34 2019\n",
      "0:\ttest: 0.5100487\tbest: 0.5100487 (0)\ttotal: 41.8ms\tremaining: 11h 36m 46s\n",
      "1000:\ttest: 0.8509598\tbest: 0.8509834 (999)\ttotal: 30.2s\tremaining: 8h 22m 57s\n",
      "2000:\ttest: 0.8711211\tbest: 0.8711211 (2000)\ttotal: 1m 1s\tremaining: 8h 29m 48s\n",
      "3000:\ttest: 0.8825437\tbest: 0.8825437 (3000)\ttotal: 1m 34s\tremaining: 8h 42m 12s\n",
      "4000:\ttest: 0.8890869\tbest: 0.8891040 (3998)\ttotal: 2m 8s\tremaining: 8h 51m 32s\n",
      "5000:\ttest: 0.8933340\tbest: 0.8933351 (4998)\ttotal: 2m 41s\tremaining: 8h 55m 5s\n",
      "6000:\ttest: 0.8963549\tbest: 0.8963549 (6000)\ttotal: 3m 14s\tremaining: 8h 57m 27s\n",
      "7000:\ttest: 0.8983072\tbest: 0.8983083 (6999)\ttotal: 3m 47s\tremaining: 8h 57m 45s\n",
      "8000:\ttest: 0.8995710\tbest: 0.8995767 (7997)\ttotal: 4m 21s\tremaining: 9h\n",
      "9000:\ttest: 0.9006471\tbest: 0.9006471 (9000)\ttotal: 4m 53s\tremaining: 8h 59m 23s\n",
      "10000:\ttest: 0.9014560\tbest: 0.9014575 (9997)\ttotal: 5m 26s\tremaining: 8h 58m 50s\n",
      "11000:\ttest: 0.9021801\tbest: 0.9021801 (11000)\ttotal: 5m 59s\tremaining: 8h 58m 17s\n",
      "12000:\ttest: 0.9026939\tbest: 0.9026945 (11999)\ttotal: 6m 31s\tremaining: 8h 57m 8s\n",
      "13000:\ttest: 0.9031321\tbest: 0.9031341 (12997)\ttotal: 7m 3s\tremaining: 8h 56m 22s\n",
      "14000:\ttest: 0.9034445\tbest: 0.9034470 (13994)\ttotal: 7m 35s\tremaining: 8h 54m 55s\n",
      "15000:\ttest: 0.9037486\tbest: 0.9037503 (14983)\ttotal: 8m 8s\tremaining: 8h 54m 49s\n",
      "16000:\ttest: 0.9039251\tbest: 0.9039260 (15940)\ttotal: 8m 41s\tremaining: 8h 54m 45s\n",
      "17000:\ttest: 0.9041107\tbest: 0.9041108 (16999)\ttotal: 9m 14s\tremaining: 8h 54m 40s\n",
      "18000:\ttest: 0.9042469\tbest: 0.9042512 (17996)\ttotal: 9m 47s\tremaining: 8h 54m 23s\n",
      "19000:\ttest: 0.9043761\tbest: 0.9043800 (18990)\ttotal: 10m 19s\tremaining: 8h 53m 21s\n",
      "20000:\ttest: 0.9044757\tbest: 0.9044771 (19998)\ttotal: 10m 52s\tremaining: 8h 52m 40s\n",
      "21000:\ttest: 0.9045474\tbest: 0.9045532 (20970)\ttotal: 11m 24s\tremaining: 8h 51m 33s\n",
      "22000:\ttest: 0.9045486\tbest: 0.9045667 (21137)\ttotal: 11m 57s\tremaining: 8h 51m 27s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9045666609\n",
      "bestIteration = 21137\n",
      "\n",
      "Shrink model to first 21138 iterations.\n",
      "Fold 7 started at Fri Mar  8 04:11:26 2019\n",
      "0:\ttest: 0.5205562\tbest: 0.5205562 (0)\ttotal: 40.4ms\tremaining: 11h 13m 59s\n",
      "1000:\ttest: 0.8434485\tbest: 0.8434551 (999)\ttotal: 31.7s\tremaining: 8h 47m 32s\n",
      "2000:\ttest: 0.8639248\tbest: 0.8639248 (2000)\ttotal: 1m 3s\tremaining: 8h 45m 3s\n",
      "3000:\ttest: 0.8751342\tbest: 0.8751342 (3000)\ttotal: 1m 35s\tremaining: 8h 51m 32s\n",
      "4000:\ttest: 0.8810854\tbest: 0.8810854 (4000)\ttotal: 2m 9s\tremaining: 8h 55m 38s\n",
      "5000:\ttest: 0.8847894\tbest: 0.8847894 (5000)\ttotal: 2m 41s\tremaining: 8h 57m 5s\n",
      "6000:\ttest: 0.8874000\tbest: 0.8874137 (5997)\ttotal: 3m 14s\tremaining: 8h 57m 37s\n",
      "7000:\ttest: 0.8890534\tbest: 0.8890534 (7000)\ttotal: 3m 48s\tremaining: 8h 59m 49s\n",
      "8000:\ttest: 0.8902587\tbest: 0.8902587 (8000)\ttotal: 4m 20s\tremaining: 8h 58m 27s\n",
      "9000:\ttest: 0.8912510\tbest: 0.8912510 (9000)\ttotal: 4m 52s\tremaining: 8h 56m 28s\n",
      "10000:\ttest: 0.8921004\tbest: 0.8921004 (10000)\ttotal: 5m 24s\tremaining: 8h 55m 13s\n",
      "11000:\ttest: 0.8926459\tbest: 0.8926465 (10949)\ttotal: 5m 57s\tremaining: 8h 55m 11s\n",
      "12000:\ttest: 0.8932622\tbest: 0.8932672 (11982)\ttotal: 6m 29s\tremaining: 8h 54m 37s\n",
      "13000:\ttest: 0.8937143\tbest: 0.8937196 (12984)\ttotal: 7m 1s\tremaining: 8h 53m 54s\n",
      "14000:\ttest: 0.8939964\tbest: 0.8940043 (13960)\ttotal: 7m 35s\tremaining: 8h 54m 9s\n",
      "15000:\ttest: 0.8942497\tbest: 0.8942575 (14984)\ttotal: 8m 7s\tremaining: 8h 53m 40s\n",
      "16000:\ttest: 0.8943656\tbest: 0.8943656 (16000)\ttotal: 8m 39s\tremaining: 8h 52m 21s\n",
      "17000:\ttest: 0.8946357\tbest: 0.8946439 (16960)\ttotal: 9m 11s\tremaining: 8h 51m 32s\n",
      "18000:\ttest: 0.8947219\tbest: 0.8947248 (17992)\ttotal: 9m 43s\tremaining: 8h 50m 47s\n",
      "19000:\ttest: 0.8948458\tbest: 0.8948649 (18951)\ttotal: 10m 16s\tremaining: 8h 50m 31s\n",
      "20000:\ttest: 0.8949383\tbest: 0.8949443 (19978)\ttotal: 10m 48s\tremaining: 8h 49m 40s\n",
      "21000:\ttest: 0.8949661\tbest: 0.8949942 (20473)\ttotal: 11m 21s\tremaining: 8h 49m 37s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8949942063\n",
      "bestIteration = 20473\n",
      "\n",
      "Shrink model to first 20474 iterations.\n",
      "Fold 8 started at Fri Mar  8 04:23:49 2019\n",
      "0:\ttest: 0.5374390\tbest: 0.5374390 (0)\ttotal: 59.5ms\tremaining: 16h 32m 5s\n",
      "1000:\ttest: 0.8426943\tbest: 0.8426943 (1000)\ttotal: 31.7s\tremaining: 8h 47m 41s\n",
      "2000:\ttest: 0.8632026\tbest: 0.8632026 (2000)\ttotal: 1m 3s\tremaining: 8h 47m 40s\n",
      "3000:\ttest: 0.8737885\tbest: 0.8737930 (2999)\ttotal: 1m 35s\tremaining: 8h 48m 11s\n",
      "4000:\ttest: 0.8804679\tbest: 0.8804689 (3997)\ttotal: 2m 8s\tremaining: 8h 54m 28s\n",
      "5000:\ttest: 0.8848639\tbest: 0.8848639 (5000)\ttotal: 2m 42s\tremaining: 8h 59m 29s\n",
      "6000:\ttest: 0.8879678\tbest: 0.8879718 (5998)\ttotal: 3m 16s\tremaining: 9h 1m 48s\n",
      "7000:\ttest: 0.8901838\tbest: 0.8901838 (7000)\ttotal: 3m 50s\tremaining: 9h 4m 12s\n",
      "8000:\ttest: 0.8918691\tbest: 0.8918691 (8000)\ttotal: 4m 24s\tremaining: 9h 5m 34s\n",
      "9000:\ttest: 0.8931010\tbest: 0.8931090 (8998)\ttotal: 4m 58s\tremaining: 9h 7m 35s\n",
      "10000:\ttest: 0.8938298\tbest: 0.8938298 (10000)\ttotal: 5m 31s\tremaining: 9h 6m 44s\n",
      "11000:\ttest: 0.8943962\tbest: 0.8943962 (11000)\ttotal: 6m 4s\tremaining: 9h 6m 18s\n",
      "12000:\ttest: 0.8949534\tbest: 0.8949552 (11999)\ttotal: 6m 38s\tremaining: 9h 6m 12s\n",
      "13000:\ttest: 0.8954316\tbest: 0.8954586 (12951)\ttotal: 7m 11s\tremaining: 9h 6m 25s\n",
      "14000:\ttest: 0.8956947\tbest: 0.8957010 (13994)\ttotal: 7m 45s\tremaining: 9h 6m 27s\n",
      "15000:\ttest: 0.8959529\tbest: 0.8959603 (14964)\ttotal: 8m 20s\tremaining: 9h 7m 45s\n",
      "16000:\ttest: 0.8961902\tbest: 0.8961957 (15968)\ttotal: 8m 54s\tremaining: 9h 7m 23s\n",
      "17000:\ttest: 0.8963666\tbest: 0.8963725 (16949)\ttotal: 9m 27s\tremaining: 9h 6m 36s\n",
      "18000:\ttest: 0.8964632\tbest: 0.8964659 (17995)\ttotal: 10m 1s\tremaining: 9h 7m 2s\n",
      "19000:\ttest: 0.8966157\tbest: 0.8966216 (18987)\ttotal: 10m 35s\tremaining: 9h 6m 37s\n",
      "20000:\ttest: 0.8967381\tbest: 0.8967415 (19984)\ttotal: 11m 9s\tremaining: 9h 6m 24s\n",
      "21000:\ttest: 0.8968108\tbest: 0.8968125 (20999)\ttotal: 11m 41s\tremaining: 9h 5m 9s\n",
      "22000:\ttest: 0.8968773\tbest: 0.8968799 (21995)\ttotal: 12m 16s\tremaining: 9h 5m 34s\n",
      "23000:\ttest: 0.8969651\tbest: 0.8969686 (22974)\ttotal: 12m 50s\tremaining: 9h 5m 9s\n",
      "24000:\ttest: 0.8969784\tbest: 0.8970080 (23794)\ttotal: 13m 23s\tremaining: 9h 4m 54s\n",
      "25000:\ttest: 0.8970447\tbest: 0.8970490 (24994)\ttotal: 13m 56s\tremaining: 9h 3m 45s\n",
      "26000:\ttest: 0.8970191\tbest: 0.8970565 (25852)\ttotal: 14m 30s\tremaining: 9h 3m 25s\n",
      "27000:\ttest: 0.8970945\tbest: 0.8970992 (26986)\ttotal: 15m 3s\tremaining: 9h 2m 30s\n",
      "28000:\ttest: 0.8971344\tbest: 0.8971353 (27979)\ttotal: 15m 35s\tremaining: 9h 1m 19s\n",
      "29000:\ttest: 0.8971690\tbest: 0.8971749 (28995)\ttotal: 16m 8s\tremaining: 9h 40s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8971749418\n",
      "bestIteration = 28995\n",
      "\n",
      "Shrink model to first 28996 iterations.\n",
      "Fold 9 started at Fri Mar  8 04:41:46 2019\n",
      "0:\ttest: 0.5113147\tbest: 0.5113147 (0)\ttotal: 53.8ms\tremaining: 14h 55m 53s\n",
      "1000:\ttest: 0.8504426\tbest: 0.8505758 (995)\ttotal: 31.9s\tremaining: 8h 50m 58s\n",
      "2000:\ttest: 0.8681409\tbest: 0.8681409 (2000)\ttotal: 1m 5s\tremaining: 9h 7m 17s\n",
      "3000:\ttest: 0.8784789\tbest: 0.8784789 (3000)\ttotal: 1m 38s\tremaining: 9h 3m 29s\n",
      "4000:\ttest: 0.8846382\tbest: 0.8846382 (4000)\ttotal: 2m 10s\tremaining: 9h 3m 2s\n",
      "5000:\ttest: 0.8887434\tbest: 0.8887448 (4999)\ttotal: 2m 43s\tremaining: 9h 1m 32s\n",
      "6000:\ttest: 0.8912742\tbest: 0.8912742 (6000)\ttotal: 3m 16s\tremaining: 9h 2m 4s\n",
      "7000:\ttest: 0.8930297\tbest: 0.8930313 (6999)\ttotal: 3m 49s\tremaining: 9h 3m 1s\n",
      "8000:\ttest: 0.8942457\tbest: 0.8942457 (8000)\ttotal: 4m 22s\tremaining: 9h 2m 33s\n",
      "9000:\ttest: 0.8953029\tbest: 0.8953029 (9000)\ttotal: 4m 55s\tremaining: 9h 1m 27s\n",
      "10000:\ttest: 0.8960238\tbest: 0.8960251 (9999)\ttotal: 5m 28s\tremaining: 9h 2m\n",
      "11000:\ttest: 0.8965832\tbest: 0.8965845 (10974)\ttotal: 6m\tremaining: 9h 38s\n",
      "12000:\ttest: 0.8969703\tbest: 0.8969703 (12000)\ttotal: 6m 33s\tremaining: 9h 6s\n",
      "13000:\ttest: 0.8972638\tbest: 0.8972745 (12986)\ttotal: 7m 5s\tremaining: 8h 58m 36s\n",
      "14000:\ttest: 0.8973602\tbest: 0.8973721 (13902)\ttotal: 7m 39s\tremaining: 8h 59m 23s\n",
      "15000:\ttest: 0.8975067\tbest: 0.8975192 (14970)\ttotal: 8m 12s\tremaining: 8h 58m 39s\n",
      "16000:\ttest: 0.8976936\tbest: 0.8976960 (15984)\ttotal: 8m 44s\tremaining: 8h 57m 14s\n",
      "17000:\ttest: 0.8977846\tbest: 0.8977887 (16983)\ttotal: 9m 16s\tremaining: 8h 56m 36s\n",
      "18000:\ttest: 0.8978717\tbest: 0.8978826 (17971)\ttotal: 9m 49s\tremaining: 8h 56m 18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19000:\ttest: 0.8979249\tbest: 0.8979376 (18369)\ttotal: 10m 22s\tremaining: 8h 55m 54s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8979376297\n",
      "bestIteration = 18369\n",
      "\n",
      "Shrink model to first 18370 iterations.\n",
      "CV mean score: 0.9008, std: 0.0040.\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Fold 0 started at Fri Mar  8 04:53:05 2019\n",
      "0:\ttest: 0.5293296\tbest: 0.5293296 (0)\ttotal: 65.5ms\tremaining: 18h 10m 59s\n",
      "1000:\ttest: 0.8520829\tbest: 0.8520829 (1000)\ttotal: 29.4s\tremaining: 8h 9m 25s\n",
      "2000:\ttest: 0.8711622\tbest: 0.8711622 (2000)\ttotal: 1m\tremaining: 8h 22m 57s\n",
      "3000:\ttest: 0.8815037\tbest: 0.8815037 (2999)\ttotal: 1m 31s\tremaining: 8h 26m 27s\n",
      "4000:\ttest: 0.8870731\tbest: 0.8870731 (4000)\ttotal: 2m 2s\tremaining: 8h 27m 13s\n",
      "5000:\ttest: 0.8908544\tbest: 0.8908544 (5000)\ttotal: 2m 32s\tremaining: 8h 26m 42s\n",
      "6000:\ttest: 0.8933405\tbest: 0.8933405 (6000)\ttotal: 3m 2s\tremaining: 8h 23m 48s\n",
      "7000:\ttest: 0.8952875\tbest: 0.8952875 (7000)\ttotal: 3m 33s\tremaining: 8h 25m 3s\n",
      "8000:\ttest: 0.8964704\tbest: 0.8964766 (7995)\ttotal: 4m 4s\tremaining: 8h 26m 2s\n",
      "9000:\ttest: 0.8974201\tbest: 0.8974208 (8997)\ttotal: 4m 37s\tremaining: 8h 28m 41s\n",
      "10000:\ttest: 0.8982182\tbest: 0.8982182 (10000)\ttotal: 5m 9s\tremaining: 8h 31m 1s\n",
      "11000:\ttest: 0.8986699\tbest: 0.8986732 (10991)\ttotal: 5m 41s\tremaining: 8h 31m 44s\n",
      "12000:\ttest: 0.8991181\tbest: 0.8991206 (11984)\ttotal: 6m 12s\tremaining: 8h 31m 4s\n",
      "13000:\ttest: 0.8994898\tbest: 0.8994908 (12987)\ttotal: 6m 44s\tremaining: 8h 31m 18s\n",
      "14000:\ttest: 0.8998569\tbest: 0.8998678 (13896)\ttotal: 7m 15s\tremaining: 8h 30m 46s\n",
      "15000:\ttest: 0.9001190\tbest: 0.9001287 (14989)\ttotal: 7m 46s\tremaining: 8h 30m 26s\n",
      "16000:\ttest: 0.9001924\tbest: 0.9001976 (15608)\ttotal: 8m 18s\tremaining: 8h 30m 35s\n",
      "17000:\ttest: 0.9003453\tbest: 0.9003591 (16868)\ttotal: 8m 50s\tremaining: 8h 30m 49s\n",
      "18000:\ttest: 0.9004290\tbest: 0.9004552 (17534)\ttotal: 9m 22s\tremaining: 8h 31m 9s\n",
      "19000:\ttest: 0.9005598\tbest: 0.9005848 (18666)\ttotal: 9m 52s\tremaining: 8h 30m 13s\n",
      "20000:\ttest: 0.9006674\tbest: 0.9007000 (19839)\ttotal: 10m 23s\tremaining: 8h 29m 20s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9007000183\n",
      "bestIteration = 19839\n",
      "\n",
      "Shrink model to first 19840 iterations.\n",
      "Fold 1 started at Fri Mar  8 05:04:41 2019\n",
      "0:\ttest: 0.5113807\tbest: 0.5113807 (0)\ttotal: 41.7ms\tremaining: 11h 35m 6s\n",
      "1000:\ttest: 0.8477730\tbest: 0.8477730 (1000)\ttotal: 31.3s\tremaining: 8h 41m 25s\n",
      "2000:\ttest: 0.8671983\tbest: 0.8671983 (2000)\ttotal: 1m 4s\tremaining: 8h 55m 1s\n",
      "3000:\ttest: 0.8774662\tbest: 0.8774662 (3000)\ttotal: 1m 37s\tremaining: 8h 58m 55s\n",
      "4000:\ttest: 0.8839343\tbest: 0.8839371 (3999)\ttotal: 2m 10s\tremaining: 8h 59m 27s\n",
      "5000:\ttest: 0.8877738\tbest: 0.8877833 (4998)\ttotal: 2m 42s\tremaining: 8h 58m 39s\n",
      "6000:\ttest: 0.8906763\tbest: 0.8906763 (6000)\ttotal: 3m 15s\tremaining: 8h 59m 34s\n",
      "7000:\ttest: 0.8926257\tbest: 0.8926281 (6996)\ttotal: 3m 49s\tremaining: 9h 1m 41s\n",
      "8000:\ttest: 0.8939816\tbest: 0.8939816 (8000)\ttotal: 4m 22s\tremaining: 9h 1m 38s\n",
      "9000:\ttest: 0.8950673\tbest: 0.8950699 (8999)\ttotal: 4m 55s\tremaining: 9h 2m 29s\n",
      "10000:\ttest: 0.8959461\tbest: 0.8959505 (9991)\ttotal: 5m 27s\tremaining: 9h 28s\n",
      "11000:\ttest: 0.8965653\tbest: 0.8965653 (11000)\ttotal: 6m\tremaining: 9h 50s\n",
      "12000:\ttest: 0.8971168\tbest: 0.8971224 (11999)\ttotal: 6m 32s\tremaining: 8h 59m 3s\n",
      "13000:\ttest: 0.8973902\tbest: 0.8973902 (13000)\ttotal: 7m 5s\tremaining: 8h 58m 2s\n",
      "14000:\ttest: 0.8975402\tbest: 0.8975413 (13988)\ttotal: 7m 37s\tremaining: 8h 57m 32s\n",
      "15000:\ttest: 0.8977275\tbest: 0.8977304 (14991)\ttotal: 8m 10s\tremaining: 8h 57m 17s\n",
      "16000:\ttest: 0.8978501\tbest: 0.8978637 (15955)\ttotal: 8m 44s\tremaining: 8h 57m 45s\n",
      "17000:\ttest: 0.8980169\tbest: 0.8980249 (16828)\ttotal: 9m 17s\tremaining: 8h 57m 12s\n",
      "18000:\ttest: 0.8980921\tbest: 0.8981017 (17896)\ttotal: 9m 50s\tremaining: 8h 56m 44s\n",
      "19000:\ttest: 0.8981837\tbest: 0.8981926 (18815)\ttotal: 10m 22s\tremaining: 8h 55m 29s\n",
      "20000:\ttest: 0.8982322\tbest: 0.8982397 (19966)\ttotal: 10m 53s\tremaining: 8h 53m 51s\n",
      "21000:\ttest: 0.8983066\tbest: 0.8983100 (20932)\ttotal: 11m 25s\tremaining: 8h 52m 53s\n",
      "22000:\ttest: 0.8983569\tbest: 0.8983679 (21270)\ttotal: 12m\tremaining: 8h 53m 31s\n",
      "23000:\ttest: 0.8984191\tbest: 0.8984290 (22942)\ttotal: 12m 32s\tremaining: 8h 52m 44s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8984290376\n",
      "bestIteration = 22942\n",
      "\n",
      "Shrink model to first 22943 iterations.\n",
      "Fold 2 started at Fri Mar  8 05:18:39 2019\n",
      "0:\ttest: 0.5235278\tbest: 0.5235278 (0)\ttotal: 53.7ms\tremaining: 14h 55m 47s\n",
      "1000:\ttest: 0.8533899\tbest: 0.8533899 (1000)\ttotal: 30.6s\tremaining: 8h 29m 26s\n",
      "2000:\ttest: 0.8717329\tbest: 0.8717329 (2000)\ttotal: 1m 2s\tremaining: 8h 39m 3s\n",
      "3000:\ttest: 0.8814882\tbest: 0.8814882 (3000)\ttotal: 1m 36s\tremaining: 8h 52m 58s\n",
      "4000:\ttest: 0.8871614\tbest: 0.8871614 (4000)\ttotal: 2m 8s\tremaining: 8h 52m 35s\n",
      "5000:\ttest: 0.8907280\tbest: 0.8907280 (5000)\ttotal: 2m 42s\tremaining: 8h 57m 58s\n",
      "6000:\ttest: 0.8928963\tbest: 0.8929069 (5992)\ttotal: 3m 16s\tremaining: 9h 1m 40s\n",
      "7000:\ttest: 0.8946584\tbest: 0.8946584 (7000)\ttotal: 3m 49s\tremaining: 9h 2m 17s\n",
      "8000:\ttest: 0.8956990\tbest: 0.8956990 (8000)\ttotal: 4m 21s\tremaining: 9h 50s\n",
      "9000:\ttest: 0.8967316\tbest: 0.8967401 (8994)\ttotal: 4m 55s\tremaining: 9h 2m 48s\n",
      "10000:\ttest: 0.8973036\tbest: 0.8973179 (9974)\ttotal: 5m 29s\tremaining: 9h 2m 53s\n",
      "11000:\ttest: 0.8976776\tbest: 0.8976776 (11000)\ttotal: 6m 1s\tremaining: 9h 1m 39s\n",
      "12000:\ttest: 0.8980256\tbest: 0.8980256 (12000)\ttotal: 6m 35s\tremaining: 9h 2m 22s\n",
      "13000:\ttest: 0.8983062\tbest: 0.8983079 (12998)\ttotal: 7m 8s\tremaining: 9h 2m\n",
      "14000:\ttest: 0.8984688\tbest: 0.8984746 (13929)\ttotal: 7m 40s\tremaining: 9h 27s\n",
      "15000:\ttest: 0.8986355\tbest: 0.8986421 (14993)\ttotal: 8m 12s\tremaining: 8h 59m 22s\n",
      "16000:\ttest: 0.8987743\tbest: 0.8987816 (15977)\ttotal: 8m 47s\tremaining: 9h 32s\n",
      "17000:\ttest: 0.8988732\tbest: 0.8988790 (16846)\ttotal: 9m 20s\tremaining: 9h 4s\n",
      "18000:\ttest: 0.8988529\tbest: 0.8988916 (17059)\ttotal: 9m 52s\tremaining: 8h 59m\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8988915899\n",
      "bestIteration = 17059\n",
      "\n",
      "Shrink model to first 17060 iterations.\n",
      "Fold 3 started at Fri Mar  8 05:29:14 2019\n",
      "0:\ttest: 0.5204108\tbest: 0.5204108 (0)\ttotal: 42.7ms\tremaining: 11h 52m 25s\n",
      "1000:\ttest: 0.8540318\tbest: 0.8541203 (995)\ttotal: 32.2s\tremaining: 8h 56m 12s\n",
      "2000:\ttest: 0.8720413\tbest: 0.8720413 (2000)\ttotal: 1m 4s\tremaining: 8h 59m 37s\n",
      "3000:\ttest: 0.8822647\tbest: 0.8822647 (3000)\ttotal: 1m 37s\tremaining: 8h 59m 45s\n",
      "4000:\ttest: 0.8880706\tbest: 0.8880706 (4000)\ttotal: 2m 10s\tremaining: 9h 3m 1s\n",
      "5000:\ttest: 0.8922183\tbest: 0.8922206 (4996)\ttotal: 2m 44s\tremaining: 9h 4m 37s\n",
      "6000:\ttest: 0.8949523\tbest: 0.8949526 (5998)\ttotal: 3m 17s\tremaining: 9h 4m 26s\n",
      "7000:\ttest: 0.8966768\tbest: 0.8966915 (6996)\ttotal: 3m 49s\tremaining: 9h 3m 41s\n",
      "8000:\ttest: 0.8979091\tbest: 0.8979268 (7968)\ttotal: 4m 23s\tremaining: 9h 4m 20s\n",
      "9000:\ttest: 0.8990266\tbest: 0.8990266 (9000)\ttotal: 4m 55s\tremaining: 9h 2m 2s\n",
      "10000:\ttest: 0.8997898\tbest: 0.8997898 (10000)\ttotal: 5m 27s\tremaining: 9h 1m 3s\n",
      "11000:\ttest: 0.9002933\tbest: 0.9002933 (11000)\ttotal: 6m\tremaining: 9h 16s\n",
      "12000:\ttest: 0.9007050\tbest: 0.9007081 (11957)\ttotal: 6m 33s\tremaining: 9h 29s\n",
      "13000:\ttest: 0.9010673\tbest: 0.9010673 (13000)\ttotal: 7m 7s\tremaining: 9h 44s\n",
      "14000:\ttest: 0.9013260\tbest: 0.9013333 (13962)\ttotal: 7m 41s\tremaining: 9h 1m 34s\n",
      "15000:\ttest: 0.9014866\tbest: 0.9015024 (14958)\ttotal: 8m 14s\tremaining: 9h 57s\n",
      "16000:\ttest: 0.9016372\tbest: 0.9016489 (15934)\ttotal: 8m 48s\tremaining: 9h 1m 10s\n",
      "17000:\ttest: 0.9016961\tbest: 0.9017115 (16550)\ttotal: 9m 20s\tremaining: 9h 2s\n",
      "18000:\ttest: 0.9017624\tbest: 0.9017642 (17996)\ttotal: 9m 53s\tremaining: 8h 59m 48s\n",
      "19000:\ttest: 0.9019131\tbest: 0.9019145 (18997)\ttotal: 10m 26s\tremaining: 8h 58m 55s\n",
      "20000:\ttest: 0.9019638\tbest: 0.9019689 (19989)\ttotal: 11m\tremaining: 8h 59m 15s\n",
      "21000:\ttest: 0.9019669\tbest: 0.9019754 (20489)\ttotal: 11m 32s\tremaining: 8h 58m 24s\n",
      "22000:\ttest: 0.9019864\tbest: 0.9020052 (21345)\ttotal: 12m 5s\tremaining: 8h 57m 18s\n",
      "23000:\ttest: 0.9020131\tbest: 0.9020241 (22530)\ttotal: 12m 38s\tremaining: 8h 57m 13s\n",
      "24000:\ttest: 0.9020030\tbest: 0.9020342 (23590)\ttotal: 13m 11s\tremaining: 8h 56m 13s\n",
      "25000:\ttest: 0.9020426\tbest: 0.9020555 (24660)\ttotal: 13m 43s\tremaining: 8h 55m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9020555367\n",
      "bestIteration = 24660\n",
      "\n",
      "Shrink model to first 24661 iterations.\n",
      "Fold 4 started at Fri Mar  8 05:44:18 2019\n",
      "0:\ttest: 0.5141316\tbest: 0.5141316 (0)\ttotal: 61.1ms\tremaining: 16h 58m 42s\n",
      "1000:\ttest: 0.8550761\tbest: 0.8550768 (996)\ttotal: 31.6s\tremaining: 8h 46m 4s\n",
      "2000:\ttest: 0.8739521\tbest: 0.8739521 (2000)\ttotal: 1m 3s\tremaining: 8h 44m 46s\n",
      "3000:\ttest: 0.8862658\tbest: 0.8862753 (2995)\ttotal: 1m 35s\tremaining: 8h 48m 10s\n",
      "4000:\ttest: 0.8933018\tbest: 0.8933018 (4000)\ttotal: 2m 7s\tremaining: 8h 48m 23s\n",
      "5000:\ttest: 0.8977514\tbest: 0.8977525 (4996)\ttotal: 2m 40s\tremaining: 8h 53m 24s\n",
      "6000:\ttest: 0.9008064\tbest: 0.9008064 (6000)\ttotal: 3m 12s\tremaining: 8h 52m 38s\n",
      "7000:\ttest: 0.9030199\tbest: 0.9030221 (6996)\ttotal: 3m 45s\tremaining: 8h 52m 48s\n",
      "8000:\ttest: 0.9045796\tbest: 0.9045859 (7997)\ttotal: 4m 18s\tremaining: 8h 53m 55s\n",
      "9000:\ttest: 0.9057177\tbest: 0.9057177 (9000)\ttotal: 4m 51s\tremaining: 8h 54m 39s\n",
      "10000:\ttest: 0.9065052\tbest: 0.9065052 (10000)\ttotal: 5m 24s\tremaining: 8h 55m 12s\n",
      "11000:\ttest: 0.9071071\tbest: 0.9071090 (10985)\ttotal: 5m 58s\tremaining: 8h 56m 35s\n",
      "12000:\ttest: 0.9076144\tbest: 0.9076144 (12000)\ttotal: 6m 32s\tremaining: 8h 58m 27s\n",
      "13000:\ttest: 0.9079196\tbest: 0.9079196 (13000)\ttotal: 7m 6s\tremaining: 8h 59m 28s\n",
      "14000:\ttest: 0.9082333\tbest: 0.9082350 (13995)\ttotal: 7m 40s\tremaining: 9h 28s\n",
      "15000:\ttest: 0.9084168\tbest: 0.9084181 (14999)\ttotal: 8m 13s\tremaining: 8h 59m 45s\n",
      "16000:\ttest: 0.9085745\tbest: 0.9085758 (15813)\ttotal: 8m 46s\tremaining: 8h 59m 26s\n",
      "17000:\ttest: 0.9088247\tbest: 0.9088247 (17000)\ttotal: 9m 19s\tremaining: 8h 58m 50s\n",
      "18000:\ttest: 0.9089278\tbest: 0.9089419 (17945)\ttotal: 9m 51s\tremaining: 8h 57m 50s\n",
      "19000:\ttest: 0.9090825\tbest: 0.9090870 (18979)\ttotal: 10m 24s\tremaining: 8h 57m 45s\n",
      "20000:\ttest: 0.9091445\tbest: 0.9091540 (19747)\ttotal: 10m 57s\tremaining: 8h 57m 4s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9091539523\n",
      "bestIteration = 19747\n",
      "\n",
      "Shrink model to first 19748 iterations.\n",
      "Fold 5 started at Fri Mar  8 05:56:24 2019\n",
      "0:\ttest: 0.5183733\tbest: 0.5183733 (0)\ttotal: 74.2ms\tremaining: 20h 36m 3s\n",
      "1000:\ttest: 0.8524177\tbest: 0.8524460 (995)\ttotal: 30.8s\tremaining: 8h 31m 40s\n",
      "2000:\ttest: 0.8720215\tbest: 0.8720227 (1998)\ttotal: 1m 2s\tremaining: 8h 38m 36s\n",
      "3000:\ttest: 0.8825498\tbest: 0.8825498 (3000)\ttotal: 1m 34s\tremaining: 8h 43m 9s\n",
      "4000:\ttest: 0.8890457\tbest: 0.8890495 (3999)\ttotal: 2m 5s\tremaining: 8h 39m 36s\n",
      "5000:\ttest: 0.8932082\tbest: 0.8932100 (4999)\ttotal: 2m 36s\tremaining: 8h 38m 50s\n",
      "6000:\ttest: 0.8957302\tbest: 0.8957308 (5998)\ttotal: 3m 9s\tremaining: 8h 42m 43s\n",
      "7000:\ttest: 0.8977335\tbest: 0.8977365 (6999)\ttotal: 3m 40s\tremaining: 8h 40m 54s\n",
      "8000:\ttest: 0.8991641\tbest: 0.8991641 (8000)\ttotal: 4m 11s\tremaining: 8h 39m 34s\n",
      "9000:\ttest: 0.9000588\tbest: 0.9000609 (8998)\ttotal: 4m 42s\tremaining: 8h 38m 25s\n",
      "10000:\ttest: 0.9008127\tbest: 0.9008179 (9994)\ttotal: 5m 15s\tremaining: 8h 40m 17s\n",
      "11000:\ttest: 0.9014663\tbest: 0.9014663 (11000)\ttotal: 5m 46s\tremaining: 8h 39m 8s\n",
      "12000:\ttest: 0.9018196\tbest: 0.9018366 (11981)\ttotal: 6m 18s\tremaining: 8h 39m 49s\n",
      "13000:\ttest: 0.9021652\tbest: 0.9021652 (13000)\ttotal: 6m 51s\tremaining: 8h 40m 33s\n",
      "14000:\ttest: 0.9024667\tbest: 0.9024667 (14000)\ttotal: 7m 23s\tremaining: 8h 40m 40s\n",
      "15000:\ttest: 0.9027058\tbest: 0.9027076 (14997)\ttotal: 7m 54s\tremaining: 8h 39m 25s\n",
      "16000:\ttest: 0.9028562\tbest: 0.9028650 (15841)\ttotal: 8m 26s\tremaining: 8h 39m 8s\n",
      "17000:\ttest: 0.9029844\tbest: 0.9029879 (16998)\ttotal: 8m 59s\tremaining: 8h 39m 34s\n",
      "18000:\ttest: 0.9030854\tbest: 0.9030901 (17975)\ttotal: 9m 30s\tremaining: 8h 38m 17s\n",
      "19000:\ttest: 0.9031390\tbest: 0.9031430 (18995)\ttotal: 10m\tremaining: 8h 36m 22s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.903143012\n",
      "bestIteration = 18995\n",
      "\n",
      "Shrink model to first 18996 iterations.\n",
      "Fold 6 started at Fri Mar  8 06:07:38 2019\n",
      "0:\ttest: 0.5258488\tbest: 0.5258488 (0)\ttotal: 47.6ms\tremaining: 13h 13m\n",
      "1000:\ttest: 0.8509334\tbest: 0.8509334 (1000)\ttotal: 31s\tremaining: 8h 35m 10s\n",
      "2000:\ttest: 0.8703932\tbest: 0.8703988 (1999)\ttotal: 1m 2s\tremaining: 8h 41m 21s\n",
      "3000:\ttest: 0.8815644\tbest: 0.8815818 (2996)\ttotal: 1m 34s\tremaining: 8h 44m 21s\n",
      "4000:\ttest: 0.8885812\tbest: 0.8885812 (4000)\ttotal: 2m 7s\tremaining: 8h 50m 11s\n",
      "5000:\ttest: 0.8930094\tbest: 0.8930102 (4998)\ttotal: 2m 40s\tremaining: 8h 51m 2s\n",
      "6000:\ttest: 0.8961822\tbest: 0.8961822 (6000)\ttotal: 3m 12s\tremaining: 8h 52m 2s\n",
      "7000:\ttest: 0.8982512\tbest: 0.8982567 (6999)\ttotal: 3m 45s\tremaining: 8h 52m 38s\n",
      "8000:\ttest: 0.8997351\tbest: 0.8997351 (8000)\ttotal: 4m 18s\tremaining: 8h 54m 40s\n",
      "9000:\ttest: 0.9008568\tbest: 0.9008583 (8999)\ttotal: 4m 50s\tremaining: 8h 53m 45s\n",
      "10000:\ttest: 0.9017728\tbest: 0.9017733 (9996)\ttotal: 5m 23s\tremaining: 8h 54m 8s\n",
      "11000:\ttest: 0.9023902\tbest: 0.9023902 (11000)\ttotal: 5m 56s\tremaining: 8h 53m 55s\n",
      "12000:\ttest: 0.9028116\tbest: 0.9028119 (11999)\ttotal: 6m 29s\tremaining: 8h 54m 15s\n",
      "13000:\ttest: 0.9031932\tbest: 0.9031932 (13000)\ttotal: 7m\tremaining: 8h 52m 1s\n",
      "14000:\ttest: 0.9034532\tbest: 0.9034552 (13992)\ttotal: 7m 32s\tremaining: 8h 51m 6s\n",
      "15000:\ttest: 0.9037662\tbest: 0.9037662 (15000)\ttotal: 8m 4s\tremaining: 8h 50m 6s\n",
      "16000:\ttest: 0.9039883\tbest: 0.9039912 (15989)\ttotal: 8m 37s\tremaining: 8h 50m 33s\n",
      "17000:\ttest: 0.9041545\tbest: 0.9041553 (16999)\ttotal: 9m 10s\tremaining: 8h 50m 29s\n",
      "18000:\ttest: 0.9042666\tbest: 0.9042673 (17985)\ttotal: 9m 43s\tremaining: 8h 50m 20s\n",
      "19000:\ttest: 0.9044351\tbest: 0.9044422 (18991)\ttotal: 10m 16s\tremaining: 8h 50m 49s\n",
      "20000:\ttest: 0.9045212\tbest: 0.9045294 (19598)\ttotal: 10m 50s\tremaining: 8h 51m 9s\n",
      "21000:\ttest: 0.9046216\tbest: 0.9046231 (20995)\ttotal: 11m 23s\tremaining: 8h 50m 44s\n",
      "22000:\ttest: 0.9046806\tbest: 0.9046937 (21571)\ttotal: 11m 59s\tremaining: 8h 53m 14s\n",
      "23000:\ttest: 0.9047092\tbest: 0.9047157 (22718)\ttotal: 12m 32s\tremaining: 8h 52m 33s\n",
      "24000:\ttest: 0.9047286\tbest: 0.9047414 (23762)\ttotal: 13m 5s\tremaining: 8h 52m 29s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9047414124\n",
      "bestIteration = 23762\n",
      "\n",
      "Shrink model to first 23763 iterations.\n",
      "Fold 7 started at Fri Mar  8 06:22:02 2019\n",
      "0:\ttest: 0.5174963\tbest: 0.5174963 (0)\ttotal: 55.9ms\tremaining: 15h 30m 56s\n",
      "1000:\ttest: 0.8427891\tbest: 0.8427891 (1000)\ttotal: 32.1s\tremaining: 8h 53m 40s\n",
      "2000:\ttest: 0.8632045\tbest: 0.8632045 (2000)\ttotal: 1m 3s\tremaining: 8h 49m 55s\n",
      "3000:\ttest: 0.8742418\tbest: 0.8742418 (3000)\ttotal: 1m 35s\tremaining: 8h 50m 20s\n",
      "4000:\ttest: 0.8802874\tbest: 0.8802874 (4000)\ttotal: 2m 6s\tremaining: 8h 44m 32s\n",
      "5000:\ttest: 0.8843544\tbest: 0.8843723 (4988)\ttotal: 2m 37s\tremaining: 8h 40m 55s\n",
      "6000:\ttest: 0.8870187\tbest: 0.8870369 (5995)\ttotal: 3m 8s\tremaining: 8h 39m 28s\n",
      "7000:\ttest: 0.8889204\tbest: 0.8889204 (7000)\ttotal: 3m 39s\tremaining: 8h 39m 45s\n",
      "8000:\ttest: 0.8902898\tbest: 0.8902898 (8000)\ttotal: 4m 11s\tremaining: 8h 39m 6s\n",
      "9000:\ttest: 0.8913055\tbest: 0.8913196 (8986)\ttotal: 4m 43s\tremaining: 8h 40m 34s\n",
      "10000:\ttest: 0.8920723\tbest: 0.8920787 (9997)\ttotal: 5m 15s\tremaining: 8h 40m 9s\n",
      "11000:\ttest: 0.8927189\tbest: 0.8927273 (10983)\ttotal: 5m 47s\tremaining: 8h 40m 42s\n",
      "12000:\ttest: 0.8932616\tbest: 0.8932628 (11984)\ttotal: 6m 19s\tremaining: 8h 40m 15s\n",
      "13000:\ttest: 0.8936436\tbest: 0.8936440 (12998)\ttotal: 6m 51s\tremaining: 8h 40m 11s\n",
      "14000:\ttest: 0.8939728\tbest: 0.8939729 (13999)\ttotal: 7m 22s\tremaining: 8h 39m 22s\n",
      "15000:\ttest: 0.8942270\tbest: 0.8942309 (14960)\ttotal: 7m 53s\tremaining: 8h 37m 57s\n",
      "16000:\ttest: 0.8943674\tbest: 0.8943766 (15993)\ttotal: 8m 25s\tremaining: 8h 38m 17s\n",
      "17000:\ttest: 0.8945494\tbest: 0.8945494 (17000)\ttotal: 8m 56s\tremaining: 8h 37m 20s\n",
      "18000:\ttest: 0.8947125\tbest: 0.8947132 (17983)\ttotal: 9m 28s\tremaining: 8h 37m 18s\n",
      "19000:\ttest: 0.8947594\tbest: 0.8947929 (18799)\ttotal: 10m 1s\tremaining: 8h 37m 47s\n",
      "20000:\ttest: 0.8948385\tbest: 0.8948466 (19781)\ttotal: 10m 32s\tremaining: 8h 36m 27s\n",
      "21000:\ttest: 0.8948839\tbest: 0.8948849 (20964)\ttotal: 11m 4s\tremaining: 8h 36m 4s\n",
      "22000:\ttest: 0.8949500\tbest: 0.8949527 (21645)\ttotal: 11m 35s\tremaining: 8h 35m 29s\n",
      "23000:\ttest: 0.8950590\tbest: 0.8950715 (22808)\ttotal: 12m 6s\tremaining: 8h 34m 20s\n",
      "24000:\ttest: 0.8951560\tbest: 0.8951653 (23957)\ttotal: 12m 37s\tremaining: 8h 33m 25s\n",
      "25000:\ttest: 0.8951952\tbest: 0.8952089 (24866)\ttotal: 13m 9s\tremaining: 8h 33m 10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26000:\ttest: 0.8952171\tbest: 0.8952185 (25992)\ttotal: 13m 41s\tremaining: 8h 32m 41s\n",
      "27000:\ttest: 0.8952679\tbest: 0.8952762 (26940)\ttotal: 14m 13s\tremaining: 8h 32m 22s\n",
      "28000:\ttest: 0.8953068\tbest: 0.8953225 (27881)\ttotal: 14m 45s\tremaining: 8h 32m 32s\n",
      "29000:\ttest: 0.8953495\tbest: 0.8953532 (28986)\ttotal: 15m 17s\tremaining: 8h 31m 47s\n",
      "30000:\ttest: 0.8952995\tbest: 0.8953533 (29037)\ttotal: 15m 47s\tremaining: 8h 30m 38s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8953532781\n",
      "bestIteration = 29037\n",
      "\n",
      "Shrink model to first 29038 iterations.\n",
      "Fold 8 started at Fri Mar  8 06:38:57 2019\n",
      "0:\ttest: 0.5233556\tbest: 0.5233556 (0)\ttotal: 52ms\tremaining: 14h 27m 14s\n",
      "1000:\ttest: 0.8441595\tbest: 0.8441595 (1000)\ttotal: 31.3s\tremaining: 8h 40m 21s\n",
      "2000:\ttest: 0.8629145\tbest: 0.8629145 (2000)\ttotal: 1m 2s\tremaining: 8h 43m 14s\n",
      "3000:\ttest: 0.8735582\tbest: 0.8735582 (3000)\ttotal: 1m 35s\tremaining: 8h 46m 46s\n",
      "4000:\ttest: 0.8798576\tbest: 0.8798618 (3997)\ttotal: 2m 7s\tremaining: 8h 49m 5s\n",
      "5000:\ttest: 0.8844316\tbest: 0.8844316 (5000)\ttotal: 2m 41s\tremaining: 8h 55m 27s\n",
      "6000:\ttest: 0.8876894\tbest: 0.8876937 (5999)\ttotal: 3m 14s\tremaining: 8h 57m 26s\n",
      "7000:\ttest: 0.8898791\tbest: 0.8898791 (7000)\ttotal: 3m 48s\tremaining: 8h 59m 45s\n",
      "8000:\ttest: 0.8916372\tbest: 0.8916372 (8000)\ttotal: 4m 20s\tremaining: 8h 58m 1s\n",
      "9000:\ttest: 0.8928233\tbest: 0.8928293 (8993)\ttotal: 4m 53s\tremaining: 8h 58m 55s\n",
      "10000:\ttest: 0.8937061\tbest: 0.8937063 (9994)\ttotal: 5m 25s\tremaining: 8h 57m 6s\n",
      "11000:\ttest: 0.8943046\tbest: 0.8943046 (11000)\ttotal: 5m 57s\tremaining: 8h 55m 57s\n",
      "12000:\ttest: 0.8948478\tbest: 0.8948481 (11999)\ttotal: 6m 30s\tremaining: 8h 55m 26s\n",
      "13000:\ttest: 0.8952473\tbest: 0.8952473 (13000)\ttotal: 7m 2s\tremaining: 8h 54m 55s\n",
      "14000:\ttest: 0.8955992\tbest: 0.8956013 (13996)\ttotal: 7m 34s\tremaining: 8h 53m 38s\n",
      "15000:\ttest: 0.8959447\tbest: 0.8959547 (14984)\ttotal: 8m 6s\tremaining: 8h 52m 39s\n",
      "16000:\ttest: 0.8961266\tbest: 0.8961311 (15979)\ttotal: 8m 39s\tremaining: 8h 52m 28s\n",
      "17000:\ttest: 0.8962497\tbest: 0.8962528 (16996)\ttotal: 9m 11s\tremaining: 8h 51m 28s\n",
      "18000:\ttest: 0.8963673\tbest: 0.8963729 (17978)\ttotal: 9m 44s\tremaining: 8h 51m 33s\n",
      "19000:\ttest: 0.8964828\tbest: 0.8964863 (18967)\ttotal: 10m 18s\tremaining: 8h 52m 3s\n",
      "20000:\ttest: 0.8965538\tbest: 0.8965769 (19451)\ttotal: 10m 51s\tremaining: 8h 52m 9s\n",
      "21000:\ttest: 0.8966249\tbest: 0.8966379 (20977)\ttotal: 11m 24s\tremaining: 8h 51m 59s\n",
      "22000:\ttest: 0.8967359\tbest: 0.8967367 (21999)\ttotal: 11m 57s\tremaining: 8h 51m 16s\n",
      "23000:\ttest: 0.8968329\tbest: 0.8968357 (22999)\ttotal: 12m 28s\tremaining: 8h 50m 10s\n",
      "24000:\ttest: 0.8968808\tbest: 0.8968976 (23816)\ttotal: 13m 1s\tremaining: 8h 49m 50s\n",
      "25000:\ttest: 0.8969500\tbest: 0.8969610 (24717)\ttotal: 13m 34s\tremaining: 8h 49m 12s\n",
      "26000:\ttest: 0.8970109\tbest: 0.8970110 (25953)\ttotal: 14m 6s\tremaining: 8h 48m 22s\n",
      "27000:\ttest: 0.8970367\tbest: 0.8970501 (26881)\ttotal: 14m 38s\tremaining: 8h 47m 21s\n",
      "28000:\ttest: 0.8971444\tbest: 0.8971544 (27910)\ttotal: 15m 10s\tremaining: 8h 46m 41s\n",
      "29000:\ttest: 0.8971839\tbest: 0.8971964 (28884)\ttotal: 15m 42s\tremaining: 8h 45m 53s\n",
      "30000:\ttest: 0.8971937\tbest: 0.8972230 (29695)\ttotal: 16m 15s\tremaining: 8h 45m 38s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.89722303\n",
      "bestIteration = 29695\n",
      "\n",
      "Shrink model to first 29696 iterations.\n",
      "Fold 9 started at Fri Mar  8 06:56:49 2019\n",
      "0:\ttest: 0.5233504\tbest: 0.5233504 (0)\ttotal: 40.5ms\tremaining: 11h 15m 41s\n",
      "1000:\ttest: 0.8491234\tbest: 0.8491234 (1000)\ttotal: 32.3s\tremaining: 8h 57m 23s\n",
      "2000:\ttest: 0.8668156\tbest: 0.8668156 (2000)\ttotal: 1m 4s\tremaining: 8h 58m 28s\n",
      "3000:\ttest: 0.8773947\tbest: 0.8774101 (2997)\ttotal: 1m 38s\tremaining: 9h 2m 42s\n",
      "4000:\ttest: 0.8837467\tbest: 0.8837499 (3997)\ttotal: 2m 11s\tremaining: 9h 3m 44s\n",
      "5000:\ttest: 0.8878397\tbest: 0.8878397 (5000)\ttotal: 2m 44s\tremaining: 9h 6m 25s\n",
      "6000:\ttest: 0.8905275\tbest: 0.8905282 (5996)\ttotal: 3m 19s\tremaining: 9h 9m 57s\n",
      "7000:\ttest: 0.8925573\tbest: 0.8925594 (6999)\ttotal: 3m 53s\tremaining: 9h 11m 29s\n",
      "8000:\ttest: 0.8939269\tbest: 0.8939269 (8000)\ttotal: 4m 26s\tremaining: 9h 10m 40s\n",
      "9000:\ttest: 0.8948658\tbest: 0.8948658 (9000)\ttotal: 5m\tremaining: 9h 11m 57s\n",
      "10000:\ttest: 0.8955005\tbest: 0.8955120 (9988)\ttotal: 5m 33s\tremaining: 9h 10m 55s\n",
      "11000:\ttest: 0.8959928\tbest: 0.8959928 (11000)\ttotal: 6m 5s\tremaining: 9h 8m 18s\n",
      "12000:\ttest: 0.8964475\tbest: 0.8964515 (11998)\ttotal: 6m 38s\tremaining: 9h 7m 17s\n",
      "13000:\ttest: 0.8967221\tbest: 0.8967318 (12992)\ttotal: 7m 10s\tremaining: 9h 4m 43s\n",
      "14000:\ttest: 0.8969410\tbest: 0.8969432 (13999)\ttotal: 7m 42s\tremaining: 9h 2m 56s\n",
      "15000:\ttest: 0.8971112\tbest: 0.8971158 (14985)\ttotal: 8m 14s\tremaining: 9h 52s\n",
      "16000:\ttest: 0.8972137\tbest: 0.8972353 (15806)\ttotal: 8m 45s\tremaining: 8h 58m 54s\n",
      "17000:\ttest: 0.8973866\tbest: 0.8973948 (16989)\ttotal: 9m 17s\tremaining: 8h 57m 1s\n",
      "18000:\ttest: 0.8974930\tbest: 0.8974995 (17980)\ttotal: 9m 49s\tremaining: 8h 56m 17s\n",
      "19000:\ttest: 0.8975790\tbest: 0.8975841 (18792)\ttotal: 10m 23s\tremaining: 8h 56m 23s\n",
      "20000:\ttest: 0.8976578\tbest: 0.8976624 (19933)\ttotal: 10m 56s\tremaining: 8h 55m 45s\n",
      "21000:\ttest: 0.8976967\tbest: 0.8977006 (20998)\ttotal: 11m 28s\tremaining: 8h 55m\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8977005919\n",
      "bestIteration = 20998\n",
      "\n",
      "Shrink model to first 20999 iterations.\n",
      "CV mean score: 0.9007, std: 0.0039.\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Fold 0 started at Fri Mar  8 07:09:38 2019\n",
      "0:\ttest: 0.5249855\tbest: 0.5249855 (0)\ttotal: 62.8ms\tremaining: 17h 26m 23s\n",
      "1000:\ttest: 0.8526664\tbest: 0.8526699 (999)\ttotal: 30.6s\tremaining: 8h 29m 2s\n",
      "2000:\ttest: 0.8718355\tbest: 0.8718587 (1994)\ttotal: 1m 2s\tremaining: 8h 41m 27s\n",
      "3000:\ttest: 0.8819545\tbest: 0.8819545 (3000)\ttotal: 1m 36s\tremaining: 8h 57m\n",
      "4000:\ttest: 0.8879355\tbest: 0.8879355 (4000)\ttotal: 2m 11s\tremaining: 9h 4m 8s\n",
      "5000:\ttest: 0.8914334\tbest: 0.8914381 (4994)\ttotal: 2m 42s\tremaining: 9h 12s\n",
      "6000:\ttest: 0.8939969\tbest: 0.8939969 (6000)\ttotal: 3m 14s\tremaining: 8h 58m 1s\n",
      "7000:\ttest: 0.8957528\tbest: 0.8957576 (6995)\ttotal: 3m 47s\tremaining: 8h 57m 38s\n",
      "8000:\ttest: 0.8969107\tbest: 0.8969107 (8000)\ttotal: 4m 20s\tremaining: 8h 58m 44s\n",
      "9000:\ttest: 0.8979157\tbest: 0.8979157 (9000)\ttotal: 4m 52s\tremaining: 8h 57m 17s\n",
      "10000:\ttest: 0.8986343\tbest: 0.8986444 (9991)\ttotal: 5m 24s\tremaining: 8h 55m 20s\n",
      "11000:\ttest: 0.8991869\tbest: 0.8991878 (10982)\ttotal: 5m 58s\tremaining: 8h 57m 16s\n",
      "12000:\ttest: 0.8996331\tbest: 0.8996444 (11948)\ttotal: 6m 31s\tremaining: 8h 56m 39s\n",
      "13000:\ttest: 0.8999914\tbest: 0.8999964 (12989)\ttotal: 7m 3s\tremaining: 8h 55m 42s\n",
      "14000:\ttest: 0.9002567\tbest: 0.9002700 (13937)\ttotal: 7m 35s\tremaining: 8h 54m 28s\n",
      "15000:\ttest: 0.9003375\tbest: 0.9003711 (14950)\ttotal: 8m 8s\tremaining: 8h 54m 33s\n",
      "16000:\ttest: 0.9004915\tbest: 0.9004915 (16000)\ttotal: 8m 41s\tremaining: 8h 54m 22s\n",
      "17000:\ttest: 0.9006121\tbest: 0.9006121 (17000)\ttotal: 9m 13s\tremaining: 8h 53m 20s\n",
      "18000:\ttest: 0.9007678\tbest: 0.9007721 (17956)\ttotal: 9m 46s\tremaining: 8h 52m 57s\n",
      "19000:\ttest: 0.9008962\tbest: 0.9009002 (18997)\ttotal: 10m 19s\tremaining: 8h 52m 41s\n",
      "20000:\ttest: 0.9009926\tbest: 0.9009975 (19949)\ttotal: 10m 50s\tremaining: 8h 51m 21s\n",
      "21000:\ttest: 0.9010454\tbest: 0.9010473 (20994)\ttotal: 11m 21s\tremaining: 8h 49m 27s\n",
      "22000:\ttest: 0.9010417\tbest: 0.9010859 (21707)\ttotal: 11m 53s\tremaining: 8h 48m 50s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9010859216\n",
      "bestIteration = 21707\n",
      "\n",
      "Shrink model to first 21708 iterations.\n",
      "Fold 1 started at Fri Mar  8 07:22:46 2019\n",
      "0:\ttest: 0.5129713\tbest: 0.5129713 (0)\ttotal: 42.4ms\tremaining: 11h 47m 9s\n",
      "1000:\ttest: 0.8470641\tbest: 0.8470847 (997)\ttotal: 30.4s\tremaining: 8h 25m 1s\n",
      "2000:\ttest: 0.8665971\tbest: 0.8665971 (2000)\ttotal: 1m 2s\tremaining: 8h 35m 32s\n",
      "3000:\ttest: 0.8772385\tbest: 0.8772385 (3000)\ttotal: 1m 36s\tremaining: 8h 52m 17s\n",
      "4000:\ttest: 0.8837952\tbest: 0.8837970 (3997)\ttotal: 2m 8s\tremaining: 8h 54m 32s\n",
      "5000:\ttest: 0.8881736\tbest: 0.8881736 (5000)\ttotal: 2m 40s\tremaining: 8h 53m 20s\n",
      "6000:\ttest: 0.8905751\tbest: 0.8905751 (6000)\ttotal: 3m 13s\tremaining: 8h 55m 1s\n",
      "7000:\ttest: 0.8927814\tbest: 0.8927814 (7000)\ttotal: 3m 45s\tremaining: 8h 53m 23s\n",
      "8000:\ttest: 0.8940483\tbest: 0.8940490 (7997)\ttotal: 4m 18s\tremaining: 8h 53m 34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000:\ttest: 0.8949992\tbest: 0.8949992 (9000)\ttotal: 4m 50s\tremaining: 8h 53m 12s\n",
      "10000:\ttest: 0.8958546\tbest: 0.8958546 (10000)\ttotal: 5m 22s\tremaining: 8h 51m 27s\n",
      "11000:\ttest: 0.8964539\tbest: 0.8964547 (10995)\ttotal: 5m 53s\tremaining: 8h 50m 5s\n",
      "12000:\ttest: 0.8968371\tbest: 0.8968371 (12000)\ttotal: 6m 27s\tremaining: 8h 51m 2s\n",
      "13000:\ttest: 0.8971146\tbest: 0.8971158 (12999)\ttotal: 7m\tremaining: 8h 52m\n",
      "14000:\ttest: 0.8974625\tbest: 0.8974663 (13989)\ttotal: 7m 33s\tremaining: 8h 51m 46s\n",
      "15000:\ttest: 0.8976907\tbest: 0.8977009 (14978)\ttotal: 8m 5s\tremaining: 8h 50m 46s\n",
      "16000:\ttest: 0.8978149\tbest: 0.8978269 (15959)\ttotal: 8m 37s\tremaining: 8h 50m 12s\n",
      "17000:\ttest: 0.8979411\tbest: 0.8979467 (16992)\ttotal: 9m 9s\tremaining: 8h 49m 28s\n",
      "18000:\ttest: 0.8979958\tbest: 0.8980235 (17801)\ttotal: 9m 40s\tremaining: 8h 48m 12s\n",
      "19000:\ttest: 0.8981161\tbest: 0.8981281 (18916)\ttotal: 10m 12s\tremaining: 8h 47m 4s\n",
      "20000:\ttest: 0.8981521\tbest: 0.8981737 (19664)\ttotal: 10m 44s\tremaining: 8h 46m 11s\n",
      "21000:\ttest: 0.8981840\tbest: 0.8981904 (20695)\ttotal: 11m 16s\tremaining: 8h 45m 40s\n",
      "22000:\ttest: 0.8982126\tbest: 0.8982233 (21915)\ttotal: 11m 48s\tremaining: 8h 44m 59s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8982232686\n",
      "bestIteration = 21915\n",
      "\n",
      "Shrink model to first 21916 iterations.\n",
      "Fold 2 started at Fri Mar  8 07:35:54 2019\n",
      "0:\ttest: 0.5081412\tbest: 0.5081412 (0)\ttotal: 62.5ms\tremaining: 17h 21m 23s\n",
      "1000:\ttest: 0.8546443\tbest: 0.8546727 (996)\ttotal: 31.5s\tremaining: 8h 43m 24s\n",
      "2000:\ttest: 0.8724945\tbest: 0.8725018 (1997)\ttotal: 1m 3s\tremaining: 8h 49m 48s\n",
      "3000:\ttest: 0.8829788\tbest: 0.8829788 (3000)\ttotal: 1m 35s\tremaining: 8h 49m 56s\n",
      "4000:\ttest: 0.8883050\tbest: 0.8883050 (4000)\ttotal: 2m 8s\tremaining: 8h 52m 20s\n",
      "5000:\ttest: 0.8916127\tbest: 0.8916205 (4998)\ttotal: 2m 40s\tremaining: 8h 52m 24s\n",
      "6000:\ttest: 0.8939042\tbest: 0.8939042 (6000)\ttotal: 3m 12s\tremaining: 8h 51m 36s\n",
      "7000:\ttest: 0.8951869\tbest: 0.8951892 (6994)\ttotal: 3m 45s\tremaining: 8h 52m 12s\n",
      "8000:\ttest: 0.8962697\tbest: 0.8962735 (7999)\ttotal: 4m 17s\tremaining: 8h 52m 14s\n",
      "9000:\ttest: 0.8970280\tbest: 0.8970280 (9000)\ttotal: 4m 50s\tremaining: 8h 53m 40s\n",
      "10000:\ttest: 0.8976132\tbest: 0.8976132 (10000)\ttotal: 5m 23s\tremaining: 8h 53m 38s\n",
      "11000:\ttest: 0.8980276\tbest: 0.8980372 (10993)\ttotal: 5m 56s\tremaining: 8h 53m 38s\n",
      "12000:\ttest: 0.8983828\tbest: 0.8983934 (11829)\ttotal: 6m 30s\tremaining: 8h 55m 27s\n",
      "13000:\ttest: 0.8985870\tbest: 0.8985870 (13000)\ttotal: 7m 3s\tremaining: 8h 55m 29s\n",
      "14000:\ttest: 0.8987740\tbest: 0.8987819 (13994)\ttotal: 7m 37s\tremaining: 8h 56m 30s\n",
      "15000:\ttest: 0.8989254\tbest: 0.8989349 (14767)\ttotal: 8m 9s\tremaining: 8h 56m 8s\n",
      "16000:\ttest: 0.8990371\tbest: 0.8990387 (15996)\ttotal: 8m 42s\tremaining: 8h 55m 30s\n",
      "17000:\ttest: 0.8991405\tbest: 0.8991437 (16882)\ttotal: 9m 14s\tremaining: 8h 54m 40s\n",
      "18000:\ttest: 0.8992274\tbest: 0.8992417 (17880)\ttotal: 9m 48s\tremaining: 8h 54m 43s\n",
      "19000:\ttest: 0.8992978\tbest: 0.8993032 (18992)\ttotal: 10m 20s\tremaining: 8h 53m 45s\n",
      "20000:\ttest: 0.8993223\tbest: 0.8993294 (19751)\ttotal: 10m 53s\tremaining: 8h 53m 23s\n",
      "21000:\ttest: 0.8993195\tbest: 0.8993597 (20643)\ttotal: 11m 26s\tremaining: 8h 53m 5s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8993597051\n",
      "bestIteration = 20643\n",
      "\n",
      "Shrink model to first 20644 iterations.\n",
      "Fold 3 started at Fri Mar  8 07:48:26 2019\n",
      "0:\ttest: 0.5145267\tbest: 0.5145267 (0)\ttotal: 45ms\tremaining: 12h 29m 54s\n",
      "1000:\ttest: 0.8536393\tbest: 0.8538087 (993)\ttotal: 31.6s\tremaining: 8h 46m 18s\n",
      "2000:\ttest: 0.8727840\tbest: 0.8727840 (2000)\ttotal: 1m 5s\tremaining: 9h 1m 45s\n",
      "3000:\ttest: 0.8830855\tbest: 0.8830855 (3000)\ttotal: 1m 38s\tremaining: 9h 4m 29s\n",
      "4000:\ttest: 0.8893412\tbest: 0.8893472 (3999)\ttotal: 2m 10s\tremaining: 9h 2m 19s\n",
      "5000:\ttest: 0.8928745\tbest: 0.8928745 (5000)\ttotal: 2m 44s\tremaining: 9h 5m 5s\n",
      "6000:\ttest: 0.8954920\tbest: 0.8954920 (6000)\ttotal: 3m 18s\tremaining: 9h 7m 8s\n",
      "7000:\ttest: 0.8973073\tbest: 0.8973236 (6995)\ttotal: 3m 51s\tremaining: 9h 7m 35s\n",
      "8000:\ttest: 0.8985820\tbest: 0.8985820 (8000)\ttotal: 4m 24s\tremaining: 9h 5m 49s\n",
      "9000:\ttest: 0.8996049\tbest: 0.8996049 (9000)\ttotal: 4m 55s\tremaining: 9h 2m 27s\n",
      "10000:\ttest: 0.9002507\tbest: 0.9002617 (9997)\ttotal: 5m 28s\tremaining: 9h 1m 28s\n",
      "11000:\ttest: 0.9006360\tbest: 0.9006389 (10999)\ttotal: 6m\tremaining: 9h 15s\n",
      "12000:\ttest: 0.9010287\tbest: 0.9010312 (11997)\ttotal: 6m 32s\tremaining: 8h 58m 52s\n",
      "13000:\ttest: 0.9013906\tbest: 0.9014057 (12962)\ttotal: 7m 6s\tremaining: 8h 59m 1s\n",
      "14000:\ttest: 0.9015472\tbest: 0.9015503 (13991)\ttotal: 7m 38s\tremaining: 8h 58m 21s\n",
      "15000:\ttest: 0.9017512\tbest: 0.9017561 (14989)\ttotal: 8m 11s\tremaining: 8h 57m 26s\n",
      "16000:\ttest: 0.9018881\tbest: 0.9018916 (15889)\ttotal: 8m 44s\tremaining: 8h 57m 16s\n",
      "17000:\ttest: 0.9019987\tbest: 0.9020055 (16988)\ttotal: 9m 18s\tremaining: 8h 58m 11s\n",
      "18000:\ttest: 0.9020771\tbest: 0.9020927 (17865)\ttotal: 9m 51s\tremaining: 8h 58m 8s\n",
      "19000:\ttest: 0.9021462\tbest: 0.9021606 (18778)\ttotal: 10m 22s\tremaining: 8h 56m\n",
      "20000:\ttest: 0.9021925\tbest: 0.9022088 (19579)\ttotal: 10m 55s\tremaining: 8h 55m 32s\n",
      "21000:\ttest: 0.9022478\tbest: 0.9022577 (20927)\ttotal: 11m 28s\tremaining: 8h 54m 47s\n",
      "22000:\ttest: 0.9022691\tbest: 0.9023016 (21486)\ttotal: 11m 59s\tremaining: 8h 53m 19s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9023015827\n",
      "bestIteration = 21486\n",
      "\n",
      "Shrink model to first 21487 iterations.\n",
      "Fold 4 started at Fri Mar  8 08:01:32 2019\n",
      "0:\ttest: 0.5216312\tbest: 0.5216312 (0)\ttotal: 51.3ms\tremaining: 14h 15m 10s\n",
      "1000:\ttest: 0.8549159\tbest: 0.8549335 (999)\ttotal: 31.2s\tremaining: 8h 39m 22s\n",
      "2000:\ttest: 0.8753426\tbest: 0.8753426 (2000)\ttotal: 1m 3s\tremaining: 8h 44m 13s\n",
      "3000:\ttest: 0.8871322\tbest: 0.8871322 (3000)\ttotal: 1m 34s\tremaining: 8h 41m 35s\n",
      "4000:\ttest: 0.8941416\tbest: 0.8941483 (3999)\ttotal: 2m 5s\tremaining: 8h 42m\n",
      "5000:\ttest: 0.8983186\tbest: 0.8983186 (5000)\ttotal: 2m 38s\tremaining: 8h 44m 21s\n",
      "6000:\ttest: 0.9013798\tbest: 0.9013798 (6000)\ttotal: 3m 11s\tremaining: 8h 48m 7s\n",
      "7000:\ttest: 0.9035649\tbest: 0.9035649 (7000)\ttotal: 3m 44s\tremaining: 8h 50m 42s\n",
      "8000:\ttest: 0.9050179\tbest: 0.9050179 (8000)\ttotal: 4m 17s\tremaining: 8h 51m 58s\n",
      "9000:\ttest: 0.9061569\tbest: 0.9061606 (8998)\ttotal: 4m 51s\tremaining: 8h 54m 14s\n",
      "10000:\ttest: 0.9069613\tbest: 0.9069616 (9999)\ttotal: 5m 25s\tremaining: 8h 56m 43s\n",
      "11000:\ttest: 0.9075052\tbest: 0.9075072 (10997)\ttotal: 5m 59s\tremaining: 8h 57m 59s\n",
      "12000:\ttest: 0.9079539\tbest: 0.9079619 (11973)\ttotal: 6m 32s\tremaining: 8h 59m\n",
      "13000:\ttest: 0.9083400\tbest: 0.9083438 (12985)\ttotal: 7m 6s\tremaining: 9h 8s\n",
      "14000:\ttest: 0.9086027\tbest: 0.9086036 (13995)\ttotal: 7m 39s\tremaining: 8h 58m 59s\n",
      "15000:\ttest: 0.9087430\tbest: 0.9087644 (14953)\ttotal: 8m 12s\tremaining: 8h 58m 27s\n",
      "16000:\ttest: 0.9089228\tbest: 0.9089319 (15991)\ttotal: 8m 44s\tremaining: 8h 57m 38s\n",
      "17000:\ttest: 0.9089554\tbest: 0.9089600 (16937)\ttotal: 9m 18s\tremaining: 8h 57m 44s\n",
      "18000:\ttest: 0.9089979\tbest: 0.9090065 (17935)\ttotal: 9m 50s\tremaining: 8h 57m 13s\n",
      "19000:\ttest: 0.9090651\tbest: 0.9090660 (18995)\ttotal: 10m 22s\tremaining: 8h 55m 52s\n",
      "20000:\ttest: 0.9090828\tbest: 0.9090934 (19597)\ttotal: 10m 55s\tremaining: 8h 55m 13s\n",
      "21000:\ttest: 0.9090855\tbest: 0.9091079 (20245)\ttotal: 11m 27s\tremaining: 8h 54m 21s\n",
      "22000:\ttest: 0.9091433\tbest: 0.9091450 (21916)\ttotal: 12m\tremaining: 8h 53m 43s\n",
      "23000:\ttest: 0.9092348\tbest: 0.9092348 (23000)\ttotal: 12m 32s\tremaining: 8h 52m 56s\n",
      "24000:\ttest: 0.9092567\tbest: 0.9092776 (23740)\ttotal: 13m 5s\tremaining: 8h 52m 29s\n",
      "25000:\ttest: 0.9092935\tbest: 0.9092943 (24984)\ttotal: 13m 37s\tremaining: 8h 51m 31s\n",
      "26000:\ttest: 0.9093072\tbest: 0.9093386 (25726)\ttotal: 14m 9s\tremaining: 8h 50m 36s\n",
      "27000:\ttest: 0.9093699\tbest: 0.9093764 (26874)\ttotal: 14m 42s\tremaining: 8h 50m 6s\n",
      "28000:\ttest: 0.9093709\tbest: 0.9093889 (27782)\ttotal: 15m 14s\tremaining: 8h 49m 6s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9093888534\n",
      "bestIteration = 27782\n",
      "\n",
      "Shrink model to first 27783 iterations.\n",
      "Fold 5 started at Fri Mar  8 08:18:18 2019\n",
      "0:\ttest: 0.5450061\tbest: 0.5450061 (0)\ttotal: 55.5ms\tremaining: 15h 24m 25s\n",
      "1000:\ttest: 0.8533788\tbest: 0.8533788 (1000)\ttotal: 30.3s\tremaining: 8h 23m 11s\n",
      "2000:\ttest: 0.8715738\tbest: 0.8715738 (2000)\ttotal: 1m 1s\tremaining: 8h 32m 50s\n",
      "3000:\ttest: 0.8827477\tbest: 0.8827477 (3000)\ttotal: 1m 34s\tremaining: 8h 41m 36s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000:\ttest: 0.8893600\tbest: 0.8893600 (4000)\ttotal: 2m 5s\tremaining: 8h 41m 29s\n",
      "5000:\ttest: 0.8933021\tbest: 0.8933021 (5000)\ttotal: 2m 37s\tremaining: 8h 43m 25s\n",
      "6000:\ttest: 0.8960230\tbest: 0.8960230 (6000)\ttotal: 3m 10s\tremaining: 8h 45m 18s\n",
      "7000:\ttest: 0.8980116\tbest: 0.8980208 (6996)\ttotal: 3m 41s\tremaining: 8h 43m 45s\n",
      "8000:\ttest: 0.8993614\tbest: 0.8993614 (8000)\ttotal: 4m 12s\tremaining: 8h 41m 35s\n",
      "9000:\ttest: 0.9002933\tbest: 0.9002942 (8999)\ttotal: 4m 45s\tremaining: 8h 43m 58s\n",
      "10000:\ttest: 0.9010167\tbest: 0.9010215 (9995)\ttotal: 5m 17s\tremaining: 8h 43m 39s\n",
      "11000:\ttest: 0.9015745\tbest: 0.9015761 (10996)\ttotal: 5m 50s\tremaining: 8h 44m 28s\n",
      "12000:\ttest: 0.9019095\tbest: 0.9019145 (11988)\ttotal: 6m 22s\tremaining: 8h 44m 18s\n",
      "13000:\ttest: 0.9022436\tbest: 0.9022452 (12998)\ttotal: 6m 54s\tremaining: 8h 44m 2s\n",
      "14000:\ttest: 0.9024609\tbest: 0.9024658 (13980)\ttotal: 7m 25s\tremaining: 8h 42m 55s\n",
      "15000:\ttest: 0.9026471\tbest: 0.9026471 (15000)\ttotal: 7m 57s\tremaining: 8h 42m 12s\n",
      "16000:\ttest: 0.9027775\tbest: 0.9027786 (15999)\ttotal: 8m 29s\tremaining: 8h 41m 46s\n",
      "17000:\ttest: 0.9028978\tbest: 0.9028978 (17000)\ttotal: 9m 1s\tremaining: 8h 41m 56s\n",
      "18000:\ttest: 0.9029681\tbest: 0.9029826 (17872)\ttotal: 9m 33s\tremaining: 8h 41m 36s\n",
      "19000:\ttest: 0.9030193\tbest: 0.9030243 (18969)\ttotal: 10m 6s\tremaining: 8h 41m 36s\n",
      "20000:\ttest: 0.9030468\tbest: 0.9030584 (19724)\ttotal: 10m 37s\tremaining: 8h 40m 54s\n",
      "21000:\ttest: 0.9030944\tbest: 0.9031058 (20830)\ttotal: 11m 9s\tremaining: 8h 40m 16s\n",
      "22000:\ttest: 0.9031461\tbest: 0.9031461 (22000)\ttotal: 11m 40s\tremaining: 8h 39m 5s\n",
      "23000:\ttest: 0.9031704\tbest: 0.9031846 (22608)\ttotal: 12m 12s\tremaining: 8h 38m 17s\n",
      "24000:\ttest: 0.9031776\tbest: 0.9031906 (23470)\ttotal: 12m 43s\tremaining: 8h 37m 41s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9031905509\n",
      "bestIteration = 23470\n",
      "\n",
      "Shrink model to first 23471 iterations.\n",
      "Fold 6 started at Fri Mar  8 08:32:11 2019\n",
      "0:\ttest: 0.5100487\tbest: 0.5100487 (0)\ttotal: 50.2ms\tremaining: 13h 56m 46s\n",
      "1000:\ttest: 0.8502897\tbest: 0.8503289 (986)\ttotal: 29.8s\tremaining: 8h 16m 23s\n",
      "2000:\ttest: 0.8711614\tbest: 0.8711614 (2000)\ttotal: 1m 1s\tremaining: 8h 34m\n",
      "3000:\ttest: 0.8825659\tbest: 0.8825659 (3000)\ttotal: 1m 33s\tremaining: 8h 38m 5s\n",
      "4000:\ttest: 0.8894331\tbest: 0.8894387 (3999)\ttotal: 2m 6s\tremaining: 8h 45m 5s\n",
      "5000:\ttest: 0.8934800\tbest: 0.8934806 (4997)\ttotal: 2m 38s\tremaining: 8h 45m 22s\n",
      "6000:\ttest: 0.8964419\tbest: 0.8964450 (5999)\ttotal: 3m 10s\tremaining: 8h 45m 35s\n",
      "7000:\ttest: 0.8983745\tbest: 0.8983776 (6999)\ttotal: 3m 42s\tremaining: 8h 46m 4s\n",
      "8000:\ttest: 0.8998094\tbest: 0.8998099 (7996)\ttotal: 4m 14s\tremaining: 8h 45m 21s\n",
      "9000:\ttest: 0.9008720\tbest: 0.9008720 (9000)\ttotal: 4m 45s\tremaining: 8h 44m 40s\n",
      "10000:\ttest: 0.9016538\tbest: 0.9016550 (9999)\ttotal: 5m 18s\tremaining: 8h 45m 9s\n",
      "11000:\ttest: 0.9022850\tbest: 0.9022850 (11000)\ttotal: 5m 52s\tremaining: 8h 47m 27s\n",
      "12000:\ttest: 0.9028569\tbest: 0.9028598 (11992)\ttotal: 6m 24s\tremaining: 8h 47m 38s\n",
      "13000:\ttest: 0.9032759\tbest: 0.9032787 (12997)\ttotal: 6m 57s\tremaining: 8h 48m 32s\n",
      "14000:\ttest: 0.9035548\tbest: 0.9035587 (13997)\ttotal: 7m 30s\tremaining: 8h 48m 13s\n",
      "15000:\ttest: 0.9038873\tbest: 0.9038895 (14989)\ttotal: 8m 1s\tremaining: 8h 47m 13s\n",
      "16000:\ttest: 0.9040946\tbest: 0.9040994 (15985)\ttotal: 8m 33s\tremaining: 8h 46m 42s\n",
      "17000:\ttest: 0.9042687\tbest: 0.9042687 (17000)\ttotal: 9m 5s\tremaining: 8h 45m 46s\n",
      "18000:\ttest: 0.9044226\tbest: 0.9044226 (18000)\ttotal: 9m 38s\tremaining: 8h 45m 34s\n",
      "19000:\ttest: 0.9045839\tbest: 0.9045899 (18971)\ttotal: 10m 10s\tremaining: 8h 45m 21s\n",
      "20000:\ttest: 0.9047179\tbest: 0.9047179 (20000)\ttotal: 10m 42s\tremaining: 8h 44m 35s\n",
      "21000:\ttest: 0.9047331\tbest: 0.9047872 (20808)\ttotal: 11m 14s\tremaining: 8h 44m 8s\n",
      "22000:\ttest: 0.9047595\tbest: 0.9047881 (21720)\ttotal: 11m 47s\tremaining: 8h 43m 48s\n",
      "23000:\ttest: 0.9048412\tbest: 0.9048420 (22998)\ttotal: 12m 19s\tremaining: 8h 43m 19s\n",
      "24000:\ttest: 0.9048891\tbest: 0.9049238 (23565)\ttotal: 12m 51s\tremaining: 8h 42m 59s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9049237968\n",
      "bestIteration = 23565\n",
      "\n",
      "Shrink model to first 23566 iterations.\n",
      "Fold 7 started at Fri Mar  8 08:46:14 2019\n",
      "0:\ttest: 0.5205562\tbest: 0.5205562 (0)\ttotal: 41.3ms\tremaining: 11h 29m 1s\n",
      "1000:\ttest: 0.8433224\tbest: 0.8433495 (999)\ttotal: 31.9s\tremaining: 8h 49m 51s\n",
      "2000:\ttest: 0.8643427\tbest: 0.8643427 (2000)\ttotal: 1m 3s\tremaining: 8h 48m 35s\n",
      "3000:\ttest: 0.8750325\tbest: 0.8750325 (3000)\ttotal: 1m 34s\tremaining: 8h 45m 25s\n",
      "4000:\ttest: 0.8810439\tbest: 0.8810500 (3999)\ttotal: 2m 8s\tremaining: 8h 51m 48s\n",
      "5000:\ttest: 0.8849544\tbest: 0.8849564 (4998)\ttotal: 2m 40s\tremaining: 8h 52m 58s\n",
      "6000:\ttest: 0.8875246\tbest: 0.8875285 (5997)\ttotal: 3m 13s\tremaining: 8h 52m 59s\n",
      "7000:\ttest: 0.8892563\tbest: 0.8892563 (7000)\ttotal: 3m 45s\tremaining: 8h 52m 58s\n",
      "8000:\ttest: 0.8906106\tbest: 0.8906106 (8000)\ttotal: 4m 18s\tremaining: 8h 53m 54s\n",
      "9000:\ttest: 0.8916090\tbest: 0.8916090 (9000)\ttotal: 4m 50s\tremaining: 8h 52m 37s\n",
      "10000:\ttest: 0.8924144\tbest: 0.8924167 (9998)\ttotal: 5m 23s\tremaining: 8h 53m 11s\n",
      "11000:\ttest: 0.8929638\tbest: 0.8929638 (11000)\ttotal: 5m 56s\tremaining: 8h 53m 32s\n",
      "12000:\ttest: 0.8934548\tbest: 0.8934610 (11991)\ttotal: 6m 29s\tremaining: 8h 54m 54s\n",
      "13000:\ttest: 0.8938917\tbest: 0.8939006 (12984)\ttotal: 7m 2s\tremaining: 8h 53m 58s\n",
      "14000:\ttest: 0.8941455\tbest: 0.8941567 (13936)\ttotal: 7m 33s\tremaining: 8h 52m 45s\n",
      "15000:\ttest: 0.8943509\tbest: 0.8943563 (14997)\ttotal: 8m 7s\tremaining: 8h 53m 3s\n",
      "16000:\ttest: 0.8945238\tbest: 0.8945240 (15999)\ttotal: 8m 40s\tremaining: 8h 53m 39s\n",
      "17000:\ttest: 0.8947609\tbest: 0.8947646 (16970)\ttotal: 9m 12s\tremaining: 8h 52m 27s\n",
      "18000:\ttest: 0.8948533\tbest: 0.8948570 (17993)\ttotal: 9m 45s\tremaining: 8h 52m 3s\n",
      "19000:\ttest: 0.8950266\tbest: 0.8950291 (18972)\ttotal: 10m 17s\tremaining: 8h 51m 27s\n",
      "20000:\ttest: 0.8951115\tbest: 0.8951215 (19841)\ttotal: 10m 51s\tremaining: 8h 51m 58s\n",
      "21000:\ttest: 0.8951859\tbest: 0.8951872 (20997)\ttotal: 11m 24s\tremaining: 8h 51m 41s\n",
      "22000:\ttest: 0.8952548\tbest: 0.8952661 (21984)\ttotal: 11m 58s\tremaining: 8h 52m 11s\n",
      "23000:\ttest: 0.8953010\tbest: 0.8953010 (23000)\ttotal: 12m 30s\tremaining: 8h 51m 36s\n",
      "24000:\ttest: 0.8952675\tbest: 0.8953254 (23266)\ttotal: 13m 3s\tremaining: 8h 51m 15s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8953254296\n",
      "bestIteration = 23266\n",
      "\n",
      "Shrink model to first 23267 iterations.\n",
      "Fold 8 started at Fri Mar  8 09:00:21 2019\n",
      "0:\ttest: 0.5094628\tbest: 0.5094628 (0)\ttotal: 72ms\tremaining: 20h 22s\n",
      "1000:\ttest: 0.8445805\tbest: 0.8445931 (998)\ttotal: 31.4s\tremaining: 8h 41m 49s\n",
      "2000:\ttest: 0.8631985\tbest: 0.8631985 (2000)\ttotal: 1m 3s\tremaining: 8h 45m 48s\n",
      "3000:\ttest: 0.8743652\tbest: 0.8743657 (2999)\ttotal: 1m 35s\tremaining: 8h 47m 58s\n",
      "4000:\ttest: 0.8804521\tbest: 0.8804539 (3999)\ttotal: 2m 8s\tremaining: 8h 51m 55s\n",
      "5000:\ttest: 0.8850089\tbest: 0.8850091 (4999)\ttotal: 2m 39s\tremaining: 8h 50m 7s\n",
      "6000:\ttest: 0.8880653\tbest: 0.8880715 (5998)\ttotal: 3m 13s\tremaining: 8h 53m 16s\n",
      "7000:\ttest: 0.8901157\tbest: 0.8901266 (6999)\ttotal: 3m 45s\tremaining: 8h 53m 5s\n",
      "8000:\ttest: 0.8917607\tbest: 0.8917625 (7993)\ttotal: 4m 18s\tremaining: 8h 54m 4s\n",
      "9000:\ttest: 0.8927688\tbest: 0.8927775 (8991)\ttotal: 4m 51s\tremaining: 8h 54m 35s\n",
      "10000:\ttest: 0.8938224\tbest: 0.8938248 (9999)\ttotal: 5m 24s\tremaining: 8h 55m 28s\n",
      "11000:\ttest: 0.8945604\tbest: 0.8945604 (11000)\ttotal: 5m 56s\tremaining: 8h 54m\n",
      "12000:\ttest: 0.8950973\tbest: 0.8951005 (11999)\ttotal: 6m 27s\tremaining: 8h 51m 8s\n",
      "13000:\ttest: 0.8955387\tbest: 0.8955398 (12998)\ttotal: 6m 59s\tremaining: 8h 50m 21s\n",
      "14000:\ttest: 0.8959008\tbest: 0.8959122 (13938)\ttotal: 7m 31s\tremaining: 8h 50m 15s\n",
      "15000:\ttest: 0.8961927\tbest: 0.8961927 (15000)\ttotal: 8m 4s\tremaining: 8h 50m 45s\n",
      "16000:\ttest: 0.8963527\tbest: 0.8963683 (15733)\ttotal: 8m 37s\tremaining: 8h 50m 47s\n",
      "17000:\ttest: 0.8965170\tbest: 0.8965239 (16985)\ttotal: 9m 11s\tremaining: 8h 51m 45s\n",
      "18000:\ttest: 0.8966292\tbest: 0.8966410 (17794)\ttotal: 9m 43s\tremaining: 8h 50m 51s\n",
      "19000:\ttest: 0.8967379\tbest: 0.8967387 (18994)\ttotal: 10m 15s\tremaining: 8h 49m 23s\n",
      "20000:\ttest: 0.8968308\tbest: 0.8968345 (19933)\ttotal: 10m 46s\tremaining: 8h 48m 14s\n",
      "21000:\ttest: 0.8968527\tbest: 0.8968747 (20830)\ttotal: 11m 18s\tremaining: 8h 47m 13s\n",
      "22000:\ttest: 0.8969264\tbest: 0.8969545 (21839)\ttotal: 11m 51s\tremaining: 8h 47m 2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23000:\ttest: 0.8969270\tbest: 0.8969574 (22736)\ttotal: 12m 23s\tremaining: 8h 46m 14s\n",
      "24000:\ttest: 0.8969706\tbest: 0.8969848 (23906)\ttotal: 12m 55s\tremaining: 8h 45m 46s\n",
      "25000:\ttest: 0.8969694\tbest: 0.8969872 (24075)\ttotal: 13m 28s\tremaining: 8h 45m 23s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8969871819\n",
      "bestIteration = 24075\n",
      "\n",
      "Shrink model to first 24076 iterations.\n",
      "Fold 9 started at Fri Mar  8 09:14:48 2019\n",
      "0:\ttest: 0.5454428\tbest: 0.5454428 (0)\ttotal: 64.8ms\tremaining: 17h 59m 13s\n",
      "1000:\ttest: 0.8497704\tbest: 0.8498016 (999)\ttotal: 30.6s\tremaining: 8h 29m 31s\n",
      "2000:\ttest: 0.8679338\tbest: 0.8679338 (2000)\ttotal: 1m 3s\tremaining: 8h 44m 37s\n",
      "3000:\ttest: 0.8783526\tbest: 0.8783526 (3000)\ttotal: 1m 35s\tremaining: 8h 49m 43s\n",
      "4000:\ttest: 0.8843614\tbest: 0.8843614 (4000)\ttotal: 2m 8s\tremaining: 8h 51m 27s\n",
      "5000:\ttest: 0.8884443\tbest: 0.8884514 (4995)\ttotal: 2m 40s\tremaining: 8h 51m 54s\n",
      "6000:\ttest: 0.8912048\tbest: 0.8912048 (6000)\ttotal: 3m 12s\tremaining: 8h 51m 29s\n",
      "7000:\ttest: 0.8931197\tbest: 0.8931209 (6997)\ttotal: 3m 45s\tremaining: 8h 52m 42s\n",
      "8000:\ttest: 0.8943751\tbest: 0.8943784 (7997)\ttotal: 4m 16s\tremaining: 8h 49m 17s\n",
      "9000:\ttest: 0.8953333\tbest: 0.8953333 (9000)\ttotal: 4m 47s\tremaining: 8h 48m 20s\n",
      "10000:\ttest: 0.8961397\tbest: 0.8961399 (9996)\ttotal: 5m 20s\tremaining: 8h 47m 57s\n",
      "11000:\ttest: 0.8966309\tbest: 0.8966367 (10905)\ttotal: 5m 53s\tremaining: 8h 49m 22s\n",
      "12000:\ttest: 0.8970640\tbest: 0.8970671 (11996)\ttotal: 6m 26s\tremaining: 8h 49m 45s\n",
      "13000:\ttest: 0.8973720\tbest: 0.8973758 (12983)\ttotal: 6m 58s\tremaining: 8h 49m 43s\n",
      "14000:\ttest: 0.8975462\tbest: 0.8975520 (13990)\ttotal: 7m 31s\tremaining: 8h 50m 6s\n",
      "15000:\ttest: 0.8976812\tbest: 0.8976891 (14985)\ttotal: 8m 4s\tremaining: 8h 49m 48s\n",
      "16000:\ttest: 0.8977706\tbest: 0.8977734 (15986)\ttotal: 8m 36s\tremaining: 8h 49m 15s\n",
      "17000:\ttest: 0.8978902\tbest: 0.8978948 (16989)\ttotal: 9m 8s\tremaining: 8h 48m 44s\n",
      "18000:\ttest: 0.8980354\tbest: 0.8980520 (17980)\ttotal: 9m 41s\tremaining: 8h 48m 22s\n",
      "19000:\ttest: 0.8981143\tbest: 0.8981201 (18974)\ttotal: 10m 12s\tremaining: 8h 46m 47s\n",
      "20000:\ttest: 0.8981079\tbest: 0.8981265 (19133)\ttotal: 10m 43s\tremaining: 8h 45m 53s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8981264964\n",
      "bestIteration = 19133\n",
      "\n",
      "Shrink model to first 19134 iterations.\n",
      "CV mean score: 0.9009, std: 0.0040.\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Fold 0 started at Fri Mar  8 09:26:19 2019\n",
      "0:\ttest: 0.5091275\tbest: 0.5091275 (0)\ttotal: 49.3ms\tremaining: 13h 40m 52s\n",
      "1000:\ttest: 0.8528398\tbest: 0.8528698 (996)\ttotal: 31.1s\tremaining: 8h 36m 33s\n",
      "2000:\ttest: 0.8717697\tbest: 0.8717697 (2000)\ttotal: 1m 3s\tremaining: 8h 48m 7s\n",
      "3000:\ttest: 0.8819129\tbest: 0.8819129 (3000)\ttotal: 1m 35s\tremaining: 8h 48m 54s\n",
      "4000:\ttest: 0.8877525\tbest: 0.8877525 (4000)\ttotal: 2m 7s\tremaining: 8h 48m 4s\n",
      "5000:\ttest: 0.8913227\tbest: 0.8913227 (5000)\ttotal: 2m 40s\tremaining: 8h 50m 49s\n",
      "6000:\ttest: 0.8938616\tbest: 0.8938616 (6000)\ttotal: 3m 11s\tremaining: 8h 49m 47s\n",
      "7000:\ttest: 0.8956727\tbest: 0.8956772 (6998)\ttotal: 3m 44s\tremaining: 8h 49m 31s\n",
      "8000:\ttest: 0.8970143\tbest: 0.8970143 (8000)\ttotal: 4m 16s\tremaining: 8h 49m 13s\n",
      "9000:\ttest: 0.8979515\tbest: 0.8979515 (9000)\ttotal: 4m 48s\tremaining: 8h 49m 28s\n",
      "10000:\ttest: 0.8986708\tbest: 0.8986708 (10000)\ttotal: 5m 20s\tremaining: 8h 49m 6s\n",
      "11000:\ttest: 0.8992072\tbest: 0.8992092 (10985)\ttotal: 5m 53s\tremaining: 8h 50m 23s\n",
      "12000:\ttest: 0.8996770\tbest: 0.8996770 (12000)\ttotal: 6m 26s\tremaining: 8h 50m 2s\n",
      "13000:\ttest: 0.8999618\tbest: 0.8999618 (13000)\ttotal: 6m 59s\tremaining: 8h 50m 10s\n",
      "14000:\ttest: 0.9002055\tbest: 0.9002085 (13983)\ttotal: 7m 31s\tremaining: 8h 50m 2s\n",
      "15000:\ttest: 0.9004220\tbest: 0.9004223 (14992)\ttotal: 8m 2s\tremaining: 8h 48m 15s\n",
      "16000:\ttest: 0.9006060\tbest: 0.9006095 (15998)\ttotal: 8m 35s\tremaining: 8h 48m 30s\n",
      "17000:\ttest: 0.9007104\tbest: 0.9007171 (16911)\ttotal: 9m 7s\tremaining: 8h 47m 45s\n",
      "18000:\ttest: 0.9007636\tbest: 0.9007716 (17991)\ttotal: 9m 39s\tremaining: 8h 47m 16s\n",
      "19000:\ttest: 0.9007851\tbest: 0.9008042 (18228)\ttotal: 10m 12s\tremaining: 8h 47m 5s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9008042164\n",
      "bestIteration = 18228\n",
      "\n",
      "Shrink model to first 18229 iterations.\n",
      "Fold 1 started at Fri Mar  8 09:37:21 2019\n",
      "0:\ttest: 0.5141810\tbest: 0.5141810 (0)\ttotal: 46.6ms\tremaining: 12h 57m 9s\n",
      "1000:\ttest: 0.8475366\tbest: 0.8476504 (993)\ttotal: 29s\tremaining: 8h 2m 59s\n",
      "2000:\ttest: 0.8656685\tbest: 0.8656685 (2000)\ttotal: 59.3s\tremaining: 8h 12m 56s\n",
      "3000:\ttest: 0.8768534\tbest: 0.8768534 (3000)\ttotal: 1m 30s\tremaining: 8h 19m 50s\n",
      "4000:\ttest: 0.8829744\tbest: 0.8829744 (4000)\ttotal: 2m 1s\tremaining: 8h 22m 14s\n",
      "5000:\ttest: 0.8871589\tbest: 0.8871589 (5000)\ttotal: 2m 32s\tremaining: 8h 26m\n",
      "6000:\ttest: 0.8900903\tbest: 0.8900974 (5998)\ttotal: 3m 3s\tremaining: 8h 25m 19s\n",
      "7000:\ttest: 0.8920693\tbest: 0.8920756 (6999)\ttotal: 3m 33s\tremaining: 8h 25m 47s\n",
      "8000:\ttest: 0.8935743\tbest: 0.8935810 (7996)\ttotal: 4m 4s\tremaining: 8h 24m 37s\n",
      "9000:\ttest: 0.8945755\tbest: 0.8945803 (8996)\ttotal: 4m 35s\tremaining: 8h 25m 11s\n",
      "10000:\ttest: 0.8953469\tbest: 0.8953469 (10000)\ttotal: 5m 7s\tremaining: 8h 26m 54s\n",
      "11000:\ttest: 0.8958638\tbest: 0.8958640 (10998)\ttotal: 5m 37s\tremaining: 8h 24m 56s\n",
      "12000:\ttest: 0.8962887\tbest: 0.8962916 (11994)\ttotal: 6m 8s\tremaining: 8h 25m 37s\n",
      "13000:\ttest: 0.8967141\tbest: 0.8967180 (12997)\ttotal: 6m 39s\tremaining: 8h 25m 9s\n",
      "14000:\ttest: 0.8970008\tbest: 0.8970045 (13994)\ttotal: 7m 9s\tremaining: 8h 23m 47s\n",
      "15000:\ttest: 0.8971971\tbest: 0.8972015 (14983)\ttotal: 7m 41s\tremaining: 8h 24m 59s\n",
      "16000:\ttest: 0.8974416\tbest: 0.8974416 (16000)\ttotal: 8m 12s\tremaining: 8h 24m 28s\n",
      "17000:\ttest: 0.8976422\tbest: 0.8976521 (16970)\ttotal: 8m 43s\tremaining: 8h 24m 22s\n",
      "18000:\ttest: 0.8977921\tbest: 0.8977954 (17967)\ttotal: 9m 14s\tremaining: 8h 23m 43s\n",
      "19000:\ttest: 0.8978463\tbest: 0.8978513 (18738)\ttotal: 9m 44s\tremaining: 8h 23m 16s\n",
      "20000:\ttest: 0.8979459\tbest: 0.8979587 (19943)\ttotal: 10m 16s\tremaining: 8h 23m 13s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8979586532\n",
      "bestIteration = 19943\n",
      "\n",
      "Shrink model to first 19944 iterations.\n",
      "Fold 2 started at Fri Mar  8 09:48:51 2019\n",
      "0:\ttest: 0.5193180\tbest: 0.5193180 (0)\ttotal: 41.7ms\tremaining: 11h 34m 59s\n",
      "1000:\ttest: 0.8530487\tbest: 0.8530941 (999)\ttotal: 31.2s\tremaining: 8h 39m 29s\n",
      "2000:\ttest: 0.8702930\tbest: 0.8702930 (2000)\ttotal: 1m 3s\tremaining: 8h 46m 44s\n",
      "3000:\ttest: 0.8807977\tbest: 0.8807978 (2999)\ttotal: 1m 35s\tremaining: 8h 47m 5s\n",
      "4000:\ttest: 0.8863461\tbest: 0.8863522 (3998)\ttotal: 2m 8s\tremaining: 8h 52m 2s\n",
      "5000:\ttest: 0.8900404\tbest: 0.8900410 (4999)\ttotal: 2m 39s\tremaining: 8h 48m 40s\n",
      "6000:\ttest: 0.8923907\tbest: 0.8923936 (5992)\ttotal: 3m 11s\tremaining: 8h 48m 28s\n",
      "7000:\ttest: 0.8940838\tbest: 0.8940838 (7000)\ttotal: 3m 44s\tremaining: 8h 51m 52s\n",
      "8000:\ttest: 0.8953382\tbest: 0.8953434 (7997)\ttotal: 4m 18s\tremaining: 8h 53m 26s\n",
      "9000:\ttest: 0.8961865\tbest: 0.8961906 (8998)\ttotal: 4m 52s\tremaining: 8h 56m 11s\n",
      "10000:\ttest: 0.8969368\tbest: 0.8969479 (9963)\ttotal: 5m 25s\tremaining: 8h 56m 47s\n",
      "11000:\ttest: 0.8975082\tbest: 0.8975185 (10987)\ttotal: 5m 59s\tremaining: 8h 58m 34s\n",
      "12000:\ttest: 0.8978865\tbest: 0.8978888 (11971)\ttotal: 6m 32s\tremaining: 8h 59m 4s\n",
      "13000:\ttest: 0.8981501\tbest: 0.8981568 (12997)\ttotal: 7m 6s\tremaining: 8h 59m 46s\n",
      "14000:\ttest: 0.8983981\tbest: 0.8983989 (13999)\ttotal: 7m 39s\tremaining: 8h 59m 19s\n",
      "15000:\ttest: 0.8985606\tbest: 0.8985627 (14997)\ttotal: 8m 12s\tremaining: 8h 58m 39s\n",
      "16000:\ttest: 0.8986397\tbest: 0.8986561 (15931)\ttotal: 8m 46s\tremaining: 8h 59m 7s\n",
      "17000:\ttest: 0.8987615\tbest: 0.8987630 (16997)\ttotal: 9m 19s\tremaining: 8h 59m 24s\n",
      "18000:\ttest: 0.8987057\tbest: 0.8987834 (17281)\ttotal: 9m 53s\tremaining: 8h 59m 15s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8987834314\n",
      "bestIteration = 17281\n",
      "\n",
      "Shrink model to first 17282 iterations.\n",
      "Fold 3 started at Fri Mar  8 09:59:34 2019\n",
      "0:\ttest: 0.5274213\tbest: 0.5274213 (0)\ttotal: 38.6ms\tremaining: 10h 43m 59s\n",
      "1000:\ttest: 0.8521165\tbest: 0.8521165 (1000)\ttotal: 30.9s\tremaining: 8h 34m 35s\n",
      "2000:\ttest: 0.8719472\tbest: 0.8719622 (1997)\ttotal: 1m 2s\tremaining: 8h 41m 41s\n",
      "3000:\ttest: 0.8825046\tbest: 0.8825162 (2995)\ttotal: 1m 35s\tremaining: 8h 50m 26s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000:\ttest: 0.8887714\tbest: 0.8887714 (4000)\ttotal: 2m 8s\tremaining: 8h 51m 15s\n",
      "5000:\ttest: 0.8926368\tbest: 0.8926369 (4990)\ttotal: 2m 41s\tremaining: 8h 55m 54s\n",
      "6000:\ttest: 0.8953684\tbest: 0.8953719 (5999)\ttotal: 3m 16s\tremaining: 9h 3m 25s\n",
      "7000:\ttest: 0.8971056\tbest: 0.8971080 (6999)\ttotal: 3m 50s\tremaining: 9h 4m 10s\n",
      "8000:\ttest: 0.8984538\tbest: 0.8984550 (7999)\ttotal: 4m 24s\tremaining: 9h 6m 41s\n",
      "9000:\ttest: 0.8994353\tbest: 0.8994426 (8990)\ttotal: 4m 57s\tremaining: 9h 6m 7s\n",
      "10000:\ttest: 0.9001397\tbest: 0.9001442 (9998)\ttotal: 5m 29s\tremaining: 9h 4m 15s\n",
      "11000:\ttest: 0.9007197\tbest: 0.9007242 (10995)\ttotal: 6m 2s\tremaining: 9h 3m 42s\n",
      "12000:\ttest: 0.9010526\tbest: 0.9010526 (12000)\ttotal: 6m 35s\tremaining: 9h 2m 2s\n",
      "13000:\ttest: 0.9013131\tbest: 0.9013173 (12985)\ttotal: 7m 6s\tremaining: 8h 59m 46s\n",
      "14000:\ttest: 0.9014514\tbest: 0.9014876 (13894)\ttotal: 7m 40s\tremaining: 9h 51s\n",
      "15000:\ttest: 0.9016858\tbest: 0.9016924 (14955)\ttotal: 8m 13s\tremaining: 8h 59m 52s\n",
      "16000:\ttest: 0.9018528\tbest: 0.9018528 (16000)\ttotal: 8m 46s\tremaining: 8h 59m 7s\n",
      "17000:\ttest: 0.9019490\tbest: 0.9019600 (16891)\ttotal: 9m 19s\tremaining: 8h 59m 30s\n",
      "18000:\ttest: 0.9019092\tbest: 0.9019698 (17155)\ttotal: 9m 52s\tremaining: 8h 58m 48s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.901969751\n",
      "bestIteration = 17155\n",
      "\n",
      "Shrink model to first 17156 iterations.\n",
      "Fold 4 started at Fri Mar  8 10:10:12 2019\n",
      "0:\ttest: 0.5141316\tbest: 0.5141316 (0)\ttotal: 48.4ms\tremaining: 13h 26m 3s\n",
      "1000:\ttest: 0.8547196\tbest: 0.8547252 (996)\ttotal: 30.6s\tremaining: 8h 29m 8s\n",
      "2000:\ttest: 0.8743888\tbest: 0.8743888 (2000)\ttotal: 1m 2s\tremaining: 8h 40m 14s\n",
      "3000:\ttest: 0.8865080\tbest: 0.8865135 (2995)\ttotal: 1m 36s\tremaining: 8h 52m 2s\n",
      "4000:\ttest: 0.8932730\tbest: 0.8932730 (4000)\ttotal: 2m 8s\tremaining: 8h 54m 37s\n",
      "5000:\ttest: 0.8976256\tbest: 0.8976256 (5000)\ttotal: 2m 42s\tremaining: 8h 59m 20s\n",
      "6000:\ttest: 0.9006768\tbest: 0.9006768 (6000)\ttotal: 3m 15s\tremaining: 8h 59m 38s\n",
      "7000:\ttest: 0.9028281\tbest: 0.9028296 (6996)\ttotal: 3m 48s\tremaining: 8h 59m 50s\n",
      "8000:\ttest: 0.9044033\tbest: 0.9044067 (7999)\ttotal: 4m 20s\tremaining: 8h 57m 48s\n",
      "9000:\ttest: 0.9055316\tbest: 0.9055316 (9000)\ttotal: 4m 53s\tremaining: 8h 57m 54s\n",
      "10000:\ttest: 0.9063331\tbest: 0.9063343 (9996)\ttotal: 5m 25s\tremaining: 8h 57m 35s\n",
      "11000:\ttest: 0.9069232\tbest: 0.9069296 (10985)\ttotal: 5m 59s\tremaining: 8h 58m 19s\n",
      "12000:\ttest: 0.9074110\tbest: 0.9074111 (11997)\ttotal: 6m 32s\tremaining: 8h 58m 7s\n",
      "13000:\ttest: 0.9077128\tbest: 0.9077128 (13000)\ttotal: 7m 5s\tremaining: 8h 58m 25s\n",
      "14000:\ttest: 0.9080096\tbest: 0.9080132 (13995)\ttotal: 7m 38s\tremaining: 8h 57m 57s\n",
      "15000:\ttest: 0.9082955\tbest: 0.9082955 (15000)\ttotal: 8m 10s\tremaining: 8h 57m 5s\n",
      "16000:\ttest: 0.9084441\tbest: 0.9084528 (15916)\ttotal: 8m 43s\tremaining: 8h 56m 42s\n",
      "17000:\ttest: 0.9086791\tbest: 0.9086791 (16999)\ttotal: 9m 17s\tremaining: 8h 57m 15s\n",
      "18000:\ttest: 0.9087659\tbest: 0.9087720 (17889)\ttotal: 9m 50s\tremaining: 8h 56m 29s\n",
      "19000:\ttest: 0.9088648\tbest: 0.9088715 (18988)\ttotal: 10m 23s\tremaining: 8h 56m 20s\n",
      "20000:\ttest: 0.9089261\tbest: 0.9089343 (19773)\ttotal: 10m 56s\tremaining: 8h 56m 5s\n",
      "21000:\ttest: 0.9089688\tbest: 0.9089761 (20885)\ttotal: 11m 29s\tremaining: 8h 55m 40s\n",
      "22000:\ttest: 0.9090212\tbest: 0.9090274 (21962)\ttotal: 12m 2s\tremaining: 8h 55m 8s\n",
      "23000:\ttest: 0.9089880\tbest: 0.9090428 (22660)\ttotal: 12m 33s\tremaining: 8h 53m 43s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9090427794\n",
      "bestIteration = 22660\n",
      "\n",
      "Shrink model to first 22661 iterations.\n",
      "Fold 5 started at Fri Mar  8 10:24:00 2019\n",
      "0:\ttest: 0.5250585\tbest: 0.5250585 (0)\ttotal: 50.3ms\tremaining: 13h 58m 50s\n",
      "1000:\ttest: 0.8518423\tbest: 0.8518556 (995)\ttotal: 30.8s\tremaining: 8h 33m 7s\n",
      "2000:\ttest: 0.8711233\tbest: 0.8711233 (2000)\ttotal: 1m 3s\tremaining: 8h 47m 13s\n",
      "3000:\ttest: 0.8824506\tbest: 0.8824629 (2990)\ttotal: 1m 35s\tremaining: 8h 46m 47s\n",
      "4000:\ttest: 0.8887392\tbest: 0.8887392 (4000)\ttotal: 2m 6s\tremaining: 8h 46m 16s\n",
      "5000:\ttest: 0.8931211\tbest: 0.8931243 (4997)\ttotal: 2m 38s\tremaining: 8h 45m 58s\n",
      "6000:\ttest: 0.8958661\tbest: 0.8958661 (6000)\ttotal: 3m 10s\tremaining: 8h 45m 2s\n",
      "7000:\ttest: 0.8977219\tbest: 0.8977324 (6998)\ttotal: 3m 42s\tremaining: 8h 46m 4s\n",
      "8000:\ttest: 0.8989984\tbest: 0.8990074 (7993)\ttotal: 4m 14s\tremaining: 8h 46m 44s\n",
      "9000:\ttest: 0.9001595\tbest: 0.9001640 (8999)\ttotal: 4m 48s\tremaining: 8h 49m 38s\n",
      "10000:\ttest: 0.9009100\tbest: 0.9009100 (10000)\ttotal: 5m 20s\tremaining: 8h 48m 35s\n",
      "11000:\ttest: 0.9014884\tbest: 0.9014884 (11000)\ttotal: 5m 53s\tremaining: 8h 50m 3s\n",
      "12000:\ttest: 0.9018629\tbest: 0.9018686 (11985)\ttotal: 6m 27s\tremaining: 8h 51m 35s\n",
      "13000:\ttest: 0.9021440\tbest: 0.9021511 (12967)\ttotal: 6m 59s\tremaining: 8h 51m\n",
      "14000:\ttest: 0.9024492\tbest: 0.9024504 (13981)\ttotal: 7m 31s\tremaining: 8h 50m 28s\n",
      "15000:\ttest: 0.9026333\tbest: 0.9026333 (15000)\ttotal: 8m 4s\tremaining: 8h 49m 54s\n",
      "16000:\ttest: 0.9028298\tbest: 0.9028298 (16000)\ttotal: 8m 36s\tremaining: 8h 49m 19s\n",
      "17000:\ttest: 0.9029925\tbest: 0.9029963 (16988)\ttotal: 9m 8s\tremaining: 8h 48m 46s\n",
      "18000:\ttest: 0.9031147\tbest: 0.9031227 (17992)\ttotal: 9m 41s\tremaining: 8h 48m 15s\n",
      "19000:\ttest: 0.9031755\tbest: 0.9031829 (18977)\ttotal: 10m 13s\tremaining: 8h 47m 56s\n",
      "20000:\ttest: 0.9031981\tbest: 0.9032184 (19848)\ttotal: 10m 45s\tremaining: 8h 47m 1s\n",
      "21000:\ttest: 0.9032376\tbest: 0.9032384 (20999)\ttotal: 11m 18s\tremaining: 8h 47m 3s\n",
      "22000:\ttest: 0.9032811\tbest: 0.9032863 (21981)\ttotal: 11m 51s\tremaining: 8h 46m 48s\n",
      "23000:\ttest: 0.9033304\tbest: 0.9033330 (22981)\ttotal: 12m 24s\tremaining: 8h 46m 46s\n",
      "24000:\ttest: 0.9033410\tbest: 0.9033526 (23190)\ttotal: 12m 56s\tremaining: 8h 46m 9s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9033526365\n",
      "bestIteration = 23190\n",
      "\n",
      "Shrink model to first 23191 iterations.\n",
      "Fold 6 started at Fri Mar  8 10:37:57 2019\n",
      "0:\ttest: 0.5100487\tbest: 0.5100487 (0)\ttotal: 60.3ms\tremaining: 16h 45m 43s\n",
      "1000:\ttest: 0.8501523\tbest: 0.8501825 (999)\ttotal: 30.7s\tremaining: 8h 30m 6s\n",
      "2000:\ttest: 0.8703304\tbest: 0.8703304 (2000)\ttotal: 1m 3s\tremaining: 8h 45m 17s\n",
      "3000:\ttest: 0.8816561\tbest: 0.8816561 (3000)\ttotal: 1m 35s\tremaining: 8h 48m 27s\n",
      "4000:\ttest: 0.8887844\tbest: 0.8887932 (3999)\ttotal: 2m 7s\tremaining: 8h 48m 43s\n",
      "5000:\ttest: 0.8930142\tbest: 0.8930180 (4998)\ttotal: 2m 38s\tremaining: 8h 46m 29s\n",
      "6000:\ttest: 0.8961143\tbest: 0.8961211 (5996)\ttotal: 3m 11s\tremaining: 8h 49m 47s\n",
      "7000:\ttest: 0.8980479\tbest: 0.8980536 (6999)\ttotal: 3m 45s\tremaining: 8h 53m 31s\n",
      "8000:\ttest: 0.8995476\tbest: 0.8995477 (7998)\ttotal: 4m 19s\tremaining: 8h 55m 27s\n",
      "9000:\ttest: 0.9006119\tbest: 0.9006153 (8999)\ttotal: 4m 52s\tremaining: 8h 55m 59s\n",
      "10000:\ttest: 0.9013891\tbest: 0.9013909 (9997)\ttotal: 5m 25s\tremaining: 8h 57m 1s\n",
      "11000:\ttest: 0.9020860\tbest: 0.9020860 (11000)\ttotal: 5m 57s\tremaining: 8h 56m 18s\n",
      "12000:\ttest: 0.9026938\tbest: 0.9026951 (11999)\ttotal: 6m 30s\tremaining: 8h 56m 6s\n",
      "13000:\ttest: 0.9031255\tbest: 0.9031284 (12973)\ttotal: 7m 3s\tremaining: 8h 55m 17s\n",
      "14000:\ttest: 0.9034113\tbest: 0.9034149 (13986)\ttotal: 7m 34s\tremaining: 8h 53m 19s\n",
      "15000:\ttest: 0.9037537\tbest: 0.9037566 (14989)\ttotal: 8m 7s\tremaining: 8h 52m 57s\n",
      "16000:\ttest: 0.9039738\tbest: 0.9039767 (15973)\ttotal: 8m 40s\tremaining: 8h 53m 5s\n",
      "17000:\ttest: 0.9041713\tbest: 0.9041768 (16948)\ttotal: 9m 14s\tremaining: 8h 54m 44s\n",
      "18000:\ttest: 0.9042930\tbest: 0.9042939 (17998)\ttotal: 9m 47s\tremaining: 8h 54m 1s\n",
      "19000:\ttest: 0.9044431\tbest: 0.9044451 (18969)\ttotal: 10m 19s\tremaining: 8h 53m 8s\n",
      "20000:\ttest: 0.9045657\tbest: 0.9045672 (19985)\ttotal: 10m 50s\tremaining: 8h 51m 32s\n",
      "21000:\ttest: 0.9045845\tbest: 0.9046195 (20818)\ttotal: 11m 23s\tremaining: 8h 51m 18s\n",
      "22000:\ttest: 0.9046267\tbest: 0.9046463 (21667)\ttotal: 11m 56s\tremaining: 8h 50m 52s\n",
      "23000:\ttest: 0.9046893\tbest: 0.9046907 (22998)\ttotal: 12m 27s\tremaining: 8h 49m 21s\n",
      "24000:\ttest: 0.9047464\tbest: 0.9047686 (23543)\ttotal: 13m\tremaining: 8h 48m 54s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9047685973\n",
      "bestIteration = 23543\n",
      "\n",
      "Shrink model to first 23544 iterations.\n",
      "Fold 7 started at Fri Mar  8 10:52:12 2019\n",
      "0:\ttest: 0.5274470\tbest: 0.5274470 (0)\ttotal: 50.6ms\tremaining: 14h 2m 41s\n",
      "1000:\ttest: 0.8427401\tbest: 0.8427401 (1000)\ttotal: 31.8s\tremaining: 8h 49m 43s\n",
      "2000:\ttest: 0.8628208\tbest: 0.8628208 (2000)\ttotal: 1m 3s\tremaining: 8h 45m 32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000:\ttest: 0.8741031\tbest: 0.8741031 (3000)\ttotal: 1m 35s\tremaining: 8h 47m 54s\n",
      "4000:\ttest: 0.8802498\tbest: 0.8802498 (4000)\ttotal: 2m 11s\tremaining: 9h 3m 48s\n",
      "5000:\ttest: 0.8841635\tbest: 0.8841635 (5000)\ttotal: 2m 43s\tremaining: 9h 44s\n",
      "6000:\ttest: 0.8867029\tbest: 0.8867029 (6000)\ttotal: 3m 15s\tremaining: 9h 5s\n",
      "7000:\ttest: 0.8886431\tbest: 0.8886431 (7000)\ttotal: 3m 47s\tremaining: 8h 58m 42s\n",
      "8000:\ttest: 0.8900491\tbest: 0.8900491 (8000)\ttotal: 4m 19s\tremaining: 8h 55m 55s\n",
      "9000:\ttest: 0.8909984\tbest: 0.8909984 (9000)\ttotal: 4m 50s\tremaining: 8h 52m 41s\n",
      "10000:\ttest: 0.8918032\tbest: 0.8918134 (9992)\ttotal: 5m 22s\tremaining: 8h 52m 10s\n",
      "11000:\ttest: 0.8923371\tbest: 0.8923371 (11000)\ttotal: 5m 54s\tremaining: 8h 51m 14s\n",
      "12000:\ttest: 0.8928660\tbest: 0.8928677 (11998)\ttotal: 6m 25s\tremaining: 8h 49m 3s\n",
      "13000:\ttest: 0.8932509\tbest: 0.8932513 (12998)\ttotal: 6m 55s\tremaining: 8h 46m 8s\n",
      "14000:\ttest: 0.8935576\tbest: 0.8935676 (13997)\ttotal: 7m 26s\tremaining: 8h 44m 18s\n",
      "15000:\ttest: 0.8938177\tbest: 0.8938236 (14911)\ttotal: 7m 57s\tremaining: 8h 42m 13s\n",
      "16000:\ttest: 0.8940645\tbest: 0.8940662 (15982)\ttotal: 8m 26s\tremaining: 8h 39m 35s\n",
      "17000:\ttest: 0.8941928\tbest: 0.8942063 (16892)\ttotal: 8m 56s\tremaining: 8h 37m 4s\n",
      "18000:\ttest: 0.8943505\tbest: 0.8943518 (17984)\ttotal: 9m 27s\tremaining: 8h 35m 48s\n",
      "19000:\ttest: 0.8944667\tbest: 0.8944672 (18988)\ttotal: 10m\tremaining: 8h 36m 34s\n",
      "20000:\ttest: 0.8945769\tbest: 0.8945769 (19999)\ttotal: 10m 34s\tremaining: 8h 38m 14s\n",
      "21000:\ttest: 0.8946512\tbest: 0.8946619 (20938)\ttotal: 11m 8s\tremaining: 8h 39m 14s\n",
      "22000:\ttest: 0.8947209\tbest: 0.8947212 (21999)\ttotal: 11m 41s\tremaining: 8h 39m 28s\n",
      "23000:\ttest: 0.8947797\tbest: 0.8947813 (22998)\ttotal: 12m 10s\tremaining: 8h 37m 9s\n",
      "24000:\ttest: 0.8948620\tbest: 0.8948620 (24000)\ttotal: 12m 40s\tremaining: 8h 35m 18s\n",
      "25000:\ttest: 0.8949504\tbest: 0.8949658 (24961)\ttotal: 13m 10s\tremaining: 8h 33m 39s\n",
      "26000:\ttest: 0.8949927\tbest: 0.8950121 (25756)\ttotal: 13m 42s\tremaining: 8h 33m 28s\n",
      "27000:\ttest: 0.8949989\tbest: 0.8950161 (26693)\ttotal: 14m 12s\tremaining: 8h 32m 2s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.895016109\n",
      "bestIteration = 26693\n",
      "\n",
      "Shrink model to first 26694 iterations.\n",
      "Fold 8 started at Fri Mar  8 11:07:48 2019\n",
      "0:\ttest: 0.5190881\tbest: 0.5190881 (0)\ttotal: 45ms\tremaining: 12h 30m 40s\n",
      "1000:\ttest: 0.8441825\tbest: 0.8442031 (999)\ttotal: 31s\tremaining: 8h 35m 25s\n",
      "2000:\ttest: 0.8629546\tbest: 0.8629814 (1997)\ttotal: 1m 2s\tremaining: 8h 40m 47s\n",
      "3000:\ttest: 0.8732149\tbest: 0.8732149 (3000)\ttotal: 1m 35s\tremaining: 8h 49m 39s\n",
      "4000:\ttest: 0.8799275\tbest: 0.8799275 (4000)\ttotal: 2m 7s\tremaining: 8h 48m 7s\n",
      "5000:\ttest: 0.8844086\tbest: 0.8844086 (5000)\ttotal: 2m 38s\tremaining: 8h 46m 25s\n",
      "6000:\ttest: 0.8875207\tbest: 0.8875207 (6000)\ttotal: 3m 10s\tremaining: 8h 44m 42s\n",
      "7000:\ttest: 0.8898675\tbest: 0.8898696 (6999)\ttotal: 3m 41s\tremaining: 8h 43m 43s\n",
      "8000:\ttest: 0.8913682\tbest: 0.8913682 (8000)\ttotal: 4m 13s\tremaining: 8h 43m 11s\n",
      "9000:\ttest: 0.8926855\tbest: 0.8926855 (9000)\ttotal: 4m 46s\tremaining: 8h 45m 29s\n",
      "10000:\ttest: 0.8936411\tbest: 0.8936498 (9993)\ttotal: 5m 18s\tremaining: 8h 45m 4s\n",
      "11000:\ttest: 0.8944236\tbest: 0.8944254 (10998)\ttotal: 5m 50s\tremaining: 8h 45m 39s\n",
      "12000:\ttest: 0.8949234\tbest: 0.8949243 (11999)\ttotal: 6m 23s\tremaining: 8h 45m 42s\n",
      "13000:\ttest: 0.8952029\tbest: 0.8952055 (12931)\ttotal: 6m 55s\tremaining: 8h 45m 40s\n",
      "14000:\ttest: 0.8955935\tbest: 0.8955998 (13982)\ttotal: 7m 27s\tremaining: 8h 44m 43s\n",
      "15000:\ttest: 0.8958171\tbest: 0.8958259 (14995)\ttotal: 7m 59s\tremaining: 8h 44m 51s\n",
      "16000:\ttest: 0.8959958\tbest: 0.8959987 (15998)\ttotal: 8m 33s\tremaining: 8h 46m 3s\n",
      "17000:\ttest: 0.8961719\tbest: 0.8961732 (16999)\ttotal: 9m 6s\tremaining: 8h 46m 35s\n",
      "18000:\ttest: 0.8963055\tbest: 0.8963055 (17999)\ttotal: 9m 38s\tremaining: 8h 45m 35s\n",
      "19000:\ttest: 0.8964615\tbest: 0.8964668 (18963)\ttotal: 10m 10s\tremaining: 8h 45m 14s\n",
      "20000:\ttest: 0.8965132\tbest: 0.8965211 (19307)\ttotal: 10m 44s\tremaining: 8h 46m 34s\n",
      "21000:\ttest: 0.8966349\tbest: 0.8966492 (20930)\ttotal: 11m 17s\tremaining: 8h 46m 8s\n",
      "22000:\ttest: 0.8967225\tbest: 0.8967278 (21991)\ttotal: 11m 49s\tremaining: 8h 45m 38s\n",
      "23000:\ttest: 0.8967684\tbest: 0.8967884 (22879)\ttotal: 12m 21s\tremaining: 8h 44m 58s\n",
      "24000:\ttest: 0.8967693\tbest: 0.8967968 (23911)\ttotal: 12m 53s\tremaining: 8h 44m 32s\n",
      "25000:\ttest: 0.8968203\tbest: 0.8968361 (24891)\ttotal: 13m 26s\tremaining: 8h 43m 55s\n",
      "26000:\ttest: 0.8968856\tbest: 0.8968863 (25374)\ttotal: 13m 57s\tremaining: 8h 43m 10s\n",
      "27000:\ttest: 0.8968740\tbest: 0.8968968 (26069)\ttotal: 14m 31s\tremaining: 8h 43m 8s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8968968159\n",
      "bestIteration = 26069\n",
      "\n",
      "Shrink model to first 26070 iterations.\n",
      "Fold 9 started at Fri Mar  8 11:23:23 2019\n",
      "0:\ttest: 0.5287782\tbest: 0.5287782 (0)\ttotal: 58.5ms\tremaining: 16h 15m 21s\n",
      "1000:\ttest: 0.8489395\tbest: 0.8489733 (995)\ttotal: 31.3s\tremaining: 8h 40m 25s\n",
      "2000:\ttest: 0.8677044\tbest: 0.8677044 (2000)\ttotal: 1m 2s\tremaining: 8h 38m 11s\n",
      "3000:\ttest: 0.8779018\tbest: 0.8779018 (3000)\ttotal: 1m 34s\tremaining: 8h 45m 39s\n",
      "4000:\ttest: 0.8840177\tbest: 0.8840177 (4000)\ttotal: 2m 8s\tremaining: 8h 54m 40s\n",
      "5000:\ttest: 0.8878650\tbest: 0.8878756 (4998)\ttotal: 2m 41s\tremaining: 8h 56m 13s\n",
      "6000:\ttest: 0.8907088\tbest: 0.8907088 (6000)\ttotal: 3m 13s\tremaining: 8h 53m 55s\n",
      "7000:\ttest: 0.8925288\tbest: 0.8925371 (6979)\ttotal: 3m 45s\tremaining: 8h 52m 48s\n",
      "8000:\ttest: 0.8938438\tbest: 0.8938438 (8000)\ttotal: 4m 18s\tremaining: 8h 54m 11s\n",
      "9000:\ttest: 0.8948970\tbest: 0.8948984 (8996)\ttotal: 4m 51s\tremaining: 8h 54m 6s\n",
      "10000:\ttest: 0.8955574\tbest: 0.8955574 (10000)\ttotal: 5m 23s\tremaining: 8h 53m 33s\n",
      "11000:\ttest: 0.8960205\tbest: 0.8960214 (10998)\ttotal: 5m 56s\tremaining: 8h 54m 23s\n",
      "12000:\ttest: 0.8964724\tbest: 0.8964761 (11985)\ttotal: 6m 29s\tremaining: 8h 54m\n",
      "13000:\ttest: 0.8968193\tbest: 0.8968222 (12998)\ttotal: 7m 1s\tremaining: 8h 52m 55s\n",
      "14000:\ttest: 0.8969931\tbest: 0.8969997 (13965)\ttotal: 7m 32s\tremaining: 8h 51m 16s\n",
      "15000:\ttest: 0.8971756\tbest: 0.8971838 (14973)\ttotal: 8m 4s\tremaining: 8h 50m 33s\n",
      "16000:\ttest: 0.8973450\tbest: 0.8973475 (15994)\ttotal: 8m 36s\tremaining: 8h 49m 41s\n",
      "17000:\ttest: 0.8974283\tbest: 0.8974289 (16998)\ttotal: 9m 8s\tremaining: 8h 48m 53s\n",
      "18000:\ttest: 0.8975750\tbest: 0.8975767 (17997)\ttotal: 9m 41s\tremaining: 8h 48m 28s\n",
      "19000:\ttest: 0.8976664\tbest: 0.8976672 (18982)\ttotal: 10m 13s\tremaining: 8h 48m 5s\n",
      "20000:\ttest: 0.8977318\tbest: 0.8977334 (19998)\ttotal: 10m 46s\tremaining: 8h 47m 36s\n",
      "21000:\ttest: 0.8977998\tbest: 0.8978100 (20863)\ttotal: 11m 19s\tremaining: 8h 47m 37s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8978099663\n",
      "bestIteration = 20863\n",
      "\n",
      "Shrink model to first 20864 iterations.\n",
      "CV mean score: 0.9006, std: 0.0040.\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# var_12 9559 (yes)\n",
    "# var_68 459 (no)\n",
    "# var_91 7959 (yes)\n",
    "# var_103 9376 (no)\n",
    "# var_108 8524 (no)\n",
    "oof = []\n",
    "preds = []\n",
    "\n",
    "for col in ['var_12', 'var_68', 'var_91', 'var_103', 'var_108']:\n",
    "    bins = np.linspace(-6.0,6.0,40)\n",
    "    X = pd.get_dummies(pd.cut(X1[col].values, bins))\n",
    "    xcols = [col + str(i) for i in range(X.shape[1])]\n",
    "    X.columns = xcols\n",
    "    X3 = X1.copy()\n",
    "    X4 = X2.copy()\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X3 = pd.concat([X3, X], axis=1)\n",
    "    X4 = X4.drop(col, axis=1)\n",
    "    X4 = pd.concat([X4, X], axis=1)\n",
    "    oof_cat, prediction_cat, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, model_type='cat', plot_feature_importance=False)\n",
    "    oof.append(oof_cat)\n",
    "    preds.append(prediction_cat)\n",
    "    print('--------------------------------------------------------')\n",
    "    print('--------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_quant_5_bins_1_10_1', oof)\n",
    "np.save('../cache/preds_quant_5_bins_1_10_1', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb      cat2\n",
      "0  0.010545  0.010837  0.008313  0.010250\n",
      "1  0.455996  0.423705  0.392034  0.486984\n",
      "2  0.004485  0.003918  0.004919  0.004351\n",
      "3  0.243523  0.253600  0.340744  0.277500\n",
      "4  0.097492  0.088431  0.110311  0.096738\n",
      "0.92538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_lgb_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_lgb_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb[0]\n",
    "stage2_test['lgb'] = preds_lgb[0]\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "\n",
    "oof_cat2 = np.load('../cache/oof_quant_5_bins_1_10_1.npy')\n",
    "preds_cat2= np.load('../cache/preds_quant_5_bins_1_10_1.npy')\n",
    "stage2['cat2'] = oof_cat2[4]\n",
    "stage2_test['cat2'] = preds_cat2[4]\n",
    "\n",
    "\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12k-2.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Fri Mar  8 14:58:54 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883519\n",
      "[2000]\tvalid_0's auc: 0.891737\n",
      "[3000]\tvalid_0's auc: 0.896032\n",
      "[4000]\tvalid_0's auc: 0.898201\n",
      "[5000]\tvalid_0's auc: 0.899417\n",
      "[6000]\tvalid_0's auc: 0.900282\n",
      "[7000]\tvalid_0's auc: 0.900679\n",
      "[8000]\tvalid_0's auc: 0.900953\n",
      "[9000]\tvalid_0's auc: 0.900843\n",
      "[10000]\tvalid_0's auc: 0.900839\n",
      "[11000]\tvalid_0's auc: 0.900952\n",
      "[12000]\tvalid_0's auc: 0.901016\n",
      "[13000]\tvalid_0's auc: 0.900866\n",
      "[14000]\tvalid_0's auc: 0.900728\n",
      "[15000]\tvalid_0's auc: 0.900601\n",
      "Early stopping, best iteration is:\n",
      "[12160]\tvalid_0's auc: 0.901118\n",
      "Fold 1 started at Fri Mar  8 15:01:41 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.87974\n",
      "[2000]\tvalid_0's auc: 0.889015\n",
      "[3000]\tvalid_0's auc: 0.893396\n",
      "[4000]\tvalid_0's auc: 0.895845\n",
      "[5000]\tvalid_0's auc: 0.897134\n",
      "[6000]\tvalid_0's auc: 0.897756\n",
      "[7000]\tvalid_0's auc: 0.898074\n",
      "[8000]\tvalid_0's auc: 0.898289\n",
      "[9000]\tvalid_0's auc: 0.89819\n",
      "[10000]\tvalid_0's auc: 0.898194\n",
      "[11000]\tvalid_0's auc: 0.898115\n",
      "Early stopping, best iteration is:\n",
      "[8013]\tvalid_0's auc: 0.898318\n",
      "Fold 2 started at Fri Mar  8 15:04:42 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.882893\n",
      "[2000]\tvalid_0's auc: 0.891475\n",
      "[3000]\tvalid_0's auc: 0.895537\n",
      "[4000]\tvalid_0's auc: 0.897651\n",
      "[5000]\tvalid_0's auc: 0.898628\n",
      "[6000]\tvalid_0's auc: 0.899061\n",
      "[7000]\tvalid_0's auc: 0.899154\n",
      "[8000]\tvalid_0's auc: 0.899214\n",
      "[9000]\tvalid_0's auc: 0.899101\n",
      "Early stopping, best iteration is:\n",
      "[6327]\tvalid_0's auc: 0.899272\n",
      "Fold 3 started at Fri Mar  8 15:30:32 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885554\n",
      "[2000]\tvalid_0's auc: 0.893337\n",
      "[3000]\tvalid_0's auc: 0.897368\n",
      "[4000]\tvalid_0's auc: 0.8996\n",
      "[5000]\tvalid_0's auc: 0.900856\n",
      "[6000]\tvalid_0's auc: 0.901568\n",
      "[7000]\tvalid_0's auc: 0.901884\n",
      "[8000]\tvalid_0's auc: 0.901961\n",
      "[9000]\tvalid_0's auc: 0.901773\n",
      "[10000]\tvalid_0's auc: 0.901625\n",
      "Early stopping, best iteration is:\n",
      "[7485]\tvalid_0's auc: 0.901972\n",
      "Fold 4 started at Fri Mar  8 16:03:48 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.889924\n",
      "[2000]\tvalid_0's auc: 0.899316\n",
      "[3000]\tvalid_0's auc: 0.903619\n",
      "[4000]\tvalid_0's auc: 0.906201\n",
      "[5000]\tvalid_0's auc: 0.907546\n",
      "[6000]\tvalid_0's auc: 0.908417\n",
      "[7000]\tvalid_0's auc: 0.909007\n",
      "[8000]\tvalid_0's auc: 0.909272\n",
      "[9000]\tvalid_0's auc: 0.909348\n",
      "[10000]\tvalid_0's auc: 0.909342\n",
      "[11000]\tvalid_0's auc: 0.909375\n",
      "[12000]\tvalid_0's auc: 0.909298\n",
      "Early stopping, best iteration is:\n",
      "[9640]\tvalid_0's auc: 0.909501\n",
      "Fold 5 started at Fri Mar  8 16:41:35 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885172\n",
      "[2000]\tvalid_0's auc: 0.89398\n",
      "[3000]\tvalid_0's auc: 0.898149\n",
      "[4000]\tvalid_0's auc: 0.900545\n",
      "[5000]\tvalid_0's auc: 0.901623\n",
      "[6000]\tvalid_0's auc: 0.902353\n",
      "[7000]\tvalid_0's auc: 0.90289\n",
      "[8000]\tvalid_0's auc: 0.903213\n",
      "[9000]\tvalid_0's auc: 0.903125\n",
      "[10000]\tvalid_0's auc: 0.903171\n",
      "[11000]\tvalid_0's auc: 0.903119\n",
      "[12000]\tvalid_0's auc: 0.903033\n",
      "Early stopping, best iteration is:\n",
      "[9770]\tvalid_0's auc: 0.903261\n",
      "Fold 6 started at Fri Mar  8 17:23:51 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88553\n",
      "[2000]\tvalid_0's auc: 0.893916\n",
      "[3000]\tvalid_0's auc: 0.898801\n",
      "[4000]\tvalid_0's auc: 0.901008\n",
      "[5000]\tvalid_0's auc: 0.902606\n",
      "[6000]\tvalid_0's auc: 0.903616\n",
      "[7000]\tvalid_0's auc: 0.904072\n",
      "[8000]\tvalid_0's auc: 0.904364\n",
      "[9000]\tvalid_0's auc: 0.904338\n",
      "[10000]\tvalid_0's auc: 0.904568\n",
      "[11000]\tvalid_0's auc: 0.904525\n",
      "[12000]\tvalid_0's auc: 0.904345\n",
      "Early stopping, best iteration is:\n",
      "[9887]\tvalid_0's auc: 0.904577\n",
      "Fold 7 started at Fri Mar  8 18:05:06 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.875096\n",
      "[2000]\tvalid_0's auc: 0.884301\n",
      "[3000]\tvalid_0's auc: 0.888829\n",
      "[4000]\tvalid_0's auc: 0.891139\n",
      "[5000]\tvalid_0's auc: 0.89259\n",
      "[6000]\tvalid_0's auc: 0.893679\n",
      "[7000]\tvalid_0's auc: 0.893917\n",
      "[8000]\tvalid_0's auc: 0.894267\n",
      "[9000]\tvalid_0's auc: 0.894352\n",
      "[10000]\tvalid_0's auc: 0.894408\n",
      "[11000]\tvalid_0's auc: 0.894488\n",
      "[12000]\tvalid_0's auc: 0.894308\n",
      "[13000]\tvalid_0's auc: 0.894185\n",
      "Early stopping, best iteration is:\n",
      "[10484]\tvalid_0's auc: 0.894592\n",
      "Fold 8 started at Fri Mar  8 18:46:42 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.875854\n",
      "[2000]\tvalid_0's auc: 0.884967\n",
      "[3000]\tvalid_0's auc: 0.889971\n",
      "[4000]\tvalid_0's auc: 0.892839\n",
      "[5000]\tvalid_0's auc: 0.894558\n",
      "[6000]\tvalid_0's auc: 0.895637\n",
      "[7000]\tvalid_0's auc: 0.896019\n",
      "[8000]\tvalid_0's auc: 0.896331\n",
      "[9000]\tvalid_0's auc: 0.896377\n",
      "[10000]\tvalid_0's auc: 0.896419\n",
      "[11000]\tvalid_0's auc: 0.896256\n",
      "[12000]\tvalid_0's auc: 0.896028\n",
      "Early stopping, best iteration is:\n",
      "[9514]\tvalid_0's auc: 0.89655\n",
      "Fold 9 started at Fri Mar  8 19:31:57 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.878455\n",
      "[2000]\tvalid_0's auc: 0.88804\n",
      "[3000]\tvalid_0's auc: 0.892692\n",
      "[4000]\tvalid_0's auc: 0.895032\n",
      "[5000]\tvalid_0's auc: 0.896426\n",
      "[6000]\tvalid_0's auc: 0.896928\n",
      "[7000]\tvalid_0's auc: 0.897457\n",
      "[8000]\tvalid_0's auc: 0.897679\n",
      "[9000]\tvalid_0's auc: 0.897609\n",
      "[10000]\tvalid_0's auc: 0.897858\n",
      "[11000]\tvalid_0's auc: 0.89767\n",
      "[12000]\tvalid_0's auc: 0.897559\n",
      "Early stopping, best iteration is:\n",
      "[9941]\tvalid_0's auc: 0.89788\n",
      "CV mean score: 0.9007, std: 0.0041.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X1, X2, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_1_10_1', oof_lgb)\n",
    "np.save('../cache/preds_new_quant_1_10_1', prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = prediction_lgb\n",
    "sub.to_csv('../submissions/sub12l.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Mar 12 23:17:58 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884499\n",
      "[2000]\tvalid_0's auc: 0.89276\n",
      "[3000]\tvalid_0's auc: 0.896099\n",
      "[4000]\tvalid_0's auc: 0.898238\n",
      "[5000]\tvalid_0's auc: 0.899494\n",
      "[6000]\tvalid_0's auc: 0.900361\n",
      "[7000]\tvalid_0's auc: 0.900833\n",
      "[8000]\tvalid_0's auc: 0.901153\n",
      "[9000]\tvalid_0's auc: 0.901485\n",
      "[10000]\tvalid_0's auc: 0.901415\n",
      "[11000]\tvalid_0's auc: 0.901532\n",
      "[12000]\tvalid_0's auc: 0.901403\n",
      "[13000]\tvalid_0's auc: 0.901521\n",
      "Early stopping, best iteration is:\n",
      "[10886]\tvalid_0's auc: 0.90159\n",
      "Fold 1 started at Tue Mar 12 23:57:02 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.881999\n",
      "[2000]\tvalid_0's auc: 0.889122\n",
      "[3000]\tvalid_0's auc: 0.892677\n",
      "[4000]\tvalid_0's auc: 0.894946\n",
      "[5000]\tvalid_0's auc: 0.896122\n",
      "[6000]\tvalid_0's auc: 0.896902\n",
      "[7000]\tvalid_0's auc: 0.89755\n",
      "[8000]\tvalid_0's auc: 0.897763\n",
      "[9000]\tvalid_0's auc: 0.89791\n",
      "[10000]\tvalid_0's auc: 0.898184\n",
      "[11000]\tvalid_0's auc: 0.898258\n",
      "[12000]\tvalid_0's auc: 0.898189\n",
      "[13000]\tvalid_0's auc: 0.898222\n",
      "Early stopping, best iteration is:\n",
      "[10914]\tvalid_0's auc: 0.898279\n",
      "Fold 2 started at Wed Mar 13 00:53:21 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885601\n",
      "[2000]\tvalid_0's auc: 0.891769\n",
      "[3000]\tvalid_0's auc: 0.895775\n",
      "[4000]\tvalid_0's auc: 0.897505\n",
      "[5000]\tvalid_0's auc: 0.898614\n",
      "[6000]\tvalid_0's auc: 0.899251\n",
      "[7000]\tvalid_0's auc: 0.899486\n",
      "[8000]\tvalid_0's auc: 0.899741\n",
      "[9000]\tvalid_0's auc: 0.899639\n",
      "[10000]\tvalid_0's auc: 0.8996\n",
      "[11000]\tvalid_0's auc: 0.899595\n",
      "Early stopping, best iteration is:\n",
      "[8038]\tvalid_0's auc: 0.899789\n",
      "Fold 3 started at Wed Mar 13 01:37:49 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88644\n",
      "[2000]\tvalid_0's auc: 0.893424\n",
      "[3000]\tvalid_0's auc: 0.89699\n",
      "[4000]\tvalid_0's auc: 0.899043\n",
      "[5000]\tvalid_0's auc: 0.900142\n",
      "[6000]\tvalid_0's auc: 0.901026\n",
      "[7000]\tvalid_0's auc: 0.901531\n",
      "[8000]\tvalid_0's auc: 0.901701\n",
      "[9000]\tvalid_0's auc: 0.901846\n",
      "[10000]\tvalid_0's auc: 0.901924\n",
      "[11000]\tvalid_0's auc: 0.902101\n",
      "[12000]\tvalid_0's auc: 0.902258\n",
      "[13000]\tvalid_0's auc: 0.902219\n",
      "[14000]\tvalid_0's auc: 0.902321\n",
      "[15000]\tvalid_0's auc: 0.902181\n",
      "[16000]\tvalid_0's auc: 0.901981\n",
      "[17000]\tvalid_0's auc: 0.901787\n",
      "Early stopping, best iteration is:\n",
      "[14017]\tvalid_0's auc: 0.902347\n",
      "Fold 4 started at Wed Mar 13 02:45:05 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.892367\n",
      "[2000]\tvalid_0's auc: 0.899899\n",
      "[3000]\tvalid_0's auc: 0.903822\n",
      "[4000]\tvalid_0's auc: 0.906002\n",
      "[5000]\tvalid_0's auc: 0.907428\n",
      "[6000]\tvalid_0's auc: 0.908152\n",
      "[7000]\tvalid_0's auc: 0.908662\n",
      "[8000]\tvalid_0's auc: 0.908904\n",
      "[9000]\tvalid_0's auc: 0.909363\n",
      "[10000]\tvalid_0's auc: 0.90944\n",
      "[11000]\tvalid_0's auc: 0.909581\n",
      "[12000]\tvalid_0's auc: 0.909578\n",
      "[13000]\tvalid_0's auc: 0.909516\n",
      "[14000]\tvalid_0's auc: 0.909577\n",
      "[15000]\tvalid_0's auc: 0.909396\n",
      "Early stopping, best iteration is:\n",
      "[12184]\tvalid_0's auc: 0.909672\n",
      "Fold 5 started at Wed Mar 13 03:44:19 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886618\n",
      "[2000]\tvalid_0's auc: 0.89422\n",
      "[3000]\tvalid_0's auc: 0.898083\n",
      "[4000]\tvalid_0's auc: 0.900418\n",
      "[5000]\tvalid_0's auc: 0.901939\n",
      "[6000]\tvalid_0's auc: 0.902981\n",
      "[7000]\tvalid_0's auc: 0.903552\n",
      "[8000]\tvalid_0's auc: 0.903886\n",
      "[9000]\tvalid_0's auc: 0.904128\n",
      "[10000]\tvalid_0's auc: 0.903933\n",
      "[11000]\tvalid_0's auc: 0.904053\n",
      "Early stopping, best iteration is:\n",
      "[8602]\tvalid_0's auc: 0.904183\n",
      "Fold 6 started at Wed Mar 13 04:28:40 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886702\n",
      "[2000]\tvalid_0's auc: 0.894628\n",
      "[3000]\tvalid_0's auc: 0.898131\n",
      "[4000]\tvalid_0's auc: 0.900627\n",
      "[5000]\tvalid_0's auc: 0.901831\n",
      "[6000]\tvalid_0's auc: 0.902751\n",
      "[7000]\tvalid_0's auc: 0.903417\n",
      "[8000]\tvalid_0's auc: 0.9039\n",
      "[9000]\tvalid_0's auc: 0.90388\n",
      "[10000]\tvalid_0's auc: 0.903974\n",
      "[11000]\tvalid_0's auc: 0.904066\n",
      "[12000]\tvalid_0's auc: 0.904087\n",
      "[13000]\tvalid_0's auc: 0.903967\n",
      "[14000]\tvalid_0's auc: 0.903774\n",
      "Early stopping, best iteration is:\n",
      "[11615]\tvalid_0's auc: 0.904154\n",
      "Fold 7 started at Wed Mar 13 05:28:15 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.877059\n",
      "[2000]\tvalid_0's auc: 0.885391\n",
      "[3000]\tvalid_0's auc: 0.889076\n",
      "[4000]\tvalid_0's auc: 0.891149\n",
      "[5000]\tvalid_0's auc: 0.892406\n",
      "[6000]\tvalid_0's auc: 0.893432\n",
      "[7000]\tvalid_0's auc: 0.893997\n",
      "[8000]\tvalid_0's auc: 0.89455\n",
      "[9000]\tvalid_0's auc: 0.894778\n",
      "[10000]\tvalid_0's auc: 0.894909\n",
      "[11000]\tvalid_0's auc: 0.894995\n",
      "[12000]\tvalid_0's auc: 0.895017\n",
      "[13000]\tvalid_0's auc: 0.894931\n",
      "[14000]\tvalid_0's auc: 0.894973\n",
      "[15000]\tvalid_0's auc: 0.894857\n",
      "Early stopping, best iteration is:\n",
      "[12474]\tvalid_0's auc: 0.895137\n",
      "Fold 8 started at Wed Mar 13 06:30:45 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.878676\n",
      "[2000]\tvalid_0's auc: 0.885423\n",
      "[3000]\tvalid_0's auc: 0.889441\n",
      "[4000]\tvalid_0's auc: 0.892023\n",
      "[5000]\tvalid_0's auc: 0.893651\n",
      "[6000]\tvalid_0's auc: 0.894766\n",
      "[7000]\tvalid_0's auc: 0.895386\n",
      "[8000]\tvalid_0's auc: 0.895632\n",
      "[9000]\tvalid_0's auc: 0.895825\n",
      "[10000]\tvalid_0's auc: 0.896013\n",
      "[11000]\tvalid_0's auc: 0.896113\n",
      "[12000]\tvalid_0's auc: 0.896105\n",
      "[13000]\tvalid_0's auc: 0.896215\n",
      "[14000]\tvalid_0's auc: 0.896344\n",
      "[15000]\tvalid_0's auc: 0.896047\n",
      "[16000]\tvalid_0's auc: 0.89597\n",
      "Early stopping, best iteration is:\n",
      "[13627]\tvalid_0's auc: 0.896375\n",
      "Fold 9 started at Wed Mar 13 07:36:27 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.879952\n",
      "[2000]\tvalid_0's auc: 0.888238\n",
      "[3000]\tvalid_0's auc: 0.891741\n",
      "[4000]\tvalid_0's auc: 0.894255\n",
      "[5000]\tvalid_0's auc: 0.895464\n",
      "[6000]\tvalid_0's auc: 0.896245\n",
      "[7000]\tvalid_0's auc: 0.896648\n",
      "[8000]\tvalid_0's auc: 0.896967\n",
      "[9000]\tvalid_0's auc: 0.897027\n",
      "[10000]\tvalid_0's auc: 0.896944\n",
      "[11000]\tvalid_0's auc: 0.896927\n",
      "[12000]\tvalid_0's auc: 0.896876\n",
      "Early stopping, best iteration is:\n",
      "[9617]\tvalid_0's auc: 0.897109\n",
      "CV mean score: 0.9009, std: 0.0042.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X1, X2, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_1_10_2', oof_lgb)\n",
    "np.save('../cache/preds_new_quant_1_10_2', prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = prediction_lgb\n",
    "sub.to_csv('../submissions/sub12l1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cat       lgb       xgb      cat2\n",
      "0  0.010545  0.011048  0.008313  0.011734\n",
      "1  0.455996  0.469107  0.392034  0.468508\n",
      "2  0.004485  0.004675  0.004919  0.004418\n",
      "3  0.243523  0.262949  0.340744  0.263807\n",
      "4  0.097492  0.093573  0.110311  0.092751\n",
      "0.92544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_new_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_new_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb\n",
    "stage2_test['lgb'] = preds_lgb\n",
    "\n",
    "oof_xgb = np.load('../cache/oof_xgb_quant_1_10_1.npy')\n",
    "preds_xgb = np.load('../cache/preds_xgb_quant_1_10_1.npy')\n",
    "stage2['xgb'] = oof_xgb[0]\n",
    "stage2_test['xgb'] = preds_xgb[0]\n",
    "\n",
    "oof_cat2 = np.load('../cache/oof_cat_quant_bin_1_10_1.npy')\n",
    "preds_cat2= np.load('../cache/preds_cat_quant_bin_1_10_1.npy')\n",
    "stage2['cat2'] = oof_cat2[0]\n",
    "stage2_test['cat2'] = preds_cat2[0]\n",
    "\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12m.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01104833, 0.46910688, 0.00467471, ..., 0.06627787, 0.05451603,\n",
       "       0.0071865 ])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10474364, 0.20477118, 0.16730939, ..., 0.00454418, 0.0903125 ,\n",
       "       0.06661956])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10474364, 0.20477118, 0.16730939, ..., 0.00454418, 0.0903125 ,\n",
       "       0.06661956])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>lgb</th>\n",
       "      <th>xgb</th>\n",
       "      <th>cat2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.088812</td>\n",
       "      <td>0.104744</td>\n",
       "      <td>0.063761</td>\n",
       "      <td>0.083926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.202222</td>\n",
       "      <td>0.204771</td>\n",
       "      <td>0.228061</td>\n",
       "      <td>0.192124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.212390</td>\n",
       "      <td>0.167309</td>\n",
       "      <td>0.208203</td>\n",
       "      <td>0.214763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.182568</td>\n",
       "      <td>0.212251</td>\n",
       "      <td>0.211347</td>\n",
       "      <td>0.198181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040704</td>\n",
       "      <td>0.040589</td>\n",
       "      <td>0.050059</td>\n",
       "      <td>0.041046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.001779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005347</td>\n",
       "      <td>0.005839</td>\n",
       "      <td>0.006722</td>\n",
       "      <td>0.004904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.185684</td>\n",
       "      <td>0.167742</td>\n",
       "      <td>0.175542</td>\n",
       "      <td>0.197578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.002579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007116</td>\n",
       "      <td>0.008125</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>0.007028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.304082</td>\n",
       "      <td>0.225350</td>\n",
       "      <td>0.233025</td>\n",
       "      <td>0.317354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.029463</td>\n",
       "      <td>0.030189</td>\n",
       "      <td>0.033986</td>\n",
       "      <td>0.028669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.040072</td>\n",
       "      <td>0.047292</td>\n",
       "      <td>0.034760</td>\n",
       "      <td>0.037035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.024482</td>\n",
       "      <td>0.029274</td>\n",
       "      <td>0.026401</td>\n",
       "      <td>0.024183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005904</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>0.007521</td>\n",
       "      <td>0.005299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.033733</td>\n",
       "      <td>0.037513</td>\n",
       "      <td>0.039988</td>\n",
       "      <td>0.032537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.432158</td>\n",
       "      <td>0.396484</td>\n",
       "      <td>0.379295</td>\n",
       "      <td>0.405342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.023617</td>\n",
       "      <td>0.025091</td>\n",
       "      <td>0.028354</td>\n",
       "      <td>0.026488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.183893</td>\n",
       "      <td>0.175888</td>\n",
       "      <td>0.173244</td>\n",
       "      <td>0.182410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.007933</td>\n",
       "      <td>0.011825</td>\n",
       "      <td>0.008435</td>\n",
       "      <td>0.007754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.332179</td>\n",
       "      <td>0.335981</td>\n",
       "      <td>0.405426</td>\n",
       "      <td>0.294309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.073929</td>\n",
       "      <td>0.079478</td>\n",
       "      <td>0.098319</td>\n",
       "      <td>0.073351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.006682</td>\n",
       "      <td>0.007971</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.006680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.033723</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.031991</td>\n",
       "      <td>0.029169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.009059</td>\n",
       "      <td>0.010429</td>\n",
       "      <td>0.012706</td>\n",
       "      <td>0.007117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.092351</td>\n",
       "      <td>0.108473</td>\n",
       "      <td>0.103159</td>\n",
       "      <td>0.077922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.090522</td>\n",
       "      <td>0.082901</td>\n",
       "      <td>0.093131</td>\n",
       "      <td>0.086625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.014668</td>\n",
       "      <td>0.013663</td>\n",
       "      <td>0.014113</td>\n",
       "      <td>0.014180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.322316</td>\n",
       "      <td>0.290937</td>\n",
       "      <td>0.401441</td>\n",
       "      <td>0.341494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.089752</td>\n",
       "      <td>0.088743</td>\n",
       "      <td>0.094623</td>\n",
       "      <td>0.075793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199970</th>\n",
       "      <td>0.044471</td>\n",
       "      <td>0.056815</td>\n",
       "      <td>0.037927</td>\n",
       "      <td>0.044509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199971</th>\n",
       "      <td>0.166861</td>\n",
       "      <td>0.155851</td>\n",
       "      <td>0.190176</td>\n",
       "      <td>0.181472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199972</th>\n",
       "      <td>0.004132</td>\n",
       "      <td>0.004928</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.004662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199973</th>\n",
       "      <td>0.060858</td>\n",
       "      <td>0.057374</td>\n",
       "      <td>0.063601</td>\n",
       "      <td>0.060691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199974</th>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.005459</td>\n",
       "      <td>0.006951</td>\n",
       "      <td>0.004787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199975</th>\n",
       "      <td>0.082498</td>\n",
       "      <td>0.088390</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.085742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199976</th>\n",
       "      <td>0.151468</td>\n",
       "      <td>0.213669</td>\n",
       "      <td>0.212621</td>\n",
       "      <td>0.155392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199977</th>\n",
       "      <td>0.191030</td>\n",
       "      <td>0.193039</td>\n",
       "      <td>0.147367</td>\n",
       "      <td>0.183323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199978</th>\n",
       "      <td>0.028185</td>\n",
       "      <td>0.032552</td>\n",
       "      <td>0.026185</td>\n",
       "      <td>0.030465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199979</th>\n",
       "      <td>0.062224</td>\n",
       "      <td>0.089105</td>\n",
       "      <td>0.072573</td>\n",
       "      <td>0.067117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199980</th>\n",
       "      <td>0.050486</td>\n",
       "      <td>0.060319</td>\n",
       "      <td>0.060105</td>\n",
       "      <td>0.053236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199981</th>\n",
       "      <td>0.041497</td>\n",
       "      <td>0.036972</td>\n",
       "      <td>0.035663</td>\n",
       "      <td>0.046190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199982</th>\n",
       "      <td>0.024625</td>\n",
       "      <td>0.027160</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>0.023368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199983</th>\n",
       "      <td>0.031026</td>\n",
       "      <td>0.031191</td>\n",
       "      <td>0.032238</td>\n",
       "      <td>0.029396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199984</th>\n",
       "      <td>0.014854</td>\n",
       "      <td>0.016647</td>\n",
       "      <td>0.016544</td>\n",
       "      <td>0.012907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199985</th>\n",
       "      <td>0.164624</td>\n",
       "      <td>0.168101</td>\n",
       "      <td>0.151474</td>\n",
       "      <td>0.151713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199986</th>\n",
       "      <td>0.522062</td>\n",
       "      <td>0.443600</td>\n",
       "      <td>0.439022</td>\n",
       "      <td>0.560769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199987</th>\n",
       "      <td>0.046251</td>\n",
       "      <td>0.046598</td>\n",
       "      <td>0.050058</td>\n",
       "      <td>0.046032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199988</th>\n",
       "      <td>0.011506</td>\n",
       "      <td>0.013305</td>\n",
       "      <td>0.015333</td>\n",
       "      <td>0.011095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199989</th>\n",
       "      <td>0.032320</td>\n",
       "      <td>0.033683</td>\n",
       "      <td>0.039366</td>\n",
       "      <td>0.033909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199990</th>\n",
       "      <td>0.004535</td>\n",
       "      <td>0.005530</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>0.004996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199991</th>\n",
       "      <td>0.006074</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.006126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199992</th>\n",
       "      <td>0.336565</td>\n",
       "      <td>0.298959</td>\n",
       "      <td>0.324612</td>\n",
       "      <td>0.339624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199993</th>\n",
       "      <td>0.008708</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>0.012822</td>\n",
       "      <td>0.008838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>0.066050</td>\n",
       "      <td>0.078504</td>\n",
       "      <td>0.064764</td>\n",
       "      <td>0.069827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0.035813</td>\n",
       "      <td>0.037860</td>\n",
       "      <td>0.028425</td>\n",
       "      <td>0.038429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0.006524</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.007260</td>\n",
       "      <td>0.006772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>0.004480</td>\n",
       "      <td>0.002884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0.089904</td>\n",
       "      <td>0.090312</td>\n",
       "      <td>0.088191</td>\n",
       "      <td>0.096151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0.063892</td>\n",
       "      <td>0.066620</td>\n",
       "      <td>0.075325</td>\n",
       "      <td>0.064138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             cat       lgb       xgb      cat2\n",
       "0       0.088812  0.104744  0.063761  0.083926\n",
       "1       0.202222  0.204771  0.228061  0.192124\n",
       "2       0.212390  0.167309  0.208203  0.214763\n",
       "3       0.182568  0.212251  0.211347  0.198181\n",
       "4       0.040704  0.040589  0.050059  0.041046\n",
       "5       0.001644  0.001893  0.002022  0.001779\n",
       "6       0.005347  0.005839  0.006722  0.004904\n",
       "7       0.185684  0.167742  0.175542  0.197578\n",
       "8       0.002262  0.002816  0.002666  0.002579\n",
       "9       0.007116  0.008125  0.006610  0.007028\n",
       "10      0.304082  0.225350  0.233025  0.317354\n",
       "11      0.029463  0.030189  0.033986  0.028669\n",
       "12      0.040072  0.047292  0.034760  0.037035\n",
       "13      0.024482  0.029274  0.026401  0.024183\n",
       "14      0.005904  0.006603  0.007521  0.005299\n",
       "15      0.033733  0.037513  0.039988  0.032537\n",
       "16      0.432158  0.396484  0.379295  0.405342\n",
       "17      0.023617  0.025091  0.028354  0.026488\n",
       "18      0.183893  0.175888  0.173244  0.182410\n",
       "19      0.007933  0.011825  0.008435  0.007754\n",
       "20      0.332179  0.335981  0.405426  0.294309\n",
       "21      0.073929  0.079478  0.098319  0.073351\n",
       "22      0.006682  0.007971  0.006381  0.006680\n",
       "23      0.033723  0.033520  0.031991  0.029169\n",
       "24      0.009059  0.010429  0.012706  0.007117\n",
       "25      0.092351  0.108473  0.103159  0.077922\n",
       "26      0.090522  0.082901  0.093131  0.086625\n",
       "27      0.014668  0.013663  0.014113  0.014180\n",
       "28      0.322316  0.290937  0.401441  0.341494\n",
       "29      0.089752  0.088743  0.094623  0.075793\n",
       "...          ...       ...       ...       ...\n",
       "199970  0.044471  0.056815  0.037927  0.044509\n",
       "199971  0.166861  0.155851  0.190176  0.181472\n",
       "199972  0.004132  0.004928  0.004755  0.004662\n",
       "199973  0.060858  0.057374  0.063601  0.060691\n",
       "199974  0.005156  0.005459  0.006951  0.004787\n",
       "199975  0.082498  0.088390  0.060547  0.085742\n",
       "199976  0.151468  0.213669  0.212621  0.155392\n",
       "199977  0.191030  0.193039  0.147367  0.183323\n",
       "199978  0.028185  0.032552  0.026185  0.030465\n",
       "199979  0.062224  0.089105  0.072573  0.067117\n",
       "199980  0.050486  0.060319  0.060105  0.053236\n",
       "199981  0.041497  0.036972  0.035663  0.046190\n",
       "199982  0.024625  0.027160  0.028533  0.023368\n",
       "199983  0.031026  0.031191  0.032238  0.029396\n",
       "199984  0.014854  0.016647  0.016544  0.012907\n",
       "199985  0.164624  0.168101  0.151474  0.151713\n",
       "199986  0.522062  0.443600  0.439022  0.560769\n",
       "199987  0.046251  0.046598  0.050058  0.046032\n",
       "199988  0.011506  0.013305  0.015333  0.011095\n",
       "199989  0.032320  0.033683  0.039366  0.033909\n",
       "199990  0.004535  0.005530  0.005067  0.004996\n",
       "199991  0.006074  0.006751  0.005635  0.006126\n",
       "199992  0.336565  0.298959  0.324612  0.339624\n",
       "199993  0.008708  0.010708  0.012822  0.008838\n",
       "199994  0.066050  0.078504  0.064764  0.069827\n",
       "199995  0.035813  0.037860  0.028425  0.038429\n",
       "199996  0.006524  0.007682  0.007260  0.006772\n",
       "199997  0.002899  0.004544  0.004480  0.002884\n",
       "199998  0.089904  0.090312  0.088191  0.096151\n",
       "199999  0.063892  0.066620  0.075325  0.064138\n",
       "\n",
       "[200000 rows x 4 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sat Mar  9 07:14:56 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884342\n",
      "[2000]\tvalid_0's auc: 0.891529\n",
      "[3000]\tvalid_0's auc: 0.895224\n",
      "[4000]\tvalid_0's auc: 0.897491\n",
      "[5000]\tvalid_0's auc: 0.898981\n",
      "[6000]\tvalid_0's auc: 0.899905\n",
      "[7000]\tvalid_0's auc: 0.900284\n",
      "[8000]\tvalid_0's auc: 0.900579\n",
      "[9000]\tvalid_0's auc: 0.900514\n",
      "[10000]\tvalid_0's auc: 0.900475\n",
      "[11000]\tvalid_0's auc: 0.900328\n",
      "Early stopping, best iteration is:\n",
      "[8214]\tvalid_0's auc: 0.90063\n",
      "Fold 1 started at Sat Mar  9 07:17:12 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88137\n",
      "[2000]\tvalid_0's auc: 0.888849\n",
      "[3000]\tvalid_0's auc: 0.893039\n",
      "[4000]\tvalid_0's auc: 0.895283\n",
      "[5000]\tvalid_0's auc: 0.896659\n",
      "[6000]\tvalid_0's auc: 0.897312\n",
      "[7000]\tvalid_0's auc: 0.897902\n",
      "[8000]\tvalid_0's auc: 0.898059\n",
      "[9000]\tvalid_0's auc: 0.898419\n",
      "[10000]\tvalid_0's auc: 0.898639\n",
      "[11000]\tvalid_0's auc: 0.898532\n",
      "[12000]\tvalid_0's auc: 0.898491\n",
      "[13000]\tvalid_0's auc: 0.89844\n",
      "Early stopping, best iteration is:\n",
      "[10151]\tvalid_0's auc: 0.89871\n",
      "Fold 2 started at Sat Mar  9 07:20:01 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883866\n",
      "[2000]\tvalid_0's auc: 0.890982\n",
      "[3000]\tvalid_0's auc: 0.894123\n",
      "[4000]\tvalid_0's auc: 0.896053\n",
      "[5000]\tvalid_0's auc: 0.897486\n",
      "[6000]\tvalid_0's auc: 0.898246\n",
      "[7000]\tvalid_0's auc: 0.898343\n",
      "[8000]\tvalid_0's auc: 0.898562\n",
      "[9000]\tvalid_0's auc: 0.898651\n",
      "[10000]\tvalid_0's auc: 0.898549\n",
      "[11000]\tvalid_0's auc: 0.898635\n",
      "[12000]\tvalid_0's auc: 0.898625\n",
      "Early stopping, best iteration is:\n",
      "[9092]\tvalid_0's auc: 0.898698\n",
      "Fold 3 started at Sat Mar  9 07:44:01 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886489\n",
      "[2000]\tvalid_0's auc: 0.893169\n",
      "[3000]\tvalid_0's auc: 0.896781\n",
      "[4000]\tvalid_0's auc: 0.898961\n",
      "[5000]\tvalid_0's auc: 0.900179\n",
      "[6000]\tvalid_0's auc: 0.900874\n",
      "[7000]\tvalid_0's auc: 0.901286\n",
      "[8000]\tvalid_0's auc: 0.901301\n",
      "[9000]\tvalid_0's auc: 0.901446\n",
      "[10000]\tvalid_0's auc: 0.901637\n",
      "[11000]\tvalid_0's auc: 0.901529\n",
      "[12000]\tvalid_0's auc: 0.901554\n",
      "Early stopping, best iteration is:\n",
      "[9966]\tvalid_0's auc: 0.901648\n",
      "Fold 4 started at Sat Mar  9 08:26:30 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.889894\n",
      "[2000]\tvalid_0's auc: 0.898543\n",
      "[3000]\tvalid_0's auc: 0.903328\n",
      "[4000]\tvalid_0's auc: 0.905644\n",
      "[5000]\tvalid_0's auc: 0.907279\n",
      "[6000]\tvalid_0's auc: 0.908111\n",
      "[7000]\tvalid_0's auc: 0.908671\n",
      "[8000]\tvalid_0's auc: 0.908891\n",
      "[9000]\tvalid_0's auc: 0.909047\n",
      "[10000]\tvalid_0's auc: 0.909043\n",
      "[11000]\tvalid_0's auc: 0.909093\n",
      "[12000]\tvalid_0's auc: 0.908961\n",
      "Early stopping, best iteration is:\n",
      "[9558]\tvalid_0's auc: 0.909132\n",
      "Fold 5 started at Sat Mar  9 09:06:11 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885742\n",
      "[2000]\tvalid_0's auc: 0.893519\n",
      "[3000]\tvalid_0's auc: 0.897492\n",
      "[4000]\tvalid_0's auc: 0.900051\n",
      "[5000]\tvalid_0's auc: 0.901641\n",
      "[6000]\tvalid_0's auc: 0.902585\n",
      "[7000]\tvalid_0's auc: 0.902879\n",
      "[8000]\tvalid_0's auc: 0.903024\n",
      "[9000]\tvalid_0's auc: 0.903122\n",
      "[10000]\tvalid_0's auc: 0.903095\n",
      "[11000]\tvalid_0's auc: 0.903005\n",
      "[12000]\tvalid_0's auc: 0.902772\n",
      "Early stopping, best iteration is:\n",
      "[9605]\tvalid_0's auc: 0.9032\n",
      "Fold 6 started at Sat Mar  9 09:40:48 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886626\n",
      "[2000]\tvalid_0's auc: 0.89404\n",
      "[3000]\tvalid_0's auc: 0.898199\n",
      "[4000]\tvalid_0's auc: 0.900701\n",
      "[5000]\tvalid_0's auc: 0.902285\n",
      "[6000]\tvalid_0's auc: 0.903418\n",
      "[7000]\tvalid_0's auc: 0.903803\n",
      "[8000]\tvalid_0's auc: 0.904041\n",
      "[9000]\tvalid_0's auc: 0.90411\n",
      "[10000]\tvalid_0's auc: 0.904116\n",
      "[11000]\tvalid_0's auc: 0.903938\n",
      "[12000]\tvalid_0's auc: 0.903861\n",
      "[13000]\tvalid_0's auc: 0.903852\n",
      "Early stopping, best iteration is:\n",
      "[10469]\tvalid_0's auc: 0.904184\n",
      "Fold 7 started at Sat Mar  9 10:20:02 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.876083\n",
      "[2000]\tvalid_0's auc: 0.883398\n",
      "[3000]\tvalid_0's auc: 0.887631\n",
      "[4000]\tvalid_0's auc: 0.89002\n",
      "[5000]\tvalid_0's auc: 0.891961\n",
      "[6000]\tvalid_0's auc: 0.892952\n",
      "[7000]\tvalid_0's auc: 0.893915\n",
      "[8000]\tvalid_0's auc: 0.89421\n",
      "[9000]\tvalid_0's auc: 0.894413\n",
      "[10000]\tvalid_0's auc: 0.894379\n",
      "[11000]\tvalid_0's auc: 0.894507\n",
      "[12000]\tvalid_0's auc: 0.894603\n",
      "[13000]\tvalid_0's auc: 0.894703\n",
      "[14000]\tvalid_0's auc: 0.894759\n",
      "[15000]\tvalid_0's auc: 0.894703\n",
      "[16000]\tvalid_0's auc: 0.894703\n",
      "Early stopping, best iteration is:\n",
      "[13548]\tvalid_0's auc: 0.894853\n",
      "Fold 8 started at Sat Mar  9 11:08:38 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.876183\n",
      "[2000]\tvalid_0's auc: 0.88432\n",
      "[3000]\tvalid_0's auc: 0.888831\n",
      "[4000]\tvalid_0's auc: 0.891366\n",
      "[5000]\tvalid_0's auc: 0.893175\n",
      "[6000]\tvalid_0's auc: 0.894252\n",
      "[7000]\tvalid_0's auc: 0.8948\n",
      "[8000]\tvalid_0's auc: 0.895234\n",
      "[9000]\tvalid_0's auc: 0.895351\n",
      "[10000]\tvalid_0's auc: 0.895469\n",
      "[11000]\tvalid_0's auc: 0.895445\n",
      "[12000]\tvalid_0's auc: 0.895365\n",
      "Early stopping, best iteration is:\n",
      "[9873]\tvalid_0's auc: 0.895594\n",
      "Fold 9 started at Sat Mar  9 11:46:22 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.879807\n",
      "[2000]\tvalid_0's auc: 0.886759\n",
      "[3000]\tvalid_0's auc: 0.890893\n",
      "[4000]\tvalid_0's auc: 0.893315\n",
      "[5000]\tvalid_0's auc: 0.894559\n",
      "[6000]\tvalid_0's auc: 0.895369\n",
      "[7000]\tvalid_0's auc: 0.896045\n",
      "[8000]\tvalid_0's auc: 0.89619\n",
      "[9000]\tvalid_0's auc: 0.896257\n",
      "[10000]\tvalid_0's auc: 0.896339\n",
      "[11000]\tvalid_0's auc: 0.896333\n",
      "[12000]\tvalid_0's auc: 0.896055\n",
      "Early stopping, best iteration is:\n",
      "[9798]\tvalid_0's auc: 0.896403\n",
      "CV mean score: 0.9003, std: 0.0042.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_2bins_1_10_1', oof)\n",
    "np.save('../cache/preds_new_quant_2bins_1_10_1', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lgb      lgb2\n",
      "0  0.011048  0.012400\n",
      "1  0.469107  0.542452\n",
      "2  0.004675  0.003715\n",
      "3  0.262949  0.239456\n",
      "4  0.093573  0.077072\n",
      "0.92513\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_new_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_new_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb\n",
    "stage2_test['lgb'] = preds_lgb\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_2bins_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_new_quant_2bins_1_10_1.npy')\n",
    "stage2['lgb2'] = oof[0]\n",
    "stage2_test['lgb2'] = preds[0]\n",
    "\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12n.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sat Mar  9 15:19:30 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883678\n",
      "[2000]\tvalid_0's auc: 0.891247\n",
      "[3000]\tvalid_0's auc: 0.895543\n",
      "[4000]\tvalid_0's auc: 0.898219\n",
      "[5000]\tvalid_0's auc: 0.899597\n",
      "[6000]\tvalid_0's auc: 0.900086\n",
      "[7000]\tvalid_0's auc: 0.900408\n",
      "[8000]\tvalid_0's auc: 0.900715\n",
      "[9000]\tvalid_0's auc: 0.900812\n",
      "[10000]\tvalid_0's auc: 0.900864\n",
      "[11000]\tvalid_0's auc: 0.900831\n",
      "[12000]\tvalid_0's auc: 0.900692\n",
      "[13000]\tvalid_0's auc: 0.900741\n",
      "Early stopping, best iteration is:\n",
      "[10555]\tvalid_0's auc: 0.900935\n",
      "Fold 1 started at Sat Mar  9 15:26:13 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.880218\n",
      "[2000]\tvalid_0's auc: 0.888838\n",
      "[3000]\tvalid_0's auc: 0.893072\n",
      "[4000]\tvalid_0's auc: 0.895576\n",
      "[5000]\tvalid_0's auc: 0.89687\n",
      "[6000]\tvalid_0's auc: 0.897726\n",
      "[7000]\tvalid_0's auc: 0.898101\n",
      "[8000]\tvalid_0's auc: 0.898141\n",
      "[9000]\tvalid_0's auc: 0.89822\n",
      "[10000]\tvalid_0's auc: 0.898391\n",
      "[11000]\tvalid_0's auc: 0.898488\n",
      "[12000]\tvalid_0's auc: 0.898605\n",
      "[13000]\tvalid_0's auc: 0.898448\n",
      "[14000]\tvalid_0's auc: 0.898434\n",
      "Early stopping, best iteration is:\n",
      "[11604]\tvalid_0's auc: 0.898659\n",
      "Fold 2 started at Sat Mar  9 16:07:19 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883579\n",
      "[2000]\tvalid_0's auc: 0.89173\n",
      "[3000]\tvalid_0's auc: 0.89538\n",
      "[4000]\tvalid_0's auc: 0.897468\n",
      "[5000]\tvalid_0's auc: 0.898883\n",
      "[6000]\tvalid_0's auc: 0.899407\n",
      "[7000]\tvalid_0's auc: 0.899817\n",
      "[8000]\tvalid_0's auc: 0.899977\n",
      "[9000]\tvalid_0's auc: 0.899924\n",
      "[10000]\tvalid_0's auc: 0.899853\n",
      "[11000]\tvalid_0's auc: 0.899809\n",
      "Early stopping, best iteration is:\n",
      "[8811]\tvalid_0's auc: 0.900045\n",
      "Fold 3 started at Sat Mar  9 16:43:49 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885529\n",
      "[2000]\tvalid_0's auc: 0.893245\n",
      "[3000]\tvalid_0's auc: 0.897281\n",
      "[4000]\tvalid_0's auc: 0.899832\n",
      "[5000]\tvalid_0's auc: 0.900907\n",
      "[6000]\tvalid_0's auc: 0.901616\n",
      "[7000]\tvalid_0's auc: 0.90189\n",
      "[8000]\tvalid_0's auc: 0.902178\n",
      "[9000]\tvalid_0's auc: 0.902369\n",
      "[10000]\tvalid_0's auc: 0.902367\n",
      "[11000]\tvalid_0's auc: 0.902364\n",
      "[12000]\tvalid_0's auc: 0.902258\n",
      "[13000]\tvalid_0's auc: 0.902052\n",
      "Early stopping, best iteration is:\n",
      "[10468]\tvalid_0's auc: 0.902472\n",
      "Fold 4 started at Sat Mar  9 17:18:46 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.889278\n",
      "[2000]\tvalid_0's auc: 0.898358\n",
      "[3000]\tvalid_0's auc: 0.903076\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-b003f81760ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mX4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mX4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moof_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lgb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_feature_importance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0moof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-8a9bee3ee549>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, averaging, model)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 early_stopping_rounds=3000)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;31m#             y_pred_valid = model.predict(X_valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#             y_pred = model.predict(X_test, num_iteration=model.best_iteration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# var_12 9559 (yes)\n",
    "# var_68 459 (no)\n",
    "# var_91 7959 (yes)\n",
    "# var_103 9376 (no)\n",
    "# var_108 8524 (no)\n",
    "oof = []\n",
    "preds = []\n",
    "\n",
    "for col in ['var_12', 'var_68', 'var_91', 'var_103', 'var_108']:\n",
    "    bins = np.linspace(-6.0,6.0,40)\n",
    "    X = pd.get_dummies(pd.cut(X1[col].values, bins))\n",
    "    xcols = [col + str(i) for i in range(X.shape[1])]\n",
    "    X.columns = xcols\n",
    "    X3 = X1.copy()\n",
    "    X4 = X2.copy()\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X3 = pd.concat([X3, X], axis=1)\n",
    "    X4 = X4.drop(col, axis=1)\n",
    "    X4 = pd.concat([X4, X], axis=1)\n",
    "    oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, model_type='lgb', plot_feature_importance=False)\n",
    "    oof.append(oof_lgb)\n",
    "    preds.append(prediction_lgb)\n",
    "    print('--------------------------------------------------------')\n",
    "    print('--------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900927\tvalid_1's auc: 0.880968\n",
      "[2000]\ttraining's auc: 0.913444\tvalid_1's auc: 0.890252\n",
      "[3000]\ttraining's auc: 0.921559\tvalid_1's auc: 0.894417\n",
      "[4000]\ttraining's auc: 0.927662\tvalid_1's auc: 0.896401\n",
      "[5000]\ttraining's auc: 0.932907\tvalid_1's auc: 0.897907\n",
      "[6000]\ttraining's auc: 0.937484\tvalid_1's auc: 0.898466\n",
      "[7000]\ttraining's auc: 0.941681\tvalid_1's auc: 0.898818\n",
      "[8000]\ttraining's auc: 0.945592\tvalid_1's auc: 0.899014\n",
      "[9000]\ttraining's auc: 0.94931\tvalid_1's auc: 0.898972\n",
      "[10000]\ttraining's auc: 0.952843\tvalid_1's auc: 0.898945\n",
      "Early stopping, best iteration is:\n",
      "[7730]\ttraining's auc: 0.944544\tvalid_1's auc: 0.899034\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.900927\tvalid_1's auc: 0.880968\n",
      "[2000]\ttraining's auc: 0.913444\tvalid_1's auc: 0.890252\n",
      "[3000]\ttraining's auc: 0.921559\tvalid_1's auc: 0.894417\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-a4758f1c7c55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#     np.random.seed(123)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#     X_train, X_valid, y_train, y_valid = train_test_split(X3, y, test_size=0.2, stratify=y, random_state=42)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# var_12 9559\n",
    "# var_68 459\n",
    "# var_91 7959\n",
    "# var_103 9376\n",
    "# var_108 8524 (no)\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.01,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.05,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.4,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1, y, test_size=0.2, stratify=y, random_state=42)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=3000)\n",
    "\n",
    "for col in ['var_12', 'var_68', 'var_91', 'var_103', 'var_108']:\n",
    "    bins = np.linspace(-6.0,6.0,40)\n",
    "    X = pd.get_dummies(pd.cut(X1[col].values, bins))\n",
    "\n",
    "    X3 = X1.copy()\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X3 = pd.concat([X3, X], axis=1)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.01,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.05,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.4,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "    np.random.seed(123)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X3, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 248), (200000, 248))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "\n",
    "# bins = np.linspace(-6.0,6.0,40)\n",
    "# X3_12_bins = pd.get_dummies(pd.cut(X3['var_12'].values, bins))\n",
    "# X4_12_bins = pd.get_dummies(pd.cut(X4['var_12'].values, bins))\n",
    "\n",
    "bins = np.linspace(-6.0,6.0,50)\n",
    "X3_91_bins = pd.get_dummies(pd.cut(X3['var_91'].values, bins))\n",
    "X4_91_bins = pd.get_dummies(pd.cut(X4['var_91'].values, bins))\n",
    "\n",
    "# X3_12_bins = pd.DataFrame(X3_12_bins)\n",
    "# cols = ['var_12_' + str(i) for i in range(X3_12_bins.shape[1])]\n",
    "# X3_12_bins.columns = cols\n",
    "\n",
    "# X4_12_bins = pd.DataFrame(X4_12_bins)\n",
    "# X4_12_bins.columns = cols\n",
    "\n",
    "X3_91_bins = pd.DataFrame(X3_91_bins)\n",
    "cols = ['var_91_' + str(i) for i in range(X3_91_bins.shape[1])]\n",
    "X3_91_bins.columns = cols\n",
    "\n",
    "X4_91_bins = pd.DataFrame(X4_91_bins)\n",
    "X4_91_bins.columns = cols\n",
    "\n",
    "# X3 = pd.concat([X3, X3_12_bins], axis=1)\n",
    "X3 = pd.concat([X3, X3_91_bins], axis=1)\n",
    "\n",
    "# X4 = pd.concat([X4, X4_12_bins], axis=1)\n",
    "X4 = pd.concat([X4, X4_91_bins], axis=1)\n",
    "\n",
    "# X3 = X3.drop(['var_12', 'var_91'], axis=1)\n",
    "# X4 = X4.drop(['var_12', 'var_91'], axis=1)\n",
    "\n",
    "X3 = X3.drop(['var_91'], axis=1)\n",
    "X4 = X4.drop(['var_91'], axis=1)\n",
    "\n",
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sat Mar  9 19:25:24 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884311\n",
      "[2000]\tvalid_0's auc: 0.891256\n",
      "[3000]\tvalid_0's auc: 0.895289\n",
      "[4000]\tvalid_0's auc: 0.897523\n",
      "[5000]\tvalid_0's auc: 0.898782\n",
      "[6000]\tvalid_0's auc: 0.899689\n",
      "[7000]\tvalid_0's auc: 0.900272\n",
      "[8000]\tvalid_0's auc: 0.900332\n",
      "[9000]\tvalid_0's auc: 0.900355\n",
      "[10000]\tvalid_0's auc: 0.900452\n",
      "[11000]\tvalid_0's auc: 0.900362\n",
      "[12000]\tvalid_0's auc: 0.900242\n",
      "Early stopping, best iteration is:\n",
      "[9690]\tvalid_0's auc: 0.900536\n",
      "Fold 1 started at Sat Mar  9 19:27:40 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88041\n",
      "[2000]\tvalid_0's auc: 0.889129\n",
      "[3000]\tvalid_0's auc: 0.893111\n",
      "[4000]\tvalid_0's auc: 0.895589\n",
      "[5000]\tvalid_0's auc: 0.897119\n",
      "[6000]\tvalid_0's auc: 0.897733\n",
      "[7000]\tvalid_0's auc: 0.898036\n",
      "[8000]\tvalid_0's auc: 0.898298\n",
      "[9000]\tvalid_0's auc: 0.898462\n",
      "[10000]\tvalid_0's auc: 0.898545\n",
      "[11000]\tvalid_0's auc: 0.898381\n",
      "[12000]\tvalid_0's auc: 0.898379\n",
      "[13000]\tvalid_0's auc: 0.89823\n",
      "Early stopping, best iteration is:\n",
      "[10185]\tvalid_0's auc: 0.898597\n",
      "Fold 2 started at Sat Mar  9 19:58:50 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883797\n",
      "[2000]\tvalid_0's auc: 0.891138\n",
      "[3000]\tvalid_0's auc: 0.894513\n",
      "[4000]\tvalid_0's auc: 0.896624\n",
      "[5000]\tvalid_0's auc: 0.898028\n",
      "[6000]\tvalid_0's auc: 0.898596\n",
      "[7000]\tvalid_0's auc: 0.898996\n",
      "[8000]\tvalid_0's auc: 0.899153\n",
      "[9000]\tvalid_0's auc: 0.899077\n",
      "[10000]\tvalid_0's auc: 0.899104\n",
      "[11000]\tvalid_0's auc: 0.899047\n",
      "[12000]\tvalid_0's auc: 0.898833\n",
      "[13000]\tvalid_0's auc: 0.898699\n",
      "Early stopping, best iteration is:\n",
      "[10316]\tvalid_0's auc: 0.899197\n",
      "Fold 3 started at Sat Mar  9 20:29:47 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886203\n",
      "[2000]\tvalid_0's auc: 0.893087\n",
      "[3000]\tvalid_0's auc: 0.896958\n",
      "[4000]\tvalid_0's auc: 0.899143\n",
      "[5000]\tvalid_0's auc: 0.900534\n",
      "[6000]\tvalid_0's auc: 0.901227\n",
      "[7000]\tvalid_0's auc: 0.901639\n",
      "[8000]\tvalid_0's auc: 0.901702\n",
      "[9000]\tvalid_0's auc: 0.901853\n",
      "[10000]\tvalid_0's auc: 0.902062\n",
      "[11000]\tvalid_0's auc: 0.901923\n",
      "[12000]\tvalid_0's auc: 0.902037\n",
      "Early stopping, best iteration is:\n",
      "[9638]\tvalid_0's auc: 0.902143\n",
      "Fold 4 started at Sat Mar  9 20:56:15 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.890962\n",
      "[2000]\tvalid_0's auc: 0.899368\n",
      "[3000]\tvalid_0's auc: 0.90372\n",
      "[4000]\tvalid_0's auc: 0.906329\n",
      "[5000]\tvalid_0's auc: 0.907683\n",
      "[6000]\tvalid_0's auc: 0.90855\n",
      "[7000]\tvalid_0's auc: 0.909213\n",
      "[8000]\tvalid_0's auc: 0.909522\n",
      "[9000]\tvalid_0's auc: 0.909621\n",
      "[10000]\tvalid_0's auc: 0.909547\n",
      "[11000]\tvalid_0's auc: 0.909566\n",
      "[12000]\tvalid_0's auc: 0.909418\n",
      "Early stopping, best iteration is:\n",
      "[9247]\tvalid_0's auc: 0.909732\n",
      "Fold 5 started at Sat Mar  9 21:37:10 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88651\n",
      "[2000]\tvalid_0's auc: 0.894582\n",
      "[3000]\tvalid_0's auc: 0.898861\n",
      "[4000]\tvalid_0's auc: 0.901032\n",
      "[5000]\tvalid_0's auc: 0.902151\n",
      "[6000]\tvalid_0's auc: 0.902621\n",
      "[7000]\tvalid_0's auc: 0.902953\n",
      "[8000]\tvalid_0's auc: 0.90322\n",
      "[9000]\tvalid_0's auc: 0.903339\n",
      "[10000]\tvalid_0's auc: 0.9033\n",
      "[11000]\tvalid_0's auc: 0.903191\n",
      "[12000]\tvalid_0's auc: 0.903088\n",
      "Early stopping, best iteration is:\n",
      "[9223]\tvalid_0's auc: 0.903472\n",
      "Fold 6 started at Sat Mar  9 22:10:16 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885848\n",
      "[2000]\tvalid_0's auc: 0.893814\n",
      "[3000]\tvalid_0's auc: 0.897463\n",
      "[4000]\tvalid_0's auc: 0.900498\n",
      "[5000]\tvalid_0's auc: 0.902289\n",
      "[6000]\tvalid_0's auc: 0.903119\n",
      "[7000]\tvalid_0's auc: 0.903676\n",
      "[8000]\tvalid_0's auc: 0.904049\n",
      "[9000]\tvalid_0's auc: 0.904118\n",
      "[10000]\tvalid_0's auc: 0.904201\n",
      "[11000]\tvalid_0's auc: 0.904182\n",
      "[12000]\tvalid_0's auc: 0.904166\n",
      "[13000]\tvalid_0's auc: 0.904\n",
      "Early stopping, best iteration is:\n",
      "[10416]\tvalid_0's auc: 0.904331\n",
      "Fold 7 started at Sat Mar  9 22:44:40 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.876817\n",
      "[2000]\tvalid_0's auc: 0.884959\n",
      "[3000]\tvalid_0's auc: 0.889002\n",
      "[4000]\tvalid_0's auc: 0.891308\n",
      "[5000]\tvalid_0's auc: 0.892766\n",
      "[6000]\tvalid_0's auc: 0.893645\n",
      "[7000]\tvalid_0's auc: 0.894075\n",
      "[8000]\tvalid_0's auc: 0.894538\n",
      "[9000]\tvalid_0's auc: 0.894891\n",
      "[10000]\tvalid_0's auc: 0.895136\n",
      "[11000]\tvalid_0's auc: 0.895204\n",
      "[12000]\tvalid_0's auc: 0.895155\n",
      "[13000]\tvalid_0's auc: 0.895312\n",
      "[14000]\tvalid_0's auc: 0.895311\n",
      "[15000]\tvalid_0's auc: 0.895391\n",
      "[16000]\tvalid_0's auc: 0.895354\n",
      "[17000]\tvalid_0's auc: 0.895342\n",
      "[18000]\tvalid_0's auc: 0.895217\n",
      "Early stopping, best iteration is:\n",
      "[15535]\tvalid_0's auc: 0.895464\n",
      "Fold 8 started at Sat Mar  9 23:52:19 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.876238\n",
      "[2000]\tvalid_0's auc: 0.885502\n",
      "[3000]\tvalid_0's auc: 0.889673\n",
      "[4000]\tvalid_0's auc: 0.892466\n",
      "[5000]\tvalid_0's auc: 0.893923\n",
      "[6000]\tvalid_0's auc: 0.894807\n",
      "[7000]\tvalid_0's auc: 0.895311\n",
      "[8000]\tvalid_0's auc: 0.895664\n",
      "[9000]\tvalid_0's auc: 0.895854\n",
      "[10000]\tvalid_0's auc: 0.89605\n",
      "[11000]\tvalid_0's auc: 0.89604\n",
      "[12000]\tvalid_0's auc: 0.895978\n",
      "Early stopping, best iteration is:\n",
      "[9592]\tvalid_0's auc: 0.896085\n",
      "Fold 9 started at Sun Mar 10 00:34:58 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.878508\n",
      "[2000]\tvalid_0's auc: 0.887504\n",
      "[3000]\tvalid_0's auc: 0.891634\n",
      "[4000]\tvalid_0's auc: 0.894223\n",
      "[5000]\tvalid_0's auc: 0.895872\n",
      "[6000]\tvalid_0's auc: 0.896419\n",
      "[7000]\tvalid_0's auc: 0.896793\n",
      "[8000]\tvalid_0's auc: 0.89702\n",
      "[9000]\tvalid_0's auc: 0.897019\n",
      "[10000]\tvalid_0's auc: 0.897027\n",
      "[11000]\tvalid_0's auc: 0.896812\n",
      "[12000]\tvalid_0's auc: 0.89662\n",
      "Early stopping, best iteration is:\n",
      "[9440]\tvalid_0's auc: 0.897108\n",
      "CV mean score: 0.9007, std: 0.0042.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_var91_1_10_1', oof)\n",
    "np.save('../cache/preds_new_quant_var91_1_10_1', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lgb      lgb2\n",
      "0  0.011048  0.013086\n",
      "1  0.469107  0.437488\n",
      "2  0.004675  0.003175\n",
      "3  0.262949  0.230912\n",
      "4  0.093573  0.078222\n",
      "0.925145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_new_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_new_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb\n",
    "stage2_test['lgb'] = preds_lgb\n",
    "\n",
    "# oof = np.load('../cache/oof_new_quant_2bins_1_10_1.npy')\n",
    "# preds = np.load('../cache/preds_new_quant_2bins_1_10_1.npy')\n",
    "stage2['lgb2'] = oof[0]\n",
    "stage2_test['lgb2'] = preds[0]\n",
    "\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = prediction_lgb\n",
    "sub.to_csv('../submissions/sub12o.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 287), (200000, 287))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "\n",
    "bins = np.linspace(-6.0,6.0,40)\n",
    "X3_12_bins = pd.get_dummies(pd.cut(X3['var_12'].values, bins))\n",
    "X4_12_bins = pd.get_dummies(pd.cut(X4['var_12'].values, bins))\n",
    "\n",
    "bins = np.linspace(-6.0,6.0,50)\n",
    "X3_91_bins = pd.get_dummies(pd.cut(X3['var_91'].values, bins))\n",
    "X4_91_bins = pd.get_dummies(pd.cut(X4['var_91'].values, bins))\n",
    "\n",
    "X3_12_bins = pd.DataFrame(X3_12_bins)\n",
    "cols = ['var_12_' + str(i) for i in range(X3_12_bins.shape[1])]\n",
    "X3_12_bins.columns = cols\n",
    "\n",
    "X4_12_bins = pd.DataFrame(X4_12_bins)\n",
    "X4_12_bins.columns = cols\n",
    "\n",
    "X3_91_bins = pd.DataFrame(X3_91_bins)\n",
    "cols = ['var_91_' + str(i) for i in range(X3_91_bins.shape[1])]\n",
    "X3_91_bins.columns = cols\n",
    "\n",
    "X4_91_bins = pd.DataFrame(X4_91_bins)\n",
    "X4_91_bins.columns = cols\n",
    "\n",
    "X3 = pd.concat([X3, X3_12_bins], axis=1)\n",
    "X3 = pd.concat([X3, X3_91_bins], axis=1)\n",
    "\n",
    "X4 = pd.concat([X4, X4_12_bins], axis=1)\n",
    "X4 = pd.concat([X4, X4_91_bins], axis=1)\n",
    "\n",
    "data = pd.concat([X3, X4], axis=0)\n",
    "data['var_12_var_91'] = data['var_12'].astype('str') + '_' + data['var_91'].astype('str')\n",
    "for col in ['var_12_var_91']:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = data[col].astype('str')\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "X3 = data[:len(X3)]\n",
    "X4 = data[len(X3):]\n",
    "\n",
    "X3 = X3.drop(['var_12', 'var_91'], axis=1)\n",
    "X4 = X4.drop(['var_12', 'var_91'], axis=1)\n",
    "\n",
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sun Mar 10 02:13:11 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88335\n",
      "[2000]\tvalid_0's auc: 0.891272\n",
      "[3000]\tvalid_0's auc: 0.895955\n",
      "[4000]\tvalid_0's auc: 0.898515\n",
      "[5000]\tvalid_0's auc: 0.899661\n",
      "[6000]\tvalid_0's auc: 0.900289\n",
      "[7000]\tvalid_0's auc: 0.900955\n",
      "[8000]\tvalid_0's auc: 0.901037\n",
      "[9000]\tvalid_0's auc: 0.901189\n",
      "[10000]\tvalid_0's auc: 0.901078\n",
      "[11000]\tvalid_0's auc: 0.901199\n",
      "Early stopping, best iteration is:\n",
      "[8899]\tvalid_0's auc: 0.901237\n",
      "Fold 1 started at Sun Mar 10 02:15:17 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.879385\n",
      "[2000]\tvalid_0's auc: 0.888093\n",
      "[3000]\tvalid_0's auc: 0.892678\n",
      "[4000]\tvalid_0's auc: 0.895504\n",
      "[5000]\tvalid_0's auc: 0.896989\n",
      "[6000]\tvalid_0's auc: 0.897882\n",
      "[7000]\tvalid_0's auc: 0.89863\n",
      "[8000]\tvalid_0's auc: 0.898811\n",
      "[9000]\tvalid_0's auc: 0.899042\n",
      "[10000]\tvalid_0's auc: 0.899211\n",
      "[11000]\tvalid_0's auc: 0.899044\n",
      "[12000]\tvalid_0's auc: 0.899045\n",
      "Early stopping, best iteration is:\n",
      "[9705]\tvalid_0's auc: 0.899312\n",
      "Fold 2 started at Sun Mar 10 02:17:42 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884053\n",
      "[2000]\tvalid_0's auc: 0.891147\n",
      "[3000]\tvalid_0's auc: 0.894677\n",
      "[4000]\tvalid_0's auc: 0.896708\n",
      "[5000]\tvalid_0's auc: 0.897803\n",
      "[6000]\tvalid_0's auc: 0.898563\n",
      "[7000]\tvalid_0's auc: 0.898776\n",
      "[8000]\tvalid_0's auc: 0.898862\n",
      "[9000]\tvalid_0's auc: 0.898901\n",
      "[10000]\tvalid_0's auc: 0.898883\n",
      "[11000]\tvalid_0's auc: 0.898642\n",
      "Early stopping, best iteration is:\n",
      "[8754]\tvalid_0's auc: 0.899045\n",
      "Fold 3 started at Sun Mar 10 02:38:11 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883824\n",
      "[2000]\tvalid_0's auc: 0.89207\n",
      "[3000]\tvalid_0's auc: 0.896127\n",
      "[4000]\tvalid_0's auc: 0.89831\n",
      "[5000]\tvalid_0's auc: 0.899831\n",
      "[6000]\tvalid_0's auc: 0.900693\n",
      "[7000]\tvalid_0's auc: 0.901173\n",
      "[8000]\tvalid_0's auc: 0.901504\n",
      "[9000]\tvalid_0's auc: 0.901878\n",
      "[10000]\tvalid_0's auc: 0.901833\n",
      "[11000]\tvalid_0's auc: 0.901982\n",
      "[12000]\tvalid_0's auc: 0.902021\n",
      "[13000]\tvalid_0's auc: 0.901988\n",
      "[14000]\tvalid_0's auc: 0.90208\n",
      "[15000]\tvalid_0's auc: 0.902068\n",
      "[16000]\tvalid_0's auc: 0.902116\n",
      "[17000]\tvalid_0's auc: 0.901963\n",
      "[18000]\tvalid_0's auc: 0.901774\n",
      "Early stopping, best iteration is:\n",
      "[15875]\tvalid_0's auc: 0.902137\n",
      "Fold 4 started at Sun Mar 10 03:32:56 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.889775\n",
      "[2000]\tvalid_0's auc: 0.898103\n",
      "[3000]\tvalid_0's auc: 0.902889\n",
      "[4000]\tvalid_0's auc: 0.905584\n",
      "[5000]\tvalid_0's auc: 0.907072\n",
      "[6000]\tvalid_0's auc: 0.908015\n",
      "[7000]\tvalid_0's auc: 0.90854\n",
      "[8000]\tvalid_0's auc: 0.908891\n",
      "[9000]\tvalid_0's auc: 0.909097\n",
      "[10000]\tvalid_0's auc: 0.90894\n",
      "[11000]\tvalid_0's auc: 0.908956\n",
      "Early stopping, best iteration is:\n",
      "[8776]\tvalid_0's auc: 0.909157\n",
      "Fold 5 started at Sun Mar 10 04:06:03 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884249\n",
      "[2000]\tvalid_0's auc: 0.892342\n",
      "[3000]\tvalid_0's auc: 0.897279\n",
      "[4000]\tvalid_0's auc: 0.899825\n",
      "[5000]\tvalid_0's auc: 0.901234\n",
      "[6000]\tvalid_0's auc: 0.901996\n",
      "[7000]\tvalid_0's auc: 0.902292\n",
      "[8000]\tvalid_0's auc: 0.902517\n",
      "[9000]\tvalid_0's auc: 0.902461\n",
      "[10000]\tvalid_0's auc: 0.902603\n",
      "[11000]\tvalid_0's auc: 0.902519\n",
      "[12000]\tvalid_0's auc: 0.902653\n",
      "[13000]\tvalid_0's auc: 0.902488\n",
      "[14000]\tvalid_0's auc: 0.902342\n",
      "[15000]\tvalid_0's auc: 0.902242\n",
      "Early stopping, best iteration is:\n",
      "[12114]\tvalid_0's auc: 0.902715\n",
      "Fold 6 started at Sun Mar 10 04:52:50 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883862\n",
      "[2000]\tvalid_0's auc: 0.892292\n",
      "[3000]\tvalid_0's auc: 0.897121\n",
      "[4000]\tvalid_0's auc: 0.90037\n",
      "[5000]\tvalid_0's auc: 0.901922\n",
      "[6000]\tvalid_0's auc: 0.903043\n",
      "[7000]\tvalid_0's auc: 0.903883\n",
      "[8000]\tvalid_0's auc: 0.904232\n",
      "[9000]\tvalid_0's auc: 0.904174\n",
      "[10000]\tvalid_0's auc: 0.904226\n",
      "[11000]\tvalid_0's auc: 0.90425\n",
      "[12000]\tvalid_0's auc: 0.904263\n",
      "[13000]\tvalid_0's auc: 0.904204\n",
      "Early stopping, best iteration is:\n",
      "[10652]\tvalid_0's auc: 0.904392\n",
      "Fold 7 started at Sun Mar 10 05:33:52 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.875794\n",
      "[2000]\tvalid_0's auc: 0.88359\n",
      "[3000]\tvalid_0's auc: 0.888041\n",
      "[4000]\tvalid_0's auc: 0.890411\n",
      "[5000]\tvalid_0's auc: 0.892033\n",
      "[6000]\tvalid_0's auc: 0.893019\n",
      "[7000]\tvalid_0's auc: 0.893494\n",
      "[8000]\tvalid_0's auc: 0.893807\n",
      "[9000]\tvalid_0's auc: 0.894071\n",
      "[10000]\tvalid_0's auc: 0.894205\n",
      "[11000]\tvalid_0's auc: 0.894348\n",
      "[12000]\tvalid_0's auc: 0.894379\n",
      "[13000]\tvalid_0's auc: 0.894221\n",
      "[14000]\tvalid_0's auc: 0.894237\n",
      "Early stopping, best iteration is:\n",
      "[11461]\tvalid_0's auc: 0.894424\n",
      "Fold 8 started at Sun Mar 10 06:16:35 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.87603\n",
      "[2000]\tvalid_0's auc: 0.884731\n",
      "[3000]\tvalid_0's auc: 0.889176\n",
      "[4000]\tvalid_0's auc: 0.892111\n",
      "[5000]\tvalid_0's auc: 0.893752\n",
      "[6000]\tvalid_0's auc: 0.894777\n",
      "[7000]\tvalid_0's auc: 0.895118\n",
      "[8000]\tvalid_0's auc: 0.895627\n",
      "[9000]\tvalid_0's auc: 0.89576\n",
      "[10000]\tvalid_0's auc: 0.895763\n",
      "[11000]\tvalid_0's auc: 0.895667\n",
      "[12000]\tvalid_0's auc: 0.895584\n",
      "Early stopping, best iteration is:\n",
      "[9550]\tvalid_0's auc: 0.895877\n",
      "Fold 9 started at Sun Mar 10 07:06:25 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.880043\n",
      "[2000]\tvalid_0's auc: 0.887438\n",
      "[3000]\tvalid_0's auc: 0.891706\n",
      "[4000]\tvalid_0's auc: 0.894325\n",
      "[5000]\tvalid_0's auc: 0.895339\n",
      "[6000]\tvalid_0's auc: 0.896004\n",
      "[7000]\tvalid_0's auc: 0.896521\n",
      "[8000]\tvalid_0's auc: 0.896446\n",
      "[9000]\tvalid_0's auc: 0.896486\n",
      "[10000]\tvalid_0's auc: 0.89647\n",
      "[11000]\tvalid_0's auc: 0.89625\n",
      "Early stopping, best iteration is:\n",
      "[8816]\tvalid_0's auc: 0.896579\n",
      "CV mean score: 0.9005, std: 0.0042.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_var12_91_inter_1_10_1', oof)\n",
    "np.save('../cache/preds_new_quant_var12_91_inter_1_10_1', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lgb      lgb2\n",
      "0  0.011048  0.011934\n",
      "1  0.469107  0.462192\n",
      "2  0.004675  0.004977\n",
      "3  0.262949  0.228097\n",
      "4  0.093573  0.101013\n",
      "0.925245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_new_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_new_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb\n",
    "stage2_test['lgb'] = preds_lgb\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_var12_91_inter_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_new_quant_var12_91_inter_1_10_1.npy')\n",
    "stage2['lgb2'] = oof[0]\n",
    "stage2_test['lgb2'] = preds[0]\n",
    "\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = prediction_lgb\n",
    "sub.to_csv('../submissions/sub12p.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 287), (200000, 287))"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "\n",
    "bins = np.linspace(-6.0,6.0,40)\n",
    "\n",
    "X3_12_bins = pd.get_dummies(pd.cut(X3['var_12'].values, bins))\n",
    "X4_12_bins = pd.get_dummies(pd.cut(X4['var_12'].values, bins))\n",
    "X3['var_12'] = pd.cut(X3['var_12'].values, bins)\n",
    "X4['var_12'] = pd.cut(X4['var_12'].values, bins)\n",
    "\n",
    "bins = np.linspace(-6.0,6.0,50)\n",
    "\n",
    "X3_91_bins = pd.get_dummies(pd.cut(X3['var_91'].values, bins))\n",
    "X4_91_bins = pd.get_dummies(pd.cut(X4['var_91'].values, bins))\n",
    "X3['var_91'] = pd.cut(X3['var_91'].values, bins)\n",
    "X4['var_91'] = pd.cut(X4['var_91'].values, bins)\n",
    "\n",
    "X3_12_bins = pd.DataFrame(X3_12_bins)\n",
    "cols = ['var_12_' + str(i) for i in range(X3_12_bins.shape[1])]\n",
    "X3_12_bins.columns = cols\n",
    "\n",
    "X4_12_bins = pd.DataFrame(X4_12_bins)\n",
    "X4_12_bins.columns = cols\n",
    "\n",
    "X3_91_bins = pd.DataFrame(X3_91_bins)\n",
    "cols = ['var_91_' + str(i) for i in range(X3_91_bins.shape[1])]\n",
    "X3_91_bins.columns = cols\n",
    "\n",
    "X4_91_bins = pd.DataFrame(X4_91_bins)\n",
    "X4_91_bins.columns = cols\n",
    "\n",
    "X3 = pd.concat([X3, X3_12_bins], axis=1)\n",
    "X3 = pd.concat([X3, X3_91_bins], axis=1)\n",
    "\n",
    "X4 = pd.concat([X4, X4_12_bins], axis=1)\n",
    "X4 = pd.concat([X4, X4_91_bins], axis=1)\n",
    "\n",
    "data = pd.concat([X3, X4], axis=0)\n",
    "data['var_12_var_91'] = data['var_12'].astype('str') + '_' + data['var_91'].astype('str')\n",
    "for col in ['var_12_var_91']:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = data[col].astype('str')\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "X3 = data[:len(X3)]\n",
    "X4 = data[len(X3):]\n",
    "\n",
    "X3 = X3.drop(['var_12', 'var_91'], axis=1)\n",
    "X4 = X4.drop(['var_12', 'var_91'], axis=1)\n",
    "\n",
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sun Mar 10 09:04:17 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883304\n",
      "[2000]\tvalid_0's auc: 0.891166\n",
      "[3000]\tvalid_0's auc: 0.895655\n",
      "[4000]\tvalid_0's auc: 0.89836\n",
      "[5000]\tvalid_0's auc: 0.899562\n",
      "[6000]\tvalid_0's auc: 0.900143\n",
      "[7000]\tvalid_0's auc: 0.900718\n",
      "[8000]\tvalid_0's auc: 0.900789\n",
      "[9000]\tvalid_0's auc: 0.900868\n",
      "[10000]\tvalid_0's auc: 0.900789\n",
      "[11000]\tvalid_0's auc: 0.900846\n",
      "Early stopping, best iteration is:\n",
      "[8729]\tvalid_0's auc: 0.900906\n",
      "Fold 1 started at Sun Mar 10 09:06:16 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.879737\n",
      "[2000]\tvalid_0's auc: 0.888183\n",
      "[3000]\tvalid_0's auc: 0.89264\n",
      "[4000]\tvalid_0's auc: 0.895284\n",
      "[5000]\tvalid_0's auc: 0.8967\n",
      "[6000]\tvalid_0's auc: 0.897541\n",
      "[7000]\tvalid_0's auc: 0.898344\n",
      "[8000]\tvalid_0's auc: 0.898617\n",
      "[9000]\tvalid_0's auc: 0.898824\n",
      "[10000]\tvalid_0's auc: 0.898945\n",
      "[11000]\tvalid_0's auc: 0.898859\n",
      "[12000]\tvalid_0's auc: 0.898841\n",
      "Early stopping, best iteration is:\n",
      "[9705]\tvalid_0's auc: 0.899029\n",
      "Fold 2 started at Sun Mar 10 09:08:00 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884234\n",
      "[2000]\tvalid_0's auc: 0.891246\n",
      "[3000]\tvalid_0's auc: 0.894797\n",
      "[4000]\tvalid_0's auc: 0.896789\n",
      "[5000]\tvalid_0's auc: 0.897883\n",
      "[6000]\tvalid_0's auc: 0.898602\n",
      "[7000]\tvalid_0's auc: 0.898771\n",
      "[8000]\tvalid_0's auc: 0.898725\n",
      "[9000]\tvalid_0's auc: 0.898618\n",
      "Early stopping, best iteration is:\n",
      "[6611]\tvalid_0's auc: 0.898894\n",
      "Fold 3 started at Sun Mar 10 09:27:44 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884573\n",
      "[2000]\tvalid_0's auc: 0.892563\n",
      "[3000]\tvalid_0's auc: 0.89662\n",
      "[4000]\tvalid_0's auc: 0.898773\n",
      "[5000]\tvalid_0's auc: 0.900125\n",
      "[6000]\tvalid_0's auc: 0.900937\n",
      "[7000]\tvalid_0's auc: 0.901498\n",
      "[8000]\tvalid_0's auc: 0.901733\n",
      "[9000]\tvalid_0's auc: 0.902124\n",
      "[10000]\tvalid_0's auc: 0.902078\n",
      "[11000]\tvalid_0's auc: 0.902224\n",
      "[12000]\tvalid_0's auc: 0.902272\n",
      "[13000]\tvalid_0's auc: 0.902217\n",
      "[14000]\tvalid_0's auc: 0.902145\n",
      "[15000]\tvalid_0's auc: 0.902184\n",
      "[16000]\tvalid_0's auc: 0.90219\n",
      "Early stopping, best iteration is:\n",
      "[13363]\tvalid_0's auc: 0.90234\n",
      "Fold 4 started at Sun Mar 10 10:31:50 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.890168\n",
      "[2000]\tvalid_0's auc: 0.898428\n",
      "[3000]\tvalid_0's auc: 0.903155\n",
      "[4000]\tvalid_0's auc: 0.905788\n",
      "[5000]\tvalid_0's auc: 0.907258\n",
      "[6000]\tvalid_0's auc: 0.908131\n",
      "[7000]\tvalid_0's auc: 0.908716\n",
      "[8000]\tvalid_0's auc: 0.909099\n",
      "[9000]\tvalid_0's auc: 0.90923\n",
      "[10000]\tvalid_0's auc: 0.909044\n",
      "[11000]\tvalid_0's auc: 0.909133\n",
      "Early stopping, best iteration is:\n",
      "[8787]\tvalid_0's auc: 0.909284\n",
      "Fold 5 started at Sun Mar 10 11:13:47 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88477\n",
      "[2000]\tvalid_0's auc: 0.892636\n",
      "[3000]\tvalid_0's auc: 0.897541\n",
      "[4000]\tvalid_0's auc: 0.900088\n",
      "[5000]\tvalid_0's auc: 0.90147\n",
      "[6000]\tvalid_0's auc: 0.902187\n",
      "[7000]\tvalid_0's auc: 0.9024\n",
      "[8000]\tvalid_0's auc: 0.90262\n",
      "[9000]\tvalid_0's auc: 0.902677\n",
      "[10000]\tvalid_0's auc: 0.902703\n",
      "[11000]\tvalid_0's auc: 0.902646\n",
      "[12000]\tvalid_0's auc: 0.902754\n",
      "[13000]\tvalid_0's auc: 0.902648\n",
      "[14000]\tvalid_0's auc: 0.902507\n",
      "Early stopping, best iteration is:\n",
      "[11720]\tvalid_0's auc: 0.902771\n",
      "Fold 6 started at Sun Mar 10 12:02:08 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884176\n",
      "[2000]\tvalid_0's auc: 0.892296\n",
      "[3000]\tvalid_0's auc: 0.897139\n",
      "[4000]\tvalid_0's auc: 0.90033\n",
      "[5000]\tvalid_0's auc: 0.901857\n",
      "[6000]\tvalid_0's auc: 0.903003\n",
      "[7000]\tvalid_0's auc: 0.903829\n",
      "[8000]\tvalid_0's auc: 0.90417\n",
      "[9000]\tvalid_0's auc: 0.904222\n",
      "[10000]\tvalid_0's auc: 0.904377\n",
      "[11000]\tvalid_0's auc: 0.904383\n",
      "[12000]\tvalid_0's auc: 0.904348\n",
      "[13000]\tvalid_0's auc: 0.904309\n",
      "Early stopping, best iteration is:\n",
      "[10663]\tvalid_0's auc: 0.904484\n",
      "Fold 7 started at Sun Mar 10 12:50:00 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.876436\n",
      "[2000]\tvalid_0's auc: 0.883856\n",
      "[3000]\tvalid_0's auc: 0.888214\n",
      "[4000]\tvalid_0's auc: 0.890726\n",
      "[5000]\tvalid_0's auc: 0.892335\n",
      "[6000]\tvalid_0's auc: 0.893181\n",
      "[7000]\tvalid_0's auc: 0.893554\n",
      "[8000]\tvalid_0's auc: 0.893815\n",
      "[9000]\tvalid_0's auc: 0.894112\n",
      "[10000]\tvalid_0's auc: 0.894305\n",
      "[11000]\tvalid_0's auc: 0.894485\n",
      "[12000]\tvalid_0's auc: 0.894418\n",
      "[13000]\tvalid_0's auc: 0.894354\n",
      "[14000]\tvalid_0's auc: 0.89445\n",
      "Early stopping, best iteration is:\n",
      "[11624]\tvalid_0's auc: 0.894544\n",
      "Fold 8 started at Sun Mar 10 13:38:10 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.876383\n",
      "[2000]\tvalid_0's auc: 0.885016\n",
      "[3000]\tvalid_0's auc: 0.889384\n",
      "[4000]\tvalid_0's auc: 0.892251\n",
      "[5000]\tvalid_0's auc: 0.894044\n",
      "[6000]\tvalid_0's auc: 0.895058\n",
      "[7000]\tvalid_0's auc: 0.895488\n",
      "[8000]\tvalid_0's auc: 0.89609\n",
      "[9000]\tvalid_0's auc: 0.896156\n",
      "[10000]\tvalid_0's auc: 0.896072\n",
      "[11000]\tvalid_0's auc: 0.895977\n",
      "Early stopping, best iteration is:\n",
      "[8817]\tvalid_0's auc: 0.896213\n",
      "Fold 9 started at Sun Mar 10 14:21:12 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.880036\n",
      "[2000]\tvalid_0's auc: 0.887359\n",
      "[3000]\tvalid_0's auc: 0.89181\n",
      "[4000]\tvalid_0's auc: 0.894411\n",
      "[5000]\tvalid_0's auc: 0.895521\n",
      "[6000]\tvalid_0's auc: 0.896117\n",
      "[7000]\tvalid_0's auc: 0.89648\n",
      "[8000]\tvalid_0's auc: 0.89653\n",
      "[9000]\tvalid_0's auc: 0.896493\n",
      "[10000]\tvalid_0's auc: 0.896439\n",
      "[11000]\tvalid_0's auc: 0.896322\n",
      "[12000]\tvalid_0's auc: 0.896166\n",
      "[13000]\tvalid_0's auc: 0.896022\n",
      "Early stopping, best iteration is:\n",
      "[10279]\tvalid_0's auc: 0.896603\n",
      "CV mean score: 0.9005, std: 0.0042.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_var12_91_inter_1_10_2', oof)\n",
    "np.save('../cache/preds_new_quant_var12_91_inter_1_10_2', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 958), (200000, 958))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "\n",
    "bins = np.linspace(-6.0,6.0,40)\n",
    "\n",
    "X3_12_bins = pd.get_dummies(pd.cut(X3['var_12'].values, bins))\n",
    "X4_12_bins = pd.get_dummies(pd.cut(X4['var_12'].values, bins))\n",
    "X3['var_12'] = pd.cut(X3['var_12'].values, bins)\n",
    "X4['var_12'] = pd.cut(X4['var_12'].values, bins)\n",
    "\n",
    "bins = np.linspace(-6.0,6.0,50)\n",
    "\n",
    "X3_91_bins = pd.get_dummies(pd.cut(X3['var_91'].values, bins))\n",
    "X4_91_bins = pd.get_dummies(pd.cut(X4['var_91'].values, bins))\n",
    "X3['var_91'] = pd.cut(X3['var_91'].values, bins)\n",
    "X4['var_91'] = pd.cut(X4['var_91'].values, bins)\n",
    "\n",
    "X3_12_bins = pd.DataFrame(X3_12_bins)\n",
    "cols = ['var_12_' + str(i) for i in range(X3_12_bins.shape[1])]\n",
    "X3_12_bins.columns = cols\n",
    "\n",
    "X4_12_bins = pd.DataFrame(X4_12_bins)\n",
    "X4_12_bins.columns = cols\n",
    "\n",
    "X3_91_bins = pd.DataFrame(X3_91_bins)\n",
    "cols = ['var_91_' + str(i) for i in range(X3_91_bins.shape[1])]\n",
    "X3_91_bins.columns = cols\n",
    "\n",
    "X4_91_bins = pd.DataFrame(X4_91_bins)\n",
    "X4_91_bins.columns = cols\n",
    "\n",
    "X3 = pd.concat([X3, X3_12_bins], axis=1)\n",
    "X3 = pd.concat([X3, X3_91_bins], axis=1)\n",
    "\n",
    "X4 = pd.concat([X4, X4_12_bins], axis=1)\n",
    "X4 = pd.concat([X4, X4_91_bins], axis=1)\n",
    "\n",
    "data = pd.concat([X3, X4], axis=0)\n",
    "data['var_12_var_91'] = data['var_12'].astype('str') + '_' + data['var_91'].astype('str')\n",
    "for col in ['var_12_var_91']:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = data[col].astype('str')\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "data_12_91_bins = pd.get_dummies(data['var_12_var_91'])\n",
    "data_12_91_bins = pd.DataFrame(data_12_91_bins)\n",
    "cols = ['var_12_var_91_' + str(i) for i in range(data_12_91_bins.shape[1])]\n",
    "data_12_91_bins.columns = cols\n",
    "\n",
    "data = pd.concat([data, data_12_91_bins], axis=1)\n",
    "\n",
    "data = data.drop(['var_12', 'var_91', 'var_12_var_91'], axis=1)\n",
    "\n",
    "X3 = data[:len(X3)]\n",
    "X4 = data[len(X3):]\n",
    "\n",
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sun Mar 10 15:38:16 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884072\n",
      "[2000]\tvalid_0's auc: 0.891063\n",
      "[3000]\tvalid_0's auc: 0.894816\n",
      "[4000]\tvalid_0's auc: 0.897656\n",
      "[5000]\tvalid_0's auc: 0.89913\n",
      "[6000]\tvalid_0's auc: 0.899912\n",
      "[7000]\tvalid_0's auc: 0.900428\n",
      "[8000]\tvalid_0's auc: 0.900595\n",
      "[9000]\tvalid_0's auc: 0.900793\n",
      "[10000]\tvalid_0's auc: 0.900874\n",
      "[11000]\tvalid_0's auc: 0.900896\n",
      "[12000]\tvalid_0's auc: 0.900971\n",
      "[13000]\tvalid_0's auc: 0.900892\n",
      "[14000]\tvalid_0's auc: 0.90112\n",
      "[15000]\tvalid_0's auc: 0.900903\n",
      "[16000]\tvalid_0's auc: 0.900654\n",
      "[17000]\tvalid_0's auc: 0.900681\n",
      "Early stopping, best iteration is:\n",
      "[14010]\tvalid_0's auc: 0.901147\n",
      "Fold 1 started at Sun Mar 10 15:42:01 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88047\n",
      "[2000]\tvalid_0's auc: 0.888522\n",
      "[3000]\tvalid_0's auc: 0.891952\n",
      "[4000]\tvalid_0's auc: 0.894706\n",
      "[5000]\tvalid_0's auc: 0.896175\n",
      "[6000]\tvalid_0's auc: 0.896949\n",
      "[7000]\tvalid_0's auc: 0.897331\n",
      "[8000]\tvalid_0's auc: 0.897641\n",
      "[9000]\tvalid_0's auc: 0.897868\n",
      "[10000]\tvalid_0's auc: 0.898086\n",
      "[11000]\tvalid_0's auc: 0.897809\n",
      "[12000]\tvalid_0's auc: 0.897795\n",
      "Early stopping, best iteration is:\n",
      "[9883]\tvalid_0's auc: 0.898089\n",
      "Fold 2 started at Sun Mar 10 16:28:23 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88429\n",
      "[2000]\tvalid_0's auc: 0.890769\n",
      "[3000]\tvalid_0's auc: 0.894624\n",
      "[4000]\tvalid_0's auc: 0.896553\n",
      "[5000]\tvalid_0's auc: 0.897648\n",
      "[6000]\tvalid_0's auc: 0.898362\n",
      "[7000]\tvalid_0's auc: 0.898544\n",
      "[8000]\tvalid_0's auc: 0.898835\n",
      "[9000]\tvalid_0's auc: 0.898926\n",
      "[10000]\tvalid_0's auc: 0.898938\n",
      "[11000]\tvalid_0's auc: 0.898791\n",
      "[12000]\tvalid_0's auc: 0.898711\n",
      "Early stopping, best iteration is:\n",
      "[9107]\tvalid_0's auc: 0.899021\n",
      "Fold 3 started at Sun Mar 10 17:15:51 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884128\n",
      "[2000]\tvalid_0's auc: 0.892117\n",
      "[3000]\tvalid_0's auc: 0.895822\n",
      "[4000]\tvalid_0's auc: 0.898561\n",
      "[5000]\tvalid_0's auc: 0.899967\n",
      "[6000]\tvalid_0's auc: 0.900604\n",
      "[7000]\tvalid_0's auc: 0.900919\n",
      "[8000]\tvalid_0's auc: 0.901039\n",
      "[9000]\tvalid_0's auc: 0.901101\n",
      "[10000]\tvalid_0's auc: 0.901259\n",
      "[11000]\tvalid_0's auc: 0.90123\n",
      "[12000]\tvalid_0's auc: 0.901379\n",
      "[13000]\tvalid_0's auc: 0.901162\n",
      "[14000]\tvalid_0's auc: 0.901312\n",
      "[15000]\tvalid_0's auc: 0.901434\n",
      "[16000]\tvalid_0's auc: 0.901523\n",
      "[17000]\tvalid_0's auc: 0.901605\n",
      "[18000]\tvalid_0's auc: 0.901525\n",
      "[19000]\tvalid_0's auc: 0.90135\n",
      "Early stopping, best iteration is:\n",
      "[16669]\tvalid_0's auc: 0.90168\n",
      "Fold 4 started at Sun Mar 10 18:28:25 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.888722\n",
      "[2000]\tvalid_0's auc: 0.897124\n",
      "[3000]\tvalid_0's auc: 0.901351\n",
      "[4000]\tvalid_0's auc: 0.904445\n",
      "[5000]\tvalid_0's auc: 0.906402\n",
      "[6000]\tvalid_0's auc: 0.907219\n",
      "[7000]\tvalid_0's auc: 0.907745\n",
      "[8000]\tvalid_0's auc: 0.908334\n",
      "[9000]\tvalid_0's auc: 0.908559\n",
      "[10000]\tvalid_0's auc: 0.908765\n",
      "[11000]\tvalid_0's auc: 0.908652\n",
      "[12000]\tvalid_0's auc: 0.90841\n",
      "Early stopping, best iteration is:\n",
      "[9567]\tvalid_0's auc: 0.908795\n",
      "Fold 5 started at Sun Mar 10 19:13:41 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885098\n",
      "[2000]\tvalid_0's auc: 0.893191\n",
      "[3000]\tvalid_0's auc: 0.897656\n",
      "[4000]\tvalid_0's auc: 0.900236\n",
      "[5000]\tvalid_0's auc: 0.902079\n",
      "[6000]\tvalid_0's auc: 0.902787\n",
      "[7000]\tvalid_0's auc: 0.903254\n",
      "[8000]\tvalid_0's auc: 0.903554\n",
      "[9000]\tvalid_0's auc: 0.903495\n",
      "[10000]\tvalid_0's auc: 0.903543\n",
      "[11000]\tvalid_0's auc: 0.903441\n",
      "[12000]\tvalid_0's auc: 0.903248\n",
      "Early stopping, best iteration is:\n",
      "[9524]\tvalid_0's auc: 0.903637\n",
      "Fold 6 started at Sun Mar 10 19:59:08 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885108\n",
      "[2000]\tvalid_0's auc: 0.893741\n",
      "[3000]\tvalid_0's auc: 0.897799\n",
      "[4000]\tvalid_0's auc: 0.900472\n",
      "[5000]\tvalid_0's auc: 0.902358\n",
      "[6000]\tvalid_0's auc: 0.903441\n",
      "[7000]\tvalid_0's auc: 0.903945\n",
      "[8000]\tvalid_0's auc: 0.903943\n",
      "[9000]\tvalid_0's auc: 0.904024\n",
      "[10000]\tvalid_0's auc: 0.903928\n",
      "[11000]\tvalid_0's auc: 0.903979\n",
      "[12000]\tvalid_0's auc: 0.903951\n",
      "[13000]\tvalid_0's auc: 0.90389\n",
      "Early stopping, best iteration is:\n",
      "[10385]\tvalid_0's auc: 0.904063\n",
      "Fold 7 started at Sun Mar 10 20:47:54 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.874142\n",
      "[2000]\tvalid_0's auc: 0.883137\n",
      "[3000]\tvalid_0's auc: 0.887604\n",
      "[4000]\tvalid_0's auc: 0.890498\n",
      "[5000]\tvalid_0's auc: 0.891903\n",
      "[6000]\tvalid_0's auc: 0.892808\n",
      "[7000]\tvalid_0's auc: 0.893637\n",
      "[8000]\tvalid_0's auc: 0.893837\n",
      "[9000]\tvalid_0's auc: 0.894294\n",
      "[10000]\tvalid_0's auc: 0.894557\n",
      "[11000]\tvalid_0's auc: 0.894607\n",
      "[12000]\tvalid_0's auc: 0.894482\n",
      "[13000]\tvalid_0's auc: 0.894413\n",
      "Early stopping, best iteration is:\n",
      "[10626]\tvalid_0's auc: 0.894711\n",
      "Fold 8 started at Sun Mar 10 21:38:28 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.877203\n",
      "[2000]\tvalid_0's auc: 0.885001\n",
      "[3000]\tvalid_0's auc: 0.889147\n",
      "[4000]\tvalid_0's auc: 0.891698\n",
      "[5000]\tvalid_0's auc: 0.893341\n",
      "[6000]\tvalid_0's auc: 0.894378\n",
      "[7000]\tvalid_0's auc: 0.895096\n",
      "[8000]\tvalid_0's auc: 0.895607\n",
      "[9000]\tvalid_0's auc: 0.895776\n",
      "[10000]\tvalid_0's auc: 0.89592\n",
      "[11000]\tvalid_0's auc: 0.89595\n",
      "[12000]\tvalid_0's auc: 0.895817\n",
      "[13000]\tvalid_0's auc: 0.895758\n",
      "Early stopping, best iteration is:\n",
      "[10555]\tvalid_0's auc: 0.896085\n",
      "Fold 9 started at Sun Mar 10 22:30:39 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.87782\n",
      "[2000]\tvalid_0's auc: 0.886236\n",
      "[3000]\tvalid_0's auc: 0.890924\n",
      "[4000]\tvalid_0's auc: 0.89351\n",
      "[5000]\tvalid_0's auc: 0.895048\n",
      "[6000]\tvalid_0's auc: 0.895684\n",
      "[7000]\tvalid_0's auc: 0.896206\n",
      "[8000]\tvalid_0's auc: 0.896439\n",
      "[9000]\tvalid_0's auc: 0.896487\n",
      "[10000]\tvalid_0's auc: 0.896599\n",
      "[11000]\tvalid_0's auc: 0.896646\n",
      "[12000]\tvalid_0's auc: 0.896587\n",
      "[13000]\tvalid_0's auc: 0.896389\n",
      "[14000]\tvalid_0's auc: 0.896372\n",
      "Early stopping, best iteration is:\n",
      "[11076]\tvalid_0's auc: 0.896687\n",
      "CV mean score: 0.9004, std: 0.0041.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_var12_91_inter_1_10_3', oof)\n",
    "np.save('../cache/preds_new_quant_var12_91_inter_1_10_3', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lgb      lgb2\n",
      "0  0.011048  0.011295\n",
      "1  0.469107  0.452181\n",
      "2  0.004675  0.004297\n",
      "3  0.262949  0.221609\n",
      "4  0.093573  0.101964\n",
      "0.92514\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_new_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_new_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb\n",
    "stage2_test['lgb'] = preds_lgb\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_var12_91_inter_1_10_2.npy')\n",
    "preds = np.load('../cache/preds_new_quant_var12_91_inter_1_10_2.npy')\n",
    "stage2['lgb2'] = oof[0]\n",
    "stage2_test['lgb2'] = preds[0]\n",
    "\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = prediction_lgb\n",
    "# sub.to_csv('../submissions/sub12q.csv', index=False)\n",
    "sub.to_csv('../submissions/sub12r.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Mar 11 04:13:58 2019\n",
      "[0]\tvalidation_0-auc:0.57416\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.88952\n",
      "[2000]\tvalidation_0-auc:0.893355\n",
      "[3000]\tvalidation_0-auc:0.89473\n",
      "[4000]\tvalidation_0-auc:0.894854\n",
      "[5000]\tvalidation_0-auc:0.895338\n",
      "[6000]\tvalidation_0-auc:0.895997\n",
      "[7000]\tvalidation_0-auc:0.896114\n",
      "[8000]\tvalidation_0-auc:0.896375\n",
      "[9000]\tvalidation_0-auc:0.89631\n",
      "[10000]\tvalidation_0-auc:0.89646\n",
      "[11000]\tvalidation_0-auc:0.896569\n",
      "[12000]\tvalidation_0-auc:0.896601\n",
      "[13000]\tvalidation_0-auc:0.896477\n",
      "[14000]\tvalidation_0-auc:0.896399\n",
      "Stopping. Best iteration:\n",
      "[11678]\tvalidation_0-auc:0.896645\n",
      "\n",
      "Fold 1 started at Mon Mar 11 05:53:25 2019\n",
      "[0]\tvalidation_0-auc:0.559198\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.887124\n",
      "[2000]\tvalidation_0-auc:0.892149\n",
      "[3000]\tvalidation_0-auc:0.893067\n",
      "[4000]\tvalidation_0-auc:0.89432\n",
      "[5000]\tvalidation_0-auc:0.894742\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-286-ed7f302a9c0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moof_xgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_xgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xgb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_feature_importance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof_xgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_xgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-283-ffc81e1a994c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, averaging, model)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 early_stopping_rounds=3000)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    698\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1045\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_xgb, prediction_xgb, _ = train_model(X1, X2, y, params=np.random.randint(1,101), folds=folds, model_type='xgb', plot_feature_importance=False)\n",
    "oof.append(oof_xgb)\n",
    "preds.append(prediction_xgb)\n",
    "max_depth=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Mar 11 06:31:24 2019\n",
      "[0]\tvalidation_0-auc:0.581947\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.887187\n",
      "[2000]\tvalidation_0-auc:0.894534\n",
      "[3000]\tvalidation_0-auc:0.898192\n",
      "[4000]\tvalidation_0-auc:0.89913\n",
      "[5000]\tvalidation_0-auc:0.899868\n",
      "[6000]\tvalidation_0-auc:0.900172\n",
      "[7000]\tvalidation_0-auc:0.900063\n",
      "[8000]\tvalidation_0-auc:0.900131\n",
      "[9000]\tvalidation_0-auc:0.90009\n",
      "[10000]\tvalidation_0-auc:0.900096\n",
      "[11000]\tvalidation_0-auc:0.899807\n",
      "Stopping. Best iteration:\n",
      "[8519]\tvalidation_0-auc:0.900277\n",
      "\n",
      "Fold 1 started at Mon Mar 11 07:07:43 2019\n",
      "[0]\tvalidation_0-auc:0.572054\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.885368\n",
      "[2000]\tvalidation_0-auc:0.892807\n",
      "[3000]\tvalidation_0-auc:0.89583\n",
      "[4000]\tvalidation_0-auc:0.897314\n",
      "[5000]\tvalidation_0-auc:0.897849\n",
      "[6000]\tvalidation_0-auc:0.89816\n",
      "[7000]\tvalidation_0-auc:0.898226\n",
      "[8000]\tvalidation_0-auc:0.898104\n",
      "[9000]\tvalidation_0-auc:0.898244\n",
      "[10000]\tvalidation_0-auc:0.8982\n",
      "[11000]\tvalidation_0-auc:0.898132\n",
      "Stopping. Best iteration:\n",
      "[8538]\tvalidation_0-auc:0.898455\n",
      "\n",
      "Fold 2 started at Mon Mar 11 07:44:47 2019\n",
      "[0]\tvalidation_0-auc:0.598637\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.890135\n",
      "[2000]\tvalidation_0-auc:0.895508\n",
      "[3000]\tvalidation_0-auc:0.897495\n",
      "[4000]\tvalidation_0-auc:0.898013\n",
      "[5000]\tvalidation_0-auc:0.8981\n",
      "[6000]\tvalidation_0-auc:0.89868\n",
      "[7000]\tvalidation_0-auc:0.898484\n",
      "[8000]\tvalidation_0-auc:0.898548\n",
      "[9000]\tvalidation_0-auc:0.898319\n",
      "[10000]\tvalidation_0-auc:0.898071\n",
      "Stopping. Best iteration:\n",
      "[7523]\tvalidation_0-auc:0.898686\n",
      "\n",
      "Fold 3 started at Mon Mar 11 08:17:56 2019\n",
      "[0]\tvalidation_0-auc:0.603394\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.890518\n",
      "[2000]\tvalidation_0-auc:0.897002\n",
      "[3000]\tvalidation_0-auc:0.899381\n",
      "[4000]\tvalidation_0-auc:0.900759\n",
      "[5000]\tvalidation_0-auc:0.90143\n",
      "[6000]\tvalidation_0-auc:0.901711\n",
      "[7000]\tvalidation_0-auc:0.901292\n",
      "[8000]\tvalidation_0-auc:0.90131\n",
      "Stopping. Best iteration:\n",
      "[5982]\tvalidation_0-auc:0.901758\n",
      "\n",
      "Fold 4 started at Mon Mar 11 08:46:29 2019\n",
      "[0]\tvalidation_0-auc:0.605081\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.895868\n",
      "[2000]\tvalidation_0-auc:0.903888\n",
      "[3000]\tvalidation_0-auc:0.906825\n",
      "[4000]\tvalidation_0-auc:0.908146\n",
      "[5000]\tvalidation_0-auc:0.908747\n",
      "[6000]\tvalidation_0-auc:0.909309\n",
      "[7000]\tvalidation_0-auc:0.909281\n",
      "[8000]\tvalidation_0-auc:0.909395\n",
      "[9000]\tvalidation_0-auc:0.9093\n",
      "[10000]\tvalidation_0-auc:0.909185\n",
      "[11000]\tvalidation_0-auc:0.909169\n",
      "Stopping. Best iteration:\n",
      "[8068]\tvalidation_0-auc:0.909417\n",
      "\n",
      "Fold 5 started at Mon Mar 11 09:20:50 2019\n",
      "[0]\tvalidation_0-auc:0.610918\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.889678\n",
      "[2000]\tvalidation_0-auc:0.897392\n",
      "[3000]\tvalidation_0-auc:0.900027\n",
      "[4000]\tvalidation_0-auc:0.901454\n",
      "[5000]\tvalidation_0-auc:0.902343\n",
      "[6000]\tvalidation_0-auc:0.902544\n",
      "[7000]\tvalidation_0-auc:0.902323\n",
      "[8000]\tvalidation_0-auc:0.902548\n",
      "[9000]\tvalidation_0-auc:0.902317\n",
      "Stopping. Best iteration:\n",
      "[6157]\tvalidation_0-auc:0.902679\n",
      "\n",
      "Fold 6 started at Mon Mar 11 09:49:40 2019\n",
      "[0]\tvalidation_0-auc:0.604602\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.891646\n",
      "[2000]\tvalidation_0-auc:0.898184\n",
      "[3000]\tvalidation_0-auc:0.901027\n",
      "[4000]\tvalidation_0-auc:0.902584\n",
      "[5000]\tvalidation_0-auc:0.903362\n",
      "[6000]\tvalidation_0-auc:0.903907\n",
      "[7000]\tvalidation_0-auc:0.903682\n",
      "[8000]\tvalidation_0-auc:0.903635\n",
      "[9000]\tvalidation_0-auc:0.903455\n",
      "Stopping. Best iteration:\n",
      "[6341]\tvalidation_0-auc:0.904042\n",
      "\n",
      "Fold 7 started at Mon Mar 11 10:19:03 2019\n",
      "[0]\tvalidation_0-auc:0.609295\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.881373\n",
      "[2000]\tvalidation_0-auc:0.888895\n",
      "[3000]\tvalidation_0-auc:0.891697\n",
      "[4000]\tvalidation_0-auc:0.892723\n",
      "[5000]\tvalidation_0-auc:0.893155\n",
      "[6000]\tvalidation_0-auc:0.893352\n",
      "[7000]\tvalidation_0-auc:0.893575\n",
      "[8000]\tvalidation_0-auc:0.89373\n",
      "[9000]\tvalidation_0-auc:0.893899\n",
      "[10000]\tvalidation_0-auc:0.894051\n",
      "[11000]\tvalidation_0-auc:0.893809\n",
      "[12000]\tvalidation_0-auc:0.893696\n",
      "Stopping. Best iteration:\n",
      "[9487]\tvalidation_0-auc:0.894118\n",
      "\n",
      "Fold 8 started at Mon Mar 11 10:59:28 2019\n",
      "[0]\tvalidation_0-auc:0.555243\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.883673\n",
      "[2000]\tvalidation_0-auc:0.889673\n",
      "[3000]\tvalidation_0-auc:0.892507\n",
      "[4000]\tvalidation_0-auc:0.894484\n",
      "[5000]\tvalidation_0-auc:0.895022\n",
      "[6000]\tvalidation_0-auc:0.895129\n",
      "[7000]\tvalidation_0-auc:0.895203\n",
      "[8000]\tvalidation_0-auc:0.894973\n",
      "[9000]\tvalidation_0-auc:0.89481\n",
      "Stopping. Best iteration:\n",
      "[6713]\tvalidation_0-auc:0.895277\n",
      "\n",
      "Fold 9 started at Mon Mar 11 11:30:30 2019\n",
      "[0]\tvalidation_0-auc:0.557933\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.884874\n",
      "[2000]\tvalidation_0-auc:0.892272\n",
      "[3000]\tvalidation_0-auc:0.89547\n",
      "[4000]\tvalidation_0-auc:0.896799\n",
      "[5000]\tvalidation_0-auc:0.897058\n",
      "[6000]\tvalidation_0-auc:0.897139\n",
      "[7000]\tvalidation_0-auc:0.896722\n",
      "Stopping. Best iteration:\n",
      "[4861]\tvalidation_0-auc:0.897189\n",
      "\n",
      "CV mean score: 0.9002, std: 0.0043.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_xgb, prediction_xgb, _ = train_model(X1, X2, y, params=np.random.randint(1,101), folds=folds, model_type='xgb', plot_feature_importance=False)\n",
    "oof.append(oof_xgb)\n",
    "preds.append(prediction_xgb)\n",
    "# max_depth=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_xgb_inter_1_10_1', oof)\n",
    "np.save('../cache/preds_new_quant_xgb_1_10_1', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lgb       xgb      lgb2\n",
      "0  0.011048  0.011639  0.012400\n",
      "1  0.469107  0.360481  0.542452\n",
      "2  0.004675  0.004766  0.003715\n",
      "3  0.262949  0.227772  0.239456\n",
      "4  0.093573  0.111758  0.077072\n",
      "0.925275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_new_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_new_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb\n",
    "stage2_test['lgb'] = preds_lgb\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_xgb_inter_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_new_quant_xgb_1_10_1.npy')\n",
    "stage2['xgb'] = oof[0]\n",
    "stage2_test['xgb'] = preds[0]\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_2bins_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_new_quant_2bins_1_10_1.npy')\n",
    "stage2['lgb2'] = oof[0]\n",
    "stage2_test['lgb2'] = preds[0]\n",
    "\n",
    "print(stage2.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12s.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lgb       xgb       cat      lgb2\n",
      "0  0.011048  0.011639  0.010545  0.012400\n",
      "1  0.469107  0.360481  0.455996  0.542452\n",
      "2  0.004675  0.004766  0.004485  0.003715\n",
      "3  0.262949  0.227772  0.243523  0.239456\n",
      "4  0.093573  0.111758  0.097492  0.077072\n",
      "------------\n",
      "        lgb       xgb       cat      lgb2\n",
      "0  0.104744  0.088794  0.088812  0.094076\n",
      "1  0.204771  0.200059  0.202222  0.211254\n",
      "2  0.167309  0.155445  0.212390  0.173490\n",
      "3  0.212251  0.192163  0.182568  0.221021\n",
      "4  0.040589  0.042701  0.040704  0.042458\n",
      "0.92528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_new_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_new_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb\n",
    "stage2_test['lgb'] = preds_lgb\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_xgb_inter_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_new_quant_xgb_1_10_1.npy')\n",
    "stage2['xgb'] = oof[0]\n",
    "stage2_test['xgb'] = preds[0]\n",
    "\n",
    "oof = np.load('../cache/oof_cat_quant_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_cat_quant_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_2bins_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_new_quant_2bins_1_10_1.npy')\n",
    "stage2['lgb2'] = oof[0]\n",
    "stage2_test['lgb2'] = preds[0]\n",
    "\n",
    "print(stage2.head())\n",
    "print('------------')\n",
    "print(stage2_test.head())\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12t.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Mar 11 15:18:28 2019\n",
      "0:\ttest: 0.5510707\tbest: 0.5510707 (0)\ttotal: 236ms\tremaining: 2d 17h 26m 29s\n",
      "1000:\ttest: 0.8778151\tbest: 0.8778151 (1000)\ttotal: 3m 44s\tremaining: 2d 14h 15m 37s\n",
      "2000:\ttest: 0.8889260\tbest: 0.8889263 (1999)\ttotal: 7m 37s\tremaining: 2d 15h 18m 52s\n",
      "3000:\ttest: 0.8923433\tbest: 0.8923906 (2985)\ttotal: 11m 34s\tremaining: 2d 16h 4m 33s\n",
      "4000:\ttest: 0.8940555\tbest: 0.8940954 (3968)\ttotal: 15m 40s\tremaining: 2d 17h 4m 3s\n",
      "5000:\ttest: 0.8945775\tbest: 0.8946647 (4931)\ttotal: 19m 38s\tremaining: 2d 17h 7m 34s\n",
      "6000:\ttest: 0.8953281\tbest: 0.8953406 (5996)\ttotal: 23m 38s\tremaining: 2d 17h 15m 44s\n",
      "7000:\ttest: 0.8957099\tbest: 0.8957245 (6975)\ttotal: 27m 37s\tremaining: 2d 17h 18m 9s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-319-b78fc70de7e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moof_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_feature_importance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-318-555b75e3bf42>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, averaging, model)\u001b[0m\n\u001b[1;32m    165\u001b[0m                                   objective=\"Logloss\")\n\u001b[1;32m    166\u001b[0m             model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], \n\u001b[0;32m--> 167\u001b[0;31m                       use_best_model=True, verbose=1000,early_stopping_rounds=1000)\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0my_pred_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval)\u001b[0m\n\u001b[1;32m   2181\u001b[0m         self._fit(X, y, cat_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[1;32m   2182\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2183\u001b[0;31m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval)\n\u001b[0m\u001b[1;32m   2184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_leaf_weights_in_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_cat, prediction_cat, _ = train_model(X1, X2, y, params=np.random.randint(1,101), folds=folds, model_type='cat', plot_feature_importance=False)\n",
    "oof.append(oof_cat)\n",
    "preds.append(prediction_cat)\n",
    "\n",
    "#max-depth = 12\n",
    "# Fold 0 started at Mon Mar 11 15:18:28 2019\n",
    "# 0:\ttest: 0.5510707\tbest: 0.5510707 (0)\ttotal: 236ms\tremaining: 2d 17h 26m 29s\n",
    "# 1000:\ttest: 0.8778151\tbest: 0.8778151 (1000)\ttotal: 3m 44s\tremaining: 2d 14h 15m 37s\n",
    "# 2000:\ttest: 0.8889260\tbest: 0.8889263 (1999)\ttotal: 7m 37s\tremaining: 2d 15h 18m 52s\n",
    "# 3000:\ttest: 0.8923433\tbest: 0.8923906 (2985)\ttotal: 11m 34s\tremaining: 2d 16h 4m 33s\n",
    "# 4000:\ttest: 0.8940555\tbest: 0.8940954 (3968)\ttotal: 15m 40s\tremaining: 2d 17h 4m 3s\n",
    "# 5000:\ttest: 0.8945775\tbest: 0.8946647 (4931)\ttotal: 19m 38s\tremaining: 2d 17h 7m 34s\n",
    "# 6000:\ttest: 0.8953281\tbest: 0.8953406 (5996)\ttotal: 23m 38s\tremaining: 2d 17h 15m 44s\n",
    "# 7000:\ttest: 0.8957099\tbest: 0.8957245 (6975)\ttotal: 27m 37s\tremaining: 2d 17h 18m 9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Mar 11 16:06:59 2019\n",
      "0:\ttest: 0.5511600\tbest: 0.5511600 (0)\ttotal: 86ms\tremaining: 23h 53m 39s\n",
      "1000:\ttest: 0.8648258\tbest: 0.8648334 (999)\ttotal: 43.4s\tremaining: 12h 1m 4s\n",
      "2000:\ttest: 0.8814357\tbest: 0.8814357 (2000)\ttotal: 1m 26s\tremaining: 12h 28s\n",
      "3000:\ttest: 0.8890985\tbest: 0.8890985 (3000)\ttotal: 2m 9s\tremaining: 11h 56m 49s\n",
      "4000:\ttest: 0.8933395\tbest: 0.8933395 (4000)\ttotal: 2m 51s\tremaining: 11h 51m 29s\n",
      "5000:\ttest: 0.8958567\tbest: 0.8958567 (5000)\ttotal: 3m 34s\tremaining: 11h 51m 31s\n",
      "6000:\ttest: 0.8971586\tbest: 0.8971620 (5999)\ttotal: 4m 17s\tremaining: 11h 51m 47s\n",
      "7000:\ttest: 0.8981640\tbest: 0.8981654 (6991)\ttotal: 5m 5s\tremaining: 12h 1m 42s\n",
      "8000:\ttest: 0.8988964\tbest: 0.8988976 (7999)\ttotal: 5m 54s\tremaining: 12h 12m 12s\n",
      "9000:\ttest: 0.8993959\tbest: 0.8994039 (8999)\ttotal: 6m 45s\tremaining: 12h 23m 14s\n",
      "10000:\ttest: 0.8997430\tbest: 0.8997517 (9980)\ttotal: 7m 34s\tremaining: 12h 30m 3s\n",
      "11000:\ttest: 0.9000645\tbest: 0.9000748 (10941)\ttotal: 8m 23s\tremaining: 12h 34m 28s\n",
      "12000:\ttest: 0.9001558\tbest: 0.9001812 (11784)\ttotal: 9m 13s\tremaining: 12h 39m 40s\n",
      "13000:\ttest: 0.9003126\tbest: 0.9003152 (12999)\ttotal: 10m 3s\tremaining: 12h 43m 22s\n",
      "14000:\ttest: 0.9004369\tbest: 0.9004715 (13655)\ttotal: 10m 53s\tremaining: 12h 46m 38s\n",
      "15000:\ttest: 0.9005149\tbest: 0.9005176 (14998)\ttotal: 11m 41s\tremaining: 12h 48m 9s\n",
      "16000:\ttest: 0.9006025\tbest: 0.9006210 (15878)\ttotal: 12m 31s\tremaining: 12h 50m 42s\n",
      "17000:\ttest: 0.9006369\tbest: 0.9006842 (16500)\ttotal: 13m 22s\tremaining: 12h 52m 58s\n",
      "18000:\ttest: 0.9006863\tbest: 0.9006898 (17941)\ttotal: 14m 14s\tremaining: 12h 56m 49s\n",
      "19000:\ttest: 0.9006614\tbest: 0.9006915 (18719)\ttotal: 15m 6s\tremaining: 13h 3s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9006915287\n",
      "bestIteration = 18719\n",
      "\n",
      "Shrink model to first 18720 iterations.\n",
      "Fold 1 started at Mon Mar 11 16:24:21 2019\n",
      "0:\ttest: 0.5576158\tbest: 0.5576158 (0)\ttotal: 98.5ms\tremaining: 1d 3h 21m 9s\n",
      "1000:\ttest: 0.8588067\tbest: 0.8588067 (1000)\ttotal: 50.7s\tremaining: 14h 4m 2s\n",
      "2000:\ttest: 0.8767846\tbest: 0.8767846 (2000)\ttotal: 1m 43s\tremaining: 14h 17m 21s\n",
      "3000:\ttest: 0.8852686\tbest: 0.8852686 (3000)\ttotal: 2m 36s\tremaining: 14h 26m 41s\n",
      "4000:\ttest: 0.8897502\tbest: 0.8897502 (4000)\ttotal: 3m 29s\tremaining: 14h 29m 29s\n",
      "5000:\ttest: 0.8926370\tbest: 0.8926370 (5000)\ttotal: 4m 23s\tremaining: 14h 34m 13s\n",
      "6000:\ttest: 0.8942558\tbest: 0.8942558 (6000)\ttotal: 5m 17s\tremaining: 14h 36m 52s\n",
      "7000:\ttest: 0.8953322\tbest: 0.8953358 (6997)\ttotal: 6m 14s\tremaining: 14h 44m 27s\n",
      "8000:\ttest: 0.8961639\tbest: 0.8961644 (7998)\ttotal: 7m 10s\tremaining: 14h 48m 39s\n",
      "9000:\ttest: 0.8966141\tbest: 0.8966183 (8998)\ttotal: 8m 4s\tremaining: 14h 49m 31s\n",
      "10000:\ttest: 0.8969160\tbest: 0.8969160 (10000)\ttotal: 8m 58s\tremaining: 14h 48m 55s\n",
      "11000:\ttest: 0.8971582\tbest: 0.8971657 (10959)\ttotal: 9m 52s\tremaining: 14h 47m 24s\n",
      "12000:\ttest: 0.8972975\tbest: 0.8973094 (11901)\ttotal: 10m 44s\tremaining: 14h 44m 34s\n",
      "13000:\ttest: 0.8975015\tbest: 0.8975052 (12999)\ttotal: 11m 36s\tremaining: 14h 41m 27s\n",
      "14000:\ttest: 0.8976337\tbest: 0.8976482 (13966)\ttotal: 12m 28s\tremaining: 14h 38m 8s\n",
      "15000:\ttest: 0.8977816\tbest: 0.8977816 (15000)\ttotal: 13m 21s\tremaining: 14h 37m 2s\n",
      "16000:\ttest: 0.8979365\tbest: 0.8979406 (15997)\ttotal: 14m 13s\tremaining: 14h 34m 25s\n",
      "17000:\ttest: 0.8979762\tbest: 0.8979982 (16607)\ttotal: 15m 6s\tremaining: 14h 33m 17s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8979981976\n",
      "bestIteration = 16607\n",
      "\n",
      "Shrink model to first 16608 iterations.\n",
      "Fold 2 started at Mon Mar 11 16:41:18 2019\n",
      "0:\ttest: 0.5581902\tbest: 0.5581902 (0)\ttotal: 87.9ms\tremaining: 1d 24m 34s\n",
      "1000:\ttest: 0.8634782\tbest: 0.8634925 (999)\ttotal: 50.8s\tremaining: 14h 4m 48s\n",
      "2000:\ttest: 0.8812616\tbest: 0.8812616 (2000)\ttotal: 1m 44s\tremaining: 14h 31m 39s\n",
      "3000:\ttest: 0.8888415\tbest: 0.8888415 (3000)\ttotal: 2m 37s\tremaining: 14h 33m 16s\n",
      "4000:\ttest: 0.8928108\tbest: 0.8928108 (3999)\ttotal: 3m 31s\tremaining: 14h 36m 9s\n",
      "5000:\ttest: 0.8951397\tbest: 0.8951397 (5000)\ttotal: 4m 24s\tremaining: 14h 35m 46s\n",
      "6000:\ttest: 0.8965088\tbest: 0.8965095 (5999)\ttotal: 5m 17s\tremaining: 14h 36m 26s\n",
      "7000:\ttest: 0.8973804\tbest: 0.8973818 (6999)\ttotal: 6m 10s\tremaining: 14h 36m 42s\n",
      "8000:\ttest: 0.8980398\tbest: 0.8980468 (7994)\ttotal: 7m 4s\tremaining: 14h 36m 37s\n",
      "9000:\ttest: 0.8983700\tbest: 0.8983707 (8998)\ttotal: 7m 56s\tremaining: 14h 34m 33s\n",
      "10000:\ttest: 0.8985794\tbest: 0.8985806 (9998)\ttotal: 8m 48s\tremaining: 14h 32m 8s\n",
      "11000:\ttest: 0.8987848\tbest: 0.8987861 (10996)\ttotal: 9m 40s\tremaining: 14h 29m 53s\n",
      "12000:\ttest: 0.8989432\tbest: 0.8989575 (11902)\ttotal: 10m 33s\tremaining: 14h 29m 31s\n",
      "13000:\ttest: 0.8991998\tbest: 0.8992006 (12999)\ttotal: 11m 28s\tremaining: 14h 31m 5s\n",
      "14000:\ttest: 0.8992583\tbest: 0.8992748 (13925)\ttotal: 12m 20s\tremaining: 14h 28m 38s\n",
      "15000:\ttest: 0.8992907\tbest: 0.8993539 (14627)\ttotal: 13m 10s\tremaining: 14h 25m 28s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8993539252\n",
      "bestIteration = 14627\n",
      "\n",
      "Shrink model to first 14628 iterations.\n",
      "Fold 3 started at Mon Mar 11 16:56:13 2019\n",
      "0:\ttest: 0.5448479\tbest: 0.5448479 (0)\ttotal: 87ms\tremaining: 1d 10m 32s\n",
      "1000:\ttest: 0.8632206\tbest: 0.8632206 (1000)\ttotal: 53.4s\tremaining: 14h 48m 39s\n",
      "2000:\ttest: 0.8823051\tbest: 0.8823051 (2000)\ttotal: 1m 44s\tremaining: 14h 28m 34s\n",
      "3000:\ttest: 0.8901970\tbest: 0.8901970 (3000)\ttotal: 2m 35s\tremaining: 14h 23m 4s\n",
      "4000:\ttest: 0.8943697\tbest: 0.8943697 (4000)\ttotal: 3m 28s\tremaining: 14h 24m 38s\n",
      "5000:\ttest: 0.8967563\tbest: 0.8967598 (4995)\ttotal: 4m 21s\tremaining: 14h 26m 23s\n",
      "6000:\ttest: 0.8983985\tbest: 0.8984017 (5997)\ttotal: 5m 12s\tremaining: 14h 23m 18s\n",
      "7000:\ttest: 0.8995269\tbest: 0.8995269 (7000)\ttotal: 6m 8s\tremaining: 14h 30m 28s\n",
      "8000:\ttest: 0.9001934\tbest: 0.9001934 (8000)\ttotal: 7m 3s\tremaining: 14h 34m 28s\n",
      "9000:\ttest: 0.9007928\tbest: 0.9008005 (8993)\ttotal: 7m 57s\tremaining: 14h 36m 32s\n",
      "10000:\ttest: 0.9011289\tbest: 0.9011289 (10000)\ttotal: 8m 50s\tremaining: 14h 34m 24s\n",
      "11000:\ttest: 0.9013803\tbest: 0.9013803 (11000)\ttotal: 9m 43s\tremaining: 14h 33m 36s\n",
      "12000:\ttest: 0.9016055\tbest: 0.9016055 (11998)\ttotal: 10m 36s\tremaining: 14h 33m 4s\n",
      "13000:\ttest: 0.9017597\tbest: 0.9017662 (12987)\ttotal: 11m 29s\tremaining: 14h 31m 51s\n",
      "14000:\ttest: 0.9017615\tbest: 0.9017682 (13971)\ttotal: 12m 21s\tremaining: 14h 30m 22s\n",
      "15000:\ttest: 0.9018306\tbest: 0.9018492 (14620)\ttotal: 13m 13s\tremaining: 14h 27m 51s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9018492308\n",
      "bestIteration = 14620\n",
      "\n",
      "Shrink model to first 14621 iterations.\n",
      "Fold 4 started at Mon Mar 11 17:11:13 2019\n",
      "0:\ttest: 0.5492991\tbest: 0.5492991 (0)\ttotal: 69.2ms\tremaining: 19h 12m 36s\n",
      "1000:\ttest: 0.8659112\tbest: 0.8659112 (1000)\ttotal: 51.2s\tremaining: 14h 11m 34s\n",
      "2000:\ttest: 0.8858178\tbest: 0.8858178 (2000)\ttotal: 1m 43s\tremaining: 14h 23m 36s\n",
      "3000:\ttest: 0.8952119\tbest: 0.8952119 (3000)\ttotal: 2m 35s\tremaining: 14h 19m 27s\n",
      "4000:\ttest: 0.9000162\tbest: 0.9000229 (3999)\ttotal: 3m 28s\tremaining: 14h 26m 30s\n",
      "5000:\ttest: 0.9029290\tbest: 0.9029290 (5000)\ttotal: 4m 21s\tremaining: 14h 26m 19s\n",
      "6000:\ttest: 0.9049128\tbest: 0.9049143 (5999)\ttotal: 5m 14s\tremaining: 14h 27m 26s\n",
      "7000:\ttest: 0.9060164\tbest: 0.9060187 (6997)\ttotal: 6m 7s\tremaining: 14h 29m 37s\n",
      "8000:\ttest: 0.9067800\tbest: 0.9067865 (7992)\ttotal: 7m\tremaining: 14h 28m 45s\n",
      "9000:\ttest: 0.9073627\tbest: 0.9073672 (8997)\ttotal: 7m 53s\tremaining: 14h 28m 49s\n",
      "10000:\ttest: 0.9077276\tbest: 0.9077276 (9998)\ttotal: 8m 46s\tremaining: 14h 27m 49s\n",
      "11000:\ttest: 0.9080563\tbest: 0.9080563 (11000)\ttotal: 9m 38s\tremaining: 14h 26m 55s\n",
      "12000:\ttest: 0.9083466\tbest: 0.9083493 (11994)\ttotal: 10m 30s\tremaining: 14h 25m 42s\n",
      "13000:\ttest: 0.9085418\tbest: 0.9085489 (12993)\ttotal: 11m 22s\tremaining: 14h 23m 21s\n",
      "14000:\ttest: 0.9087376\tbest: 0.9087428 (13988)\ttotal: 12m 13s\tremaining: 14h 20m 54s\n",
      "15000:\ttest: 0.9088476\tbest: 0.9088655 (14872)\ttotal: 13m 3s\tremaining: 14h 17m 50s\n",
      "16000:\ttest: 0.9090048\tbest: 0.9090226 (15922)\ttotal: 13m 54s\tremaining: 14h 14m 56s\n",
      "17000:\ttest: 0.9090567\tbest: 0.9090758 (16286)\ttotal: 14m 43s\tremaining: 14h 11m 27s\n",
      "18000:\ttest: 0.9091016\tbest: 0.9091030 (17179)\ttotal: 15m 33s\tremaining: 14h 9m 9s\n",
      "19000:\ttest: 0.9091351\tbest: 0.9091633 (18858)\ttotal: 16m 22s\tremaining: 14h 5m 34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9091632997\n",
      "bestIteration = 18858\n",
      "\n",
      "Shrink model to first 18859 iterations.\n",
      "Fold 5 started at Mon Mar 11 17:30:07 2019\n",
      "0:\ttest: 0.5480331\tbest: 0.5480331 (0)\ttotal: 66ms\tremaining: 18h 20m 46s\n",
      "1000:\ttest: 0.8639853\tbest: 0.8639853 (1000)\ttotal: 47.5s\tremaining: 13h 9m 59s\n",
      "2000:\ttest: 0.8819289\tbest: 0.8819289 (2000)\ttotal: 1m 39s\tremaining: 13h 45m 22s\n",
      "3000:\ttest: 0.8908033\tbest: 0.8908033 (3000)\ttotal: 2m 32s\tremaining: 14h 5m 32s\n",
      "4000:\ttest: 0.8953152\tbest: 0.8953194 (3999)\ttotal: 3m 25s\tremaining: 14h 12m 48s\n",
      "5000:\ttest: 0.8978466\tbest: 0.8978466 (5000)\ttotal: 4m 17s\tremaining: 14h 14m 18s\n",
      "6000:\ttest: 0.8994368\tbest: 0.8994491 (5987)\ttotal: 5m 10s\tremaining: 14h 18m 23s\n",
      "7000:\ttest: 0.9004354\tbest: 0.9004394 (6986)\ttotal: 6m 2s\tremaining: 14h 17m 16s\n",
      "8000:\ttest: 0.9012449\tbest: 0.9012459 (7998)\ttotal: 6m 55s\tremaining: 14h 17m 39s\n",
      "9000:\ttest: 0.9017178\tbest: 0.9017223 (8994)\ttotal: 7m 46s\tremaining: 14h 15m 56s\n",
      "10000:\ttest: 0.9020482\tbest: 0.9020534 (9922)\ttotal: 8m 38s\tremaining: 14h 15m 24s\n",
      "11000:\ttest: 0.9024042\tbest: 0.9024042 (11000)\ttotal: 9m 30s\tremaining: 14h 14m 40s\n",
      "12000:\ttest: 0.9026297\tbest: 0.9026473 (11969)\ttotal: 10m 23s\tremaining: 14h 14m 55s\n",
      "13000:\ttest: 0.9027792\tbest: 0.9027807 (12997)\ttotal: 11m 14s\tremaining: 14h 13m 4s\n",
      "14000:\ttest: 0.9028659\tbest: 0.9028667 (13999)\ttotal: 12m 7s\tremaining: 14h 13m 29s\n",
      "15000:\ttest: 0.9029240\tbest: 0.9029384 (14827)\ttotal: 12m 59s\tremaining: 14h 12m 45s\n",
      "16000:\ttest: 0.9030061\tbest: 0.9030173 (15664)\ttotal: 13m 52s\tremaining: 14h 13m 1s\n",
      "17000:\ttest: 0.9030352\tbest: 0.9030814 (16569)\ttotal: 14m 49s\tremaining: 14h 16m 47s\n",
      "18000:\ttest: 0.9030389\tbest: 0.9030904 (17256)\ttotal: 15m 43s\tremaining: 14h 17m 46s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9030903847\n",
      "bestIteration = 17256\n",
      "\n",
      "Shrink model to first 17257 iterations.\n",
      "Fold 6 started at Mon Mar 11 17:47:35 2019\n",
      "0:\ttest: 0.5536311\tbest: 0.5536311 (0)\ttotal: 66.8ms\tremaining: 18h 33m 12s\n",
      "1000:\ttest: 0.8607412\tbest: 0.8607412 (1000)\ttotal: 51s\tremaining: 14h 7m 53s\n",
      "2000:\ttest: 0.8807032\tbest: 0.8807032 (2000)\ttotal: 1m 44s\tremaining: 14h 32m 41s\n",
      "3000:\ttest: 0.8899717\tbest: 0.8899772 (2999)\ttotal: 2m 38s\tremaining: 14h 39m 19s\n",
      "4000:\ttest: 0.8951619\tbest: 0.8951619 (4000)\ttotal: 3m 33s\tremaining: 14h 43m 48s\n",
      "5000:\ttest: 0.8982107\tbest: 0.8982139 (4999)\ttotal: 4m 29s\tremaining: 14h 52m 6s\n",
      "6000:\ttest: 0.9001025\tbest: 0.9001051 (5997)\ttotal: 5m 26s\tremaining: 15h 1m 12s\n",
      "7000:\ttest: 0.9012835\tbest: 0.9012835 (7000)\ttotal: 6m 21s\tremaining: 15h 1m 15s\n",
      "8000:\ttest: 0.9020672\tbest: 0.9020702 (7993)\ttotal: 7m 18s\tremaining: 15h 5m 38s\n",
      "9000:\ttest: 0.9026634\tbest: 0.9026657 (8974)\ttotal: 8m 13s\tremaining: 15h 4m 51s\n",
      "10000:\ttest: 0.9030913\tbest: 0.9030913 (10000)\ttotal: 9m 7s\tremaining: 15h 3m 6s\n",
      "11000:\ttest: 0.9035138\tbest: 0.9035199 (10976)\ttotal: 10m 1s\tremaining: 15h 45s\n",
      "12000:\ttest: 0.9037416\tbest: 0.9037416 (12000)\ttotal: 10m 54s\tremaining: 14h 58m 40s\n",
      "13000:\ttest: 0.9039092\tbest: 0.9039159 (12991)\ttotal: 11m 49s\tremaining: 14h 58m 18s\n",
      "14000:\ttest: 0.9039845\tbest: 0.9040286 (13462)\ttotal: 12m 43s\tremaining: 14h 55m 34s\n",
      "15000:\ttest: 0.9040794\tbest: 0.9040805 (14996)\ttotal: 13m 37s\tremaining: 14h 54m 23s\n",
      "16000:\ttest: 0.9041162\tbest: 0.9041309 (15497)\ttotal: 14m 31s\tremaining: 14h 52m 50s\n",
      "17000:\ttest: 0.9041519\tbest: 0.9041944 (16651)\ttotal: 15m 25s\tremaining: 14h 51m 50s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9041943977\n",
      "bestIteration = 16651\n",
      "\n",
      "Shrink model to first 16652 iterations.\n",
      "Fold 7 started at Mon Mar 11 18:05:10 2019\n",
      "0:\ttest: 0.5283411\tbest: 0.5283411 (0)\ttotal: 76.2ms\tremaining: 21h 10m 45s\n",
      "1000:\ttest: 0.8542353\tbest: 0.8542353 (1000)\ttotal: 49.9s\tremaining: 13h 49m 11s\n",
      "2000:\ttest: 0.8737689\tbest: 0.8737689 (2000)\ttotal: 1m 40s\tremaining: 13h 53m 30s\n",
      "3000:\ttest: 0.8822008\tbest: 0.8822008 (3000)\ttotal: 2m 32s\tremaining: 14h 2m 51s\n",
      "4000:\ttest: 0.8865057\tbest: 0.8865057 (4000)\ttotal: 3m 23s\tremaining: 14h 5m 29s\n",
      "5000:\ttest: 0.8888854\tbest: 0.8888854 (5000)\ttotal: 4m 14s\tremaining: 14h 5m 27s\n",
      "6000:\ttest: 0.8905894\tbest: 0.8905894 (6000)\ttotal: 5m 6s\tremaining: 14h 4m 50s\n",
      "7000:\ttest: 0.8916496\tbest: 0.8916496 (7000)\ttotal: 5m 58s\tremaining: 14h 8m 21s\n",
      "8000:\ttest: 0.8923896\tbest: 0.8923896 (8000)\ttotal: 6m 51s\tremaining: 14h 10m 30s\n",
      "9000:\ttest: 0.8928593\tbest: 0.8928790 (8941)\ttotal: 7m 43s\tremaining: 14h 10m 43s\n",
      "10000:\ttest: 0.8933562\tbest: 0.8933562 (10000)\ttotal: 8m 35s\tremaining: 14h 10m 16s\n",
      "11000:\ttest: 0.8937142\tbest: 0.8937152 (10999)\ttotal: 9m 27s\tremaining: 14h 10m 18s\n",
      "12000:\ttest: 0.8939591\tbest: 0.8939611 (11996)\ttotal: 10m 21s\tremaining: 14h 12m 40s\n",
      "13000:\ttest: 0.8939621\tbest: 0.8940063 (12366)\ttotal: 11m 15s\tremaining: 14h 14m 7s\n",
      "14000:\ttest: 0.8941472\tbest: 0.8941524 (13996)\ttotal: 12m 6s\tremaining: 14h 12m 18s\n",
      "15000:\ttest: 0.8942104\tbest: 0.8942293 (14844)\ttotal: 12m 57s\tremaining: 14h 10m 27s\n",
      "16000:\ttest: 0.8942677\tbest: 0.8942677 (16000)\ttotal: 13m 50s\tremaining: 14h 10m 49s\n",
      "17000:\ttest: 0.8943087\tbest: 0.8943120 (16954)\ttotal: 14m 41s\tremaining: 14h 9m 52s\n",
      "18000:\ttest: 0.8944004\tbest: 0.8944005 (17998)\ttotal: 15m 33s\tremaining: 14h 9m 4s\n",
      "19000:\ttest: 0.8944623\tbest: 0.8944921 (18831)\ttotal: 16m 26s\tremaining: 14h 8m 36s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8944921031\n",
      "bestIteration = 18831\n",
      "\n",
      "Shrink model to first 18832 iterations.\n",
      "Fold 8 started at Mon Mar 11 18:24:11 2019\n",
      "0:\ttest: 0.5643837\tbest: 0.5643837 (0)\ttotal: 64.1ms\tremaining: 17h 49m 6s\n",
      "1000:\ttest: 0.8548770\tbest: 0.8548770 (1000)\ttotal: 50.6s\tremaining: 14h 1m 12s\n",
      "2000:\ttest: 0.8731978\tbest: 0.8731978 (2000)\ttotal: 1m 41s\tremaining: 14h 6m 45s\n",
      "3000:\ttest: 0.8820283\tbest: 0.8820283 (3000)\ttotal: 2m 33s\tremaining: 14h 12m 16s\n",
      "4000:\ttest: 0.8870399\tbest: 0.8870399 (4000)\ttotal: 3m 25s\tremaining: 14h 10m 54s\n",
      "5000:\ttest: 0.8902212\tbest: 0.8902212 (5000)\ttotal: 4m 17s\tremaining: 14h 14m 54s\n",
      "6000:\ttest: 0.8920205\tbest: 0.8920256 (5999)\ttotal: 5m 9s\tremaining: 14h 14m 47s\n",
      "7000:\ttest: 0.8933262\tbest: 0.8933282 (6999)\ttotal: 6m 2s\tremaining: 14h 15m 49s\n",
      "8000:\ttest: 0.8942306\tbest: 0.8942306 (8000)\ttotal: 6m 55s\tremaining: 14h 19m\n",
      "9000:\ttest: 0.8947535\tbest: 0.8947558 (8996)\ttotal: 7m 49s\tremaining: 14h 20m 55s\n",
      "10000:\ttest: 0.8951896\tbest: 0.8951937 (9999)\ttotal: 8m 43s\tremaining: 14h 23m 57s\n",
      "11000:\ttest: 0.8955982\tbest: 0.8955982 (11000)\ttotal: 9m 36s\tremaining: 14h 24m 1s\n",
      "12000:\ttest: 0.8958566\tbest: 0.8958635 (11993)\ttotal: 10m 30s\tremaining: 14h 24m 33s\n",
      "13000:\ttest: 0.8960292\tbest: 0.8960538 (12884)\ttotal: 11m 23s\tremaining: 14h 24m 47s\n",
      "14000:\ttest: 0.8962413\tbest: 0.8962583 (13922)\ttotal: 12m 16s\tremaining: 14h 24m 34s\n",
      "15000:\ttest: 0.8963334\tbest: 0.8963370 (14995)\ttotal: 13m 10s\tremaining: 14h 24m 44s\n",
      "16000:\ttest: 0.8964415\tbest: 0.8964683 (15770)\ttotal: 14m 2s\tremaining: 14h 23m 44s\n",
      "17000:\ttest: 0.8965845\tbest: 0.8966179 (16816)\ttotal: 14m 55s\tremaining: 14h 23m 4s\n",
      "18000:\ttest: 0.8966159\tbest: 0.8966392 (17851)\ttotal: 15m 47s\tremaining: 14h 21m 26s\n",
      "19000:\ttest: 0.8966490\tbest: 0.8966828 (18419)\ttotal: 16m 39s\tremaining: 14h 20m 27s\n",
      "20000:\ttest: 0.8966142\tbest: 0.8966947 (19239)\ttotal: 17m 32s\tremaining: 14h 19m 52s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8966946683\n",
      "bestIteration = 19239\n",
      "\n",
      "Shrink model to first 19240 iterations.\n",
      "Fold 9 started at Mon Mar 11 18:43:51 2019\n",
      "0:\ttest: 0.5723721\tbest: 0.5723721 (0)\ttotal: 79.3ms\tremaining: 22h 1m 49s\n",
      "1000:\ttest: 0.8603727\tbest: 0.8603727 (1000)\ttotal: 48.6s\tremaining: 13h 27m 58s\n",
      "2000:\ttest: 0.8774053\tbest: 0.8774131 (1999)\ttotal: 1m 40s\tremaining: 13h 52m 35s\n",
      "3000:\ttest: 0.8856773\tbest: 0.8856773 (3000)\ttotal: 2m 32s\tremaining: 14h 4m 1s\n",
      "4000:\ttest: 0.8901630\tbest: 0.8901630 (4000)\ttotal: 3m 24s\tremaining: 14h 10m 1s\n",
      "5000:\ttest: 0.8924981\tbest: 0.8925005 (4996)\ttotal: 4m 17s\tremaining: 14h 14m 11s\n",
      "6000:\ttest: 0.8942536\tbest: 0.8942536 (6000)\ttotal: 5m 10s\tremaining: 14h 17m 8s\n",
      "7000:\ttest: 0.8953247\tbest: 0.8953278 (6998)\ttotal: 6m 3s\tremaining: 14h 18m 59s\n",
      "8000:\ttest: 0.8959856\tbest: 0.8959904 (7999)\ttotal: 6m 56s\tremaining: 14h 19m 55s\n",
      "9000:\ttest: 0.8965080\tbest: 0.8965189 (8988)\ttotal: 7m 48s\tremaining: 14h 18m 47s\n",
      "10000:\ttest: 0.8969389\tbest: 0.8969463 (9976)\ttotal: 8m 40s\tremaining: 14h 18m 25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000:\ttest: 0.8972217\tbest: 0.8972235 (10998)\ttotal: 9m 33s\tremaining: 14h 18m 46s\n",
      "12000:\ttest: 0.8973688\tbest: 0.8973843 (11858)\ttotal: 10m 26s\tremaining: 14h 19m 20s\n",
      "13000:\ttest: 0.8974855\tbest: 0.8975097 (12766)\ttotal: 11m 18s\tremaining: 14h 19m 5s\n",
      "14000:\ttest: 0.8975308\tbest: 0.8975432 (13590)\ttotal: 12m 12s\tremaining: 14h 19m 16s\n",
      "15000:\ttest: 0.8976220\tbest: 0.8976567 (14758)\ttotal: 13m 5s\tremaining: 14h 19m 37s\n",
      "16000:\ttest: 0.8976125\tbest: 0.8976819 (15565)\ttotal: 14m 1s\tremaining: 14h 22m 36s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8976818602\n",
      "bestIteration = 15565\n",
      "\n",
      "Shrink model to first 15566 iterations.\n",
      "CV mean score: 0.9005, std: 0.0040.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_cat, prediction_cat, _ = train_model(X1, X2, y, params=np.random.randint(1,101), folds=folds, model_type='cat', plot_feature_importance=False)\n",
    "oof.append(oof_cat)\n",
    "preds.append(prediction_cat)\n",
    "\n",
    "#max-depth=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_cat_1_10_1', oof)\n",
    "np.save('../cache/preds_new_quant_cat_1_10_1', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lgb       xgb       cat      lgb2\n",
      "0  0.011048  0.011639  0.009439  0.012400\n",
      "1  0.469107  0.360481  0.466392  0.542452\n",
      "2  0.004675  0.004766  0.005296  0.003715\n",
      "3  0.262949  0.227772  0.261931  0.239456\n",
      "4  0.093573  0.111758  0.083445  0.077072\n",
      "------------\n",
      "        lgb       xgb       cat      lgb2\n",
      "0  0.104744  0.088794  0.087025  0.094076\n",
      "1  0.204771  0.200059  0.205205  0.211254\n",
      "2  0.167309  0.155445  0.213981  0.173490\n",
      "3  0.212251  0.192163  0.177912  0.221021\n",
      "4  0.040589  0.042701  0.037651  0.042458\n",
      "0.925315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_new_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_new_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb\n",
    "stage2_test['lgb'] = preds_lgb\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_xgb_inter_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_new_quant_xgb_1_10_1.npy')\n",
    "stage2['xgb'] = oof[0]\n",
    "stage2_test['xgb'] = preds[0]\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_cat_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_new_quant_cat_1_10_1.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_2bins_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_new_quant_2bins_1_10_1.npy')\n",
    "stage2['lgb2'] = oof[0]\n",
    "stage2_test['lgb2'] = preds[0]\n",
    "\n",
    "print(stage2.head())\n",
    "print('------------')\n",
    "print(stage2_test.head())\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12t.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Mar 11 19:12:20 2019\n",
      "0:\ttest: 0.5342064\tbest: 0.5342064 (0)\ttotal: 72.9ms\tremaining: 20h 14m 58s\n",
      "1000:\ttest: 0.8499962\tbest: 0.8499962 (1000)\ttotal: 38.5s\tremaining: 10h 40m 15s\n",
      "2000:\ttest: 0.8708011\tbest: 0.8708011 (2000)\ttotal: 1m 18s\tremaining: 10h 49m 53s\n",
      "3000:\ttest: 0.8814808\tbest: 0.8814867 (2999)\ttotal: 1m 58s\tremaining: 10h 57m 50s\n",
      "4000:\ttest: 0.8873863\tbest: 0.8873863 (4000)\ttotal: 2m 36s\tremaining: 10h 49m 16s\n",
      "5000:\ttest: 0.8912302\tbest: 0.8912364 (4996)\ttotal: 3m 11s\tremaining: 10h 35m 53s\n",
      "6000:\ttest: 0.8937937\tbest: 0.8937937 (6000)\ttotal: 3m 50s\tremaining: 10h 37m 14s\n",
      "7000:\ttest: 0.8956373\tbest: 0.8956373 (7000)\ttotal: 4m 33s\tremaining: 10h 46m 29s\n",
      "8000:\ttest: 0.8969506\tbest: 0.8969510 (7998)\ttotal: 5m 16s\tremaining: 10h 53m 49s\n",
      "9000:\ttest: 0.8978511\tbest: 0.8978511 (9000)\ttotal: 5m 57s\tremaining: 10h 56m 38s\n",
      "10000:\ttest: 0.8986443\tbest: 0.8986481 (9988)\ttotal: 6m 49s\tremaining: 11h 15m 6s\n",
      "11000:\ttest: 0.8992490\tbest: 0.8992490 (11000)\ttotal: 7m 35s\tremaining: 11h 21m 49s\n",
      "12000:\ttest: 0.8996158\tbest: 0.8996242 (11973)\ttotal: 8m 19s\tremaining: 11h 25m 30s\n",
      "13000:\ttest: 0.8999756\tbest: 0.8999791 (12995)\ttotal: 9m 10s\tremaining: 11h 36m 40s\n",
      "14000:\ttest: 0.9002787\tbest: 0.9002795 (13964)\ttotal: 9m 59s\tremaining: 11h 43m 10s\n",
      "15000:\ttest: 0.9004981\tbest: 0.9005028 (14998)\ttotal: 10m 47s\tremaining: 11h 49m 3s\n",
      "16000:\ttest: 0.9006838\tbest: 0.9006841 (15996)\ttotal: 11m 41s\tremaining: 11h 58m 48s\n",
      "17000:\ttest: 0.9008325\tbest: 0.9008374 (16878)\ttotal: 12m 26s\tremaining: 11h 59m 27s\n",
      "18000:\ttest: 0.9009627\tbest: 0.9009665 (17979)\ttotal: 13m 9s\tremaining: 11h 57m 42s\n",
      "19000:\ttest: 0.9010613\tbest: 0.9010627 (18999)\ttotal: 13m 51s\tremaining: 11h 55m 16s\n",
      "20000:\ttest: 0.9011383\tbest: 0.9011480 (19979)\ttotal: 14m 33s\tremaining: 11h 52m 59s\n",
      "21000:\ttest: 0.9011800\tbest: 0.9011814 (20998)\ttotal: 15m 13s\tremaining: 11h 49m 47s\n",
      "22000:\ttest: 0.9011892\tbest: 0.9011896 (21995)\ttotal: 15m 54s\tremaining: 11h 47m 13s\n",
      "23000:\ttest: 0.9012233\tbest: 0.9012266 (22990)\ttotal: 16m 35s\tremaining: 11h 45m 3s\n",
      "24000:\ttest: 0.9012475\tbest: 0.9012527 (23993)\ttotal: 17m 17s\tremaining: 11h 42m 55s\n",
      "25000:\ttest: 0.9012428\tbest: 0.9012555 (24380)\ttotal: 17m 59s\tremaining: 11h 41m 35s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9012554647\n",
      "bestIteration = 24380\n",
      "\n",
      "Shrink model to first 24381 iterations.\n",
      "Fold 1 started at Mon Mar 11 19:32:01 2019\n",
      "0:\ttest: 0.5378689\tbest: 0.5378689 (0)\ttotal: 53.4ms\tremaining: 14h 49m 43s\n",
      "1000:\ttest: 0.8442338\tbest: 0.8442338 (1000)\ttotal: 38.9s\tremaining: 10h 47m 7s\n",
      "2000:\ttest: 0.8658963\tbest: 0.8658963 (2000)\ttotal: 1m 18s\tremaining: 10h 52m 50s\n",
      "3000:\ttest: 0.8768042\tbest: 0.8768042 (3000)\ttotal: 1m 56s\tremaining: 10h 45m 30s\n",
      "4000:\ttest: 0.8832710\tbest: 0.8832710 (4000)\ttotal: 2m 37s\tremaining: 10h 52m 55s\n",
      "5000:\ttest: 0.8873850\tbest: 0.8873863 (4999)\ttotal: 3m 18s\tremaining: 10h 57m 35s\n",
      "6000:\ttest: 0.8902429\tbest: 0.8902429 (6000)\ttotal: 3m 59s\tremaining: 11h 25s\n",
      "7000:\ttest: 0.8921301\tbest: 0.8921351 (6997)\ttotal: 4m 40s\tremaining: 11h 2m 24s\n",
      "8000:\ttest: 0.8935795\tbest: 0.8935795 (8000)\ttotal: 5m 21s\tremaining: 11h 4m 22s\n",
      "9000:\ttest: 0.8947395\tbest: 0.8947395 (9000)\ttotal: 6m 3s\tremaining: 11h 6m 6s\n",
      "10000:\ttest: 0.8955614\tbest: 0.8955614 (10000)\ttotal: 6m 44s\tremaining: 11h 7m 17s\n",
      "11000:\ttest: 0.8961256\tbest: 0.8961256 (11000)\ttotal: 7m 26s\tremaining: 11h 9m 13s\n",
      "12000:\ttest: 0.8965985\tbest: 0.8965985 (12000)\ttotal: 8m 7s\tremaining: 11h 8m 30s\n",
      "13000:\ttest: 0.8970821\tbest: 0.8970833 (12999)\ttotal: 8m 48s\tremaining: 11h 8m 29s\n",
      "14000:\ttest: 0.8973505\tbest: 0.8973505 (14000)\ttotal: 9m 34s\tremaining: 11h 13m 57s\n",
      "15000:\ttest: 0.8975354\tbest: 0.8975361 (14998)\ttotal: 10m 15s\tremaining: 11h 14m 5s\n",
      "16000:\ttest: 0.8976964\tbest: 0.8977048 (15863)\ttotal: 10m 57s\tremaining: 11h 13m 54s\n",
      "17000:\ttest: 0.8978599\tbest: 0.8978611 (16997)\ttotal: 11m 39s\tremaining: 11h 14m 17s\n",
      "18000:\ttest: 0.8979358\tbest: 0.8979467 (17962)\ttotal: 12m 19s\tremaining: 11h 12m 38s\n",
      "19000:\ttest: 0.8980844\tbest: 0.8980916 (18973)\ttotal: 13m 1s\tremaining: 11h 12m 28s\n",
      "20000:\ttest: 0.8981240\tbest: 0.8981562 (19812)\ttotal: 13m 42s\tremaining: 11h 11m 47s\n",
      "21000:\ttest: 0.8981891\tbest: 0.8982040 (20769)\ttotal: 14m 25s\tremaining: 11h 12m 35s\n",
      "22000:\ttest: 0.8982315\tbest: 0.8982395 (21730)\ttotal: 15m 7s\tremaining: 11h 12m 21s\n",
      "23000:\ttest: 0.8982644\tbest: 0.8982707 (22985)\ttotal: 15m 50s\tremaining: 11h 13m 9s\n",
      "24000:\ttest: 0.8982802\tbest: 0.8982802 (24000)\ttotal: 16m 32s\tremaining: 11h 12m 52s\n",
      "25000:\ttest: 0.8982915\tbest: 0.8982982 (24903)\ttotal: 17m 14s\tremaining: 11h 12m 21s\n",
      "26000:\ttest: 0.8983262\tbest: 0.8983410 (25762)\ttotal: 17m 55s\tremaining: 11h 11m 25s\n",
      "27000:\ttest: 0.8983251\tbest: 0.8983556 (26387)\ttotal: 18m 37s\tremaining: 11h 11m 21s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8983556455\n",
      "bestIteration = 26387\n",
      "\n",
      "Shrink model to first 26388 iterations.\n",
      "Fold 2 started at Mon Mar 11 19:52:35 2019\n",
      "0:\ttest: 0.5558937\tbest: 0.5558937 (0)\ttotal: 51.6ms\tremaining: 14h 19m 17s\n",
      "1000:\ttest: 0.8509220\tbest: 0.8509220 (1000)\ttotal: 38.1s\tremaining: 10h 33m 50s\n",
      "2000:\ttest: 0.8705448\tbest: 0.8705448 (2000)\ttotal: 1m 18s\tremaining: 10h 52m 58s\n",
      "3000:\ttest: 0.8813446\tbest: 0.8813478 (2999)\ttotal: 1m 59s\tremaining: 11h 4m 6s\n",
      "4000:\ttest: 0.8873308\tbest: 0.8873308 (4000)\ttotal: 2m 42s\tremaining: 11h 15m 57s\n",
      "5000:\ttest: 0.8909422\tbest: 0.8909482 (4999)\ttotal: 3m 24s\tremaining: 11h 17m 42s\n",
      "6000:\ttest: 0.8931705\tbest: 0.8931749 (5999)\ttotal: 4m 6s\tremaining: 11h 20m 36s\n",
      "7000:\ttest: 0.8946922\tbest: 0.8947004 (6993)\ttotal: 4m 49s\tremaining: 11h 24m 12s\n",
      "8000:\ttest: 0.8958074\tbest: 0.8958074 (8000)\ttotal: 5m 30s\tremaining: 11h 23m 42s\n",
      "9000:\ttest: 0.8966506\tbest: 0.8966554 (8994)\ttotal: 6m 14s\tremaining: 11h 26m 48s\n",
      "10000:\ttest: 0.8972464\tbest: 0.8972464 (10000)\ttotal: 6m 55s\tremaining: 11h 25m 3s\n",
      "11000:\ttest: 0.8977305\tbest: 0.8977364 (10994)\ttotal: 7m 36s\tremaining: 11h 24m 39s\n",
      "12000:\ttest: 0.8980865\tbest: 0.8980884 (11993)\ttotal: 8m 19s\tremaining: 11h 24m 42s\n",
      "13000:\ttest: 0.8983988\tbest: 0.8984047 (12996)\ttotal: 9m 1s\tremaining: 11h 24m 46s\n",
      "14000:\ttest: 0.8986230\tbest: 0.8986293 (13994)\ttotal: 9m 43s\tremaining: 11h 24m 50s\n",
      "15000:\ttest: 0.8988227\tbest: 0.8988233 (14991)\ttotal: 10m 25s\tremaining: 11h 24m 5s\n",
      "16000:\ttest: 0.8989537\tbest: 0.8989547 (15999)\ttotal: 11m 6s\tremaining: 11h 22m 48s\n",
      "17000:\ttest: 0.8990586\tbest: 0.8990586 (17000)\ttotal: 11m 47s\tremaining: 11h 21m 32s\n",
      "18000:\ttest: 0.8992213\tbest: 0.8992233 (17935)\ttotal: 12m 28s\tremaining: 11h 20m 23s\n",
      "19000:\ttest: 0.8992850\tbest: 0.8992933 (18881)\ttotal: 13m 9s\tremaining: 11h 19m 18s\n",
      "20000:\ttest: 0.8993282\tbest: 0.8993349 (19550)\ttotal: 13m 49s\tremaining: 11h 17m 42s\n",
      "21000:\ttest: 0.8993733\tbest: 0.8993821 (20966)\ttotal: 14m 30s\tremaining: 11h 16m 11s\n",
      "22000:\ttest: 0.8994086\tbest: 0.8994088 (21999)\ttotal: 15m 12s\tremaining: 11h 15m 41s\n",
      "23000:\ttest: 0.8994288\tbest: 0.8994293 (22999)\ttotal: 15m 52s\tremaining: 11h 14m 35s\n",
      "24000:\ttest: 0.8994439\tbest: 0.8994468 (23820)\ttotal: 16m 33s\tremaining: 11h 13m 12s\n",
      "25000:\ttest: 0.8994253\tbest: 0.8994566 (24374)\ttotal: 17m 13s\tremaining: 11h 11m 43s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8994565804\n",
      "bestIteration = 24374\n",
      "\n",
      "Shrink model to first 24375 iterations.\n",
      "Fold 3 started at Mon Mar 11 20:11:30 2019\n",
      "0:\ttest: 0.5451856\tbest: 0.5451856 (0)\ttotal: 69.7ms\tremaining: 19h 20m 58s\n",
      "1000:\ttest: 0.8495790\tbest: 0.8495790 (1000)\ttotal: 40.3s\tremaining: 11h 10m 59s\n",
      "2000:\ttest: 0.8710830\tbest: 0.8710982 (1998)\ttotal: 1m 21s\tremaining: 11h 19m 23s\n",
      "3000:\ttest: 0.8824111\tbest: 0.8824111 (3000)\ttotal: 2m 3s\tremaining: 11h 26m 6s\n",
      "4000:\ttest: 0.8885908\tbest: 0.8885908 (4000)\ttotal: 2m 46s\tremaining: 11h 29m 2s\n",
      "5000:\ttest: 0.8926314\tbest: 0.8926316 (4998)\ttotal: 3m 29s\tremaining: 11h 34m 8s\n",
      "6000:\ttest: 0.8952862\tbest: 0.8952862 (6000)\ttotal: 4m 11s\tremaining: 11h 34m 26s\n",
      "7000:\ttest: 0.8970636\tbest: 0.8970660 (6998)\ttotal: 4m 52s\tremaining: 11h 32m 29s\n",
      "8000:\ttest: 0.8983575\tbest: 0.8983617 (7997)\ttotal: 5m 34s\tremaining: 11h 31m 35s\n",
      "9000:\ttest: 0.8992355\tbest: 0.8992355 (9000)\ttotal: 6m 15s\tremaining: 11h 29m 23s\n",
      "10000:\ttest: 0.9000322\tbest: 0.9000352 (9977)\ttotal: 6m 57s\tremaining: 11h 28m 54s\n",
      "11000:\ttest: 0.9005955\tbest: 0.9005955 (11000)\ttotal: 7m 39s\tremaining: 11h 27m 45s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000:\ttest: 0.9010363\tbest: 0.9010386 (11987)\ttotal: 8m 20s\tremaining: 11h 26m 36s\n",
      "13000:\ttest: 0.9013893\tbest: 0.9013910 (12996)\ttotal: 9m 1s\tremaining: 11h 25m 39s\n",
      "14000:\ttest: 0.9016142\tbest: 0.9016162 (13976)\ttotal: 9m 43s\tremaining: 11h 24m 46s\n",
      "15000:\ttest: 0.9017709\tbest: 0.9017712 (14999)\ttotal: 10m 25s\tremaining: 11h 24m 22s\n",
      "16000:\ttest: 0.9018646\tbest: 0.9018713 (15995)\ttotal: 11m 7s\tremaining: 11h 24m 8s\n",
      "17000:\ttest: 0.9019685\tbest: 0.9019835 (16933)\ttotal: 11m 49s\tremaining: 11h 23m 32s\n",
      "18000:\ttest: 0.9020394\tbest: 0.9020538 (17541)\ttotal: 12m 30s\tremaining: 11h 22m 8s\n",
      "19000:\ttest: 0.9021112\tbest: 0.9021289 (18846)\ttotal: 13m 11s\tremaining: 11h 21m 14s\n",
      "20000:\ttest: 0.9021679\tbest: 0.9021702 (19792)\ttotal: 13m 52s\tremaining: 11h 20m 3s\n",
      "21000:\ttest: 0.9022619\tbest: 0.9022670 (20989)\ttotal: 14m 36s\tremaining: 11h 20m 40s\n",
      "22000:\ttest: 0.9022596\tbest: 0.9022845 (21512)\ttotal: 15m 17s\tremaining: 11h 19m 25s\n",
      "23000:\ttest: 0.9023021\tbest: 0.9023147 (22854)\ttotal: 15m 59s\tremaining: 11h 19m 5s\n",
      "24000:\ttest: 0.9023488\tbest: 0.9023838 (23833)\ttotal: 16m 41s\tremaining: 11h 18m 30s\n",
      "25000:\ttest: 0.9024052\tbest: 0.9024162 (24686)\ttotal: 17m 23s\tremaining: 11h 17m 58s\n",
      "26000:\ttest: 0.9024439\tbest: 0.9024481 (25915)\ttotal: 18m 4s\tremaining: 11h 17m 3s\n",
      "27000:\ttest: 0.9024508\tbest: 0.9024901 (26475)\ttotal: 18m 45s\tremaining: 11h 15m 44s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9024900788\n",
      "bestIteration = 26475\n",
      "\n",
      "Shrink model to first 26476 iterations.\n",
      "Fold 4 started at Mon Mar 11 20:32:18 2019\n",
      "0:\ttest: 0.5464668\tbest: 0.5464668 (0)\ttotal: 74ms\tremaining: 20h 33m 16s\n",
      "1000:\ttest: 0.8512657\tbest: 0.8512657 (1000)\ttotal: 40s\tremaining: 11h 5m 34s\n",
      "2000:\ttest: 0.8736803\tbest: 0.8736803 (2000)\ttotal: 1m 21s\tremaining: 11h 17m 4s\n",
      "3000:\ttest: 0.8864793\tbest: 0.8864793 (3000)\ttotal: 2m 3s\tremaining: 11h 25m\n",
      "4000:\ttest: 0.8931761\tbest: 0.8931761 (4000)\ttotal: 2m 46s\tremaining: 11h 30m 20s\n",
      "5000:\ttest: 0.8975949\tbest: 0.8975977 (4999)\ttotal: 3m 28s\tremaining: 11h 31m 15s\n",
      "6000:\ttest: 0.9007010\tbest: 0.9007010 (6000)\ttotal: 4m 11s\tremaining: 11h 35m 9s\n",
      "7000:\ttest: 0.9028982\tbest: 0.9028982 (7000)\ttotal: 4m 54s\tremaining: 11h 36m 10s\n",
      "8000:\ttest: 0.9044544\tbest: 0.9044544 (8000)\ttotal: 5m 36s\tremaining: 11h 35m 8s\n",
      "9000:\ttest: 0.9055797\tbest: 0.9055807 (8999)\ttotal: 6m 18s\tremaining: 11h 34m 4s\n",
      "10000:\ttest: 0.9063292\tbest: 0.9063310 (9999)\ttotal: 7m\tremaining: 11h 33m 16s\n",
      "11000:\ttest: 0.9069856\tbest: 0.9069856 (11000)\ttotal: 7m 41s\tremaining: 11h 31m 49s\n",
      "12000:\ttest: 0.9074394\tbest: 0.9074401 (11998)\ttotal: 8m 22s\tremaining: 11h 29m 54s\n",
      "13000:\ttest: 0.9077449\tbest: 0.9077504 (12985)\ttotal: 9m 4s\tremaining: 11h 29m 10s\n",
      "14000:\ttest: 0.9080803\tbest: 0.9080803 (14000)\ttotal: 9m 46s\tremaining: 11h 28m 4s\n",
      "15000:\ttest: 0.9082912\tbest: 0.9082913 (14997)\ttotal: 10m 27s\tremaining: 11h 26m 45s\n",
      "16000:\ttest: 0.9085990\tbest: 0.9086057 (15977)\ttotal: 11m 9s\tremaining: 11h 25m 55s\n",
      "17000:\ttest: 0.9087820\tbest: 0.9087833 (16996)\ttotal: 11m 51s\tremaining: 11h 25m 32s\n",
      "18000:\ttest: 0.9089580\tbest: 0.9089580 (18000)\ttotal: 12m 33s\tremaining: 11h 24m 57s\n",
      "19000:\ttest: 0.9090679\tbest: 0.9090686 (18999)\ttotal: 13m 14s\tremaining: 11h 23m 24s\n",
      "20000:\ttest: 0.9092036\tbest: 0.9092066 (19936)\ttotal: 13m 55s\tremaining: 11h 22m 21s\n",
      "21000:\ttest: 0.9092541\tbest: 0.9092543 (20969)\ttotal: 14m 37s\tremaining: 11h 22m 4s\n",
      "22000:\ttest: 0.9093051\tbest: 0.9093167 (21882)\ttotal: 15m 19s\tremaining: 11h 21m 33s\n",
      "23000:\ttest: 0.9093930\tbest: 0.9093975 (22992)\ttotal: 16m 1s\tremaining: 11h 20m 42s\n",
      "24000:\ttest: 0.9093729\tbest: 0.9094079 (23140)\ttotal: 16m 42s\tremaining: 11h 19m 17s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9094079353\n",
      "bestIteration = 23140\n",
      "\n",
      "Shrink model to first 23141 iterations.\n",
      "Fold 5 started at Mon Mar 11 20:50:31 2019\n",
      "0:\ttest: 0.5489966\tbest: 0.5489966 (0)\ttotal: 57.5ms\tremaining: 15h 58m 19s\n",
      "1000:\ttest: 0.8490991\tbest: 0.8490991 (1000)\ttotal: 39.9s\tremaining: 11h 4m 17s\n",
      "2000:\ttest: 0.8705463\tbest: 0.8705463 (2000)\ttotal: 1m 20s\tremaining: 11h 10m 41s\n",
      "3000:\ttest: 0.8822621\tbest: 0.8822621 (3000)\ttotal: 2m 3s\tremaining: 11h 22m 2s\n",
      "4000:\ttest: 0.8886087\tbest: 0.8886087 (4000)\ttotal: 2m 45s\tremaining: 11h 26m 48s\n",
      "5000:\ttest: 0.8927380\tbest: 0.8927380 (5000)\ttotal: 3m 28s\tremaining: 11h 30m 23s\n",
      "6000:\ttest: 0.8954856\tbest: 0.8954878 (5999)\ttotal: 4m 11s\tremaining: 11h 33m 52s\n",
      "7000:\ttest: 0.8973010\tbest: 0.8973032 (6998)\ttotal: 4m 53s\tremaining: 11h 33m 47s\n",
      "8000:\ttest: 0.8987524\tbest: 0.8987524 (8000)\ttotal: 5m 36s\tremaining: 11h 35m 21s\n",
      "9000:\ttest: 0.8998074\tbest: 0.8998074 (9000)\ttotal: 6m 18s\tremaining: 11h 34m 25s\n",
      "10000:\ttest: 0.9005444\tbest: 0.9005446 (9992)\ttotal: 7m\tremaining: 11h 33m 27s\n",
      "11000:\ttest: 0.9011627\tbest: 0.9011643 (10998)\ttotal: 7m 41s\tremaining: 11h 31m 44s\n",
      "12000:\ttest: 0.9016292\tbest: 0.9016366 (11994)\ttotal: 8m 24s\tremaining: 11h 31m 45s\n",
      "13000:\ttest: 0.9019448\tbest: 0.9019470 (12999)\ttotal: 9m 5s\tremaining: 11h 29m 39s\n",
      "14000:\ttest: 0.9022657\tbest: 0.9022741 (13988)\ttotal: 9m 46s\tremaining: 11h 28m 28s\n",
      "15000:\ttest: 0.9024163\tbest: 0.9024221 (14988)\ttotal: 10m 28s\tremaining: 11h 27m 47s\n",
      "16000:\ttest: 0.9025676\tbest: 0.9025772 (15980)\ttotal: 11m 10s\tremaining: 11h 27m 6s\n",
      "17000:\ttest: 0.9027257\tbest: 0.9027398 (16956)\ttotal: 11m 51s\tremaining: 11h 26m\n",
      "18000:\ttest: 0.9028399\tbest: 0.9028469 (17972)\ttotal: 12m 32s\tremaining: 11h 24m 2s\n",
      "19000:\ttest: 0.9029531\tbest: 0.9029672 (18907)\ttotal: 13m 13s\tremaining: 11h 22m 24s\n",
      "20000:\ttest: 0.9029912\tbest: 0.9029960 (19989)\ttotal: 13m 54s\tremaining: 11h 21m 45s\n",
      "21000:\ttest: 0.9030507\tbest: 0.9030516 (20725)\ttotal: 14m 36s\tremaining: 11h 20m 41s\n",
      "22000:\ttest: 0.9030941\tbest: 0.9030944 (21999)\ttotal: 15m 16s\tremaining: 11h 19m 21s\n",
      "23000:\ttest: 0.9031367\tbest: 0.9031417 (22871)\ttotal: 15m 57s\tremaining: 11h 17m 50s\n",
      "24000:\ttest: 0.9031739\tbest: 0.9031849 (23928)\ttotal: 16m 37s\tremaining: 11h 16m 12s\n",
      "25000:\ttest: 0.9032131\tbest: 0.9032241 (24586)\ttotal: 17m 18s\tremaining: 11h 15m 10s\n",
      "26000:\ttest: 0.9032503\tbest: 0.9032620 (25923)\ttotal: 17m 59s\tremaining: 11h 13m 56s\n",
      "27000:\ttest: 0.9032538\tbest: 0.9032678 (26627)\ttotal: 18m 40s\tremaining: 11h 12m 51s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9032678464\n",
      "bestIteration = 26627\n",
      "\n",
      "Shrink model to first 26628 iterations.\n",
      "Fold 6 started at Mon Mar 11 21:11:22 2019\n",
      "0:\ttest: 0.5486350\tbest: 0.5486350 (0)\ttotal: 50.7ms\tremaining: 14h 5m 28s\n",
      "1000:\ttest: 0.8469391\tbest: 0.8469391 (1000)\ttotal: 38.5s\tremaining: 10h 40m 53s\n",
      "2000:\ttest: 0.8693108\tbest: 0.8693117 (1999)\ttotal: 1m 20s\tremaining: 11h 6m 55s\n",
      "3000:\ttest: 0.8813243\tbest: 0.8813243 (3000)\ttotal: 2m 1s\tremaining: 11h 11m 6s\n",
      "4000:\ttest: 0.8881197\tbest: 0.8881197 (4000)\ttotal: 2m 43s\tremaining: 11h 16m 44s\n",
      "5000:\ttest: 0.8925267\tbest: 0.8925290 (4999)\ttotal: 3m 24s\tremaining: 11h 19m 44s\n",
      "6000:\ttest: 0.8956688\tbest: 0.8956772 (5997)\ttotal: 4m 6s\tremaining: 11h 21m 43s\n",
      "7000:\ttest: 0.8977706\tbest: 0.8977706 (7000)\ttotal: 4m 47s\tremaining: 11h 19m 55s\n",
      "8000:\ttest: 0.8992527\tbest: 0.8992527 (8000)\ttotal: 5m 29s\tremaining: 11h 21m\n",
      "9000:\ttest: 0.9001928\tbest: 0.9001928 (9000)\ttotal: 6m 15s\tremaining: 11h 29m 48s\n",
      "10000:\ttest: 0.9010305\tbest: 0.9010305 (10000)\ttotal: 7m\tremaining: 11h 33m 54s\n",
      "11000:\ttest: 0.9017067\tbest: 0.9017154 (10979)\ttotal: 7m 50s\tremaining: 11h 45m 42s\n",
      "12000:\ttest: 0.9021644\tbest: 0.9021663 (11998)\ttotal: 8m 40s\tremaining: 11h 54m 34s\n",
      "13000:\ttest: 0.9026251\tbest: 0.9026251 (13000)\ttotal: 9m 26s\tremaining: 11h 56m 14s\n",
      "14000:\ttest: 0.9030762\tbest: 0.9030762 (14000)\ttotal: 10m 12s\tremaining: 11h 58m 28s\n",
      "15000:\ttest: 0.9032790\tbest: 0.9032790 (15000)\ttotal: 10m 54s\tremaining: 11h 56m 34s\n",
      "16000:\ttest: 0.9034211\tbest: 0.9034211 (16000)\ttotal: 11m 37s\tremaining: 11h 54m 48s\n",
      "17000:\ttest: 0.9035794\tbest: 0.9035794 (17000)\ttotal: 12m 25s\tremaining: 11h 58m 31s\n",
      "18000:\ttest: 0.9037067\tbest: 0.9037207 (17958)\ttotal: 13m 9s\tremaining: 11h 57m 31s\n",
      "19000:\ttest: 0.9038832\tbest: 0.9038832 (19000)\ttotal: 13m 55s\tremaining: 11h 59m 21s\n",
      "20000:\ttest: 0.9039820\tbest: 0.9039889 (19944)\ttotal: 14m 41s\tremaining: 11h 59m 32s\n",
      "21000:\ttest: 0.9040720\tbest: 0.9040749 (20967)\ttotal: 15m 21s\tremaining: 11h 56m 8s\n",
      "22000:\ttest: 0.9041477\tbest: 0.9041536 (21990)\ttotal: 16m 3s\tremaining: 11h 53m 28s\n",
      "23000:\ttest: 0.9042004\tbest: 0.9042049 (22891)\ttotal: 16m 43s\tremaining: 11h 50m 41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000:\ttest: 0.9042628\tbest: 0.9042727 (23890)\ttotal: 17m 23s\tremaining: 11h 46m 55s\n",
      "25000:\ttest: 0.9043114\tbest: 0.9043132 (24921)\ttotal: 18m 4s\tremaining: 11h 44m 47s\n",
      "26000:\ttest: 0.9043366\tbest: 0.9043484 (25907)\ttotal: 18m 44s\tremaining: 11h 42m 9s\n",
      "27000:\ttest: 0.9043537\tbest: 0.9043544 (26997)\ttotal: 19m 26s\tremaining: 11h 40m 31s\n",
      "28000:\ttest: 0.9043361\tbest: 0.9043697 (27107)\ttotal: 20m 6s\tremaining: 11h 38m 12s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.90436973\n",
      "bestIteration = 27107\n",
      "\n",
      "Shrink model to first 27108 iterations.\n",
      "Fold 7 started at Mon Mar 11 21:33:22 2019\n",
      "0:\ttest: 0.5335685\tbest: 0.5335685 (0)\ttotal: 52.2ms\tremaining: 14h 30m 6s\n",
      "1000:\ttest: 0.8399515\tbest: 0.8399775 (999)\ttotal: 40.7s\tremaining: 11h 17m 4s\n",
      "2000:\ttest: 0.8622727\tbest: 0.8622727 (2000)\ttotal: 1m 21s\tremaining: 11h 17m 4s\n",
      "3000:\ttest: 0.8742222\tbest: 0.8742222 (3000)\ttotal: 2m 3s\tremaining: 11h 23m 31s\n",
      "4000:\ttest: 0.8804109\tbest: 0.8804123 (3999)\ttotal: 2m 44s\tremaining: 11h 23m 59s\n",
      "5000:\ttest: 0.8844043\tbest: 0.8844043 (5000)\ttotal: 3m 26s\tremaining: 11h 25m 59s\n",
      "6000:\ttest: 0.8869695\tbest: 0.8869695 (6000)\ttotal: 4m 9s\tremaining: 11h 28m 30s\n",
      "7000:\ttest: 0.8887879\tbest: 0.8887879 (7000)\ttotal: 4m 53s\tremaining: 11h 32m 43s\n",
      "8000:\ttest: 0.8901874\tbest: 0.8901894 (7999)\ttotal: 5m 35s\tremaining: 11h 33m 16s\n",
      "9000:\ttest: 0.8911279\tbest: 0.8911279 (9000)\ttotal: 6m 17s\tremaining: 11h 32m 43s\n",
      "10000:\ttest: 0.8918362\tbest: 0.8918368 (9999)\ttotal: 6m 58s\tremaining: 11h 30m 28s\n",
      "11000:\ttest: 0.8924569\tbest: 0.8924569 (11000)\ttotal: 7m 39s\tremaining: 11h 28m 52s\n",
      "12000:\ttest: 0.8930114\tbest: 0.8930178 (11991)\ttotal: 8m 21s\tremaining: 11h 27m 31s\n",
      "13000:\ttest: 0.8933479\tbest: 0.8933480 (12999)\ttotal: 9m 3s\tremaining: 11h 27m 12s\n",
      "14000:\ttest: 0.8935581\tbest: 0.8935667 (13977)\ttotal: 9m 44s\tremaining: 11h 26m 23s\n",
      "15000:\ttest: 0.8938102\tbest: 0.8938218 (14923)\ttotal: 10m 25s\tremaining: 11h 24m 46s\n",
      "16000:\ttest: 0.8940063\tbest: 0.8940076 (15999)\ttotal: 11m 6s\tremaining: 11h 22m 36s\n",
      "17000:\ttest: 0.8941748\tbest: 0.8941773 (16993)\ttotal: 11m 46s\tremaining: 11h 20m 47s\n",
      "18000:\ttest: 0.8942888\tbest: 0.8942999 (17970)\ttotal: 12m 27s\tremaining: 11h 19m 14s\n",
      "19000:\ttest: 0.8944191\tbest: 0.8944267 (18860)\ttotal: 13m 6s\tremaining: 11h 17m\n",
      "20000:\ttest: 0.8945091\tbest: 0.8945101 (19987)\ttotal: 13m 47s\tremaining: 11h 15m 51s\n",
      "21000:\ttest: 0.8946294\tbest: 0.8946310 (20998)\ttotal: 14m 28s\tremaining: 11h 14m 28s\n",
      "22000:\ttest: 0.8947086\tbest: 0.8947177 (21880)\ttotal: 15m 8s\tremaining: 11h 12m 45s\n",
      "23000:\ttest: 0.8948255\tbest: 0.8948259 (22998)\ttotal: 15m 48s\tremaining: 11h 11m 19s\n",
      "24000:\ttest: 0.8949235\tbest: 0.8949238 (23999)\ttotal: 16m 28s\tremaining: 11h 10m 15s\n",
      "25000:\ttest: 0.8949998\tbest: 0.8950260 (24599)\ttotal: 17m 9s\tremaining: 11h 9m 11s\n",
      "26000:\ttest: 0.8950406\tbest: 0.8950472 (25994)\ttotal: 17m 49s\tremaining: 11h 7m 58s\n",
      "27000:\ttest: 0.8950478\tbest: 0.8950635 (26874)\ttotal: 18m 30s\tremaining: 11h 7m 3s\n",
      "28000:\ttest: 0.8950833\tbest: 0.8950912 (27283)\ttotal: 19m 10s\tremaining: 11h 5m 42s\n",
      "29000:\ttest: 0.8950985\tbest: 0.8951148 (28198)\ttotal: 19m 49s\tremaining: 11h 4m 2s\n",
      "30000:\ttest: 0.8951033\tbest: 0.8951463 (29552)\ttotal: 20m 30s\tremaining: 11h 3m 6s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8951462808\n",
      "bestIteration = 29552\n",
      "\n",
      "Shrink model to first 29553 iterations.\n",
      "Fold 8 started at Mon Mar 11 21:56:18 2019\n",
      "0:\ttest: 0.5561246\tbest: 0.5561246 (0)\ttotal: 50.1ms\tremaining: 13h 54m 59s\n",
      "1000:\ttest: 0.8415788\tbest: 0.8415788 (1000)\ttotal: 41s\tremaining: 11h 22m 33s\n",
      "2000:\ttest: 0.8615275\tbest: 0.8615275 (2000)\ttotal: 1m 23s\tremaining: 11h 30m 58s\n",
      "3000:\ttest: 0.8729614\tbest: 0.8729614 (3000)\ttotal: 2m 7s\tremaining: 11h 45m 52s\n",
      "4000:\ttest: 0.8797720\tbest: 0.8797720 (4000)\ttotal: 2m 49s\tremaining: 11h 42m\n",
      "5000:\ttest: 0.8843603\tbest: 0.8843603 (5000)\ttotal: 3m 29s\tremaining: 11h 35m 40s\n",
      "6000:\ttest: 0.8873686\tbest: 0.8873686 (6000)\ttotal: 4m 10s\tremaining: 11h 32m 51s\n",
      "7000:\ttest: 0.8897367\tbest: 0.8897367 (7000)\ttotal: 4m 51s\tremaining: 11h 29m 15s\n",
      "8000:\ttest: 0.8914300\tbest: 0.8914300 (8000)\ttotal: 5m 32s\tremaining: 11h 27m 11s\n",
      "9000:\ttest: 0.8926096\tbest: 0.8926147 (8995)\ttotal: 6m 13s\tremaining: 11h 25m 42s\n",
      "10000:\ttest: 0.8934436\tbest: 0.8934445 (9989)\ttotal: 6m 53s\tremaining: 11h 22m 44s\n",
      "11000:\ttest: 0.8940601\tbest: 0.8940605 (10989)\ttotal: 7m 35s\tremaining: 11h 22m 36s\n",
      "12000:\ttest: 0.8946691\tbest: 0.8946715 (11936)\ttotal: 8m 15s\tremaining: 11h 20m 22s\n",
      "13000:\ttest: 0.8951737\tbest: 0.8951758 (12996)\ttotal: 8m 56s\tremaining: 11h 19m 12s\n",
      "14000:\ttest: 0.8955968\tbest: 0.8955987 (13998)\ttotal: 9m 38s\tremaining: 11h 19m 2s\n",
      "15000:\ttest: 0.8958629\tbest: 0.8958657 (14996)\ttotal: 10m 18s\tremaining: 11h 17m 20s\n",
      "16000:\ttest: 0.8960816\tbest: 0.8960826 (15996)\ttotal: 10m 59s\tremaining: 11h 16m 10s\n",
      "17000:\ttest: 0.8962552\tbest: 0.8962605 (16994)\ttotal: 11m 39s\tremaining: 11h 14m 31s\n",
      "18000:\ttest: 0.8964213\tbest: 0.8964215 (17999)\ttotal: 12m 19s\tremaining: 11h 12m 45s\n",
      "19000:\ttest: 0.8965606\tbest: 0.8965644 (18996)\ttotal: 13m\tremaining: 11h 11m 43s\n",
      "20000:\ttest: 0.8967126\tbest: 0.8967153 (19996)\ttotal: 13m 40s\tremaining: 11h 9m 54s\n",
      "21000:\ttest: 0.8967558\tbest: 0.8967610 (20904)\ttotal: 14m 19s\tremaining: 11h 8m 5s\n",
      "22000:\ttest: 0.8968581\tbest: 0.8968660 (21973)\ttotal: 14m 59s\tremaining: 11h 6m 3s\n",
      "23000:\ttest: 0.8969615\tbest: 0.8969659 (22943)\ttotal: 15m 37s\tremaining: 11h 3m 55s\n",
      "24000:\ttest: 0.8970612\tbest: 0.8970657 (23989)\ttotal: 16m 16s\tremaining: 11h 1m 50s\n",
      "25000:\ttest: 0.8971561\tbest: 0.8971600 (24988)\ttotal: 16m 55s\tremaining: 11h 2s\n",
      "26000:\ttest: 0.8971554\tbest: 0.8971910 (25332)\ttotal: 17m 34s\tremaining: 10h 58m 26s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8971909896\n",
      "bestIteration = 25332\n",
      "\n",
      "Shrink model to first 25333 iterations.\n",
      "Fold 9 started at Mon Mar 11 22:15:42 2019\n",
      "0:\ttest: 0.5610281\tbest: 0.5610281 (0)\ttotal: 47.9ms\tremaining: 13h 17m 45s\n",
      "1000:\ttest: 0.8473521\tbest: 0.8473521 (1000)\ttotal: 39.4s\tremaining: 10h 55m 3s\n",
      "2000:\ttest: 0.8664234\tbest: 0.8664234 (2000)\ttotal: 1m 19s\tremaining: 11h 2m 58s\n",
      "3000:\ttest: 0.8775611\tbest: 0.8775611 (3000)\ttotal: 2m 1s\tremaining: 11h 12m 40s\n",
      "4000:\ttest: 0.8838729\tbest: 0.8838773 (3999)\ttotal: 2m 42s\tremaining: 11h 14m 22s\n",
      "5000:\ttest: 0.8879002\tbest: 0.8879002 (5000)\ttotal: 3m 23s\tremaining: 11h 16m 7s\n",
      "6000:\ttest: 0.8906282\tbest: 0.8906282 (6000)\ttotal: 4m 4s\tremaining: 11h 16m 4s\n",
      "7000:\ttest: 0.8924902\tbest: 0.8924917 (6995)\ttotal: 4m 46s\tremaining: 11h 16m 16s\n",
      "8000:\ttest: 0.8938000\tbest: 0.8938000 (8000)\ttotal: 5m 28s\tremaining: 11h 18m\n",
      "9000:\ttest: 0.8947305\tbest: 0.8947362 (8991)\ttotal: 6m 9s\tremaining: 11h 18m 36s\n",
      "10000:\ttest: 0.8954458\tbest: 0.8954458 (10000)\ttotal: 6m 51s\tremaining: 11h 18m 10s\n",
      "11000:\ttest: 0.8959856\tbest: 0.8959856 (11000)\ttotal: 7m 31s\tremaining: 11h 17m 4s\n",
      "12000:\ttest: 0.8964236\tbest: 0.8964305 (11970)\ttotal: 8m 12s\tremaining: 11h 15m 36s\n",
      "13000:\ttest: 0.8967858\tbest: 0.8967860 (12995)\ttotal: 8m 53s\tremaining: 11h 14m 38s\n",
      "14000:\ttest: 0.8970300\tbest: 0.8970354 (13995)\ttotal: 9m 34s\tremaining: 11h 14m 13s\n",
      "15000:\ttest: 0.8972633\tbest: 0.8972708 (14980)\ttotal: 10m 15s\tremaining: 11h 14m\n",
      "16000:\ttest: 0.8974396\tbest: 0.8974495 (15894)\ttotal: 10m 55s\tremaining: 11h 12m 13s\n",
      "17000:\ttest: 0.8975835\tbest: 0.8975863 (16992)\ttotal: 11m 37s\tremaining: 11h 11m 53s\n",
      "18000:\ttest: 0.8976177\tbest: 0.8976328 (17825)\ttotal: 12m 20s\tremaining: 11h 13m 8s\n",
      "19000:\ttest: 0.8976671\tbest: 0.8976729 (18982)\ttotal: 13m 1s\tremaining: 11h 12m 46s\n",
      "20000:\ttest: 0.8976978\tbest: 0.8976998 (19992)\ttotal: 13m 44s\tremaining: 11h 12m 57s\n",
      "21000:\ttest: 0.8977331\tbest: 0.8977400 (20935)\ttotal: 14m 27s\tremaining: 11h 13m 52s\n",
      "22000:\ttest: 0.8977884\tbest: 0.8977913 (21972)\ttotal: 15m 9s\tremaining: 11h 14m 10s\n",
      "23000:\ttest: 0.8977816\tbest: 0.8978039 (22561)\ttotal: 15m 52s\tremaining: 11h 14m 14s\n",
      "24000:\ttest: 0.8978231\tbest: 0.8978316 (23964)\ttotal: 16m 34s\tremaining: 11h 13m 48s\n",
      "25000:\ttest: 0.8978066\tbest: 0.8978340 (24083)\ttotal: 17m 16s\tremaining: 11h 13m 52s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8978339828\n",
      "bestIteration = 24083\n",
      "\n",
      "Shrink model to first 24084 iterations.\n",
      "CV mean score: 0.9009, std: 0.0040.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_cat, prediction_cat, _ = train_model(X1, X2, y, params=np.random.randint(1,101), folds=folds, model_type='cat', plot_feature_importance=False)\n",
    "oof.append(oof_cat)\n",
    "preds.append(prediction_cat)\n",
    "\n",
    "#max-depth=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_cat_1_10_2', oof)\n",
    "np.save('../cache/preds_new_quant_cat_1_10_2', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lgb       xgb       cat      lgb2\n",
      "0  0.011048  0.011639  0.010788  0.012400\n",
      "1  0.469107  0.360481  0.508154  0.542452\n",
      "2  0.004675  0.004766  0.004653  0.003715\n",
      "3  0.262949  0.227772  0.271351  0.239456\n",
      "4  0.093573  0.111758  0.095146  0.077072\n",
      "------------\n",
      "        lgb       xgb       cat      lgb2\n",
      "0  0.104744  0.088794  0.091315  0.094076\n",
      "1  0.204771  0.200059  0.195239  0.211254\n",
      "2  0.167309  0.155445  0.217269  0.173490\n",
      "3  0.212251  0.192163  0.183777  0.221021\n",
      "4  0.040589  0.042701  0.039297  0.042458\n",
      "0.92526\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_new_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_new_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb\n",
    "stage2_test['lgb'] = preds_lgb\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_xgb_inter_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_new_quant_xgb_1_10_1.npy')\n",
    "stage2['xgb'] = oof[0]\n",
    "stage2_test['xgb'] = preds[0]\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_cat_1_10_2.npy')\n",
    "preds = np.load('../cache/preds_new_quant_cat_1_10_2.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_2bins_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_new_quant_2bins_1_10_1.npy')\n",
    "stage2['lgb2'] = oof[0]\n",
    "stage2_test['lgb2'] = preds[0]\n",
    "\n",
    "print(stage2.head())\n",
    "print('------------')\n",
    "print(stage2_test.head())\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12u.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lgb       xgb       cat      lgb2\n",
      "0  0.011048  0.011639  0.010788  0.012400\n",
      "1  0.469107  0.360481  0.508154  0.542452\n",
      "2  0.004675  0.004766  0.004653  0.003715\n",
      "3  0.262949  0.227772  0.271351  0.239456\n",
      "4  0.093573  0.111758  0.095146  0.077072\n",
      "------------\n",
      "        lgb       xgb       cat      lgb2\n",
      "0  0.104744  0.088794  0.091315  0.094076\n",
      "1  0.204771  0.200059  0.195239  0.211254\n",
      "2  0.167309  0.155445  0.217269  0.173490\n",
      "3  0.212251  0.192163  0.183777  0.221021\n",
      "4  0.040589  0.042701  0.039297  0.042458\n",
      "0.925425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof_lgb = np.load('../cache/oof_new_quant_1_10_1.npy')\n",
    "preds_lgb = np.load('../cache/preds_new_quant_1_10_1.npy')\n",
    "stage2['lgb'] = oof_lgb\n",
    "stage2_test['lgb'] = preds_lgb\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_xgb_inter_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_new_quant_xgb_1_10_1.npy')\n",
    "stage2['xgb'] = oof[0]\n",
    "stage2_test['xgb'] = preds[0]\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_cat_1_10_2.npy')\n",
    "preds = np.load('../cache/preds_new_quant_cat_1_10_2.npy')\n",
    "stage2['cat'] = oof[0]\n",
    "stage2_test['cat'] = preds[0]\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_2bins_1_10_1.npy')\n",
    "preds = np.load('../cache/preds_new_quant_2bins_1_10_1.npy')\n",
    "stage2['lgb2'] = oof[0]\n",
    "stage2_test['lgb2'] = preds[0]\n",
    "\n",
    "print(stage2.head())\n",
    "print('------------')\n",
    "print(stage2_test.head())\n",
    "lr = LogisticRegression(C=0.01)\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12v.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 200), (200000, 200))"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "\n",
    "bins = np.linspace(-6.0,6.0,40)\n",
    "\n",
    "# X3_12_bins = pd.get_dummies(pd.cut(X3['var_12'].values, bins))\n",
    "# X4_12_bins = pd.get_dummies(pd.cut(X4['var_12'].values, bins))\n",
    "X3['var_12'] = pd.cut(X3['var_12'].values, bins)\n",
    "X4['var_12'] = pd.cut(X4['var_12'].values, bins)\n",
    "\n",
    "bins = np.linspace(-6.0,6.0,50)\n",
    "\n",
    "# X3_91_bins = pd.get_dummies(pd.cut(X3['var_91'].values, bins))\n",
    "# X4_91_bins = pd.get_dummies(pd.cut(X4['var_91'].values, bins))\n",
    "X3['var_91'] = pd.cut(X3['var_91'].values, bins)\n",
    "X4['var_91'] = pd.cut(X4['var_91'].values, bins)\n",
    "\n",
    "# X3_12_bins = pd.DataFrame(X3_12_bins)\n",
    "# cols = ['var_12_' + str(i) for i in range(X3_12_bins.shape[1])]\n",
    "# X3_12_bins.columns = cols\n",
    "\n",
    "# X4_12_bins = pd.DataFrame(X4_12_bins)\n",
    "# X4_12_bins.columns = cols\n",
    "\n",
    "# X3_91_bins = pd.DataFrame(X3_91_bins)\n",
    "# cols = ['var_91_' + str(i) for i in range(X3_91_bins.shape[1])]\n",
    "# X3_91_bins.columns = cols\n",
    "\n",
    "# X4_91_bins = pd.DataFrame(X4_91_bins)\n",
    "# X4_91_bins.columns = cols\n",
    "\n",
    "# X3 = pd.concat([X3, X3_12_bins], axis=1)\n",
    "# X3 = pd.concat([X3, X3_91_bins], axis=1)\n",
    "\n",
    "# X4 = pd.concat([X4, X4_12_bins], axis=1)\n",
    "# X4 = pd.concat([X4, X4_91_bins], axis=1)\n",
    "\n",
    "data = pd.concat([X3, X4], axis=0)\n",
    "# data['var_12_var_91'] = data['var_12'].astype('str') + '_' + data['var_91'].astype('str')\n",
    "for col in ['var_12', 'var_91']:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = data[col].astype('str')\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    data[col] = data[col].astype(int)\n",
    "    \n",
    "# data_12_91_bins = pd.get_dummies(data['var_12_var_91'])\n",
    "# data_12_91_bins = pd.DataFrame(data_12_91_bins)\n",
    "# cols = ['var_12_var_91_' + str(i) for i in range(data_12_91_bins.shape[1])]\n",
    "# data_12_91_bins.columns = cols\n",
    "\n",
    "# data = pd.concat([data, data_12_91_bins], axis=1)\n",
    "\n",
    "# data = data.drop(['var_12', 'var_91', 'var_12_var_91'], axis=1)\n",
    "\n",
    "X3 = data[:len(X3)]\n",
    "X4 = data[len(X3):]\n",
    "\n",
    "for col in ['var_12', 'var_91']:\n",
    "    X3[col] = X3[col].astype(int)\n",
    "    X4[col] = X4[col].astype(int)\n",
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Mar 11 23:19:54 2019\n",
      "0:\ttest: 0.5434013\tbest: 0.5434013 (0)\ttotal: 62ms\tremaining: 17h 12m 52s\n",
      "1000:\ttest: 0.8494045\tbest: 0.8494045 (1000)\ttotal: 42s\tremaining: 11h 39m 23s\n",
      "2000:\ttest: 0.8698722\tbest: 0.8698722 (2000)\ttotal: 1m 25s\tremaining: 11h 50m 6s\n",
      "3000:\ttest: 0.8810002\tbest: 0.8810002 (3000)\ttotal: 2m 12s\tremaining: 12h 16m 8s\n",
      "4000:\ttest: 0.8868469\tbest: 0.8868469 (4000)\ttotal: 2m 57s\tremaining: 12h 16m 53s\n",
      "5000:\ttest: 0.8906626\tbest: 0.8906634 (4994)\ttotal: 3m 41s\tremaining: 12h 13m 24s\n",
      "6000:\ttest: 0.8932475\tbest: 0.8932537 (5998)\ttotal: 4m 24s\tremaining: 12h 9m 49s\n",
      "7000:\ttest: 0.8951242\tbest: 0.8951249 (6998)\ttotal: 5m 10s\tremaining: 12h 14m 20s\n",
      "8000:\ttest: 0.8964945\tbest: 0.8964945 (8000)\ttotal: 5m 59s\tremaining: 12h 22m 57s\n",
      "9000:\ttest: 0.8973911\tbest: 0.8973930 (8998)\ttotal: 6m 47s\tremaining: 12h 27m 45s\n",
      "10000:\ttest: 0.8979988\tbest: 0.8979988 (10000)\ttotal: 7m 36s\tremaining: 12h 33m 52s\n",
      "11000:\ttest: 0.8985671\tbest: 0.8985806 (10960)\ttotal: 8m 25s\tremaining: 12h 37m 5s\n",
      "12000:\ttest: 0.8989410\tbest: 0.8989410 (12000)\ttotal: 9m 15s\tremaining: 12h 42m 10s\n",
      "13000:\ttest: 0.8993489\tbest: 0.8993501 (12996)\ttotal: 10m 6s\tremaining: 12h 47m 37s\n",
      "14000:\ttest: 0.8996112\tbest: 0.8996148 (13997)\ttotal: 10m 55s\tremaining: 12h 49m 12s\n",
      "15000:\ttest: 0.8997970\tbest: 0.8998074 (14943)\ttotal: 11m 43s\tremaining: 12h 50m 18s\n",
      "16000:\ttest: 0.8999744\tbest: 0.8999785 (15978)\ttotal: 12m 33s\tremaining: 12h 52m 5s\n",
      "17000:\ttest: 0.9001205\tbest: 0.9001205 (17000)\ttotal: 13m 22s\tremaining: 12h 53m 37s\n",
      "18000:\ttest: 0.9002872\tbest: 0.9002902 (17984)\ttotal: 14m 11s\tremaining: 12h 54m 30s\n",
      "19000:\ttest: 0.9004142\tbest: 0.9004236 (18978)\ttotal: 15m 1s\tremaining: 12h 55m 49s\n",
      "20000:\ttest: 0.9005255\tbest: 0.9005290 (19975)\ttotal: 15m 50s\tremaining: 12h 56m 21s\n",
      "21000:\ttest: 0.9005898\tbest: 0.9006007 (20955)\ttotal: 16m 39s\tremaining: 12h 56m 22s\n",
      "22000:\ttest: 0.9006340\tbest: 0.9006353 (21999)\ttotal: 17m 29s\tremaining: 12h 57m 35s\n",
      "23000:\ttest: 0.9007211\tbest: 0.9007263 (22959)\ttotal: 18m 18s\tremaining: 12h 57m 44s\n",
      "24000:\ttest: 0.9007661\tbest: 0.9007739 (23943)\ttotal: 19m 8s\tremaining: 12h 58m 38s\n",
      "25000:\ttest: 0.9007713\tbest: 0.9007864 (24378)\ttotal: 20m\tremaining: 13h 16s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9007863523\n",
      "bestIteration = 24378\n",
      "\n",
      "Shrink model to first 24379 iterations.\n",
      "Fold 1 started at Mon Mar 11 23:41:47 2019\n",
      "0:\ttest: 0.5490914\tbest: 0.5490914 (0)\ttotal: 79.6ms\tremaining: 22h 6m\n",
      "1000:\ttest: 0.8441452\tbest: 0.8441452 (1000)\ttotal: 47.8s\tremaining: 13h 15m 51s\n",
      "2000:\ttest: 0.8650906\tbest: 0.8650906 (2000)\ttotal: 1m 36s\tremaining: 13h 25m 3s\n",
      "3000:\ttest: 0.8763688\tbest: 0.8763688 (3000)\ttotal: 2m 27s\tremaining: 13h 35m 20s\n",
      "4000:\ttest: 0.8828722\tbest: 0.8828722 (4000)\ttotal: 3m 14s\tremaining: 13h 28m 52s\n",
      "5000:\ttest: 0.8870374\tbest: 0.8870374 (5000)\ttotal: 4m 5s\tremaining: 13h 32m 49s\n",
      "6000:\ttest: 0.8898062\tbest: 0.8898062 (5999)\ttotal: 4m 58s\tremaining: 13h 43m 57s\n",
      "7000:\ttest: 0.8917893\tbest: 0.8917904 (6999)\ttotal: 5m 50s\tremaining: 13h 49m 36s\n",
      "8000:\ttest: 0.8931413\tbest: 0.8931413 (8000)\ttotal: 6m 42s\tremaining: 13h 51m 44s\n",
      "9000:\ttest: 0.8942700\tbest: 0.8942811 (8977)\ttotal: 7m 34s\tremaining: 13h 54m 11s\n",
      "10000:\ttest: 0.8950469\tbest: 0.8950484 (9999)\ttotal: 8m 23s\tremaining: 13h 50m 53s\n",
      "11000:\ttest: 0.8956253\tbest: 0.8956290 (10979)\ttotal: 9m 14s\tremaining: 13h 51m 4s\n",
      "12000:\ttest: 0.8960749\tbest: 0.8960749 (12000)\ttotal: 10m 6s\tremaining: 13h 51m 35s\n",
      "13000:\ttest: 0.8964035\tbest: 0.8964047 (12996)\ttotal: 10m 59s\tremaining: 13h 54m 12s\n",
      "14000:\ttest: 0.8967347\tbest: 0.8967347 (14000)\ttotal: 11m 52s\tremaining: 13h 56m 2s\n",
      "15000:\ttest: 0.8969386\tbest: 0.8969430 (14991)\ttotal: 12m 45s\tremaining: 13h 58m 3s\n",
      "16000:\ttest: 0.8971007\tbest: 0.8971106 (15951)\ttotal: 13m 37s\tremaining: 13h 58m 10s\n",
      "17000:\ttest: 0.8971928\tbest: 0.8971928 (16999)\ttotal: 14m 28s\tremaining: 13h 56m 50s\n",
      "18000:\ttest: 0.8972916\tbest: 0.8972976 (17733)\ttotal: 15m 18s\tremaining: 13h 55m 26s\n",
      "19000:\ttest: 0.8974056\tbest: 0.8974179 (18976)\ttotal: 16m 7s\tremaining: 13h 52m 47s\n",
      "20000:\ttest: 0.8974753\tbest: 0.8974863 (19897)\ttotal: 16m 58s\tremaining: 13h 51m 20s\n",
      "21000:\ttest: 0.8975438\tbest: 0.8975546 (20937)\ttotal: 17m 50s\tremaining: 13h 51m 42s\n",
      "22000:\ttest: 0.8976745\tbest: 0.8976745 (22000)\ttotal: 18m 41s\tremaining: 13h 50m 37s\n",
      "23000:\ttest: 0.8976857\tbest: 0.8976974 (22857)\ttotal: 19m 30s\tremaining: 13h 48m 18s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8976973838\n",
      "bestIteration = 22857\n",
      "\n",
      "Shrink model to first 22858 iterations.\n",
      "Fold 2 started at Tue Mar 12 00:03:30 2019\n",
      "0:\ttest: 0.5185592\tbest: 0.5185592 (0)\ttotal: 75.8ms\tremaining: 21h 2m 55s\n",
      "1000:\ttest: 0.8502089\tbest: 0.8502089 (1000)\ttotal: 47.1s\tremaining: 13h 3m 58s\n",
      "2000:\ttest: 0.8701222\tbest: 0.8701222 (2000)\ttotal: 1m 37s\tremaining: 13h 27m 47s\n",
      "3000:\ttest: 0.8814009\tbest: 0.8814013 (2999)\ttotal: 2m 31s\tremaining: 13h 56m 45s\n",
      "4000:\ttest: 0.8871058\tbest: 0.8871058 (4000)\ttotal: 3m 23s\tremaining: 14h 2m 18s\n",
      "5000:\ttest: 0.8905621\tbest: 0.8905621 (5000)\ttotal: 4m 12s\tremaining: 13h 56m 16s\n",
      "6000:\ttest: 0.8929389\tbest: 0.8929416 (5998)\ttotal: 5m\tremaining: 13h 49m 22s\n",
      "7000:\ttest: 0.8944758\tbest: 0.8944758 (7000)\ttotal: 5m 48s\tremaining: 13h 44m 43s\n",
      "8000:\ttest: 0.8956757\tbest: 0.8956757 (8000)\ttotal: 6m 36s\tremaining: 13h 38m 21s\n",
      "9000:\ttest: 0.8963387\tbest: 0.8963460 (8994)\ttotal: 7m 23s\tremaining: 13h 34m 22s\n",
      "10000:\ttest: 0.8970060\tbest: 0.8970062 (9999)\ttotal: 8m 12s\tremaining: 13h 32m 40s\n",
      "11000:\ttest: 0.8973874\tbest: 0.8973915 (10999)\ttotal: 9m\tremaining: 13h 30m 25s\n",
      "12000:\ttest: 0.8976914\tbest: 0.8977024 (11988)\ttotal: 9m 48s\tremaining: 13h 26m 58s\n",
      "13000:\ttest: 0.8978457\tbest: 0.8978583 (12834)\ttotal: 10m 35s\tremaining: 13h 23m 42s\n",
      "14000:\ttest: 0.8980526\tbest: 0.8980545 (13995)\ttotal: 11m 23s\tremaining: 13h 22m\n",
      "15000:\ttest: 0.8981890\tbest: 0.8981957 (14957)\ttotal: 12m 12s\tremaining: 13h 21m 37s\n",
      "16000:\ttest: 0.8983202\tbest: 0.8983252 (15994)\ttotal: 13m\tremaining: 13h 19m 44s\n",
      "17000:\ttest: 0.8983494\tbest: 0.8983518 (16950)\ttotal: 13m 48s\tremaining: 13h 18m 4s\n",
      "18000:\ttest: 0.8984692\tbest: 0.8984705 (17959)\ttotal: 14m 35s\tremaining: 13h 15m 59s\n",
      "19000:\ttest: 0.8985486\tbest: 0.8985518 (18963)\ttotal: 15m 24s\tremaining: 13h 15m 29s\n",
      "20000:\ttest: 0.8985859\tbest: 0.8986026 (19643)\ttotal: 16m 13s\tremaining: 13h 14m 40s\n",
      "21000:\ttest: 0.8986244\tbest: 0.8986289 (20991)\ttotal: 16m 59s\tremaining: 13h 12m 17s\n",
      "22000:\ttest: 0.8986414\tbest: 0.8986589 (21953)\ttotal: 17m 46s\tremaining: 13h 10m 30s\n",
      "23000:\ttest: 0.8986515\tbest: 0.8986851 (22298)\ttotal: 18m 35s\tremaining: 13h 10m\n",
      "24000:\ttest: 0.8987227\tbest: 0.8987343 (23789)\ttotal: 19m 24s\tremaining: 13h 8m 58s\n",
      "25000:\ttest: 0.8987083\tbest: 0.8987363 (24254)\ttotal: 20m 11s\tremaining: 13h 7m 39s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.898736335\n",
      "bestIteration = 24254\n",
      "\n",
      "Shrink model to first 24255 iterations.\n",
      "Fold 3 started at Tue Mar 12 00:25:34 2019\n",
      "0:\ttest: 0.5313023\tbest: 0.5313023 (0)\ttotal: 66.2ms\tremaining: 18h 23m 32s\n",
      "1000:\ttest: 0.8484012\tbest: 0.8484012 (1000)\ttotal: 49.3s\tremaining: 13h 39m 12s\n",
      "2000:\ttest: 0.8702524\tbest: 0.8702524 (2000)\ttotal: 1m 39s\tremaining: 13h 48m 46s\n",
      "3000:\ttest: 0.8818107\tbest: 0.8818107 (3000)\ttotal: 2m 32s\tremaining: 14h 3m 36s\n",
      "4000:\ttest: 0.8878572\tbest: 0.8878572 (4000)\ttotal: 3m 23s\tremaining: 14h 5m 16s\n",
      "5000:\ttest: 0.8918453\tbest: 0.8918453 (5000)\ttotal: 4m 14s\tremaining: 14h 3m 20s\n",
      "6000:\ttest: 0.8943106\tbest: 0.8943133 (5998)\ttotal: 5m 5s\tremaining: 14h 4m 40s\n",
      "7000:\ttest: 0.8961669\tbest: 0.8961698 (6997)\ttotal: 5m 57s\tremaining: 14h 5m 19s\n",
      "8000:\ttest: 0.8974535\tbest: 0.8974543 (7997)\ttotal: 6m 49s\tremaining: 14h 6m 34s\n",
      "9000:\ttest: 0.8983843\tbest: 0.8983843 (9000)\ttotal: 7m 42s\tremaining: 14h 9m 12s\n",
      "10000:\ttest: 0.8991580\tbest: 0.8991608 (9997)\ttotal: 8m 35s\tremaining: 14h 10m 39s\n",
      "11000:\ttest: 0.8997481\tbest: 0.8997481 (11000)\ttotal: 9m 25s\tremaining: 14h 6m 47s\n",
      "12000:\ttest: 0.9001287\tbest: 0.9001287 (12000)\ttotal: 10m 15s\tremaining: 14h 4m 26s\n",
      "13000:\ttest: 0.9003797\tbest: 0.9003797 (13000)\ttotal: 11m 7s\tremaining: 14h 4m 44s\n",
      "14000:\ttest: 0.9005821\tbest: 0.9005980 (13894)\ttotal: 12m 1s\tremaining: 14h 6m 25s\n",
      "15000:\ttest: 0.9008133\tbest: 0.9008189 (14991)\ttotal: 12m 53s\tremaining: 14h 6m 27s\n",
      "16000:\ttest: 0.9009306\tbest: 0.9009314 (15997)\ttotal: 13m 47s\tremaining: 14h 7m 47s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000:\ttest: 0.9010341\tbest: 0.9010403 (16979)\ttotal: 14m 39s\tremaining: 14h 7m 57s\n",
      "18000:\ttest: 0.9010505\tbest: 0.9010739 (17645)\ttotal: 15m 28s\tremaining: 14h 4m 15s\n",
      "19000:\ttest: 0.9011724\tbest: 0.9011805 (18972)\ttotal: 16m 23s\tremaining: 14h 6m 34s\n",
      "20000:\ttest: 0.9012826\tbest: 0.9012991 (19959)\ttotal: 17m 14s\tremaining: 14h 5m 5s\n",
      "21000:\ttest: 0.9013535\tbest: 0.9013551 (20998)\ttotal: 18m 4s\tremaining: 14h 2m 37s\n",
      "22000:\ttest: 0.9013860\tbest: 0.9014177 (21654)\ttotal: 18m 55s\tremaining: 14h 1m 20s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9014177307\n",
      "bestIteration = 21654\n",
      "\n",
      "Shrink model to first 21655 iterations.\n",
      "Fold 4 started at Tue Mar 12 00:46:32 2019\n",
      "0:\ttest: 0.5239677\tbest: 0.5239677 (0)\ttotal: 79.9ms\tremaining: 22h 11m 49s\n",
      "1000:\ttest: 0.8490553\tbest: 0.8490553 (1000)\ttotal: 50.8s\tremaining: 14h 5m 1s\n",
      "2000:\ttest: 0.8727296\tbest: 0.8727296 (2000)\ttotal: 1m 43s\tremaining: 14h 21m 6s\n",
      "3000:\ttest: 0.8849830\tbest: 0.8849830 (3000)\ttotal: 2m 35s\tremaining: 14h 18m 41s\n",
      "4000:\ttest: 0.8922255\tbest: 0.8922255 (4000)\ttotal: 3m 27s\tremaining: 14h 22m 47s\n",
      "5000:\ttest: 0.8966586\tbest: 0.8966598 (4999)\ttotal: 4m 19s\tremaining: 14h 22m 7s\n",
      "6000:\ttest: 0.8998476\tbest: 0.8998476 (6000)\ttotal: 5m 19s\tremaining: 14h 42m 54s\n",
      "7000:\ttest: 0.9020961\tbest: 0.9020961 (7000)\ttotal: 6m 15s\tremaining: 14h 46m 34s\n",
      "8000:\ttest: 0.9035877\tbest: 0.9035877 (8000)\ttotal: 7m 5s\tremaining: 14h 40m 4s\n",
      "9000:\ttest: 0.9046260\tbest: 0.9046293 (8996)\ttotal: 7m 55s\tremaining: 14h 33m 24s\n",
      "10000:\ttest: 0.9055612\tbest: 0.9055637 (9996)\ttotal: 8m 49s\tremaining: 14h 32m 46s\n",
      "11000:\ttest: 0.9061311\tbest: 0.9061411 (10988)\ttotal: 9m 51s\tremaining: 14h 46m 38s\n",
      "12000:\ttest: 0.9066289\tbest: 0.9066373 (11982)\ttotal: 10m 49s\tremaining: 14h 50m 42s\n",
      "13000:\ttest: 0.9070569\tbest: 0.9070569 (13000)\ttotal: 11m 45s\tremaining: 14h 52m 30s\n",
      "14000:\ttest: 0.9073474\tbest: 0.9073509 (13999)\ttotal: 12m 38s\tremaining: 14h 50m 7s\n",
      "15000:\ttest: 0.9075746\tbest: 0.9075746 (15000)\ttotal: 13m 30s\tremaining: 14h 46m 35s\n",
      "16000:\ttest: 0.9078039\tbest: 0.9078096 (15984)\ttotal: 14m 22s\tremaining: 14h 44m 3s\n",
      "17000:\ttest: 0.9079215\tbest: 0.9079215 (17000)\ttotal: 15m 19s\tremaining: 14h 45m 38s\n",
      "18000:\ttest: 0.9080306\tbest: 0.9080318 (17999)\ttotal: 16m 14s\tremaining: 14h 45m 55s\n",
      "19000:\ttest: 0.9081191\tbest: 0.9081239 (18997)\ttotal: 17m 11s\tremaining: 14h 47m 22s\n",
      "20000:\ttest: 0.9082869\tbest: 0.9082880 (19997)\ttotal: 18m 7s\tremaining: 14h 48m 28s\n",
      "21000:\ttest: 0.9082938\tbest: 0.9083285 (20800)\ttotal: 19m 5s\tremaining: 14h 49m 48s\n",
      "22000:\ttest: 0.9083482\tbest: 0.9083734 (21723)\ttotal: 20m 3s\tremaining: 14h 51m 24s\n",
      "23000:\ttest: 0.9083994\tbest: 0.9084045 (22973)\ttotal: 20m 58s\tremaining: 14h 50m 38s\n",
      "24000:\ttest: 0.9084607\tbest: 0.9084747 (23728)\ttotal: 21m 53s\tremaining: 14h 49m 52s\n",
      "25000:\ttest: 0.9085566\tbest: 0.9085591 (24996)\ttotal: 22m 47s\tremaining: 14h 48m 57s\n",
      "26000:\ttest: 0.9085487\tbest: 0.9085664 (25600)\ttotal: 23m 40s\tremaining: 14h 46m 43s\n",
      "27000:\ttest: 0.9085818\tbest: 0.9085973 (26891)\ttotal: 24m 34s\tremaining: 14h 45m 18s\n",
      "28000:\ttest: 0.9086288\tbest: 0.9086319 (27778)\ttotal: 25m 26s\tremaining: 14h 43m 24s\n",
      "29000:\ttest: 0.9085875\tbest: 0.9086428 (28054)\ttotal: 26m 19s\tremaining: 14h 41m 18s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9086427507\n",
      "bestIteration = 28054\n",
      "\n",
      "Shrink model to first 28055 iterations.\n",
      "Fold 5 started at Tue Mar 12 01:15:05 2019\n",
      "0:\ttest: 0.5275758\tbest: 0.5275758 (0)\ttotal: 73.1ms\tremaining: 20h 18m 24s\n",
      "1000:\ttest: 0.8495482\tbest: 0.8495482 (1000)\ttotal: 49.6s\tremaining: 13h 44m 54s\n",
      "2000:\ttest: 0.8709490\tbest: 0.8709650 (1999)\ttotal: 1m 40s\tremaining: 13h 52m 56s\n",
      "3000:\ttest: 0.8824064\tbest: 0.8824064 (3000)\ttotal: 2m 35s\tremaining: 14h 19m 15s\n",
      "4000:\ttest: 0.8888940\tbest: 0.8888940 (4000)\ttotal: 3m 29s\tremaining: 14h 28m 6s\n",
      "5000:\ttest: 0.8928870\tbest: 0.8928870 (5000)\ttotal: 4m 24s\tremaining: 14h 36m 46s\n",
      "6000:\ttest: 0.8956606\tbest: 0.8956633 (5999)\ttotal: 5m 16s\tremaining: 14h 32m 59s\n",
      "7000:\ttest: 0.8975928\tbest: 0.8975939 (6999)\ttotal: 6m 9s\tremaining: 14h 33m 46s\n",
      "8000:\ttest: 0.8988986\tbest: 0.8988986 (8000)\ttotal: 7m 1s\tremaining: 14h 31m 30s\n",
      "9000:\ttest: 0.8997847\tbest: 0.8997859 (8995)\ttotal: 7m 57s\tremaining: 14h 35m 23s\n",
      "10000:\ttest: 0.9005707\tbest: 0.9005713 (9999)\ttotal: 8m 49s\tremaining: 14h 33m 37s\n",
      "11000:\ttest: 0.9010702\tbest: 0.9010715 (10999)\ttotal: 9m 43s\tremaining: 14h 34m 21s\n",
      "12000:\ttest: 0.9014994\tbest: 0.9014994 (12000)\ttotal: 10m 34s\tremaining: 14h 30m 57s\n",
      "13000:\ttest: 0.9018129\tbest: 0.9018141 (12986)\ttotal: 11m 22s\tremaining: 14h 24m 1s\n",
      "14000:\ttest: 0.9020769\tbest: 0.9020769 (14000)\ttotal: 12m 13s\tremaining: 14h 20m 53s\n",
      "15000:\ttest: 0.9022142\tbest: 0.9022177 (14978)\ttotal: 13m 2s\tremaining: 14h 16m 5s\n",
      "16000:\ttest: 0.9024094\tbest: 0.9024114 (15993)\ttotal: 13m 52s\tremaining: 14h 12m 45s\n",
      "17000:\ttest: 0.9025510\tbest: 0.9025545 (16996)\ttotal: 14m 42s\tremaining: 14h 10m 17s\n",
      "18000:\ttest: 0.9026318\tbest: 0.9026440 (17888)\ttotal: 15m 33s\tremaining: 14h 8m 44s\n",
      "19000:\ttest: 0.9027663\tbest: 0.9027663 (19000)\ttotal: 16m 24s\tremaining: 14h 6m 50s\n",
      "20000:\ttest: 0.9029152\tbest: 0.9029194 (19997)\ttotal: 17m 14s\tremaining: 14h 5m 8s\n",
      "21000:\ttest: 0.9029757\tbest: 0.9029781 (20989)\ttotal: 18m 4s\tremaining: 14h 2m 46s\n",
      "22000:\ttest: 0.9030134\tbest: 0.9030177 (21606)\ttotal: 18m 57s\tremaining: 14h 2m 24s\n",
      "23000:\ttest: 0.9030414\tbest: 0.9030551 (22373)\ttotal: 19m 44s\tremaining: 13h 58m 51s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9030550693\n",
      "bestIteration = 22373\n",
      "\n",
      "Shrink model to first 22374 iterations.\n",
      "Fold 6 started at Tue Mar 12 01:36:40 2019\n",
      "0:\ttest: 0.5287105\tbest: 0.5287105 (0)\ttotal: 72.2ms\tremaining: 20h 2m 37s\n",
      "1000:\ttest: 0.8466987\tbest: 0.8466987 (1000)\ttotal: 49.3s\tremaining: 13h 40m 11s\n",
      "2000:\ttest: 0.8692602\tbest: 0.8692602 (2000)\ttotal: 1m 41s\tremaining: 14h 5m 4s\n",
      "3000:\ttest: 0.8817301\tbest: 0.8817301 (3000)\ttotal: 2m 37s\tremaining: 14h 33m 3s\n",
      "4000:\ttest: 0.8886260\tbest: 0.8886260 (4000)\ttotal: 3m 28s\tremaining: 14h 23m 23s\n",
      "5000:\ttest: 0.8929816\tbest: 0.8929816 (5000)\ttotal: 4m 22s\tremaining: 14h 30m 18s\n",
      "6000:\ttest: 0.8960007\tbest: 0.8960056 (5999)\ttotal: 5m 15s\tremaining: 14h 31m 19s\n",
      "7000:\ttest: 0.8981127\tbest: 0.8981127 (7000)\ttotal: 6m 9s\tremaining: 14h 32m 41s\n",
      "8000:\ttest: 0.8995921\tbest: 0.8995921 (8000)\ttotal: 7m 2s\tremaining: 14h 32m 53s\n",
      "9000:\ttest: 0.9006553\tbest: 0.9006600 (8994)\ttotal: 7m 56s\tremaining: 14h 34m 42s\n",
      "10000:\ttest: 0.9015763\tbest: 0.9015763 (10000)\ttotal: 8m 49s\tremaining: 14h 33m 27s\n",
      "11000:\ttest: 0.9022402\tbest: 0.9022444 (10997)\ttotal: 9m 43s\tremaining: 14h 33m 43s\n",
      "12000:\ttest: 0.9028266\tbest: 0.9028283 (11997)\ttotal: 10m 36s\tremaining: 14h 33m 16s\n",
      "13000:\ttest: 0.9032375\tbest: 0.9032419 (12994)\ttotal: 11m 28s\tremaining: 14h 31m 12s\n",
      "14000:\ttest: 0.9036127\tbest: 0.9036127 (14000)\ttotal: 12m 21s\tremaining: 14h 30m 14s\n",
      "15000:\ttest: 0.9038800\tbest: 0.9038800 (15000)\ttotal: 13m 13s\tremaining: 14h 28m 14s\n",
      "16000:\ttest: 0.9041391\tbest: 0.9041416 (15959)\ttotal: 14m 4s\tremaining: 14h 25m 26s\n",
      "17000:\ttest: 0.9043409\tbest: 0.9043489 (16934)\ttotal: 14m 56s\tremaining: 14h 23m 46s\n",
      "18000:\ttest: 0.9045597\tbest: 0.9045597 (18000)\ttotal: 15m 46s\tremaining: 14h 20m 42s\n",
      "19000:\ttest: 0.9046781\tbest: 0.9046818 (18986)\ttotal: 16m 38s\tremaining: 14h 19m 32s\n",
      "20000:\ttest: 0.9047221\tbest: 0.9047244 (19974)\ttotal: 17m 32s\tremaining: 14h 19m 29s\n",
      "21000:\ttest: 0.9048756\tbest: 0.9048847 (20922)\ttotal: 18m 25s\tremaining: 14h 18m 53s\n",
      "22000:\ttest: 0.9049262\tbest: 0.9049314 (21536)\ttotal: 19m 17s\tremaining: 14h 17m 47s\n",
      "23000:\ttest: 0.9049579\tbest: 0.9049623 (22995)\ttotal: 20m 10s\tremaining: 14h 17m 11s\n",
      "24000:\ttest: 0.9049993\tbest: 0.9050045 (23940)\ttotal: 21m\tremaining: 14h 14m 33s\n",
      "25000:\ttest: 0.9050056\tbest: 0.9050251 (24392)\ttotal: 21m 52s\tremaining: 14h 13m 5s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9050250692\n",
      "bestIteration = 24392\n",
      "\n",
      "Shrink model to first 24393 iterations.\n",
      "Fold 7 started at Tue Mar 12 02:00:33 2019\n",
      "0:\ttest: 0.5314763\tbest: 0.5314763 (0)\ttotal: 79.1ms\tremaining: 21h 58m 31s\n",
      "1000:\ttest: 0.8390454\tbest: 0.8390454 (1000)\ttotal: 48.9s\tremaining: 13h 33m 43s\n",
      "2000:\ttest: 0.8618221\tbest: 0.8618221 (2000)\ttotal: 1m 38s\tremaining: 13h 42m 27s\n",
      "3000:\ttest: 0.8735320\tbest: 0.8735320 (3000)\ttotal: 2m 29s\tremaining: 13h 47m 51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000:\ttest: 0.8795871\tbest: 0.8795871 (4000)\ttotal: 3m 21s\tremaining: 13h 56m 30s\n",
      "5000:\ttest: 0.8833340\tbest: 0.8833340 (5000)\ttotal: 4m 13s\tremaining: 14h 46s\n",
      "6000:\ttest: 0.8858983\tbest: 0.8858991 (5998)\ttotal: 5m 6s\tremaining: 14h 5m 29s\n",
      "7000:\ttest: 0.8877717\tbest: 0.8877717 (7000)\ttotal: 5m 59s\tremaining: 14h 9m 8s\n",
      "8000:\ttest: 0.8890663\tbest: 0.8890663 (8000)\ttotal: 6m 50s\tremaining: 14h 7m 58s\n",
      "9000:\ttest: 0.8901304\tbest: 0.8901380 (8989)\ttotal: 7m 42s\tremaining: 14h 8m 55s\n",
      "10000:\ttest: 0.8908623\tbest: 0.8908637 (9998)\ttotal: 8m 34s\tremaining: 14h 8m 4s\n",
      "11000:\ttest: 0.8915001\tbest: 0.8915001 (11000)\ttotal: 9m 24s\tremaining: 14h 5m 17s\n",
      "12000:\ttest: 0.8919288\tbest: 0.8919294 (11993)\ttotal: 10m 15s\tremaining: 14h 4m 19s\n",
      "13000:\ttest: 0.8923877\tbest: 0.8923877 (13000)\ttotal: 11m 8s\tremaining: 14h 5m 24s\n",
      "14000:\ttest: 0.8927204\tbest: 0.8927207 (13995)\ttotal: 12m\tremaining: 14h 5m 15s\n",
      "15000:\ttest: 0.8929376\tbest: 0.8929413 (14988)\ttotal: 12m 51s\tremaining: 14h 3m 57s\n",
      "16000:\ttest: 0.8931408\tbest: 0.8931445 (15999)\ttotal: 13m 42s\tremaining: 14h 3m 20s\n",
      "17000:\ttest: 0.8933672\tbest: 0.8933672 (17000)\ttotal: 14m 33s\tremaining: 14h 1m 52s\n",
      "18000:\ttest: 0.8935819\tbest: 0.8935830 (17998)\ttotal: 15m 25s\tremaining: 14h 1m 27s\n",
      "19000:\ttest: 0.8937275\tbest: 0.8937285 (18992)\ttotal: 16m 19s\tremaining: 14h 3m 3s\n",
      "20000:\ttest: 0.8938164\tbest: 0.8938264 (19851)\ttotal: 17m 11s\tremaining: 14h 2m 38s\n",
      "21000:\ttest: 0.8939426\tbest: 0.8939457 (20919)\ttotal: 18m 4s\tremaining: 14h 2m 34s\n",
      "22000:\ttest: 0.8939682\tbest: 0.8939829 (21613)\ttotal: 18m 53s\tremaining: 14h 4s\n",
      "23000:\ttest: 0.8940570\tbest: 0.8940577 (22999)\ttotal: 19m 43s\tremaining: 13h 57m 34s\n",
      "24000:\ttest: 0.8941092\tbest: 0.8941127 (23990)\ttotal: 20m 32s\tremaining: 13h 55m 10s\n",
      "25000:\ttest: 0.8940946\tbest: 0.8941174 (24327)\ttotal: 21m 20s\tremaining: 13h 52m 31s\n",
      "26000:\ttest: 0.8940919\tbest: 0.8941203 (25204)\ttotal: 22m 10s\tremaining: 13h 50m 32s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8941203101\n",
      "bestIteration = 25204\n",
      "\n",
      "Shrink model to first 25205 iterations.\n",
      "Fold 8 started at Tue Mar 12 02:24:43 2019\n",
      "0:\ttest: 0.5534965\tbest: 0.5534965 (0)\ttotal: 82.7ms\tremaining: 22h 58m 42s\n",
      "1000:\ttest: 0.8403188\tbest: 0.8403188 (1000)\ttotal: 50.3s\tremaining: 13h 56m 33s\n",
      "2000:\ttest: 0.8599380\tbest: 0.8599380 (2000)\ttotal: 1m 44s\tremaining: 14h 26m\n",
      "3000:\ttest: 0.8718943\tbest: 0.8718943 (3000)\ttotal: 2m 37s\tremaining: 14h 32m 9s\n",
      "4000:\ttest: 0.8789563\tbest: 0.8789563 (4000)\ttotal: 3m 29s\tremaining: 14h 30m 1s\n",
      "5000:\ttest: 0.8837061\tbest: 0.8837061 (5000)\ttotal: 4m 22s\tremaining: 14h 30m 41s\n",
      "6000:\ttest: 0.8867257\tbest: 0.8867257 (6000)\ttotal: 5m 18s\tremaining: 14h 38m 19s\n",
      "7000:\ttest: 0.8891297\tbest: 0.8891297 (7000)\ttotal: 6m 11s\tremaining: 14h 37m 3s\n",
      "8000:\ttest: 0.8906373\tbest: 0.8906387 (7999)\ttotal: 7m 4s\tremaining: 14h 37m 55s\n",
      "9000:\ttest: 0.8919378\tbest: 0.8919378 (9000)\ttotal: 7m 56s\tremaining: 14h 33m 44s\n",
      "10000:\ttest: 0.8928189\tbest: 0.8928220 (9994)\ttotal: 8m 48s\tremaining: 14h 32m 20s\n",
      "11000:\ttest: 0.8935093\tbest: 0.8935122 (10989)\ttotal: 9m 40s\tremaining: 14h 29m 12s\n",
      "12000:\ttest: 0.8939897\tbest: 0.8939963 (11997)\ttotal: 10m 32s\tremaining: 14h 28m 18s\n",
      "13000:\ttest: 0.8944232\tbest: 0.8944254 (12995)\ttotal: 11m 25s\tremaining: 14h 27m 41s\n",
      "14000:\ttest: 0.8948111\tbest: 0.8948111 (14000)\ttotal: 12m 19s\tremaining: 14h 27m 40s\n",
      "15000:\ttest: 0.8950759\tbest: 0.8950801 (14991)\ttotal: 13m 10s\tremaining: 14h 24m 44s\n",
      "16000:\ttest: 0.8953291\tbest: 0.8953298 (15999)\ttotal: 14m 5s\tremaining: 14h 26m 20s\n",
      "17000:\ttest: 0.8955106\tbest: 0.8955170 (16907)\ttotal: 14m 55s\tremaining: 14h 23m 3s\n",
      "18000:\ttest: 0.8955951\tbest: 0.8955954 (17998)\ttotal: 15m 48s\tremaining: 14h 22m 35s\n",
      "19000:\ttest: 0.8956829\tbest: 0.8957025 (18625)\ttotal: 16m 43s\tremaining: 14h 23m 44s\n",
      "20000:\ttest: 0.8957945\tbest: 0.8957975 (19934)\ttotal: 17m 36s\tremaining: 14h 22m 43s\n",
      "21000:\ttest: 0.8959346\tbest: 0.8959351 (20999)\ttotal: 18m 28s\tremaining: 14h 20m 51s\n",
      "22000:\ttest: 0.8960231\tbest: 0.8960393 (21946)\ttotal: 19m 21s\tremaining: 14h 20m 52s\n",
      "23000:\ttest: 0.8961590\tbest: 0.8961657 (22843)\ttotal: 20m 13s\tremaining: 14h 19m 26s\n",
      "24000:\ttest: 0.8962467\tbest: 0.8962507 (23985)\ttotal: 21m 7s\tremaining: 14h 18m 49s\n",
      "25000:\ttest: 0.8962745\tbest: 0.8962776 (24998)\ttotal: 21m 58s\tremaining: 14h 17m 1s\n",
      "26000:\ttest: 0.8962944\tbest: 0.8963055 (25952)\ttotal: 22m 52s\tremaining: 14h 16m 41s\n",
      "27000:\ttest: 0.8963198\tbest: 0.8963537 (26402)\ttotal: 23m 44s\tremaining: 14h 15m 31s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8963536515\n",
      "bestIteration = 26402\n",
      "\n",
      "Shrink model to first 26403 iterations.\n",
      "Fold 9 started at Tue Mar 12 02:50:45 2019\n",
      "0:\ttest: 0.5678622\tbest: 0.5678622 (0)\ttotal: 76.2ms\tremaining: 21h 10m 21s\n",
      "1000:\ttest: 0.8481245\tbest: 0.8481245 (1000)\ttotal: 50.9s\tremaining: 14h 7m 1s\n",
      "2000:\ttest: 0.8664858\tbest: 0.8664858 (2000)\ttotal: 1m 44s\tremaining: 14h 29m 8s\n",
      "3000:\ttest: 0.8770064\tbest: 0.8770064 (3000)\ttotal: 2m 38s\tremaining: 14h 36m 13s\n",
      "4000:\ttest: 0.8835132\tbest: 0.8835132 (4000)\ttotal: 3m 32s\tremaining: 14h 42m 41s\n",
      "5000:\ttest: 0.8874944\tbest: 0.8874944 (5000)\ttotal: 4m 28s\tremaining: 14h 51m 9s\n",
      "6000:\ttest: 0.8900109\tbest: 0.8900109 (6000)\ttotal: 5m 25s\tremaining: 14h 58m 55s\n",
      "7000:\ttest: 0.8918311\tbest: 0.8918311 (7000)\ttotal: 6m 21s\tremaining: 15h 2m 2s\n",
      "8000:\ttest: 0.8931593\tbest: 0.8931614 (7998)\ttotal: 7m 17s\tremaining: 15h 4m 20s\n",
      "9000:\ttest: 0.8941316\tbest: 0.8941321 (8998)\ttotal: 8m 12s\tremaining: 15h 4m 11s\n",
      "10000:\ttest: 0.8948928\tbest: 0.8948937 (9998)\ttotal: 9m 6s\tremaining: 15h 1m\n",
      "11000:\ttest: 0.8954193\tbest: 0.8954200 (10998)\ttotal: 9m 59s\tremaining: 14h 58m 15s\n",
      "12000:\ttest: 0.8958141\tbest: 0.8958141 (12000)\ttotal: 10m 50s\tremaining: 14h 52m 52s\n",
      "13000:\ttest: 0.8960717\tbest: 0.8960719 (12994)\ttotal: 11m 43s\tremaining: 14h 49m 41s\n",
      "14000:\ttest: 0.8963545\tbest: 0.8963566 (13985)\ttotal: 12m 35s\tremaining: 14h 46m 29s\n",
      "15000:\ttest: 0.8964951\tbest: 0.8965103 (14967)\ttotal: 13m 27s\tremaining: 14h 43m 14s\n",
      "16000:\ttest: 0.8966523\tbest: 0.8966630 (15966)\ttotal: 14m 17s\tremaining: 14h 39m 12s\n",
      "17000:\ttest: 0.8967421\tbest: 0.8967442 (16987)\ttotal: 15m 8s\tremaining: 14h 35m 42s\n",
      "18000:\ttest: 0.8968393\tbest: 0.8968485 (17979)\ttotal: 15m 59s\tremaining: 14h 32m 42s\n",
      "19000:\ttest: 0.8969002\tbest: 0.8969028 (18988)\ttotal: 16m 52s\tremaining: 14h 31m 3s\n",
      "20000:\ttest: 0.8969437\tbest: 0.8969437 (20000)\ttotal: 17m 44s\tremaining: 14h 29m 19s\n",
      "21000:\ttest: 0.8969896\tbest: 0.8969962 (20903)\ttotal: 18m 36s\tremaining: 14h 27m 7s\n",
      "22000:\ttest: 0.8970425\tbest: 0.8970532 (21840)\ttotal: 19m 29s\tremaining: 14h 26m 46s\n",
      "23000:\ttest: 0.8970444\tbest: 0.8970637 (22134)\ttotal: 20m 21s\tremaining: 14h 25m 3s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8970636582\n",
      "bestIteration = 22134\n",
      "\n",
      "Shrink model to first 22135 iterations.\n",
      "CV mean score: 0.9003, std: 0.0042.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_cat, prediction_cat, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, model_type='cat', plot_feature_importance=False)\n",
    "oof.append(oof_cat)\n",
    "preds.append(prediction_cat)\n",
    "\n",
    "#max-depth=4, with 2 cat vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_cat_1_10_3', oof)\n",
    "np.save('../cache/preds_new_quant_cat_1_10_3', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Mar 12 06:43:46 2019\n",
      "0:\ttest: 0.5434013\tbest: 0.5434013 (0)\ttotal: 70.5ms\tremaining: 19h 34m 18s\n",
      "1000:\ttest: 0.8489139\tbest: 0.8489139 (1000)\ttotal: 41.9s\tremaining: 11h 36m 32s\n",
      "2000:\ttest: 0.8695598\tbest: 0.8695598 (2000)\ttotal: 1m 22s\tremaining: 11h 21m 53s\n",
      "3000:\ttest: 0.8798604\tbest: 0.8798604 (3000)\ttotal: 2m 2s\tremaining: 11h 20m 12s\n",
      "4000:\ttest: 0.8861244\tbest: 0.8861281 (3999)\ttotal: 2m 44s\tremaining: 11h 21m 48s\n",
      "5000:\ttest: 0.8898428\tbest: 0.8898492 (4993)\ttotal: 3m 32s\tremaining: 11h 45m 4s\n",
      "6000:\ttest: 0.8925020\tbest: 0.8925020 (6000)\ttotal: 4m 12s\tremaining: 11h 37m 25s\n",
      "7000:\ttest: 0.8943744\tbest: 0.8943744 (7000)\ttotal: 4m 52s\tremaining: 11h 30m 47s\n",
      "8000:\ttest: 0.8957074\tbest: 0.8957074 (7999)\ttotal: 5m 34s\tremaining: 11h 30m 58s\n",
      "9000:\ttest: 0.8966161\tbest: 0.8966174 (8998)\ttotal: 6m 19s\tremaining: 11h 35m 29s\n",
      "10000:\ttest: 0.8972607\tbest: 0.8972607 (10000)\ttotal: 7m 4s\tremaining: 11h 39m 59s\n",
      "11000:\ttest: 0.8977669\tbest: 0.8977700 (10987)\ttotal: 7m 50s\tremaining: 11h 44m 55s\n",
      "12000:\ttest: 0.8980858\tbest: 0.8980947 (11903)\ttotal: 8m 37s\tremaining: 11h 50m 43s\n",
      "13000:\ttest: 0.8985120\tbest: 0.8985227 (12986)\ttotal: 9m 25s\tremaining: 11h 55m 5s\n",
      "14000:\ttest: 0.8988387\tbest: 0.8988433 (13982)\ttotal: 10m 10s\tremaining: 11h 56m 5s\n",
      "15000:\ttest: 0.8990919\tbest: 0.8990949 (14984)\ttotal: 10m 57s\tremaining: 11h 59m 12s\n",
      "16000:\ttest: 0.8992147\tbest: 0.8992299 (15945)\ttotal: 11m 42s\tremaining: 11h 59m 36s\n",
      "17000:\ttest: 0.8993970\tbest: 0.8994071 (16933)\ttotal: 12m 27s\tremaining: 12h 19s\n",
      "18000:\ttest: 0.8995609\tbest: 0.8995609 (17929)\ttotal: 13m 14s\tremaining: 12h 2m 11s\n",
      "19000:\ttest: 0.8996325\tbest: 0.8996325 (19000)\ttotal: 13m 59s\tremaining: 12h 2m 30s\n",
      "20000:\ttest: 0.8997162\tbest: 0.8997261 (19892)\ttotal: 14m 46s\tremaining: 12h 4m 5s\n",
      "21000:\ttest: 0.8998202\tbest: 0.8998214 (20891)\ttotal: 15m 32s\tremaining: 12h 4m 41s\n",
      "22000:\ttest: 0.8998469\tbest: 0.8998502 (21949)\ttotal: 16m 17s\tremaining: 12h 4m 14s\n",
      "23000:\ttest: 0.8999365\tbest: 0.8999454 (22921)\ttotal: 17m 5s\tremaining: 12h 5m 47s\n",
      "24000:\ttest: 0.8999665\tbest: 0.8999683 (23447)\ttotal: 17m 52s\tremaining: 12h 6m 41s\n",
      "25000:\ttest: 0.8999652\tbest: 0.8999748 (24208)\ttotal: 18m 39s\tremaining: 12h 7m 44s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8999748354\n",
      "bestIteration = 24208\n",
      "\n",
      "Shrink model to first 24209 iterations.\n",
      "Fold 1 started at Tue Mar 12 07:04:14 2019\n",
      "0:\ttest: 0.5490914\tbest: 0.5490914 (0)\ttotal: 63.4ms\tremaining: 17h 36m 26s\n",
      "1000:\ttest: 0.8443046\tbest: 0.8443599 (998)\ttotal: 47.2s\tremaining: 13h 4m 58s\n",
      "2000:\ttest: 0.8651015\tbest: 0.8651056 (1999)\ttotal: 1m 38s\tremaining: 13h 38m 28s\n",
      "3000:\ttest: 0.8762232\tbest: 0.8762232 (3000)\ttotal: 2m 29s\tremaining: 13h 47m 58s\n",
      "4000:\ttest: 0.8825299\tbest: 0.8825299 (4000)\ttotal: 3m 19s\tremaining: 13h 46m 57s\n",
      "5000:\ttest: 0.8868302\tbest: 0.8868302 (5000)\ttotal: 4m 10s\tremaining: 13h 50m 35s\n",
      "6000:\ttest: 0.8897270\tbest: 0.8897270 (6000)\ttotal: 5m 2s\tremaining: 13h 54m 15s\n",
      "7000:\ttest: 0.8917465\tbest: 0.8917465 (7000)\ttotal: 5m 53s\tremaining: 13h 56m 9s\n",
      "8000:\ttest: 0.8932537\tbest: 0.8932537 (8000)\ttotal: 6m 46s\tremaining: 13h 59m 10s\n",
      "9000:\ttest: 0.8942881\tbest: 0.8942887 (8987)\ttotal: 7m 37s\tremaining: 13h 59m 42s\n",
      "10000:\ttest: 0.8950393\tbest: 0.8950393 (10000)\ttotal: 8m 29s\tremaining: 13h 59m 50s\n",
      "11000:\ttest: 0.8956041\tbest: 0.8956058 (10996)\ttotal: 9m 20s\tremaining: 14h 21s\n",
      "12000:\ttest: 0.8960472\tbest: 0.8960581 (11972)\ttotal: 10m 12s\tremaining: 13h 59m 55s\n",
      "13000:\ttest: 0.8963549\tbest: 0.8963590 (12970)\ttotal: 11m 3s\tremaining: 13h 59m 10s\n",
      "14000:\ttest: 0.8966350\tbest: 0.8966359 (13999)\ttotal: 11m 52s\tremaining: 13h 56m 27s\n",
      "15000:\ttest: 0.8968736\tbest: 0.8968736 (15000)\ttotal: 12m 42s\tremaining: 13h 54m 44s\n",
      "16000:\ttest: 0.8970140\tbest: 0.8970156 (15986)\ttotal: 13m 35s\tremaining: 13h 55m 55s\n",
      "17000:\ttest: 0.8971947\tbest: 0.8971977 (16994)\ttotal: 14m 27s\tremaining: 13h 56m 11s\n",
      "18000:\ttest: 0.8972791\tbest: 0.8972858 (17979)\ttotal: 15m 19s\tremaining: 13h 56m 10s\n",
      "19000:\ttest: 0.8973430\tbest: 0.8973624 (18772)\ttotal: 16m 9s\tremaining: 13h 54m 27s\n",
      "20000:\ttest: 0.8974609\tbest: 0.8974618 (19999)\ttotal: 17m 3s\tremaining: 13h 55m 47s\n",
      "21000:\ttest: 0.8975229\tbest: 0.8975303 (20830)\ttotal: 17m 55s\tremaining: 13h 55m 50s\n",
      "22000:\ttest: 0.8975770\tbest: 0.8975846 (21672)\ttotal: 18m 46s\tremaining: 13h 54m 36s\n",
      "23000:\ttest: 0.8975978\tbest: 0.8976225 (22511)\ttotal: 19m 38s\tremaining: 13h 54m 4s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8976224707\n",
      "bestIteration = 22511\n",
      "\n",
      "Shrink model to first 22512 iterations.\n",
      "Fold 2 started at Tue Mar 12 07:25:51 2019\n",
      "0:\ttest: 0.5185592\tbest: 0.5185592 (0)\ttotal: 76.3ms\tremaining: 21h 10m 54s\n",
      "1000:\ttest: 0.8474391\tbest: 0.8474697 (998)\ttotal: 47.3s\tremaining: 13h 6m 38s\n",
      "2000:\ttest: 0.8685388\tbest: 0.8685493 (1999)\ttotal: 1m 37s\tremaining: 13h 29m 35s\n",
      "3000:\ttest: 0.8793474\tbest: 0.8793474 (3000)\ttotal: 2m 30s\tremaining: 13h 50m 52s\n",
      "4000:\ttest: 0.8852718\tbest: 0.8852718 (3999)\ttotal: 3m 23s\tremaining: 14h 2m 44s\n",
      "5000:\ttest: 0.8890230\tbest: 0.8890230 (5000)\ttotal: 4m 17s\tremaining: 14h 14m 47s\n",
      "6000:\ttest: 0.8913523\tbest: 0.8913523 (6000)\ttotal: 5m 12s\tremaining: 14h 21m 40s\n",
      "7000:\ttest: 0.8930691\tbest: 0.8930691 (7000)\ttotal: 6m 3s\tremaining: 14h 18m 16s\n",
      "8000:\ttest: 0.8941120\tbest: 0.8941123 (7999)\ttotal: 6m 56s\tremaining: 14h 20m 34s\n",
      "9000:\ttest: 0.8949772\tbest: 0.8949810 (8998)\ttotal: 7m 46s\tremaining: 14h 16m 40s\n",
      "10000:\ttest: 0.8956013\tbest: 0.8956041 (9994)\ttotal: 8m 36s\tremaining: 14h 12m 24s\n",
      "11000:\ttest: 0.8960581\tbest: 0.8960703 (10956)\ttotal: 9m 28s\tremaining: 14h 12m 12s\n",
      "12000:\ttest: 0.8963378\tbest: 0.8963404 (11899)\ttotal: 10m 29s\tremaining: 14h 23m 28s\n",
      "13000:\ttest: 0.8965724\tbest: 0.8965785 (12986)\ttotal: 11m 28s\tremaining: 14h 31m 8s\n",
      "14000:\ttest: 0.8968270\tbest: 0.8968270 (14000)\ttotal: 12m 31s\tremaining: 14h 41m 35s\n",
      "15000:\ttest: 0.8970182\tbest: 0.8970221 (14994)\ttotal: 13m 23s\tremaining: 14h 38m 58s\n",
      "16000:\ttest: 0.8971593\tbest: 0.8971668 (15964)\ttotal: 14m 15s\tremaining: 14h 36m 41s\n",
      "17000:\ttest: 0.8972184\tbest: 0.8972239 (16874)\ttotal: 15m 5s\tremaining: 14h 32m 33s\n",
      "18000:\ttest: 0.8973122\tbest: 0.8973195 (17625)\ttotal: 15m 54s\tremaining: 14h 27m 43s\n",
      "19000:\ttest: 0.8973711\tbest: 0.8973727 (18956)\ttotal: 16m 44s\tremaining: 14h 24m 11s\n",
      "20000:\ttest: 0.8974059\tbest: 0.8974061 (19999)\ttotal: 17m 34s\tremaining: 14h 20m 54s\n",
      "21000:\ttest: 0.8974249\tbest: 0.8974317 (20825)\ttotal: 18m 24s\tremaining: 14h 17m 59s\n",
      "22000:\ttest: 0.8974654\tbest: 0.8974865 (21636)\ttotal: 19m 14s\tremaining: 14h 15m 37s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8974864975\n",
      "bestIteration = 21636\n",
      "\n",
      "Shrink model to first 21637 iterations.\n",
      "Fold 3 started at Tue Mar 12 07:47:03 2019\n",
      "0:\ttest: 0.5313023\tbest: 0.5313023 (0)\ttotal: 75.8ms\tremaining: 21h 4m 8s\n",
      "1000:\ttest: 0.8473886\tbest: 0.8474057 (999)\ttotal: 48.4s\tremaining: 13h 24m 55s\n",
      "2000:\ttest: 0.8703200\tbest: 0.8703200 (2000)\ttotal: 1m 37s\tremaining: 13h 34m 31s\n",
      "3000:\ttest: 0.8812804\tbest: 0.8812804 (3000)\ttotal: 2m 30s\tremaining: 13h 54m 43s\n",
      "4000:\ttest: 0.8872500\tbest: 0.8872500 (4000)\ttotal: 3m 22s\tremaining: 14h 1m 51s\n",
      "5000:\ttest: 0.8911171\tbest: 0.8911171 (5000)\ttotal: 4m 20s\tremaining: 14h 22m 50s\n",
      "6000:\ttest: 0.8939365\tbest: 0.8939365 (6000)\ttotal: 5m 13s\tremaining: 14h 24m 12s\n",
      "7000:\ttest: 0.8958140\tbest: 0.8958140 (7000)\ttotal: 6m 6s\tremaining: 14h 25m 13s\n",
      "8000:\ttest: 0.8972693\tbest: 0.8972694 (7999)\ttotal: 7m 2s\tremaining: 14h 32m 33s\n",
      "9000:\ttest: 0.8982793\tbest: 0.8982793 (9000)\ttotal: 7m 54s\tremaining: 14h 29m 58s\n",
      "10000:\ttest: 0.8989263\tbest: 0.8989288 (9998)\ttotal: 8m 44s\tremaining: 14h 25m 11s\n",
      "11000:\ttest: 0.8995033\tbest: 0.8995054 (10999)\ttotal: 9m 35s\tremaining: 14h 22m 57s\n",
      "12000:\ttest: 0.8998911\tbest: 0.8998911 (12000)\ttotal: 10m 26s\tremaining: 14h 19m 47s\n",
      "13000:\ttest: 0.9001949\tbest: 0.9001954 (12973)\ttotal: 11m 17s\tremaining: 14h 16m 55s\n",
      "14000:\ttest: 0.9004885\tbest: 0.9004971 (13994)\ttotal: 12m 7s\tremaining: 14h 14m 14s\n",
      "15000:\ttest: 0.9007071\tbest: 0.9007098 (14968)\ttotal: 12m 57s\tremaining: 14h 10m 56s\n",
      "16000:\ttest: 0.9008034\tbest: 0.9008238 (15603)\ttotal: 13m 49s\tremaining: 14h 9m 53s\n",
      "17000:\ttest: 0.9009813\tbest: 0.9009830 (16992)\ttotal: 14m 38s\tremaining: 14h 6m 54s\n",
      "18000:\ttest: 0.9010799\tbest: 0.9010899 (17710)\ttotal: 15m 31s\tremaining: 14h 6m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19000:\ttest: 0.9011159\tbest: 0.9011347 (18872)\ttotal: 16m 21s\tremaining: 14h 4m 22s\n",
      "20000:\ttest: 0.9011301\tbest: 0.9011549 (19504)\ttotal: 17m 12s\tremaining: 14h 3m 24s\n",
      "21000:\ttest: 0.9012174\tbest: 0.9012177 (20999)\ttotal: 18m 3s\tremaining: 14h 2m 11s\n",
      "22000:\ttest: 0.9012759\tbest: 0.9012869 (21813)\ttotal: 18m 53s\tremaining: 14h 4s\n",
      "23000:\ttest: 0.9012927\tbest: 0.9013204 (22571)\ttotal: 19m 43s\tremaining: 13h 57m 38s\n",
      "24000:\ttest: 0.9013609\tbest: 0.9013712 (23944)\ttotal: 20m 33s\tremaining: 13h 55m 59s\n",
      "25000:\ttest: 0.9013730\tbest: 0.9013945 (24227)\ttotal: 21m 24s\tremaining: 13h 54m 49s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9013944729\n",
      "bestIteration = 24227\n",
      "\n",
      "Shrink model to first 24228 iterations.\n",
      "Fold 4 started at Tue Mar 12 08:10:19 2019\n",
      "0:\ttest: 0.5239677\tbest: 0.5239677 (0)\ttotal: 66.8ms\tremaining: 18h 32m 43s\n",
      "1000:\ttest: 0.8481544\tbest: 0.8481544 (1000)\ttotal: 51.7s\tremaining: 14h 20m 32s\n",
      "2000:\ttest: 0.8707835\tbest: 0.8707835 (2000)\ttotal: 1m 41s\tremaining: 13h 59m 38s\n",
      "3000:\ttest: 0.8840805\tbest: 0.8840805 (3000)\ttotal: 2m 31s\tremaining: 13h 56m 14s\n",
      "4000:\ttest: 0.8912514\tbest: 0.8912514 (4000)\ttotal: 3m 21s\tremaining: 13h 55m 28s\n",
      "5000:\ttest: 0.8959024\tbest: 0.8959024 (5000)\ttotal: 4m 9s\tremaining: 13h 48m 59s\n",
      "6000:\ttest: 0.8989998\tbest: 0.8989998 (6000)\ttotal: 4m 59s\tremaining: 13h 47m 45s\n",
      "7000:\ttest: 0.9010826\tbest: 0.9010830 (6998)\ttotal: 5m 48s\tremaining: 13h 42m 43s\n",
      "8000:\ttest: 0.9026373\tbest: 0.9026373 (8000)\ttotal: 6m 39s\tremaining: 13h 44m 36s\n",
      "9000:\ttest: 0.9036398\tbest: 0.9036462 (8974)\ttotal: 7m 27s\tremaining: 13h 40m 45s\n",
      "10000:\ttest: 0.9044949\tbest: 0.9044949 (10000)\ttotal: 8m 16s\tremaining: 13h 38m 34s\n",
      "11000:\ttest: 0.9051062\tbest: 0.9051075 (10987)\ttotal: 9m 6s\tremaining: 13h 38m 18s\n",
      "12000:\ttest: 0.9055722\tbest: 0.9055759 (11994)\ttotal: 9m 53s\tremaining: 13h 34m 54s\n",
      "13000:\ttest: 0.9059523\tbest: 0.9059606 (12983)\ttotal: 10m 40s\tremaining: 13h 30m 53s\n",
      "14000:\ttest: 0.9062658\tbest: 0.9062658 (14000)\ttotal: 11m 30s\tremaining: 13h 30m 39s\n",
      "15000:\ttest: 0.9065602\tbest: 0.9065686 (14975)\ttotal: 12m 20s\tremaining: 13h 30m\n",
      "16000:\ttest: 0.9067602\tbest: 0.9067687 (15968)\ttotal: 13m 10s\tremaining: 13h 29m 59s\n",
      "17000:\ttest: 0.9069364\tbest: 0.9069409 (16995)\ttotal: 13m 58s\tremaining: 13h 28m 29s\n",
      "18000:\ttest: 0.9070500\tbest: 0.9070614 (17974)\ttotal: 14m 46s\tremaining: 13h 25m 40s\n",
      "19000:\ttest: 0.9071562\tbest: 0.9071593 (18980)\ttotal: 15m 36s\tremaining: 13h 25m 24s\n",
      "20000:\ttest: 0.9072862\tbest: 0.9072879 (19996)\ttotal: 16m 24s\tremaining: 13h 24m 19s\n",
      "21000:\ttest: 0.9073142\tbest: 0.9073171 (20618)\ttotal: 17m 12s\tremaining: 13h 22m 14s\n",
      "22000:\ttest: 0.9074328\tbest: 0.9074328 (22000)\ttotal: 18m 8s\tremaining: 13h 26m 7s\n",
      "23000:\ttest: 0.9075155\tbest: 0.9075269 (22920)\ttotal: 18m 59s\tremaining: 13h 27m 1s\n",
      "24000:\ttest: 0.9075254\tbest: 0.9075419 (23384)\ttotal: 19m 51s\tremaining: 13h 27m 16s\n",
      "25000:\ttest: 0.9075721\tbest: 0.9075770 (24962)\ttotal: 20m 41s\tremaining: 13h 26m 56s\n",
      "26000:\ttest: 0.9075807\tbest: 0.9076125 (25234)\ttotal: 21m 34s\tremaining: 13h 28m 3s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9076124657\n",
      "bestIteration = 25234\n",
      "\n",
      "Shrink model to first 25235 iterations.\n",
      "Fold 5 started at Tue Mar 12 08:33:47 2019\n",
      "0:\ttest: 0.5275758\tbest: 0.5275758 (0)\ttotal: 69.7ms\tremaining: 19h 21m 7s\n",
      "1000:\ttest: 0.8476012\tbest: 0.8476208 (998)\ttotal: 49.2s\tremaining: 13h 37m 53s\n",
      "2000:\ttest: 0.8686383\tbest: 0.8686383 (2000)\ttotal: 1m 41s\tremaining: 14h 1m 25s\n",
      "3000:\ttest: 0.8805242\tbest: 0.8805242 (3000)\ttotal: 2m 34s\tremaining: 14h 15m 24s\n",
      "4000:\ttest: 0.8871665\tbest: 0.8871665 (4000)\ttotal: 3m 34s\tremaining: 14h 51m 58s\n",
      "5000:\ttest: 0.8914828\tbest: 0.8914828 (5000)\ttotal: 4m 36s\tremaining: 15h 17m 8s\n",
      "6000:\ttest: 0.8943310\tbest: 0.8943352 (5999)\ttotal: 5m 32s\tremaining: 15h 18m 7s\n",
      "7000:\ttest: 0.8961734\tbest: 0.8961742 (6999)\ttotal: 6m 26s\tremaining: 15h 13m 36s\n",
      "8000:\ttest: 0.8976226\tbest: 0.8976226 (8000)\ttotal: 7m 28s\tremaining: 15h 27m 17s\n",
      "9000:\ttest: 0.8985411\tbest: 0.8985411 (9000)\ttotal: 8m 22s\tremaining: 15h 21m 55s\n",
      "10000:\ttest: 0.8994444\tbest: 0.8994444 (10000)\ttotal: 9m 17s\tremaining: 15h 20m 18s\n",
      "11000:\ttest: 0.8999423\tbest: 0.8999431 (10995)\ttotal: 10m 10s\tremaining: 15h 15m 4s\n",
      "12000:\ttest: 0.9004488\tbest: 0.9004488 (12000)\ttotal: 11m 4s\tremaining: 15h 11m 46s\n",
      "13000:\ttest: 0.9007491\tbest: 0.9007514 (12987)\ttotal: 11m 56s\tremaining: 15h 6m 2s\n",
      "14000:\ttest: 0.9009958\tbest: 0.9009959 (13998)\ttotal: 12m 50s\tremaining: 15h 4m 21s\n",
      "15000:\ttest: 0.9011539\tbest: 0.9011539 (15000)\ttotal: 13m 41s\tremaining: 14h 59m 18s\n",
      "16000:\ttest: 0.9013746\tbest: 0.9013777 (15993)\ttotal: 14m 35s\tremaining: 14h 57m 13s\n",
      "17000:\ttest: 0.9015167\tbest: 0.9015203 (16995)\ttotal: 15m 28s\tremaining: 14h 55m 7s\n",
      "18000:\ttest: 0.9016382\tbest: 0.9016382 (18000)\ttotal: 16m 24s\tremaining: 14h 54m 40s\n",
      "19000:\ttest: 0.9017266\tbest: 0.9017274 (18995)\ttotal: 17m 18s\tremaining: 14h 53m 36s\n",
      "20000:\ttest: 0.9018584\tbest: 0.9018597 (19990)\ttotal: 18m 13s\tremaining: 14h 52m 59s\n",
      "21000:\ttest: 0.9019339\tbest: 0.9019413 (20958)\ttotal: 19m 9s\tremaining: 14h 53m 20s\n",
      "22000:\ttest: 0.9020179\tbest: 0.9020306 (21760)\ttotal: 20m 6s\tremaining: 14h 53m 48s\n",
      "23000:\ttest: 0.9020857\tbest: 0.9020897 (22798)\ttotal: 21m 1s\tremaining: 14h 53m 23s\n",
      "24000:\ttest: 0.9021378\tbest: 0.9021420 (23802)\ttotal: 21m 57s\tremaining: 14h 53m 14s\n",
      "25000:\ttest: 0.9021786\tbest: 0.9021826 (24996)\ttotal: 22m 53s\tremaining: 14h 52m 49s\n",
      "26000:\ttest: 0.9022196\tbest: 0.9022319 (25940)\ttotal: 23m 49s\tremaining: 14h 52m 23s\n",
      "27000:\ttest: 0.9022952\tbest: 0.9022967 (26958)\ttotal: 24m 42s\tremaining: 14h 50m 29s\n",
      "28000:\ttest: 0.9023461\tbest: 0.9023474 (27740)\ttotal: 25m 33s\tremaining: 14h 47m 20s\n",
      "29000:\ttest: 0.9024136\tbest: 0.9024172 (28983)\ttotal: 26m 25s\tremaining: 14h 44m 32s\n",
      "30000:\ttest: 0.9024551\tbest: 0.9024690 (29638)\ttotal: 27m 19s\tremaining: 14h 43m 28s\n",
      "31000:\ttest: 0.9024733\tbest: 0.9024825 (30954)\ttotal: 28m 13s\tremaining: 14h 42m 2s\n",
      "32000:\ttest: 0.9025019\tbest: 0.9025029 (31998)\ttotal: 29m 5s\tremaining: 14h 40m 6s\n",
      "33000:\ttest: 0.9025216\tbest: 0.9025241 (32822)\ttotal: 29m 58s\tremaining: 14h 38m 14s\n",
      "34000:\ttest: 0.9025010\tbest: 0.9025365 (33509)\ttotal: 30m 56s\tremaining: 14h 38m 55s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9025364561\n",
      "bestIteration = 33509\n",
      "\n",
      "Shrink model to first 33510 iterations.\n",
      "Fold 6 started at Tue Mar 12 09:07:54 2019\n",
      "0:\ttest: 0.5287105\tbest: 0.5287105 (0)\ttotal: 75ms\tremaining: 20h 49m 35s\n",
      "1000:\ttest: 0.8418395\tbest: 0.8418395 (1000)\ttotal: 49.3s\tremaining: 13h 39m 31s\n",
      "2000:\ttest: 0.8663503\tbest: 0.8663503 (2000)\ttotal: 1m 41s\tremaining: 14h 56s\n",
      "3000:\ttest: 0.8787853\tbest: 0.8787853 (3000)\ttotal: 2m 34s\tremaining: 14h 14m 12s\n",
      "4000:\ttest: 0.8862304\tbest: 0.8862305 (3999)\ttotal: 3m 29s\tremaining: 14h 30m 57s\n",
      "5000:\ttest: 0.8906955\tbest: 0.8906955 (5000)\ttotal: 4m 29s\tremaining: 14h 52m 2s\n",
      "6000:\ttest: 0.8938424\tbest: 0.8938435 (5999)\ttotal: 5m 30s\tremaining: 15h 12m 10s\n",
      "7000:\ttest: 0.8960236\tbest: 0.8960240 (6999)\ttotal: 6m 26s\tremaining: 15h 14m 22s\n",
      "8000:\ttest: 0.8976072\tbest: 0.8976072 (8000)\ttotal: 7m 23s\tremaining: 15h 17m 2s\n",
      "9000:\ttest: 0.8987275\tbest: 0.8987345 (8991)\ttotal: 8m 23s\tremaining: 15h 24m 47s\n",
      "10000:\ttest: 0.8996142\tbest: 0.8996142 (10000)\ttotal: 9m 14s\tremaining: 15h 15m 28s\n",
      "11000:\ttest: 0.9003154\tbest: 0.9003154 (11000)\ttotal: 10m 4s\tremaining: 15h 5m 49s\n",
      "12000:\ttest: 0.9008607\tbest: 0.9008607 (12000)\ttotal: 10m 55s\tremaining: 14h 59m 4s\n",
      "13000:\ttest: 0.9012366\tbest: 0.9012409 (12967)\ttotal: 11m 48s\tremaining: 14h 56m 41s\n",
      "14000:\ttest: 0.9015883\tbest: 0.9015931 (13953)\ttotal: 12m 42s\tremaining: 14h 54m 23s\n",
      "15000:\ttest: 0.9018680\tbest: 0.9018684 (14998)\ttotal: 13m 32s\tremaining: 14h 49m 12s\n",
      "16000:\ttest: 0.9021433\tbest: 0.9021433 (16000)\ttotal: 14m 24s\tremaining: 14h 45m 33s\n",
      "17000:\ttest: 0.9023132\tbest: 0.9023146 (16999)\ttotal: 15m 15s\tremaining: 14h 42m 16s\n",
      "18000:\ttest: 0.9024558\tbest: 0.9024572 (17988)\ttotal: 16m 9s\tremaining: 14h 41m 6s\n",
      "19000:\ttest: 0.9025898\tbest: 0.9025898 (19000)\ttotal: 17m\tremaining: 14h 38m 19s\n",
      "20000:\ttest: 0.9026589\tbest: 0.9026615 (19986)\ttotal: 17m 51s\tremaining: 14h 35m 11s\n",
      "21000:\ttest: 0.9027274\tbest: 0.9027372 (20765)\ttotal: 18m 42s\tremaining: 14h 31m 58s\n",
      "22000:\ttest: 0.9028292\tbest: 0.9028308 (21993)\ttotal: 19m 32s\tremaining: 14h 28m 37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23000:\ttest: 0.9029093\tbest: 0.9029119 (22842)\ttotal: 20m 23s\tremaining: 14h 26m 29s\n",
      "24000:\ttest: 0.9029708\tbest: 0.9029765 (23776)\ttotal: 21m 14s\tremaining: 14h 23m 40s\n",
      "25000:\ttest: 0.9029910\tbest: 0.9030091 (24862)\ttotal: 22m 4s\tremaining: 14h 20m 50s\n",
      "26000:\ttest: 0.9030670\tbest: 0.9030685 (25960)\ttotal: 22m 56s\tremaining: 14h 19m 31s\n",
      "27000:\ttest: 0.9031002\tbest: 0.9031022 (26992)\ttotal: 23m 51s\tremaining: 14h 19m 38s\n",
      "28000:\ttest: 0.9031377\tbest: 0.9031493 (27822)\ttotal: 24m 44s\tremaining: 14h 18m 40s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.9031492897\n",
      "bestIteration = 27822\n",
      "\n",
      "Shrink model to first 27823 iterations.\n",
      "Fold 7 started at Tue Mar 12 09:35:19 2019\n",
      "0:\ttest: 0.5314763\tbest: 0.5314763 (0)\ttotal: 75.4ms\tremaining: 20h 56m 56s\n",
      "1000:\ttest: 0.8371372\tbest: 0.8371799 (998)\ttotal: 48.9s\tremaining: 13h 32m 44s\n",
      "2000:\ttest: 0.8592815\tbest: 0.8592815 (2000)\ttotal: 1m 42s\tremaining: 14h 9m 13s\n",
      "3000:\ttest: 0.8710675\tbest: 0.8710675 (3000)\ttotal: 2m 35s\tremaining: 14h 19m 26s\n",
      "4000:\ttest: 0.8777240\tbest: 0.8777280 (3998)\ttotal: 3m 28s\tremaining: 14h 23m 7s\n",
      "5000:\ttest: 0.8817449\tbest: 0.8817494 (4999)\ttotal: 4m 22s\tremaining: 14h 31m 42s\n",
      "6000:\ttest: 0.8844726\tbest: 0.8844726 (6000)\ttotal: 5m 17s\tremaining: 14h 35m 44s\n",
      "7000:\ttest: 0.8865439\tbest: 0.8865445 (6999)\ttotal: 6m 9s\tremaining: 14h 34m 11s\n",
      "8000:\ttest: 0.8879418\tbest: 0.8879445 (7999)\ttotal: 7m 5s\tremaining: 14h 38m 17s\n",
      "9000:\ttest: 0.8888324\tbest: 0.8888324 (9000)\ttotal: 8m\tremaining: 14h 41m 1s\n",
      "10000:\ttest: 0.8896322\tbest: 0.8896335 (9998)\ttotal: 8m 57s\tremaining: 14h 46m 10s\n",
      "11000:\ttest: 0.8901972\tbest: 0.8902022 (10979)\ttotal: 9m 50s\tremaining: 14h 44m 10s\n",
      "12000:\ttest: 0.8906891\tbest: 0.8906894 (11985)\ttotal: 10m 43s\tremaining: 14h 42m 26s\n",
      "13000:\ttest: 0.8910686\tbest: 0.8910686 (13000)\ttotal: 11m 36s\tremaining: 14h 41m 11s\n",
      "14000:\ttest: 0.8914105\tbest: 0.8914124 (13998)\ttotal: 12m 27s\tremaining: 14h 37m 31s\n",
      "15000:\ttest: 0.8916189\tbest: 0.8916242 (14984)\ttotal: 13m 18s\tremaining: 14h 33m 55s\n",
      "16000:\ttest: 0.8918372\tbest: 0.8918421 (15992)\ttotal: 14m 10s\tremaining: 14h 32m 5s\n",
      "17000:\ttest: 0.8920350\tbest: 0.8920404 (16997)\ttotal: 15m 1s\tremaining: 14h 29m 10s\n",
      "18000:\ttest: 0.8921714\tbest: 0.8921799 (17965)\ttotal: 15m 54s\tremaining: 14h 27m 44s\n",
      "19000:\ttest: 0.8923012\tbest: 0.8923012 (19000)\ttotal: 16m 45s\tremaining: 14h 24m 54s\n",
      "20000:\ttest: 0.8924026\tbest: 0.8924041 (19999)\ttotal: 17m 36s\tremaining: 14h 23m 8s\n",
      "21000:\ttest: 0.8924897\tbest: 0.8924913 (20991)\ttotal: 18m 30s\tremaining: 14h 23m 2s\n",
      "22000:\ttest: 0.8925468\tbest: 0.8925504 (21957)\ttotal: 19m 25s\tremaining: 14h 23m 40s\n",
      "23000:\ttest: 0.8926042\tbest: 0.8926183 (22872)\ttotal: 20m 16s\tremaining: 14h 21m 15s\n",
      "24000:\ttest: 0.8926401\tbest: 0.8926597 (23880)\ttotal: 21m 9s\tremaining: 14h 20m 3s\n",
      "25000:\ttest: 0.8926728\tbest: 0.8926756 (24405)\ttotal: 22m 1s\tremaining: 14h 18m 43s\n",
      "26000:\ttest: 0.8926979\tbest: 0.8927026 (25894)\ttotal: 22m 52s\tremaining: 14h 16m 44s\n",
      "27000:\ttest: 0.8927404\tbest: 0.8927423 (26995)\ttotal: 23m 41s\tremaining: 14h 13m 59s\n",
      "28000:\ttest: 0.8927401\tbest: 0.8927523 (27070)\ttotal: 24m 31s\tremaining: 14h 11m 21s\n",
      "29000:\ttest: 0.8927629\tbest: 0.8927844 (28356)\ttotal: 25m 24s\tremaining: 14h 10m 27s\n",
      "30000:\ttest: 0.8928142\tbest: 0.8928152 (29996)\ttotal: 26m 15s\tremaining: 14h 8m 52s\n",
      "31000:\ttest: 0.8928834\tbest: 0.8928834 (31000)\ttotal: 27m 5s\tremaining: 14h 6m 50s\n",
      "32000:\ttest: 0.8929373\tbest: 0.8929563 (31874)\ttotal: 27m 57s\tremaining: 14h 5m 40s\n",
      "33000:\ttest: 0.8929543\tbest: 0.8929681 (32911)\ttotal: 28m 50s\tremaining: 14h 5m 13s\n",
      "34000:\ttest: 0.8929768\tbest: 0.8930027 (33781)\ttotal: 29m 41s\tremaining: 14h 3m 38s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8930027461\n",
      "bestIteration = 33781\n",
      "\n",
      "Shrink model to first 33782 iterations.\n",
      "Fold 8 started at Tue Mar 12 10:08:20 2019\n",
      "0:\ttest: 0.5534965\tbest: 0.5534965 (0)\ttotal: 68.2ms\tremaining: 18h 56m 2s\n",
      "1000:\ttest: 0.8384009\tbest: 0.8384800 (998)\ttotal: 49.4s\tremaining: 13h 41m 30s\n",
      "2000:\ttest: 0.8590697\tbest: 0.8590697 (2000)\ttotal: 1m 38s\tremaining: 13h 42m 4s\n",
      "3000:\ttest: 0.8711092\tbest: 0.8711092 (3000)\ttotal: 2m 28s\tremaining: 13h 42m 43s\n",
      "4000:\ttest: 0.8783578\tbest: 0.8783578 (4000)\ttotal: 3m 19s\tremaining: 13h 47m 14s\n",
      "5000:\ttest: 0.8827641\tbest: 0.8827673 (4997)\ttotal: 4m 10s\tremaining: 13h 50m 55s\n",
      "6000:\ttest: 0.8860401\tbest: 0.8860401 (6000)\ttotal: 5m 2s\tremaining: 13h 54m 42s\n",
      "7000:\ttest: 0.8882489\tbest: 0.8882507 (6998)\ttotal: 5m 54s\tremaining: 13h 57m 13s\n",
      "8000:\ttest: 0.8898267\tbest: 0.8898279 (7999)\ttotal: 6m 43s\tremaining: 13h 53m 41s\n",
      "9000:\ttest: 0.8911176\tbest: 0.8911176 (9000)\ttotal: 7m 32s\tremaining: 13h 50m 33s\n",
      "10000:\ttest: 0.8920742\tbest: 0.8920774 (9996)\ttotal: 8m 21s\tremaining: 13h 48m 3s\n",
      "11000:\ttest: 0.8927742\tbest: 0.8927768 (10995)\ttotal: 9m 11s\tremaining: 13h 46m 41s\n",
      "12000:\ttest: 0.8933837\tbest: 0.8933897 (11990)\ttotal: 10m 1s\tremaining: 13h 45m 4s\n",
      "13000:\ttest: 0.8937990\tbest: 0.8937990 (13000)\ttotal: 10m 49s\tremaining: 13h 41m 59s\n",
      "14000:\ttest: 0.8942082\tbest: 0.8942110 (13961)\ttotal: 11m 39s\tremaining: 13h 40m 38s\n",
      "15000:\ttest: 0.8944480\tbest: 0.8944520 (14991)\ttotal: 12m 27s\tremaining: 13h 38m 5s\n",
      "16000:\ttest: 0.8947188\tbest: 0.8947201 (15958)\ttotal: 13m 17s\tremaining: 13h 37m 20s\n",
      "17000:\ttest: 0.8949112\tbest: 0.8949175 (16986)\ttotal: 14m 8s\tremaining: 13h 37m 16s\n",
      "18000:\ttest: 0.8950680\tbest: 0.8950695 (17991)\ttotal: 14m 57s\tremaining: 13h 35m 37s\n",
      "19000:\ttest: 0.8950912\tbest: 0.8951076 (18866)\ttotal: 15m 46s\tremaining: 13h 34m 42s\n",
      "20000:\ttest: 0.8952242\tbest: 0.8952312 (19959)\ttotal: 16m 35s\tremaining: 13h 32m 58s\n",
      "21000:\ttest: 0.8952740\tbest: 0.8952745 (20996)\ttotal: 17m 24s\tremaining: 13h 31m 49s\n",
      "22000:\ttest: 0.8953716\tbest: 0.8953755 (21990)\ttotal: 18m 14s\tremaining: 13h 31m 11s\n",
      "23000:\ttest: 0.8954674\tbest: 0.8954785 (22969)\ttotal: 19m 4s\tremaining: 13h 30m 28s\n",
      "24000:\ttest: 0.8955517\tbest: 0.8955572 (23966)\ttotal: 19m 54s\tremaining: 13h 29m 18s\n",
      "25000:\ttest: 0.8955967\tbest: 0.8955992 (24974)\ttotal: 20m 43s\tremaining: 13h 28m 28s\n",
      "26000:\ttest: 0.8955858\tbest: 0.8956227 (25360)\ttotal: 21m 35s\tremaining: 13h 28m 44s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8956226719\n",
      "bestIteration = 25360\n",
      "\n",
      "Shrink model to first 25361 iterations.\n",
      "Fold 9 started at Tue Mar 12 10:31:56 2019\n",
      "0:\ttest: 0.5678622\tbest: 0.5678622 (0)\ttotal: 67.6ms\tremaining: 18h 46m 8s\n",
      "1000:\ttest: 0.8451945\tbest: 0.8452126 (999)\ttotal: 48.6s\tremaining: 13h 29m 12s\n",
      "2000:\ttest: 0.8636586\tbest: 0.8636586 (2000)\ttotal: 1m 38s\tremaining: 13h 40m 21s\n",
      "3000:\ttest: 0.8744898\tbest: 0.8744898 (3000)\ttotal: 2m 30s\tremaining: 13h 51m 17s\n",
      "4000:\ttest: 0.8813571\tbest: 0.8813571 (4000)\ttotal: 3m 21s\tremaining: 13h 57m 9s\n",
      "5000:\ttest: 0.8856671\tbest: 0.8856671 (5000)\ttotal: 4m 13s\tremaining: 14h 1m 25s\n",
      "6000:\ttest: 0.8882495\tbest: 0.8882495 (6000)\ttotal: 5m 7s\tremaining: 14h 9m 43s\n",
      "7000:\ttest: 0.8902280\tbest: 0.8902280 (7000)\ttotal: 5m 59s\tremaining: 14h 9m 44s\n",
      "8000:\ttest: 0.8916222\tbest: 0.8916222 (8000)\ttotal: 6m 51s\tremaining: 14h 10m 27s\n",
      "9000:\ttest: 0.8926489\tbest: 0.8926494 (8993)\ttotal: 7m 42s\tremaining: 14h 9m 5s\n",
      "10000:\ttest: 0.8933360\tbest: 0.8933360 (10000)\ttotal: 8m 35s\tremaining: 14h 9m 42s\n",
      "11000:\ttest: 0.8938687\tbest: 0.8938687 (11000)\ttotal: 9m 26s\tremaining: 14h 9m 21s\n",
      "12000:\ttest: 0.8942735\tbest: 0.8942845 (11949)\ttotal: 10m 18s\tremaining: 14h 8m 30s\n",
      "13000:\ttest: 0.8946316\tbest: 0.8946428 (12986)\ttotal: 11m 10s\tremaining: 14h 8m 18s\n",
      "14000:\ttest: 0.8949136\tbest: 0.8949185 (13992)\ttotal: 12m 2s\tremaining: 14h 8m 2s\n",
      "15000:\ttest: 0.8950811\tbest: 0.8950811 (15000)\ttotal: 12m 53s\tremaining: 14h 6m 24s\n",
      "16000:\ttest: 0.8952308\tbest: 0.8952310 (15824)\ttotal: 13m 45s\tremaining: 14h 6m 31s\n",
      "17000:\ttest: 0.8953722\tbest: 0.8953770 (16968)\ttotal: 14m 39s\tremaining: 14h 7m 41s\n",
      "18000:\ttest: 0.8953895\tbest: 0.8954098 (17743)\ttotal: 15m 31s\tremaining: 14h 6m 56s\n",
      "19000:\ttest: 0.8955063\tbest: 0.8955070 (18998)\ttotal: 16m 22s\tremaining: 14h 5m 48s\n",
      "20000:\ttest: 0.8955594\tbest: 0.8955644 (19983)\ttotal: 17m 13s\tremaining: 14h 3m 48s\n",
      "21000:\ttest: 0.8956309\tbest: 0.8956331 (20820)\ttotal: 18m 4s\tremaining: 14h 2m 34s\n",
      "22000:\ttest: 0.8956126\tbest: 0.8956406 (21531)\ttotal: 18m 55s\tremaining: 14h 55s\n",
      "23000:\ttest: 0.8956619\tbest: 0.8956794 (22881)\ttotal: 19m 45s\tremaining: 13h 59m 2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000:\ttest: 0.8956894\tbest: 0.8957000 (23689)\ttotal: 20m 35s\tremaining: 13h 57m 1s\n",
      "Stopped by overfitting detector  (1000 iterations wait)\n",
      "\n",
      "bestTest = 0.8956999782\n",
      "bestIteration = 23689\n",
      "\n",
      "Shrink model to first 23690 iterations.\n",
      "CV mean score: 0.8994, std: 0.0041.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "\n",
    "for col in ['var_12', 'var_91']:\n",
    "    X3[col] = X3[col].astype('str')\n",
    "    X4[col] = X4[col].astype('str')\n",
    "    \n",
    "oof_cat, prediction_cat, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, model_type='cat', plot_feature_importance=False)\n",
    "oof.append(oof_cat)\n",
    "preds.append(prediction_cat)\n",
    "\n",
    "#max-depth=4, with 2 cat vars, org X1, X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_cat_1_10_4', oof)\n",
    "np.save('../cache/preds_new_quant_cat_1_10_4', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "\n",
    "features = list(X3.columns)\n",
    "# bins = np.linspace(-6.0,6.0,40)\n",
    "\n",
    "# X3_12_bins = pd.get_dummies(pd.cut(X3['var_12'].values, bins))\n",
    "# X4_12_bins = pd.get_dummies(pd.cut(X4['var_12'].values, bins))\n",
    "# X3['var_12'] = pd.cut(X3['var_12'].values, bins)\n",
    "# X4['var_12'] = pd.cut(X4['var_12'].values, bins)\n",
    "\n",
    "# bins = np.linspace(-6.0,6.0,50)\n",
    "\n",
    "# X3_91_bins = pd.get_dummies(pd.cut(X3['var_91'].values, bins))\n",
    "# X4_91_bins = pd.get_dummies(pd.cut(X4['var_91'].values, bins))\n",
    "# X3['var_91'] = pd.cut(X3['var_91'].values, bins)\n",
    "# X4['var_91'] = pd.cut(X4['var_91'].values, bins)\n",
    "\n",
    "# X3_12_bins = pd.DataFrame(X3_12_bins)\n",
    "# cols = ['var_12_' + str(i) for i in range(X3_12_bins.shape[1])]\n",
    "# X3_12_bins.columns = cols\n",
    "\n",
    "# X4_12_bins = pd.DataFrame(X4_12_bins)\n",
    "# X4_12_bins.columns = cols\n",
    "\n",
    "# X3_91_bins = pd.DataFrame(X3_91_bins)\n",
    "# cols = ['var_91_' + str(i) for i in range(X3_91_bins.shape[1])]\n",
    "# X3_91_bins.columns = cols\n",
    "\n",
    "# X4_91_bins = pd.DataFrame(X4_91_bins)\n",
    "# X4_91_bins.columns = cols\n",
    "\n",
    "# X3 = pd.concat([X3, X3_12_bins], axis=1)\n",
    "# X3 = pd.concat([X3, X3_91_bins], axis=1)\n",
    "\n",
    "# X4 = pd.concat([X4, X4_12_bins], axis=1)\n",
    "# X4 = pd.concat([X4, X4_91_bins], axis=1)\n",
    "\n",
    "# data = pd.concat([X3, X4], axis=0)\n",
    "\n",
    "# for col in ['var_12']:\n",
    "#     le = LabelEncoder()\n",
    "#     X3[col] = X3[col].astype('str')\n",
    "#     X4[col] = X4[col].astype('str')\n",
    "#     le.fit(X3[col])\n",
    "#     X3[col] = le.transform(X3[col])\n",
    "#     X4[col] = le.transform(X4[col])\n",
    "\n",
    "\n",
    "# for c in ['var_12', 'var_91']:\n",
    "#     d = pd.concat([X3[c],X4[c]]).value_counts().to_dict()\n",
    "#     X3['%s_count'%c] = X3[c].apply(lambda x:d.get(x,0))\n",
    "#     X4['%s_count'%c] = X4[c].apply(lambda x:d.get(x,0))\n",
    "    \n",
    "\n",
    "    \n",
    "# # data = data.drop(['var_12', 'var_91'], axis=1)\n",
    "\n",
    "# # X3 = data[:len(X3)]\n",
    "# # X4 = data[len(X3):]\n",
    "\n",
    "# X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features.remove('var_12')\n",
    "features.remove('var_91')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = features\n",
    "for df in [X3, X4]:\n",
    "    df['sum'] = df[idx].sum(axis=1)  \n",
    "    df['min'] = df[idx].min(axis=1)\n",
    "    df['max'] = df[idx].max(axis=1)\n",
    "    df['mean'] = df[idx].mean(axis=1)\n",
    "    df['std'] = df[idx].std(axis=1)\n",
    "    df['skew'] = df[idx].skew(axis=1)\n",
    "    df['kurt'] = df[idx].kurtosis(axis=1)\n",
    "    df['med'] = df[idx].median(axis=1)\n",
    "    \n",
    "# for feature in features:\n",
    "#     X3['r2_'+feature] = np.round(X3[feature], 2)\n",
    "#     X4['r2_'+feature] = np.round(X4[feature], 2)\n",
    "#     X3['r1_'+feature] = np.round(X3[feature], 1)\n",
    "#     X4['r1_'+feature] = np.round(X4[feature], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 208), (200000, 208))"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Mar 12 22:32:26 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.871788\n",
      "[2000]\tvalid_0's auc: 0.884019\n",
      "[3000]\tvalid_0's auc: 0.890675\n",
      "[4000]\tvalid_0's auc: 0.895005\n",
      "[5000]\tvalid_0's auc: 0.897329\n",
      "[6000]\tvalid_0's auc: 0.898824\n",
      "[7000]\tvalid_0's auc: 0.899643\n",
      "[8000]\tvalid_0's auc: 0.900213\n",
      "[9000]\tvalid_0's auc: 0.90064\n",
      "[10000]\tvalid_0's auc: 0.900797\n",
      "[11000]\tvalid_0's auc: 0.900797\n",
      "[12000]\tvalid_0's auc: 0.900673\n",
      "[13000]\tvalid_0's auc: 0.9007\n",
      "Early stopping, best iteration is:\n",
      "[10486]\tvalid_0's auc: 0.900907\n",
      "Fold 1 started at Tue Mar 12 23:05:52 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.868336\n",
      "[2000]\tvalid_0's auc: 0.88211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-392-c39f5a9d0a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n\u001b[0;32m----> 5\u001b[0;31m                                          model_type='lgb', plot_feature_importance=False)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-349-08fe1f7e85b1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, averaging, model)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 early_stopping_rounds=3000)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;31m#             y_pred_valid = model.predict(X_valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#             y_pred = model.predict(X_test, num_iteration=model.best_iteration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Mar 12 22:16:59 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.869472\n",
      "[2000]\tvalid_0's auc: 0.884875\n",
      "[3000]\tvalid_0's auc: 0.892712\n",
      "[4000]\tvalid_0's auc: 0.896717\n",
      "[5000]\tvalid_0's auc: 0.898765\n",
      "[6000]\tvalid_0's auc: 0.899794\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-387-c39f5a9d0a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n\u001b[0;32m----> 5\u001b[0;31m                                          model_type='lgb', plot_feature_importance=False)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-349-08fe1f7e85b1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, averaging, model)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 early_stopping_rounds=3000)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;31m#             y_pred_valid = model.predict(X_valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#             y_pred = model.predict(X_test, num_iteration=model.best_iteration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)\n",
    "\n",
    "# with feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Wed Mar 13 10:32:12 2019\n",
      "[0]\tvalidation_0-auc:0.575067\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.88952\n",
      "[2000]\tvalidation_0-auc:0.895126\n",
      "[3000]\tvalidation_0-auc:0.897503\n",
      "[4000]\tvalidation_0-auc:0.898602\n",
      "[5000]\tvalidation_0-auc:0.899395\n",
      "[6000]\tvalidation_0-auc:0.899667\n",
      "[7000]\tvalidation_0-auc:0.89967\n",
      "[8000]\tvalidation_0-auc:0.899908\n",
      "[9000]\tvalidation_0-auc:0.900079\n",
      "[10000]\tvalidation_0-auc:0.899997\n",
      "[11000]\tvalidation_0-auc:0.900211\n",
      "[12000]\tvalidation_0-auc:0.900008\n",
      "[13000]\tvalidation_0-auc:0.899854\n",
      "Stopping. Best iteration:\n",
      "[10717]\tvalidation_0-auc:0.900239\n",
      "\n",
      "Fold 1 started at Wed Mar 13 11:01:48 2019\n",
      "[0]\tvalidation_0-auc:0.572342\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.887849\n",
      "[2000]\tvalidation_0-auc:0.893778\n",
      "[3000]\tvalidation_0-auc:0.896319\n",
      "[4000]\tvalidation_0-auc:0.897703\n",
      "[5000]\tvalidation_0-auc:0.89848\n",
      "[6000]\tvalidation_0-auc:0.898793\n",
      "[7000]\tvalidation_0-auc:0.898764\n",
      "[8000]\tvalidation_0-auc:0.898764\n",
      "[9000]\tvalidation_0-auc:0.89898\n",
      "[10000]\tvalidation_0-auc:0.898775\n",
      "[11000]\tvalidation_0-auc:0.898829\n",
      "Stopping. Best iteration:\n",
      "[8448]\tvalidation_0-auc:0.899104\n",
      "\n",
      "Fold 2 started at Wed Mar 13 11:33:11 2019\n",
      "[0]\tvalidation_0-auc:0.571544\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.891098\n",
      "[2000]\tvalidation_0-auc:0.895511\n",
      "[3000]\tvalidation_0-auc:0.897553\n",
      "[4000]\tvalidation_0-auc:0.897994\n",
      "[5000]\tvalidation_0-auc:0.89826\n",
      "[6000]\tvalidation_0-auc:0.8985\n",
      "[7000]\tvalidation_0-auc:0.898397\n",
      "[8000]\tvalidation_0-auc:0.89848\n",
      "[9000]\tvalidation_0-auc:0.898268\n",
      "Stopping. Best iteration:\n",
      "[6580]\tvalidation_0-auc:0.898663\n",
      "\n",
      "Fold 3 started at Wed Mar 13 12:00:19 2019\n",
      "[0]\tvalidation_0-auc:0.574609\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.892285\n",
      "[2000]\tvalidation_0-auc:0.897089\n",
      "[3000]\tvalidation_0-auc:0.899371\n",
      "[4000]\tvalidation_0-auc:0.900398\n",
      "[5000]\tvalidation_0-auc:0.901239\n",
      "[6000]\tvalidation_0-auc:0.901655\n",
      "[7000]\tvalidation_0-auc:0.90147\n",
      "[8000]\tvalidation_0-auc:0.901533\n",
      "Stopping. Best iteration:\n",
      "[5872]\tvalidation_0-auc:0.901723\n",
      "\n",
      "Fold 4 started at Wed Mar 13 12:26:03 2019\n",
      "[0]\tvalidation_0-auc:0.595526\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.897858\n",
      "[2000]\tvalidation_0-auc:0.903736\n",
      "[3000]\tvalidation_0-auc:0.906434\n",
      "[4000]\tvalidation_0-auc:0.907995\n",
      "[5000]\tvalidation_0-auc:0.908493\n",
      "[6000]\tvalidation_0-auc:0.908989\n",
      "[7000]\tvalidation_0-auc:0.90919\n",
      "[8000]\tvalidation_0-auc:0.908904\n",
      "[9000]\tvalidation_0-auc:0.908973\n",
      "[10000]\tvalidation_0-auc:0.909015\n",
      "Stopping. Best iteration:\n",
      "[7275]\tvalidation_0-auc:0.909339\n",
      "\n",
      "Fold 5 started at Wed Mar 13 12:54:40 2019\n",
      "[0]\tvalidation_0-auc:0.582498\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.892043\n",
      "[2000]\tvalidation_0-auc:0.89847\n",
      "[3000]\tvalidation_0-auc:0.90025\n",
      "[4000]\tvalidation_0-auc:0.901509\n",
      "[5000]\tvalidation_0-auc:0.902491\n",
      "[6000]\tvalidation_0-auc:0.902812\n",
      "[7000]\tvalidation_0-auc:0.902849\n",
      "[8000]\tvalidation_0-auc:0.902984\n",
      "[9000]\tvalidation_0-auc:0.902626\n",
      "[10000]\tvalidation_0-auc:0.902601\n",
      "[11000]\tvalidation_0-auc:0.902581\n",
      "Stopping. Best iteration:\n",
      "[8038]\tvalidation_0-auc:0.903022\n",
      "\n",
      "Fold 6 started at Wed Mar 13 13:26:03 2019\n",
      "[0]\tvalidation_0-auc:0.584719\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.89437\n",
      "[2000]\tvalidation_0-auc:0.899093\n",
      "[3000]\tvalidation_0-auc:0.901399\n",
      "[4000]\tvalidation_0-auc:0.902828\n",
      "[5000]\tvalidation_0-auc:0.903438\n",
      "[6000]\tvalidation_0-auc:0.904106\n",
      "[7000]\tvalidation_0-auc:0.904155\n",
      "[8000]\tvalidation_0-auc:0.90411\n",
      "[9000]\tvalidation_0-auc:0.904417\n",
      "[10000]\tvalidation_0-auc:0.903994\n",
      "[11000]\tvalidation_0-auc:0.903717\n",
      "Stopping. Best iteration:\n",
      "[8688]\tvalidation_0-auc:0.904444\n",
      "\n",
      "Fold 7 started at Wed Mar 13 13:59:25 2019\n",
      "[0]\tvalidation_0-auc:0.575168\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.884267\n",
      "[2000]\tvalidation_0-auc:0.890595\n",
      "[3000]\tvalidation_0-auc:0.892301\n",
      "[4000]\tvalidation_0-auc:0.893367\n",
      "[5000]\tvalidation_0-auc:0.893966\n",
      "[6000]\tvalidation_0-auc:0.894551\n",
      "[7000]\tvalidation_0-auc:0.894567\n",
      "[8000]\tvalidation_0-auc:0.894537\n",
      "[9000]\tvalidation_0-auc:0.894635\n",
      "[10000]\tvalidation_0-auc:0.894357\n",
      "[11000]\tvalidation_0-auc:0.894292\n",
      "Stopping. Best iteration:\n",
      "[8867]\tvalidation_0-auc:0.894669\n",
      "\n",
      "Fold 8 started at Wed Mar 13 14:32:38 2019\n",
      "[0]\tvalidation_0-auc:0.548897\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.887481\n",
      "[2000]\tvalidation_0-auc:0.892116\n",
      "[3000]\tvalidation_0-auc:0.894502\n",
      "[4000]\tvalidation_0-auc:0.895714\n",
      "[5000]\tvalidation_0-auc:0.895929\n",
      "[6000]\tvalidation_0-auc:0.89651\n",
      "[7000]\tvalidation_0-auc:0.896469\n",
      "[8000]\tvalidation_0-auc:0.896403\n",
      "[9000]\tvalidation_0-auc:0.896243\n",
      "[10000]\tvalidation_0-auc:0.896132\n",
      "Stopping. Best iteration:\n",
      "[7484]\tvalidation_0-auc:0.896587\n",
      "\n",
      "Fold 9 started at Wed Mar 13 15:02:36 2019\n",
      "[0]\tvalidation_0-auc:0.555661\n",
      "Will train until validation_0-auc hasn't improved in 3000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.886905\n",
      "[2000]\tvalidation_0-auc:0.89347\n",
      "[3000]\tvalidation_0-auc:0.895572\n",
      "[4000]\tvalidation_0-auc:0.896869\n",
      "[5000]\tvalidation_0-auc:0.897386\n",
      "[6000]\tvalidation_0-auc:0.897537\n",
      "[7000]\tvalidation_0-auc:0.897587\n",
      "[8000]\tvalidation_0-auc:0.897442\n",
      "Stopping. Best iteration:\n",
      "[5598]\tvalidation_0-auc:0.897816\n",
      "\n",
      "CV mean score: 0.9006, std: 0.0040.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_xgb, prediction_xgb, _ = train_model(X1, X2, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='xgb', plot_feature_importance=False)\n",
    "oof.append(oof_xgb)\n",
    "preds.append(prediction_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_xgb_1_10_2', oof)\n",
    "np.save('../cache/preds_new_quant_xgb_1_10_2', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lgb       xgb\n",
      "0  0.009521  0.014329\n",
      "1  0.446041  0.403255\n",
      "2  0.005359  0.006500\n",
      "3  0.273077  0.225319\n",
      "4  0.115543  0.105429\n",
      "        lgb       xgb\n",
      "0  0.097262  0.102657\n",
      "1  0.209279  0.192630\n",
      "2  0.169372  0.158632\n",
      "3  0.209088  0.204888\n",
      "4  0.044063  0.046218\n",
      "0.92547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "stage2 = pd.DataFrame()\n",
    "stage2_test = pd.DataFrame()\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_1_10_2.npy')\n",
    "preds = np.load('../cache/preds_new_quant_1_10_2.npy')\n",
    "stage2['lgb'] = oof\n",
    "stage2_test['lgb'] = preds\n",
    "\n",
    "oof = np.load('../cache/oof_new_quant_xgb_1_10_2.npy')\n",
    "preds = np.load('../cache/preds_new_quant_xgb_1_10_2.npy')\n",
    "stage2['xgb'] = oof[0]\n",
    "stage2_test['xgb'] = preds[0]\n",
    "\n",
    "print(stage2.head())\n",
    "print(stage2_test.head())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(stage2, y)\n",
    "predictions = lr.predict_proba(stage2_test)[:, 1]\n",
    "\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12w.csv', index=False)\n",
    "print(lr.score(stage2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fold = 15\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Wed Mar 13 16:37:47 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885676\n",
      "[2000]\tvalid_0's auc: 0.893774\n",
      "[3000]\tvalid_0's auc: 0.897464\n",
      "[4000]\tvalid_0's auc: 0.899676\n",
      "[5000]\tvalid_0's auc: 0.900695\n",
      "[6000]\tvalid_0's auc: 0.902021\n",
      "[7000]\tvalid_0's auc: 0.902529\n",
      "[8000]\tvalid_0's auc: 0.903053\n",
      "[9000]\tvalid_0's auc: 0.903212\n",
      "[10000]\tvalid_0's auc: 0.903346\n",
      "[11000]\tvalid_0's auc: 0.903237\n",
      "[12000]\tvalid_0's auc: 0.903248\n",
      "Early stopping, best iteration is:\n",
      "[9863]\tvalid_0's auc: 0.903405\n",
      "Fold 1 started at Wed Mar 13 16:40:24 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885995\n",
      "[2000]\tvalid_0's auc: 0.892576\n",
      "[3000]\tvalid_0's auc: 0.89569\n",
      "[4000]\tvalid_0's auc: 0.897415\n",
      "[5000]\tvalid_0's auc: 0.898656\n",
      "[6000]\tvalid_0's auc: 0.899548\n",
      "[7000]\tvalid_0's auc: 0.900416\n",
      "[8000]\tvalid_0's auc: 0.900683\n",
      "[9000]\tvalid_0's auc: 0.90064\n",
      "[10000]\tvalid_0's auc: 0.900575\n",
      "[11000]\tvalid_0's auc: 0.900543\n",
      "Early stopping, best iteration is:\n",
      "[8576]\tvalid_0's auc: 0.900889\n",
      "Fold 2 started at Wed Mar 13 16:42:56 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.881426\n",
      "[2000]\tvalid_0's auc: 0.888513\n",
      "[3000]\tvalid_0's auc: 0.892105\n",
      "[4000]\tvalid_0's auc: 0.894662\n",
      "[5000]\tvalid_0's auc: 0.895925\n",
      "[6000]\tvalid_0's auc: 0.89656\n",
      "[7000]\tvalid_0's auc: 0.897074\n",
      "[8000]\tvalid_0's auc: 0.89764\n",
      "[9000]\tvalid_0's auc: 0.89772\n",
      "[10000]\tvalid_0's auc: 0.897721\n",
      "[11000]\tvalid_0's auc: 0.897706\n",
      "[12000]\tvalid_0's auc: 0.89767\n",
      "Early stopping, best iteration is:\n",
      "[9350]\tvalid_0's auc: 0.897833\n",
      "Fold 3 started at Wed Mar 13 17:32:55 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886329\n",
      "[2000]\tvalid_0's auc: 0.893187\n",
      "[3000]\tvalid_0's auc: 0.896779\n",
      "[4000]\tvalid_0's auc: 0.898881\n",
      "[5000]\tvalid_0's auc: 0.89965\n",
      "[6000]\tvalid_0's auc: 0.9001\n",
      "[7000]\tvalid_0's auc: 0.900412\n",
      "[8000]\tvalid_0's auc: 0.900785\n",
      "[9000]\tvalid_0's auc: 0.901103\n",
      "[10000]\tvalid_0's auc: 0.901063\n",
      "[11000]\tvalid_0's auc: 0.901007\n",
      "[12000]\tvalid_0's auc: 0.900763\n",
      "[13000]\tvalid_0's auc: 0.900483\n",
      "Early stopping, best iteration is:\n",
      "[10389]\tvalid_0's auc: 0.901185\n",
      "Fold 4 started at Wed Mar 13 18:27:46 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885548\n",
      "[2000]\tvalid_0's auc: 0.892849\n",
      "[3000]\tvalid_0's auc: 0.895948\n",
      "[4000]\tvalid_0's auc: 0.897811\n",
      "[5000]\tvalid_0's auc: 0.898793\n",
      "[6000]\tvalid_0's auc: 0.89944\n",
      "[7000]\tvalid_0's auc: 0.899863\n",
      "[8000]\tvalid_0's auc: 0.900204\n",
      "[9000]\tvalid_0's auc: 0.900243\n",
      "[10000]\tvalid_0's auc: 0.900267\n",
      "[11000]\tvalid_0's auc: 0.900185\n",
      "Early stopping, best iteration is:\n",
      "[8755]\tvalid_0's auc: 0.900341\n",
      "Fold 5 started at Wed Mar 13 19:15:06 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.888132\n",
      "[2000]\tvalid_0's auc: 0.894141\n",
      "[3000]\tvalid_0's auc: 0.897198\n",
      "[4000]\tvalid_0's auc: 0.899185\n",
      "[5000]\tvalid_0's auc: 0.900279\n",
      "[6000]\tvalid_0's auc: 0.900845\n",
      "[7000]\tvalid_0's auc: 0.901278\n",
      "[8000]\tvalid_0's auc: 0.901487\n",
      "[9000]\tvalid_0's auc: 0.90179\n",
      "[10000]\tvalid_0's auc: 0.902204\n",
      "[11000]\tvalid_0's auc: 0.902154\n",
      "[12000]\tvalid_0's auc: 0.902079\n",
      "[13000]\tvalid_0's auc: 0.902176\n",
      "Early stopping, best iteration is:\n",
      "[10383]\tvalid_0's auc: 0.902303\n",
      "Fold 6 started at Wed Mar 13 20:10:14 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.896529\n",
      "[2000]\tvalid_0's auc: 0.904641\n",
      "[3000]\tvalid_0's auc: 0.907893\n",
      "[4000]\tvalid_0's auc: 0.91062\n",
      "[5000]\tvalid_0's auc: 0.912419\n",
      "[6000]\tvalid_0's auc: 0.913594\n",
      "[7000]\tvalid_0's auc: 0.914259\n",
      "[8000]\tvalid_0's auc: 0.914614\n",
      "[9000]\tvalid_0's auc: 0.914972\n",
      "[10000]\tvalid_0's auc: 0.915057\n",
      "[11000]\tvalid_0's auc: 0.91524\n",
      "[12000]\tvalid_0's auc: 0.915189\n",
      "[13000]\tvalid_0's auc: 0.915067\n",
      "Early stopping, best iteration is:\n",
      "[10965]\tvalid_0's auc: 0.915274\n",
      "Fold 7 started at Wed Mar 13 21:06:25 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884504\n",
      "[2000]\tvalid_0's auc: 0.889969\n",
      "[3000]\tvalid_0's auc: 0.893574\n",
      "[4000]\tvalid_0's auc: 0.8958\n",
      "[5000]\tvalid_0's auc: 0.897209\n",
      "[6000]\tvalid_0's auc: 0.897802\n",
      "[7000]\tvalid_0's auc: 0.898153\n",
      "[8000]\tvalid_0's auc: 0.89827\n",
      "[9000]\tvalid_0's auc: 0.898365\n",
      "[10000]\tvalid_0's auc: 0.898394\n",
      "[11000]\tvalid_0's auc: 0.898327\n",
      "[12000]\tvalid_0's auc: 0.898246\n",
      "Early stopping, best iteration is:\n",
      "[9737]\tvalid_0's auc: 0.898507\n",
      "Fold 8 started at Wed Mar 13 21:57:18 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.889145\n",
      "[2000]\tvalid_0's auc: 0.89627\n",
      "[3000]\tvalid_0's auc: 0.900716\n",
      "[4000]\tvalid_0's auc: 0.903072\n",
      "[5000]\tvalid_0's auc: 0.904543\n",
      "[6000]\tvalid_0's auc: 0.905442\n",
      "[7000]\tvalid_0's auc: 0.905967\n",
      "[8000]\tvalid_0's auc: 0.906333\n",
      "[9000]\tvalid_0's auc: 0.906605\n",
      "[10000]\tvalid_0's auc: 0.906668\n",
      "[11000]\tvalid_0's auc: 0.906636\n",
      "[12000]\tvalid_0's auc: 0.906536\n",
      "Early stopping, best iteration is:\n",
      "[9660]\tvalid_0's auc: 0.906709\n",
      "Fold 9 started at Wed Mar 13 22:48:42 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88817\n",
      "[2000]\tvalid_0's auc: 0.895805\n",
      "[3000]\tvalid_0's auc: 0.89952\n",
      "[4000]\tvalid_0's auc: 0.902206\n",
      "[5000]\tvalid_0's auc: 0.903472\n",
      "[6000]\tvalid_0's auc: 0.904245\n",
      "[7000]\tvalid_0's auc: 0.904813\n",
      "[8000]\tvalid_0's auc: 0.905145\n",
      "[9000]\tvalid_0's auc: 0.905454\n",
      "[10000]\tvalid_0's auc: 0.905614\n",
      "[11000]\tvalid_0's auc: 0.90589\n",
      "[12000]\tvalid_0's auc: 0.905756\n",
      "[13000]\tvalid_0's auc: 0.905828\n",
      "[14000]\tvalid_0's auc: 0.905837\n",
      "[15000]\tvalid_0's auc: 0.906117\n",
      "[16000]\tvalid_0's auc: 0.905942\n",
      "[17000]\tvalid_0's auc: 0.905911\n",
      "[18000]\tvalid_0's auc: 0.9058\n",
      "Early stopping, best iteration is:\n",
      "[15041]\tvalid_0's auc: 0.906157\n",
      "Fold 10 started at Wed Mar 13 23:54:28 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.882103\n",
      "[2000]\tvalid_0's auc: 0.889409\n",
      "[3000]\tvalid_0's auc: 0.892913\n",
      "[4000]\tvalid_0's auc: 0.895289\n",
      "[5000]\tvalid_0's auc: 0.896658\n",
      "[6000]\tvalid_0's auc: 0.897465\n",
      "[7000]\tvalid_0's auc: 0.897664\n",
      "[8000]\tvalid_0's auc: 0.898165\n",
      "[9000]\tvalid_0's auc: 0.898246\n",
      "[10000]\tvalid_0's auc: 0.898531\n",
      "[11000]\tvalid_0's auc: 0.898503\n",
      "[12000]\tvalid_0's auc: 0.898715\n",
      "[13000]\tvalid_0's auc: 0.898658\n",
      "[14000]\tvalid_0's auc: 0.898672\n",
      "[15000]\tvalid_0's auc: 0.898585\n",
      "Early stopping, best iteration is:\n",
      "[12714]\tvalid_0's auc: 0.89881\n",
      "Fold 11 started at Thu Mar 14 00:48:48 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.876188\n",
      "[2000]\tvalid_0's auc: 0.885055\n",
      "[3000]\tvalid_0's auc: 0.888824\n",
      "[4000]\tvalid_0's auc: 0.890887\n",
      "[5000]\tvalid_0's auc: 0.892284\n",
      "[6000]\tvalid_0's auc: 0.893294\n",
      "[7000]\tvalid_0's auc: 0.893797\n",
      "[8000]\tvalid_0's auc: 0.894262\n",
      "[9000]\tvalid_0's auc: 0.89434\n",
      "[10000]\tvalid_0's auc: 0.894527\n",
      "[11000]\tvalid_0's auc: 0.894571\n",
      "[12000]\tvalid_0's auc: 0.894675\n",
      "[13000]\tvalid_0's auc: 0.894633\n",
      "[14000]\tvalid_0's auc: 0.89478\n",
      "[15000]\tvalid_0's auc: 0.894882\n",
      "[16000]\tvalid_0's auc: 0.894839\n",
      "[17000]\tvalid_0's auc: 0.89481\n",
      "Early stopping, best iteration is:\n",
      "[14617]\tvalid_0's auc: 0.894977\n",
      "Fold 12 started at Thu Mar 14 01:53:28 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.875729\n",
      "[2000]\tvalid_0's auc: 0.882411\n",
      "[3000]\tvalid_0's auc: 0.886978\n",
      "[4000]\tvalid_0's auc: 0.889952\n",
      "[5000]\tvalid_0's auc: 0.891481\n",
      "[6000]\tvalid_0's auc: 0.89259\n",
      "[7000]\tvalid_0's auc: 0.893277\n",
      "[8000]\tvalid_0's auc: 0.893922\n",
      "[9000]\tvalid_0's auc: 0.894333\n",
      "[10000]\tvalid_0's auc: 0.894617\n",
      "[11000]\tvalid_0's auc: 0.894677\n",
      "[12000]\tvalid_0's auc: 0.894738\n",
      "[13000]\tvalid_0's auc: 0.894683\n",
      "[14000]\tvalid_0's auc: 0.894454\n",
      "[15000]\tvalid_0's auc: 0.894087\n",
      "Early stopping, best iteration is:\n",
      "[12415]\tvalid_0's auc: 0.894856\n",
      "Fold 13 started at Thu Mar 14 02:51:17 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88351\n",
      "[2000]\tvalid_0's auc: 0.890552\n",
      "[3000]\tvalid_0's auc: 0.894164\n",
      "[4000]\tvalid_0's auc: 0.896308\n",
      "[5000]\tvalid_0's auc: 0.897422\n",
      "[6000]\tvalid_0's auc: 0.898533\n",
      "[7000]\tvalid_0's auc: 0.899049\n",
      "[8000]\tvalid_0's auc: 0.89917\n",
      "[9000]\tvalid_0's auc: 0.899332\n",
      "[10000]\tvalid_0's auc: 0.899461\n",
      "[11000]\tvalid_0's auc: 0.899418\n",
      "[12000]\tvalid_0's auc: 0.899197\n",
      "Early stopping, best iteration is:\n",
      "[9712]\tvalid_0's auc: 0.899568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 14 started at Thu Mar 14 03:40:27 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.881129\n",
      "[2000]\tvalid_0's auc: 0.889132\n",
      "[3000]\tvalid_0's auc: 0.892473\n",
      "[4000]\tvalid_0's auc: 0.895022\n",
      "[5000]\tvalid_0's auc: 0.89636\n",
      "[6000]\tvalid_0's auc: 0.897341\n",
      "[7000]\tvalid_0's auc: 0.897637\n",
      "[8000]\tvalid_0's auc: 0.897921\n",
      "[9000]\tvalid_0's auc: 0.898151\n",
      "[10000]\tvalid_0's auc: 0.898173\n",
      "[11000]\tvalid_0's auc: 0.898257\n",
      "[12000]\tvalid_0's auc: 0.898104\n",
      "[13000]\tvalid_0's auc: 0.897951\n",
      "[14000]\tvalid_0's auc: 0.897779\n",
      "Early stopping, best iteration is:\n",
      "[11429]\tvalid_0's auc: 0.898289\n",
      "CV mean score: 0.9013, std: 0.0050.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X1, X2, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_1_10_3', oof_lgb)\n",
    "np.save('../cache/preds_new_quant_1_10_3', prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = prediction_lgb\n",
    "sub.to_csv('../submissions/sub12l2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 202), (200000, 201))"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.csv.zip')\n",
    "test = pd.read_csv('../data/test.csv.zip')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train.drop(['ID_code', 'target'], axis=1)\n",
    "X_test = test.drop(['ID_code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Thu Mar 14 06:12:05 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886491\n",
      "[2000]\tvalid_0's auc: 0.894061\n",
      "[3000]\tvalid_0's auc: 0.897956\n",
      "[4000]\tvalid_0's auc: 0.900319\n",
      "[5000]\tvalid_0's auc: 0.901197\n",
      "[6000]\tvalid_0's auc: 0.902354\n",
      "[7000]\tvalid_0's auc: 0.902878\n",
      "[8000]\tvalid_0's auc: 0.903506\n",
      "[9000]\tvalid_0's auc: 0.903391\n",
      "[10000]\tvalid_0's auc: 0.903701\n",
      "[11000]\tvalid_0's auc: 0.903478\n",
      "[12000]\tvalid_0's auc: 0.903493\n",
      "Early stopping, best iteration is:\n",
      "[9583]\tvalid_0's auc: 0.903755\n",
      "Fold 1 started at Thu Mar 14 06:14:27 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88575\n",
      "[2000]\tvalid_0's auc: 0.892313\n",
      "[3000]\tvalid_0's auc: 0.895377\n",
      "[4000]\tvalid_0's auc: 0.897222\n",
      "[5000]\tvalid_0's auc: 0.898606\n",
      "[6000]\tvalid_0's auc: 0.89944\n",
      "[7000]\tvalid_0's auc: 0.900404\n",
      "[8000]\tvalid_0's auc: 0.900715\n",
      "[9000]\tvalid_0's auc: 0.900821\n",
      "[10000]\tvalid_0's auc: 0.900892\n",
      "[11000]\tvalid_0's auc: 0.900752\n",
      "[12000]\tvalid_0's auc: 0.900541\n",
      "Early stopping, best iteration is:\n",
      "[9921]\tvalid_0's auc: 0.900965\n",
      "Fold 2 started at Thu Mar 14 06:16:51 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.880967\n",
      "[2000]\tvalid_0's auc: 0.888228\n",
      "[3000]\tvalid_0's auc: 0.891999\n",
      "[4000]\tvalid_0's auc: 0.894251\n",
      "[5000]\tvalid_0's auc: 0.895415\n",
      "[6000]\tvalid_0's auc: 0.896159\n",
      "[7000]\tvalid_0's auc: 0.896424\n",
      "[8000]\tvalid_0's auc: 0.896872\n",
      "[9000]\tvalid_0's auc: 0.897094\n",
      "[10000]\tvalid_0's auc: 0.897162\n",
      "[11000]\tvalid_0's auc: 0.896948\n",
      "[12000]\tvalid_0's auc: 0.89683\n",
      "Early stopping, best iteration is:\n",
      "[9355]\tvalid_0's auc: 0.897297\n",
      "Fold 3 started at Thu Mar 14 06:46:51 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886264\n",
      "[2000]\tvalid_0's auc: 0.892865\n",
      "[3000]\tvalid_0's auc: 0.896362\n",
      "[4000]\tvalid_0's auc: 0.898466\n",
      "[5000]\tvalid_0's auc: 0.899275\n",
      "[6000]\tvalid_0's auc: 0.899645\n",
      "[7000]\tvalid_0's auc: 0.899956\n",
      "[8000]\tvalid_0's auc: 0.90025\n",
      "[9000]\tvalid_0's auc: 0.900501\n",
      "[10000]\tvalid_0's auc: 0.900597\n",
      "[11000]\tvalid_0's auc: 0.900546\n",
      "[12000]\tvalid_0's auc: 0.90039\n",
      "[13000]\tvalid_0's auc: 0.900252\n",
      "Early stopping, best iteration is:\n",
      "[10328]\tvalid_0's auc: 0.900676\n",
      "Fold 4 started at Thu Mar 14 07:36:57 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885374\n",
      "[2000]\tvalid_0's auc: 0.892694\n",
      "[3000]\tvalid_0's auc: 0.896077\n",
      "[4000]\tvalid_0's auc: 0.897995\n",
      "[5000]\tvalid_0's auc: 0.898917\n",
      "[6000]\tvalid_0's auc: 0.899526\n",
      "[7000]\tvalid_0's auc: 0.899899\n",
      "[8000]\tvalid_0's auc: 0.900217\n",
      "[9000]\tvalid_0's auc: 0.900143\n",
      "[10000]\tvalid_0's auc: 0.900307\n",
      "[11000]\tvalid_0's auc: 0.900281\n",
      "[12000]\tvalid_0's auc: 0.900191\n",
      "[13000]\tvalid_0's auc: 0.900026\n",
      "[14000]\tvalid_0's auc: 0.900191\n",
      "Early stopping, best iteration is:\n",
      "[11206]\tvalid_0's auc: 0.90038\n",
      "Fold 5 started at Thu Mar 14 08:28:35 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.888348\n",
      "[2000]\tvalid_0's auc: 0.894258\n",
      "[3000]\tvalid_0's auc: 0.897532\n",
      "[4000]\tvalid_0's auc: 0.899451\n",
      "[5000]\tvalid_0's auc: 0.900377\n",
      "[6000]\tvalid_0's auc: 0.901001\n",
      "[7000]\tvalid_0's auc: 0.901513\n",
      "[8000]\tvalid_0's auc: 0.901913\n",
      "[9000]\tvalid_0's auc: 0.902137\n",
      "[10000]\tvalid_0's auc: 0.902663\n",
      "[11000]\tvalid_0's auc: 0.902749\n",
      "[12000]\tvalid_0's auc: 0.902806\n",
      "[13000]\tvalid_0's auc: 0.902595\n",
      "[14000]\tvalid_0's auc: 0.902492\n",
      "Early stopping, best iteration is:\n",
      "[11657]\tvalid_0's auc: 0.902868\n",
      "Fold 6 started at Thu Mar 14 09:21:39 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.896337\n",
      "[2000]\tvalid_0's auc: 0.904588\n",
      "[3000]\tvalid_0's auc: 0.907831\n",
      "[4000]\tvalid_0's auc: 0.910607\n",
      "[5000]\tvalid_0's auc: 0.912052\n",
      "[6000]\tvalid_0's auc: 0.913134\n",
      "[7000]\tvalid_0's auc: 0.913791\n",
      "[8000]\tvalid_0's auc: 0.914289\n",
      "[9000]\tvalid_0's auc: 0.914661\n",
      "[10000]\tvalid_0's auc: 0.914821\n",
      "[11000]\tvalid_0's auc: 0.914817\n",
      "[12000]\tvalid_0's auc: 0.914785\n",
      "[13000]\tvalid_0's auc: 0.914744\n",
      "Early stopping, best iteration is:\n",
      "[10847]\tvalid_0's auc: 0.91491\n",
      "Fold 7 started at Thu Mar 14 10:14:40 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884837\n",
      "[2000]\tvalid_0's auc: 0.890388\n",
      "[3000]\tvalid_0's auc: 0.893876\n",
      "[4000]\tvalid_0's auc: 0.896066\n",
      "[5000]\tvalid_0's auc: 0.897291\n",
      "[6000]\tvalid_0's auc: 0.898014\n",
      "[7000]\tvalid_0's auc: 0.898636\n",
      "[8000]\tvalid_0's auc: 0.898761\n",
      "[9000]\tvalid_0's auc: 0.898726\n",
      "[10000]\tvalid_0's auc: 0.898709\n",
      "[11000]\tvalid_0's auc: 0.898421\n",
      "Early stopping, best iteration is:\n",
      "[8581]\tvalid_0's auc: 0.898829\n",
      "Fold 8 started at Thu Mar 14 10:58:15 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.888589\n",
      "[2000]\tvalid_0's auc: 0.895659\n",
      "[3000]\tvalid_0's auc: 0.900142\n",
      "[4000]\tvalid_0's auc: 0.902438\n",
      "[5000]\tvalid_0's auc: 0.903775\n",
      "[6000]\tvalid_0's auc: 0.904602\n",
      "[7000]\tvalid_0's auc: 0.905023\n",
      "[8000]\tvalid_0's auc: 0.905663\n",
      "[9000]\tvalid_0's auc: 0.905812\n",
      "[10000]\tvalid_0's auc: 0.905828\n",
      "[11000]\tvalid_0's auc: 0.905856\n",
      "[12000]\tvalid_0's auc: 0.905866\n",
      "[13000]\tvalid_0's auc: 0.905898\n",
      "[14000]\tvalid_0's auc: 0.905728\n",
      "[15000]\tvalid_0's auc: 0.905631\n",
      "[16000]\tvalid_0's auc: 0.90531\n",
      "Early stopping, best iteration is:\n",
      "[13046]\tvalid_0's auc: 0.905933\n",
      "Fold 9 started at Thu Mar 14 11:57:42 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.888469\n",
      "[2000]\tvalid_0's auc: 0.895952\n",
      "[3000]\tvalid_0's auc: 0.899461\n",
      "[4000]\tvalid_0's auc: 0.902348\n",
      "[5000]\tvalid_0's auc: 0.903673\n",
      "[6000]\tvalid_0's auc: 0.904467\n",
      "[7000]\tvalid_0's auc: 0.905142\n",
      "[8000]\tvalid_0's auc: 0.905512\n",
      "[9000]\tvalid_0's auc: 0.90574\n",
      "[10000]\tvalid_0's auc: 0.905762\n",
      "[11000]\tvalid_0's auc: 0.905902\n",
      "[12000]\tvalid_0's auc: 0.9058\n",
      "[13000]\tvalid_0's auc: 0.905779\n",
      "[14000]\tvalid_0's auc: 0.905681\n",
      "Early stopping, best iteration is:\n",
      "[11261]\tvalid_0's auc: 0.905937\n",
      "Fold 10 started at Thu Mar 14 12:50:08 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.881842\n",
      "[2000]\tvalid_0's auc: 0.889388\n",
      "[3000]\tvalid_0's auc: 0.892893\n",
      "[4000]\tvalid_0's auc: 0.895218\n",
      "[5000]\tvalid_0's auc: 0.89657\n",
      "[6000]\tvalid_0's auc: 0.897437\n",
      "[7000]\tvalid_0's auc: 0.897846\n",
      "[8000]\tvalid_0's auc: 0.898357\n",
      "[9000]\tvalid_0's auc: 0.898192\n",
      "[10000]\tvalid_0's auc: 0.898398\n",
      "[11000]\tvalid_0's auc: 0.898469\n",
      "[12000]\tvalid_0's auc: 0.898694\n",
      "[13000]\tvalid_0's auc: 0.89874\n",
      "[14000]\tvalid_0's auc: 0.898636\n",
      "[15000]\tvalid_0's auc: 0.898472\n",
      "Early stopping, best iteration is:\n",
      "[12706]\tvalid_0's auc: 0.898881\n",
      "Fold 11 started at Thu Mar 14 13:47:14 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.875912\n",
      "[2000]\tvalid_0's auc: 0.884764\n",
      "[3000]\tvalid_0's auc: 0.888458\n",
      "[4000]\tvalid_0's auc: 0.890755\n",
      "[5000]\tvalid_0's auc: 0.891959\n",
      "[6000]\tvalid_0's auc: 0.893162\n",
      "[7000]\tvalid_0's auc: 0.893915\n",
      "[8000]\tvalid_0's auc: 0.894374\n",
      "[9000]\tvalid_0's auc: 0.894552\n",
      "[10000]\tvalid_0's auc: 0.894858\n",
      "[11000]\tvalid_0's auc: 0.894993\n",
      "[12000]\tvalid_0's auc: 0.894898\n",
      "[13000]\tvalid_0's auc: 0.894981\n",
      "[14000]\tvalid_0's auc: 0.895099\n",
      "[15000]\tvalid_0's auc: 0.895235\n",
      "[16000]\tvalid_0's auc: 0.895266\n",
      "[17000]\tvalid_0's auc: 0.895308\n",
      "[18000]\tvalid_0's auc: 0.895285\n",
      "[19000]\tvalid_0's auc: 0.895122\n",
      "[20000]\tvalid_0's auc: 0.895047\n",
      "[21000]\tvalid_0's auc: 0.895139\n",
      "Early stopping, best iteration is:\n",
      "[18367]\tvalid_0's auc: 0.895446\n",
      "Fold 12 started at Thu Mar 14 15:06:28 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.874647\n",
      "[2000]\tvalid_0's auc: 0.881513\n",
      "[3000]\tvalid_0's auc: 0.886445\n",
      "[4000]\tvalid_0's auc: 0.889645\n",
      "[5000]\tvalid_0's auc: 0.891219\n",
      "[6000]\tvalid_0's auc: 0.892556\n",
      "[7000]\tvalid_0's auc: 0.893197\n",
      "[8000]\tvalid_0's auc: 0.89382\n",
      "[9000]\tvalid_0's auc: 0.894434\n",
      "[10000]\tvalid_0's auc: 0.894698\n",
      "[11000]\tvalid_0's auc: 0.894717\n",
      "[12000]\tvalid_0's auc: 0.894648\n",
      "[13000]\tvalid_0's auc: 0.89459\n",
      "[14000]\tvalid_0's auc: 0.894475\n",
      "[15000]\tvalid_0's auc: 0.894215\n",
      "Early stopping, best iteration is:\n",
      "[12176]\tvalid_0's auc: 0.894789\n",
      "Fold 13 started at Thu Mar 14 16:03:38 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883927\n",
      "[2000]\tvalid_0's auc: 0.89085\n",
      "[3000]\tvalid_0's auc: 0.894277\n",
      "[4000]\tvalid_0's auc: 0.896399\n",
      "[5000]\tvalid_0's auc: 0.897518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6000]\tvalid_0's auc: 0.898519\n",
      "[7000]\tvalid_0's auc: 0.899167\n",
      "[8000]\tvalid_0's auc: 0.899198\n",
      "[9000]\tvalid_0's auc: 0.899229\n",
      "[10000]\tvalid_0's auc: 0.899368\n",
      "[11000]\tvalid_0's auc: 0.89937\n",
      "[12000]\tvalid_0's auc: 0.899168\n",
      "[13000]\tvalid_0's auc: 0.898912\n",
      "Early stopping, best iteration is:\n",
      "[10860]\tvalid_0's auc: 0.899437\n",
      "Fold 14 started at Thu Mar 14 16:54:28 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88128\n",
      "[2000]\tvalid_0's auc: 0.889413\n",
      "[3000]\tvalid_0's auc: 0.89275\n",
      "[4000]\tvalid_0's auc: 0.895103\n",
      "[5000]\tvalid_0's auc: 0.896249\n",
      "[6000]\tvalid_0's auc: 0.897138\n",
      "[7000]\tvalid_0's auc: 0.897352\n",
      "[8000]\tvalid_0's auc: 0.897476\n",
      "[9000]\tvalid_0's auc: 0.897613\n",
      "[10000]\tvalid_0's auc: 0.897538\n",
      "[11000]\tvalid_0's auc: 0.897629\n",
      "[12000]\tvalid_0's auc: 0.897616\n",
      "[13000]\tvalid_0's auc: 0.897533\n",
      "[14000]\tvalid_0's auc: 0.897322\n",
      "Early stopping, best iteration is:\n",
      "[11279]\tvalid_0's auc: 0.897728\n",
      "CV mean score: 0.9012, std: 0.0049.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_old_lgb_1_10_4', oof_lgb)\n",
    "np.save('../cache/preds_old_lgb_1_10_4', prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = prediction_lgb\n",
    "sub.to_csv('../submissions/sub12l3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Thu Mar 14 18:47:44 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886195\n",
      "[2000]\tvalid_0's auc: 0.894865\n",
      "[3000]\tvalid_0's auc: 0.898807\n",
      "[4000]\tvalid_0's auc: 0.900392\n",
      "[5000]\tvalid_0's auc: 0.901534\n",
      "[6000]\tvalid_0's auc: 0.902106\n",
      "[7000]\tvalid_0's auc: 0.902659\n",
      "[8000]\tvalid_0's auc: 0.902801\n",
      "[9000]\tvalid_0's auc: 0.90277\n",
      "[10000]\tvalid_0's auc: 0.902863\n",
      "[11000]\tvalid_0's auc: 0.902752\n",
      "[12000]\tvalid_0's auc: 0.902757\n",
      "Early stopping, best iteration is:\n",
      "[9740]\tvalid_0's auc: 0.902911\n",
      "Fold 1 started at Thu Mar 14 18:50:39 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885634\n",
      "[2000]\tvalid_0's auc: 0.892644\n",
      "[3000]\tvalid_0's auc: 0.896406\n",
      "[4000]\tvalid_0's auc: 0.898081\n",
      "[5000]\tvalid_0's auc: 0.898797\n",
      "[6000]\tvalid_0's auc: 0.899409\n",
      "[7000]\tvalid_0's auc: 0.899704\n",
      "[8000]\tvalid_0's auc: 0.899592\n",
      "[9000]\tvalid_0's auc: 0.899504\n",
      "[10000]\tvalid_0's auc: 0.899392\n",
      "Early stopping, best iteration is:\n",
      "[7038]\tvalid_0's auc: 0.899748\n",
      "Fold 2 started at Thu Mar 14 18:55:25 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.881278\n",
      "[2000]\tvalid_0's auc: 0.888677\n",
      "[3000]\tvalid_0's auc: 0.893088\n",
      "[4000]\tvalid_0's auc: 0.895057\n",
      "[5000]\tvalid_0's auc: 0.896164\n",
      "[6000]\tvalid_0's auc: 0.896939\n",
      "[7000]\tvalid_0's auc: 0.897349\n",
      "[8000]\tvalid_0's auc: 0.897498\n",
      "[9000]\tvalid_0's auc: 0.897573\n",
      "[10000]\tvalid_0's auc: 0.897394\n",
      "[11000]\tvalid_0's auc: 0.897265\n",
      "Early stopping, best iteration is:\n",
      "[8398]\tvalid_0's auc: 0.897648\n",
      "Fold 3 started at Thu Mar 14 19:35:07 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886285\n",
      "[2000]\tvalid_0's auc: 0.893454\n",
      "[3000]\tvalid_0's auc: 0.897302\n",
      "[4000]\tvalid_0's auc: 0.899081\n",
      "[5000]\tvalid_0's auc: 0.899496\n",
      "[6000]\tvalid_0's auc: 0.899682\n",
      "[7000]\tvalid_0's auc: 0.899821\n",
      "[8000]\tvalid_0's auc: 0.899841\n",
      "[9000]\tvalid_0's auc: 0.899786\n",
      "[10000]\tvalid_0's auc: 0.899592\n",
      "Early stopping, best iteration is:\n",
      "[7622]\tvalid_0's auc: 0.900027\n",
      "Fold 4 started at Thu Mar 14 20:14:07 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885993\n",
      "[2000]\tvalid_0's auc: 0.893125\n",
      "[3000]\tvalid_0's auc: 0.896787\n",
      "[4000]\tvalid_0's auc: 0.898706\n",
      "[5000]\tvalid_0's auc: 0.899528\n",
      "[6000]\tvalid_0's auc: 0.899511\n",
      "[7000]\tvalid_0's auc: 0.899796\n",
      "[8000]\tvalid_0's auc: 0.899941\n",
      "[9000]\tvalid_0's auc: 0.899807\n",
      "[10000]\tvalid_0's auc: 0.899809\n",
      "Early stopping, best iteration is:\n",
      "[7385]\tvalid_0's auc: 0.89998\n",
      "Fold 5 started at Thu Mar 14 20:51:30 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88876\n",
      "[2000]\tvalid_0's auc: 0.89539\n",
      "[3000]\tvalid_0's auc: 0.898661\n",
      "[4000]\tvalid_0's auc: 0.900046\n",
      "[5000]\tvalid_0's auc: 0.901186\n",
      "[6000]\tvalid_0's auc: 0.901809\n",
      "[7000]\tvalid_0's auc: 0.901841\n",
      "[8000]\tvalid_0's auc: 0.901683\n",
      "[9000]\tvalid_0's auc: 0.901859\n",
      "[10000]\tvalid_0's auc: 0.901942\n",
      "[11000]\tvalid_0's auc: 0.902075\n",
      "[12000]\tvalid_0's auc: 0.902195\n",
      "[13000]\tvalid_0's auc: 0.902292\n",
      "[14000]\tvalid_0's auc: 0.902302\n",
      "[15000]\tvalid_0's auc: 0.902375\n",
      "[16000]\tvalid_0's auc: 0.902146\n",
      "[17000]\tvalid_0's auc: 0.902181\n",
      "Early stopping, best iteration is:\n",
      "[14892]\tvalid_0's auc: 0.902421\n",
      "Fold 6 started at Thu Mar 14 21:56:10 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.896802\n",
      "[2000]\tvalid_0's auc: 0.905678\n",
      "[3000]\tvalid_0's auc: 0.909314\n",
      "[4000]\tvalid_0's auc: 0.911685\n",
      "[5000]\tvalid_0's auc: 0.912943\n",
      "[6000]\tvalid_0's auc: 0.913311\n",
      "[7000]\tvalid_0's auc: 0.913795\n",
      "[8000]\tvalid_0's auc: 0.91379\n",
      "[9000]\tvalid_0's auc: 0.913949\n",
      "[10000]\tvalid_0's auc: 0.914273\n",
      "[11000]\tvalid_0's auc: 0.91418\n",
      "[12000]\tvalid_0's auc: 0.914158\n",
      "Early stopping, best iteration is:\n",
      "[9928]\tvalid_0's auc: 0.914329\n",
      "Fold 7 started at Thu Mar 14 22:40:42 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884297\n",
      "[2000]\tvalid_0's auc: 0.891076\n",
      "[3000]\tvalid_0's auc: 0.894677\n",
      "[4000]\tvalid_0's auc: 0.89666\n",
      "[5000]\tvalid_0's auc: 0.89773\n",
      "[6000]\tvalid_0's auc: 0.898369\n",
      "[7000]\tvalid_0's auc: 0.898407\n",
      "[8000]\tvalid_0's auc: 0.898481\n",
      "[9000]\tvalid_0's auc: 0.898188\n",
      "[10000]\tvalid_0's auc: 0.897928\n",
      "Early stopping, best iteration is:\n",
      "[7768]\tvalid_0's auc: 0.898554\n",
      "Fold 8 started at Thu Mar 14 23:20:46 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.889049\n",
      "[2000]\tvalid_0's auc: 0.897476\n",
      "[3000]\tvalid_0's auc: 0.901368\n",
      "[4000]\tvalid_0's auc: 0.903923\n",
      "[5000]\tvalid_0's auc: 0.904779\n",
      "[6000]\tvalid_0's auc: 0.905399\n",
      "[7000]\tvalid_0's auc: 0.905636\n",
      "[8000]\tvalid_0's auc: 0.905713\n",
      "[9000]\tvalid_0's auc: 0.905937\n",
      "[10000]\tvalid_0's auc: 0.905898\n",
      "[11000]\tvalid_0's auc: 0.905977\n",
      "[12000]\tvalid_0's auc: 0.90579\n",
      "[13000]\tvalid_0's auc: 0.905718\n",
      "[14000]\tvalid_0's auc: 0.905774\n",
      "Early stopping, best iteration is:\n",
      "[11356]\tvalid_0's auc: 0.906047\n",
      "Fold 9 started at Fri Mar 15 00:13:23 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.889693\n",
      "[2000]\tvalid_0's auc: 0.897205\n",
      "[3000]\tvalid_0's auc: 0.900984\n",
      "[4000]\tvalid_0's auc: 0.902908\n",
      "[5000]\tvalid_0's auc: 0.90439\n",
      "[6000]\tvalid_0's auc: 0.904802\n",
      "[7000]\tvalid_0's auc: 0.905245\n",
      "[8000]\tvalid_0's auc: 0.905658\n",
      "[9000]\tvalid_0's auc: 0.905741\n",
      "[10000]\tvalid_0's auc: 0.905989\n",
      "[11000]\tvalid_0's auc: 0.905668\n",
      "[12000]\tvalid_0's auc: 0.905604\n",
      "Early stopping, best iteration is:\n",
      "[9913]\tvalid_0's auc: 0.906034\n",
      "Fold 10 started at Fri Mar 15 00:58:17 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.882014\n",
      "[2000]\tvalid_0's auc: 0.890234\n",
      "[3000]\tvalid_0's auc: 0.894\n",
      "[4000]\tvalid_0's auc: 0.896246\n",
      "[5000]\tvalid_0's auc: 0.897527\n",
      "[6000]\tvalid_0's auc: 0.897776\n",
      "[7000]\tvalid_0's auc: 0.898438\n",
      "[8000]\tvalid_0's auc: 0.89892\n",
      "[9000]\tvalid_0's auc: 0.899355\n",
      "[10000]\tvalid_0's auc: 0.899358\n",
      "[11000]\tvalid_0's auc: 0.899182\n",
      "[12000]\tvalid_0's auc: 0.899023\n",
      "Early stopping, best iteration is:\n",
      "[9369]\tvalid_0's auc: 0.899468\n",
      "Fold 11 started at Fri Mar 15 01:41:24 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.87773\n",
      "[2000]\tvalid_0's auc: 0.885545\n",
      "[3000]\tvalid_0's auc: 0.889387\n",
      "[4000]\tvalid_0's auc: 0.891541\n",
      "[5000]\tvalid_0's auc: 0.892334\n",
      "[6000]\tvalid_0's auc: 0.893137\n",
      "[7000]\tvalid_0's auc: 0.893718\n",
      "[8000]\tvalid_0's auc: 0.893874\n",
      "[9000]\tvalid_0's auc: 0.893918\n",
      "[10000]\tvalid_0's auc: 0.894082\n",
      "[11000]\tvalid_0's auc: 0.894186\n",
      "[12000]\tvalid_0's auc: 0.894012\n",
      "[13000]\tvalid_0's auc: 0.894038\n",
      "Early stopping, best iteration is:\n",
      "[10721]\tvalid_0's auc: 0.894266\n",
      "Fold 12 started at Fri Mar 15 02:31:03 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.874941\n",
      "[2000]\tvalid_0's auc: 0.882787\n",
      "[3000]\tvalid_0's auc: 0.887924\n",
      "[4000]\tvalid_0's auc: 0.890363\n",
      "[5000]\tvalid_0's auc: 0.892085\n",
      "[6000]\tvalid_0's auc: 0.893245\n",
      "[7000]\tvalid_0's auc: 0.893498\n",
      "[8000]\tvalid_0's auc: 0.893786\n",
      "[9000]\tvalid_0's auc: 0.894033\n",
      "[10000]\tvalid_0's auc: 0.894221\n",
      "[11000]\tvalid_0's auc: 0.894102\n",
      "[12000]\tvalid_0's auc: 0.89386\n",
      "Early stopping, best iteration is:\n",
      "[9522]\tvalid_0's auc: 0.894289\n",
      "Fold 13 started at Fri Mar 15 03:11:32 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883767\n",
      "[2000]\tvalid_0's auc: 0.890995\n",
      "[3000]\tvalid_0's auc: 0.89472\n",
      "[4000]\tvalid_0's auc: 0.896865\n",
      "[5000]\tvalid_0's auc: 0.898275\n",
      "[6000]\tvalid_0's auc: 0.898861\n",
      "[7000]\tvalid_0's auc: 0.898945\n",
      "[8000]\tvalid_0's auc: 0.898872\n",
      "[9000]\tvalid_0's auc: 0.898937\n",
      "[10000]\tvalid_0's auc: 0.898796\n",
      "Early stopping, best iteration is:\n",
      "[7125]\tvalid_0's auc: 0.899045\n",
      "Fold 14 started at Fri Mar 15 03:49:02 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.879701\n",
      "[2000]\tvalid_0's auc: 0.889909\n",
      "[3000]\tvalid_0's auc: 0.893638\n",
      "[4000]\tvalid_0's auc: 0.895677\n",
      "[5000]\tvalid_0's auc: 0.897308\n",
      "[6000]\tvalid_0's auc: 0.897872\n",
      "[7000]\tvalid_0's auc: 0.897806\n",
      "[8000]\tvalid_0's auc: 0.897764\n",
      "[9000]\tvalid_0's auc: 0.897803\n",
      "Early stopping, best iteration is:\n",
      "[6118]\tvalid_0's auc: 0.898004\n",
      "CV mean score: 0.9009, std: 0.0049.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X1, X2, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_lgb_1_10_4', oof_lgb)\n",
    "np.save('../cache/preds_new_quant_lgb_1_10_4', prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = prediction_lgb\n",
    "sub.to_csv('../submissions/sub12l4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0.103815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>0.205620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0.162792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.201517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0.042881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    target\n",
       "0  test_0  0.103815\n",
       "1  test_1  0.205620\n",
       "2  test_2  0.162792\n",
       "3  test_3  0.201517\n",
       "4  test_4  0.042881"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_blend = pd.read_csv('../submissions/blend_ver10.csv')\n",
    "sub_blend.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0.059043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>0.132137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0.105327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.133768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0.038310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    target\n",
       "0  test_0  0.059043\n",
       "1  test_1  0.132137\n",
       "2  test_2  0.105327\n",
       "3  test_3  0.133768\n",
       "4  test_4  0.038310"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_12s = pd.read_csv('../submissions/sub12s.csv')\n",
    "sub_12s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0.097262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>0.209279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0.169372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.209088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0.044063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    target\n",
       "0  test_0  0.097262\n",
       "1  test_1  0.209279\n",
       "2  test_2  0.169372\n",
       "3  test_3  0.209088\n",
       "4  test_4  0.044063"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_l1 = pd.read_csv('../submissions/sub12l1.csv')\n",
    "sub_l1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0.099658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>0.198545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0.166282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.220686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0.043329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    target\n",
       "0  test_0  0.099658\n",
       "1  test_1  0.198545\n",
       "2  test_2  0.166282\n",
       "3  test_3  0.220686\n",
       "4  test_4  0.043329"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_l2 = pd.read_csv('../submissions/sub12l2.csv')\n",
    "sub_l2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0.061032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>0.136968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0.102828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.142510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0.038794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    target\n",
       "0  test_0  0.061032\n",
       "1  test_1  0.136968\n",
       "2  test_2  0.102828\n",
       "3  test_3  0.142510\n",
       "4  test_4  0.038794"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_12w = pd.read_csv('../submissions/sub12w.csv')\n",
    "sub_12w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_my = 0.7 * sub_blend['target'] + 0.2 * sub_l1['target'] + 0.1 * sub_12s['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = sub_my\n",
    "sub.to_csv('../submissions/sub12l5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_my2 = 0.6 * sub_blend['target'] + 0.2 * sub_l2['target'] + 0.1 * sub_12w['target'] +  0.1 * sub_12s['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = sub_my2\n",
    "sub.to_csv('../submissions/sub12l6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_0 94667\n",
      "var_1 108929\n",
      "var_2 86554\n",
      "var_3 74594\n",
      "var_4 63514\n",
      "var_5 141028\n",
      "var_6 38595\n",
      "var_7 103057\n",
      "var_8 98616\n",
      "var_9 49416\n",
      "var_10 128762\n",
      "var_11 130188\n",
      "var_12 9559\n",
      "var_13 115181\n",
      "var_14 79122\n",
      "var_15 19809\n",
      "var_16 86915\n",
      "var_17 137823\n",
      "var_18 139513\n",
      "var_19 144179\n",
      "var_20 127763\n",
      "var_21 140062\n",
      "var_22 90653\n",
      "var_23 24911\n",
      "var_24 105101\n",
      "var_25 14850\n",
      "var_26 127086\n",
      "var_27 60183\n",
      "var_28 35857\n",
      "var_29 88338\n",
      "var_30 145973\n",
      "var_31 77387\n",
      "var_32 85961\n",
      "var_33 112235\n",
      "var_34 25163\n",
      "var_35 122384\n",
      "var_36 96403\n",
      "var_37 79039\n",
      "var_38 115364\n",
      "var_39 112669\n",
      "var_40 141878\n",
      "var_41 131896\n",
      "var_42 31590\n",
      "var_43 15183\n",
      "var_44 127702\n",
      "var_45 169965\n",
      "var_46 93446\n",
      "var_47 154779\n",
      "var_48 152039\n",
      "var_49 140640\n",
      "var_50 32307\n",
      "var_51 143455\n",
      "var_52 121311\n",
      "var_53 33460\n",
      "var_54 144775\n",
      "var_55 128076\n",
      "var_56 103044\n",
      "var_57 35542\n",
      "var_58 113904\n",
      "var_59 37741\n",
      "var_60 113762\n",
      "var_61 159368\n",
      "var_62 74774\n",
      "var_63 97098\n",
      "var_64 59376\n",
      "var_65 108347\n",
      "var_66 47716\n",
      "var_67 137252\n",
      "var_68 459\n",
      "var_69 110346\n",
      "var_70 153193\n",
      "var_71 13526\n",
      "var_72 110112\n",
      "var_73 142582\n",
      "var_74 161057\n",
      "var_75 129382\n",
      "var_76 139316\n",
      "var_77 106808\n",
      "var_78 72253\n",
      "var_79 53212\n",
      "var_80 136426\n",
      "var_81 79060\n",
      "var_82 144827\n",
      "var_83 144277\n",
      "var_84 133766\n",
      "var_85 108435\n",
      "var_86 140593\n",
      "var_87 125295\n",
      "var_88 84914\n",
      "var_89 103520\n",
      "var_90 157207\n",
      "var_91 7959\n",
      "var_92 110740\n",
      "var_93 26707\n",
      "var_94 89145\n",
      "var_95 29387\n",
      "var_96 148098\n",
      "var_97 158735\n",
      "var_98 33266\n",
      "var_99 69295\n",
      "var_100 150725\n",
      "var_101 122293\n",
      "var_102 146237\n",
      "var_103 9376\n",
      "var_104 72624\n",
      "var_105 39111\n",
      "var_106 71061\n",
      "var_107 137826\n",
      "var_108 8524\n",
      "var_109 112171\n",
      "var_110 106116\n",
      "var_111 46462\n",
      "var_112 60482\n",
      "var_113 116491\n",
      "var_114 43081\n",
      "var_115 86725\n",
      "var_116 63467\n",
      "var_117 164465\n",
      "var_118 143667\n",
      "var_119 112399\n",
      "var_120 158266\n",
      "var_121 64695\n",
      "var_122 121766\n",
      "var_123 129886\n",
      "var_124 91022\n",
      "var_125 16056\n",
      "var_126 32410\n",
      "var_127 95709\n",
      "var_128 98200\n",
      "var_129 113425\n",
      "var_130 36636\n",
      "var_131 21463\n",
      "var_132 57922\n",
      "var_133 19233\n",
      "var_134 131618\n",
      "var_135 140774\n",
      "var_136 156614\n",
      "var_137 144394\n",
      "var_138 117425\n",
      "var_139 137294\n",
      "var_140 121383\n",
      "var_141 134441\n",
      "var_142 128612\n",
      "var_143 94372\n",
      "var_144 40595\n",
      "var_145 108525\n",
      "var_146 84314\n",
      "var_147 137557\n",
      "var_148 10605\n",
      "var_149 148501\n",
      "var_150 83659\n",
      "var_151 109665\n",
      "var_152 95822\n",
      "var_153 73724\n",
      "var_154 119339\n",
      "var_155 127454\n",
      "var_156 40634\n",
      "var_157 126534\n",
      "var_158 144554\n",
      "var_159 112828\n",
      "var_160 156274\n",
      "var_161 11067\n",
      "var_162 57395\n",
      "var_163 123166\n",
      "var_164 122735\n",
      "var_165 119401\n",
      "var_166 17900\n",
      "var_167 140954\n",
      "var_168 97224\n",
      "var_169 18242\n",
      "var_170 113720\n",
      "var_171 125912\n",
      "var_172 143366\n",
      "var_173 128119\n",
      "var_174 134944\n",
      "var_175 92658\n",
      "var_176 142521\n",
      "var_177 85716\n",
      "var_178 145234\n",
      "var_179 90089\n",
      "var_180 123473\n",
      "var_181 56164\n",
      "var_182 149195\n",
      "var_183 117527\n",
      "var_184 145174\n",
      "var_185 120745\n",
      "var_186 98058\n",
      "var_187 157031\n",
      "var_188 108813\n",
      "var_189 41764\n",
      "var_190 114958\n",
      "var_191 94265\n",
      "var_192 59062\n",
      "var_193 110556\n",
      "var_194 97067\n",
      "var_195 57868\n",
      "var_196 125558\n",
      "var_197 40533\n",
      "var_198 94148\n",
      "var_199 149430\n"
     ]
    }
   ],
   "source": [
    "for col in list(X1.columns):\n",
    "    print(col, X1[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_12 9559\n",
      "var_15 19809\n",
      "var_25 14850\n",
      "var_43 15183\n",
      "var_68 459\n",
      "var_71 13526\n",
      "var_91 7959\n",
      "var_103 9376\n",
      "var_108 8524\n",
      "var_125 16056\n",
      "var_133 19233\n",
      "var_148 10605\n",
      "var_161 11067\n",
      "var_166 17900\n",
      "var_169 18242\n"
     ]
    }
   ],
   "source": [
    "for col in list(X1.columns):\n",
    "    if(X1[col].nunique()  < 20000):\n",
    "        print(col, X1[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.550245\tvalid_1's auc: 0.546033\n",
      "Early stopping, best iteration is:\n",
      "[43]\ttraining's auc: 0.543894\tvalid_1's auc: 0.546716\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546363\tvalid_1's auc: 0.543272\n",
      "[2000]\ttraining's auc: 0.546665\tvalid_1's auc: 0.543285\n",
      "Early stopping, best iteration is:\n",
      "[1099]\ttraining's auc: 0.546484\tvalid_1's auc: 0.543341\n",
      "var_0\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.545721\tvalid_1's auc: 0.54987\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttraining's auc: 0.542974\tvalid_1's auc: 0.551547\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54265\tvalid_1's auc: 0.548994\n",
      "[2000]\ttraining's auc: 0.543158\tvalid_1's auc: 0.550778\n",
      "[3000]\ttraining's auc: 0.543238\tvalid_1's auc: 0.550674\n",
      "Early stopping, best iteration is:\n",
      "[2197]\ttraining's auc: 0.543178\tvalid_1's auc: 0.550902\n",
      "var_1\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.55392\tvalid_1's auc: 0.558183\n",
      "Early stopping, best iteration is:\n",
      "[255]\ttraining's auc: 0.549826\tvalid_1's auc: 0.558381\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.550015\tvalid_1's auc: 0.555598\n",
      "Early stopping, best iteration is:\n",
      "[521]\ttraining's auc: 0.549648\tvalid_1's auc: 0.556115\n",
      "var_2\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515693\tvalid_1's auc: 0.510284\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttraining's auc: 0.513129\tvalid_1's auc: 0.511062\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514883\tvalid_1's auc: 0.508863\n",
      "[2000]\ttraining's auc: 0.515014\tvalid_1's auc: 0.509493\n",
      "[3000]\ttraining's auc: 0.515034\tvalid_1's auc: 0.509535\n",
      "Early stopping, best iteration is:\n",
      "[2071]\ttraining's auc: 0.515034\tvalid_1's auc: 0.509535\n",
      "var_3\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516402\tvalid_1's auc: 0.506288\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.507312\tvalid_1's auc: 0.50793\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514316\tvalid_1's auc: 0.50678\n",
      "[2000]\ttraining's auc: 0.5146\tvalid_1's auc: 0.507684\n",
      "Early stopping, best iteration is:\n",
      "[1664]\ttraining's auc: 0.514567\tvalid_1's auc: 0.507989\n",
      "var_4\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532682\tvalid_1's auc: 0.524404\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttraining's auc: 0.527349\tvalid_1's auc: 0.526146\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.528303\tvalid_1's auc: 0.516182\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's auc: 0.52588\tvalid_1's auc: 0.517324\n",
      "var_5\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.562463\tvalid_1's auc: 0.557408\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttraining's auc: 0.55983\tvalid_1's auc: 0.559987\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.559163\tvalid_1's auc: 0.55661\n",
      "Early stopping, best iteration is:\n",
      "[592]\ttraining's auc: 0.558633\tvalid_1's auc: 0.557016\n",
      "var_6\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512312\tvalid_1's auc: 0.497816\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's auc: 0.506381\tvalid_1's auc: 0.499996\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510968\tvalid_1's auc: 0.498797\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.506224\tvalid_1's auc: 0.50021\n",
      "var_7\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.522807\tvalid_1's auc: 0.518589\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttraining's auc: 0.519376\tvalid_1's auc: 0.519581\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.522085\tvalid_1's auc: 0.519209\n",
      "[2000]\ttraining's auc: 0.522392\tvalid_1's auc: 0.519731\n",
      "Early stopping, best iteration is:\n",
      "[1858]\ttraining's auc: 0.522386\tvalid_1's auc: 0.519784\n",
      "var_8\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.547382\tvalid_1's auc: 0.542231\n",
      "Early stopping, best iteration is:\n",
      "[515]\ttraining's auc: 0.546513\tvalid_1's auc: 0.543088\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.544421\tvalid_1's auc: 0.539384\n",
      "[2000]\ttraining's auc: 0.544493\tvalid_1's auc: 0.539727\n",
      "[3000]\ttraining's auc: 0.544567\tvalid_1's auc: 0.539976\n",
      "Early stopping, best iteration is:\n",
      "[2551]\ttraining's auc: 0.544567\tvalid_1's auc: 0.539976\n",
      "var_9\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512114\tvalid_1's auc: 0.503713\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's auc: 0.506894\tvalid_1's auc: 0.507461\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.509159\tvalid_1's auc: 0.498951\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.505363\tvalid_1's auc: 0.504803\n",
      "var_10\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52121\tvalid_1's auc: 0.523786\n",
      "Early stopping, best iteration is:\n",
      "[213]\ttraining's auc: 0.520266\tvalid_1's auc: 0.524355\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.519226\tvalid_1's auc: 0.521207\n",
      "[2000]\ttraining's auc: 0.51962\tvalid_1's auc: 0.522558\n",
      "[3000]\ttraining's auc: 0.519623\tvalid_1's auc: 0.522514\n",
      "Early stopping, best iteration is:\n",
      "[2008]\ttraining's auc: 0.519622\tvalid_1's auc: 0.522594\n",
      "var_11\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.563783\tvalid_1's auc: 0.562899\n",
      "Early stopping, best iteration is:\n",
      "[793]\ttraining's auc: 0.56358\tvalid_1's auc: 0.562954\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.561006\tvalid_1's auc: 0.563367\n",
      "Early stopping, best iteration is:\n",
      "[924]\ttraining's auc: 0.560917\tvalid_1's auc: 0.563415\n",
      "var_12\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.557112\tvalid_1's auc: 0.543566\n",
      "Early stopping, best iteration is:\n",
      "[336]\ttraining's auc: 0.55581\tvalid_1's auc: 0.543818\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.55374\tvalid_1's auc: 0.541704\n",
      "[2000]\ttraining's auc: 0.554428\tvalid_1's auc: 0.541699\n",
      "Early stopping, best iteration is:\n",
      "[1128]\ttraining's auc: 0.554106\tvalid_1's auc: 0.542727\n",
      "var_13\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513307\tvalid_1's auc: 0.508323\n",
      "Early stopping, best iteration is:\n",
      "[95]\ttraining's auc: 0.507472\tvalid_1's auc: 0.509615\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510797\tvalid_1's auc: 0.503965\n",
      "[2000]\ttraining's auc: 0.510839\tvalid_1's auc: 0.504627\n",
      "[3000]\ttraining's auc: 0.510847\tvalid_1's auc: 0.505269\n",
      "Early stopping, best iteration is:\n",
      "[2418]\ttraining's auc: 0.510847\tvalid_1's auc: 0.505269\n",
      "var_14\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.520428\tvalid_1's auc: 0.512288\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's auc: 0.517209\tvalid_1's auc: 0.513548\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.518603\tvalid_1's auc: 0.50691\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's auc: 0.51522\tvalid_1's auc: 0.507584\n",
      "var_15\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516178\tvalid_1's auc: 0.502091\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's auc: 0.506521\tvalid_1's auc: 0.511464\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513328\tvalid_1's auc: 0.503882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\ttraining's auc: 0.513395\tvalid_1's auc: 0.503252\n",
      "Early stopping, best iteration is:\n",
      "[1000]\ttraining's auc: 0.513328\tvalid_1's auc: 0.503882\n",
      "var_16\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512121\tvalid_1's auc: 0.493472\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.50197\tvalid_1's auc: 0.49966\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.50806\tvalid_1's auc: 0.490096\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.501121\tvalid_1's auc: 0.501481\n",
      "var_17\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.539594\tvalid_1's auc: 0.539151\n",
      "Early stopping, best iteration is:\n",
      "[467]\ttraining's auc: 0.538747\tvalid_1's auc: 0.539827\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537858\tvalid_1's auc: 0.53852\n",
      "Early stopping, best iteration is:\n",
      "[849]\ttraining's auc: 0.537714\tvalid_1's auc: 0.538593\n",
      "var_18\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52048\tvalid_1's auc: 0.506219\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's auc: 0.512708\tvalid_1's auc: 0.509987\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515932\tvalid_1's auc: 0.505484\n",
      "Early stopping, best iteration is:\n",
      "[560]\ttraining's auc: 0.5157\tvalid_1's auc: 0.506211\n",
      "var_19\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.524044\tvalid_1's auc: 0.515909\n",
      "Early stopping, best iteration is:\n",
      "[261]\ttraining's auc: 0.519295\tvalid_1's auc: 0.51737\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.522018\tvalid_1's auc: 0.516432\n",
      "Early stopping, best iteration is:\n",
      "[383]\ttraining's auc: 0.52083\tvalid_1's auc: 0.517199\n",
      "var_20\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.559611\tvalid_1's auc: 0.543623\n",
      "Early stopping, best iteration is:\n",
      "[288]\ttraining's auc: 0.556971\tvalid_1's auc: 0.545686\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.556277\tvalid_1's auc: 0.5398\n",
      "[2000]\ttraining's auc: 0.556812\tvalid_1's auc: 0.540989\n",
      "[3000]\ttraining's auc: 0.556825\tvalid_1's auc: 0.541062\n",
      "Early stopping, best iteration is:\n",
      "[2849]\ttraining's auc: 0.556825\tvalid_1's auc: 0.541062\n",
      "var_21\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.555077\tvalid_1's auc: 0.550652\n",
      "Early stopping, best iteration is:\n",
      "[264]\ttraining's auc: 0.553187\tvalid_1's auc: 0.552574\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.551841\tvalid_1's auc: 0.549585\n",
      "[2000]\ttraining's auc: 0.552385\tvalid_1's auc: 0.550848\n",
      "[3000]\ttraining's auc: 0.55243\tvalid_1's auc: 0.5511\n",
      "Early stopping, best iteration is:\n",
      "[2664]\ttraining's auc: 0.55243\tvalid_1's auc: 0.5511\n",
      "var_22\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527352\tvalid_1's auc: 0.519989\n",
      "Early stopping, best iteration is:\n",
      "[254]\ttraining's auc: 0.525269\tvalid_1's auc: 0.521343\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.524219\tvalid_1's auc: 0.519635\n",
      "[2000]\ttraining's auc: 0.524412\tvalid_1's auc: 0.519987\n",
      "Early stopping, best iteration is:\n",
      "[1042]\ttraining's auc: 0.524357\tvalid_1's auc: 0.520208\n",
      "var_23\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532607\tvalid_1's auc: 0.526024\n",
      "[2000]\ttraining's auc: 0.533347\tvalid_1's auc: 0.525873\n",
      "Early stopping, best iteration is:\n",
      "[1146]\ttraining's auc: 0.532711\tvalid_1's auc: 0.526307\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529341\tvalid_1's auc: 0.522488\n",
      "Early stopping, best iteration is:\n",
      "[298]\ttraining's auc: 0.527959\tvalid_1's auc: 0.52428\n",
      "var_24\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514496\tvalid_1's auc: 0.511181\n",
      "Early stopping, best iteration is:\n",
      "[125]\ttraining's auc: 0.511722\tvalid_1's auc: 0.51487\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513274\tvalid_1's auc: 0.512036\n",
      "[2000]\ttraining's auc: 0.51334\tvalid_1's auc: 0.512556\n",
      "Early stopping, best iteration is:\n",
      "[1936]\ttraining's auc: 0.51334\tvalid_1's auc: 0.512556\n",
      "var_25\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.55479\tvalid_1's auc: 0.55414\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttraining's auc: 0.552268\tvalid_1's auc: 0.55714\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.552973\tvalid_1's auc: 0.550859\n",
      "Early stopping, best iteration is:\n",
      "[325]\ttraining's auc: 0.549873\tvalid_1's auc: 0.551643\n",
      "var_26\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511243\tvalid_1's auc: 0.501554\n",
      "Early stopping, best iteration is:\n",
      "[980]\ttraining's auc: 0.511239\tvalid_1's auc: 0.501559\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.509657\tvalid_1's auc: 0.501472\n",
      "Early stopping, best iteration is:\n",
      "[770]\ttraining's auc: 0.50953\tvalid_1's auc: 0.502109\n",
      "var_27\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.530435\tvalid_1's auc: 0.514053\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's auc: 0.525172\tvalid_1's auc: 0.514565\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527898\tvalid_1's auc: 0.517771\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's auc: 0.525624\tvalid_1's auc: 0.52026\n",
      "var_28\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513749\tvalid_1's auc: 0.490035\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.506909\tvalid_1's auc: 0.494666\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51081\tvalid_1's auc: 0.491133\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.502924\tvalid_1's auc: 0.497724\n",
      "var_29\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511179\tvalid_1's auc: 0.507241\n",
      "Early stopping, best iteration is:\n",
      "[130]\ttraining's auc: 0.50708\tvalid_1's auc: 0.508401\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.508841\tvalid_1's auc: 0.50747\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's auc: 0.507002\tvalid_1's auc: 0.508318\n",
      "var_30\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526655\tvalid_1's auc: 0.535381\n",
      "[2000]\ttraining's auc: 0.528128\tvalid_1's auc: 0.535556\n",
      "[3000]\ttraining's auc: 0.528795\tvalid_1's auc: 0.535901\n",
      "Early stopping, best iteration is:\n",
      "[2575]\ttraining's auc: 0.528649\tvalid_1's auc: 0.535978\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.525696\tvalid_1's auc: 0.534306\n",
      "[2000]\ttraining's auc: 0.525738\tvalid_1's auc: 0.534338\n",
      "Early stopping, best iteration is:\n",
      "[1860]\ttraining's auc: 0.525729\tvalid_1's auc: 0.534506\n",
      "var_31\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532574\tvalid_1's auc: 0.525689\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's auc: 0.528281\tvalid_1's auc: 0.528941\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52962\tvalid_1's auc: 0.521956\n",
      "Early stopping, best iteration is:\n",
      "[781]\ttraining's auc: 0.529066\tvalid_1's auc: 0.52558\n",
      "var_32\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.541486\tvalid_1's auc: 0.541929\n",
      "Early stopping, best iteration is:\n",
      "[106]\ttraining's auc: 0.539934\tvalid_1's auc: 0.544182\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538653\tvalid_1's auc: 0.544536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\ttraining's auc: 0.539263\tvalid_1's auc: 0.544065\n",
      "Early stopping, best iteration is:\n",
      "[1079]\ttraining's auc: 0.538793\tvalid_1's auc: 0.54479\n",
      "var_33\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.549455\tvalid_1's auc: 0.543827\n",
      "Early stopping, best iteration is:\n",
      "[78]\ttraining's auc: 0.54791\tvalid_1's auc: 0.546052\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.547249\tvalid_1's auc: 0.544815\n",
      "[2000]\ttraining's auc: 0.547606\tvalid_1's auc: 0.544171\n",
      "Early stopping, best iteration is:\n",
      "[1764]\ttraining's auc: 0.547417\tvalid_1's auc: 0.545053\n",
      "var_34\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535724\tvalid_1's auc: 0.539947\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttraining's auc: 0.533103\tvalid_1's auc: 0.541456\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53336\tvalid_1's auc: 0.542444\n",
      "Early stopping, best iteration is:\n",
      "[817]\ttraining's auc: 0.533169\tvalid_1's auc: 0.542757\n",
      "var_35\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540964\tvalid_1's auc: 0.533761\n",
      "Early stopping, best iteration is:\n",
      "[550]\ttraining's auc: 0.540503\tvalid_1's auc: 0.534142\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538658\tvalid_1's auc: 0.532433\n",
      "Early stopping, best iteration is:\n",
      "[716]\ttraining's auc: 0.538523\tvalid_1's auc: 0.534301\n",
      "var_36\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516278\tvalid_1's auc: 0.503821\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.504794\tvalid_1's auc: 0.50507\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513278\tvalid_1's auc: 0.509417\n",
      "Early stopping, best iteration is:\n",
      "[510]\ttraining's auc: 0.512957\tvalid_1's auc: 0.510697\n",
      "var_37\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515595\tvalid_1's auc: 0.506821\n",
      "[2000]\ttraining's auc: 0.51658\tvalid_1's auc: 0.506875\n",
      "[3000]\ttraining's auc: 0.517447\tvalid_1's auc: 0.507592\n",
      "[4000]\ttraining's auc: 0.518378\tvalid_1's auc: 0.507063\n",
      "Early stopping, best iteration is:\n",
      "[3355]\ttraining's auc: 0.517765\tvalid_1's auc: 0.507913\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.50975\tvalid_1's auc: 0.507718\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.507444\tvalid_1's auc: 0.508336\n",
      "var_38\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515994\tvalid_1's auc: 0.502299\n",
      "Early stopping, best iteration is:\n",
      "[472]\ttraining's auc: 0.511741\tvalid_1's auc: 0.502459\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513181\tvalid_1's auc: 0.49631\n",
      "Early stopping, best iteration is:\n",
      "[121]\ttraining's auc: 0.511265\tvalid_1's auc: 0.499956\n",
      "var_39\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.547383\tvalid_1's auc: 0.542993\n",
      "Early stopping, best iteration is:\n",
      "[251]\ttraining's auc: 0.546003\tvalid_1's auc: 0.543982\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.544583\tvalid_1's auc: 0.543364\n",
      "Early stopping, best iteration is:\n",
      "[727]\ttraining's auc: 0.544349\tvalid_1's auc: 0.543727\n",
      "var_40\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511326\tvalid_1's auc: 0.503375\n",
      "[2000]\ttraining's auc: 0.513756\tvalid_1's auc: 0.503539\n",
      "Early stopping, best iteration is:\n",
      "[1870]\ttraining's auc: 0.513649\tvalid_1's auc: 0.504006\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.508939\tvalid_1's auc: 0.50258\n",
      "Early stopping, best iteration is:\n",
      "[268]\ttraining's auc: 0.508421\tvalid_1's auc: 0.504545\n",
      "var_41\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512653\tvalid_1's auc: 0.502701\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's auc: 0.509593\tvalid_1's auc: 0.504371\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511454\tvalid_1's auc: 0.499842\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's auc: 0.505678\tvalid_1's auc: 0.503459\n",
      "var_42\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.530231\tvalid_1's auc: 0.522243\n",
      "Early stopping, best iteration is:\n",
      "[271]\ttraining's auc: 0.529227\tvalid_1's auc: 0.523617\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52751\tvalid_1's auc: 0.518673\n",
      "[2000]\ttraining's auc: 0.527673\tvalid_1's auc: 0.518747\n",
      "Early stopping, best iteration is:\n",
      "[1095]\ttraining's auc: 0.527575\tvalid_1's auc: 0.519231\n",
      "var_43\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54564\tvalid_1's auc: 0.54112\n",
      "Early stopping, best iteration is:\n",
      "[812]\ttraining's auc: 0.545059\tvalid_1's auc: 0.542686\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.542183\tvalid_1's auc: 0.542555\n",
      "[2000]\ttraining's auc: 0.542436\tvalid_1's auc: 0.543515\n",
      "[3000]\ttraining's auc: 0.54249\tvalid_1's auc: 0.544019\n",
      "Early stopping, best iteration is:\n",
      "[2821]\ttraining's auc: 0.54249\tvalid_1's auc: 0.544019\n",
      "var_44\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.524118\tvalid_1's auc: 0.516078\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's auc: 0.518652\tvalid_1's auc: 0.519995\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.520639\tvalid_1's auc: 0.520941\n",
      "[2000]\ttraining's auc: 0.520817\tvalid_1's auc: 0.521021\n",
      "Early stopping, best iteration is:\n",
      "[1552]\ttraining's auc: 0.520738\tvalid_1's auc: 0.521334\n",
      "var_45\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513131\tvalid_1's auc: 0.497677\n",
      "Early stopping, best iteration is:\n",
      "[91]\ttraining's auc: 0.507076\tvalid_1's auc: 0.503906\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510301\tvalid_1's auc: 0.501933\n",
      "[2000]\ttraining's auc: 0.510385\tvalid_1's auc: 0.503016\n",
      "Early stopping, best iteration is:\n",
      "[1982]\ttraining's auc: 0.510385\tvalid_1's auc: 0.503016\n",
      "var_46\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515178\tvalid_1's auc: 0.500097\n",
      "Early stopping, best iteration is:\n",
      "[145]\ttraining's auc: 0.510774\tvalid_1's auc: 0.500716\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510364\tvalid_1's auc: 0.497485\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's auc: 0.505715\tvalid_1's auc: 0.502183\n",
      "var_47\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53348\tvalid_1's auc: 0.533101\n",
      "[2000]\ttraining's auc: 0.53408\tvalid_1's auc: 0.533152\n",
      "Early stopping, best iteration is:\n",
      "[1680]\ttraining's auc: 0.533767\tvalid_1's auc: 0.533293\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.531739\tvalid_1's auc: 0.531645\n",
      "Early stopping, best iteration is:\n",
      "[943]\ttraining's auc: 0.531697\tvalid_1's auc: 0.531839\n",
      "var_48\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.531741\tvalid_1's auc: 0.525552\n",
      "[2000]\ttraining's auc: 0.53236\tvalid_1's auc: 0.525728\n",
      "[3000]\ttraining's auc: 0.532921\tvalid_1's auc: 0.525319\n",
      "Early stopping, best iteration is:\n",
      "[2033]\ttraining's auc: 0.532554\tvalid_1's auc: 0.525976\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.528941\tvalid_1's auc: 0.526555\n",
      "Early stopping, best iteration is:\n",
      "[742]\ttraining's auc: 0.528711\tvalid_1's auc: 0.527584\n",
      "var_49\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.520103\tvalid_1's auc: 0.511379\n",
      "Early stopping, best iteration is:\n",
      "[85]\ttraining's auc: 0.515658\tvalid_1's auc: 0.51402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517883\tvalid_1's auc: 0.509735\n",
      "Early stopping, best iteration is:\n",
      "[817]\ttraining's auc: 0.517855\tvalid_1's auc: 0.509939\n",
      "var_50\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529864\tvalid_1's auc: 0.517853\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttraining's auc: 0.524876\tvalid_1's auc: 0.518887\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.528364\tvalid_1's auc: 0.514785\n",
      "Early stopping, best iteration is:\n",
      "[548]\ttraining's auc: 0.526366\tvalid_1's auc: 0.517286\n",
      "var_51\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.531615\tvalid_1's auc: 0.520467\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.526517\tvalid_1's auc: 0.524657\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529134\tvalid_1's auc: 0.51963\n",
      "[2000]\ttraining's auc: 0.529331\tvalid_1's auc: 0.519635\n",
      "Early stopping, best iteration is:\n",
      "[1468]\ttraining's auc: 0.529224\tvalid_1's auc: 0.520324\n",
      "var_52\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.559267\tvalid_1's auc: 0.552226\n",
      "Early stopping, best iteration is:\n",
      "[368]\ttraining's auc: 0.558017\tvalid_1's auc: 0.552639\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.556289\tvalid_1's auc: 0.550482\n",
      "[2000]\ttraining's auc: 0.556869\tvalid_1's auc: 0.55161\n",
      "[3000]\ttraining's auc: 0.556886\tvalid_1's auc: 0.551965\n",
      "Early stopping, best iteration is:\n",
      "[2265]\ttraining's auc: 0.556886\tvalid_1's auc: 0.551965\n",
      "var_53\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.521739\tvalid_1's auc: 0.51733\n",
      "Early stopping, best iteration is:\n",
      "[573]\ttraining's auc: 0.52145\tvalid_1's auc: 0.517941\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51936\tvalid_1's auc: 0.51457\n",
      "Early stopping, best iteration is:\n",
      "[723]\ttraining's auc: 0.519084\tvalid_1's auc: 0.515379\n",
      "var_54\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.522336\tvalid_1's auc: 0.50739\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's auc: 0.517537\tvalid_1's auc: 0.509528\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.518302\tvalid_1's auc: 0.508796\n",
      "Early stopping, best iteration is:\n",
      "[954]\ttraining's auc: 0.518272\tvalid_1's auc: 0.508909\n",
      "var_55\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.530308\tvalid_1's auc: 0.534003\n",
      "Early stopping, best iteration is:\n",
      "[411]\ttraining's auc: 0.529044\tvalid_1's auc: 0.535409\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.528104\tvalid_1's auc: 0.533358\n",
      "[2000]\ttraining's auc: 0.528297\tvalid_1's auc: 0.533907\n",
      "Early stopping, best iteration is:\n",
      "[1403]\ttraining's auc: 0.528259\tvalid_1's auc: 0.534136\n",
      "var_56\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.519203\tvalid_1's auc: 0.508599\n",
      "Early stopping, best iteration is:\n",
      "[138]\ttraining's auc: 0.517189\tvalid_1's auc: 0.51065\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517916\tvalid_1's auc: 0.508742\n",
      "[2000]\ttraining's auc: 0.517981\tvalid_1's auc: 0.509011\n",
      "Early stopping, best iteration is:\n",
      "[1433]\ttraining's auc: 0.517959\tvalid_1's auc: 0.50923\n",
      "var_57\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52856\tvalid_1's auc: 0.526394\n",
      "Early stopping, best iteration is:\n",
      "[326]\ttraining's auc: 0.527637\tvalid_1's auc: 0.527928\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526254\tvalid_1's auc: 0.52848\n",
      "Early stopping, best iteration is:\n",
      "[820]\ttraining's auc: 0.526164\tvalid_1's auc: 0.528823\n",
      "var_58\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512284\tvalid_1's auc: 0.504858\n",
      "Early stopping, best iteration is:\n",
      "[322]\ttraining's auc: 0.509208\tvalid_1's auc: 0.515004\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511226\tvalid_1's auc: 0.499821\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.508008\tvalid_1's auc: 0.508362\n",
      "var_59\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513497\tvalid_1's auc: 0.518217\n",
      "Early stopping, best iteration is:\n",
      "[652]\ttraining's auc: 0.512259\tvalid_1's auc: 0.520146\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511636\tvalid_1's auc: 0.507926\n",
      "Early stopping, best iteration is:\n",
      "[373]\ttraining's auc: 0.510952\tvalid_1's auc: 0.508065\n",
      "var_60\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51552\tvalid_1's auc: 0.501102\n",
      "[2000]\ttraining's auc: 0.516236\tvalid_1's auc: 0.500996\n",
      "Early stopping, best iteration is:\n",
      "[1002]\ttraining's auc: 0.515547\tvalid_1's auc: 0.501115\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512376\tvalid_1's auc: 0.501582\n",
      "[2000]\ttraining's auc: 0.512639\tvalid_1's auc: 0.501556\n",
      "Early stopping, best iteration is:\n",
      "[1044]\ttraining's auc: 0.512413\tvalid_1's auc: 0.502173\n",
      "var_61\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.518358\tvalid_1's auc: 0.508182\n",
      "Early stopping, best iteration is:\n",
      "[109]\ttraining's auc: 0.512671\tvalid_1's auc: 0.511787\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517042\tvalid_1's auc: 0.503718\n",
      "Early stopping, best iteration is:\n",
      "[172]\ttraining's auc: 0.513892\tvalid_1's auc: 0.505776\n",
      "var_62\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.518262\tvalid_1's auc: 0.511906\n",
      "Early stopping, best iteration is:\n",
      "[78]\ttraining's auc: 0.513841\tvalid_1's auc: 0.514581\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515726\tvalid_1's auc: 0.509345\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's auc: 0.512031\tvalid_1's auc: 0.511049\n",
      "var_63\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.518741\tvalid_1's auc: 0.501773\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's auc: 0.514699\tvalid_1's auc: 0.504948\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516975\tvalid_1's auc: 0.502515\n",
      "[2000]\ttraining's auc: 0.517026\tvalid_1's auc: 0.503151\n",
      "[3000]\ttraining's auc: 0.517043\tvalid_1's auc: 0.503243\n",
      "[4000]\ttraining's auc: 0.517047\tvalid_1's auc: 0.503285\n",
      "[5000]\ttraining's auc: 0.517049\tvalid_1's auc: 0.503208\n",
      "Early stopping, best iteration is:\n",
      "[4016]\ttraining's auc: 0.517047\tvalid_1's auc: 0.503292\n",
      "var_64\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515177\tvalid_1's auc: 0.508934\n",
      "[2000]\ttraining's auc: 0.516203\tvalid_1's auc: 0.509246\n",
      "Early stopping, best iteration is:\n",
      "[1908]\ttraining's auc: 0.516179\tvalid_1's auc: 0.509315\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514279\tvalid_1's auc: 0.509545\n",
      "Early stopping, best iteration is:\n",
      "[67]\ttraining's auc: 0.513089\tvalid_1's auc: 0.51056\n",
      "var_65\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.524162\tvalid_1's auc: 0.52765\n",
      "Early stopping, best iteration is:\n",
      "[302]\ttraining's auc: 0.523407\tvalid_1's auc: 0.527777\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.521636\tvalid_1's auc: 0.525663\n",
      "[2000]\ttraining's auc: 0.521874\tvalid_1's auc: 0.525794\n",
      "Early stopping, best iteration is:\n",
      "[1026]\ttraining's auc: 0.521763\tvalid_1's auc: 0.526385\n",
      "var_66\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543137\tvalid_1's auc: 0.544294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[278]\ttraining's auc: 0.541856\tvalid_1's auc: 0.545012\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540335\tvalid_1's auc: 0.542496\n",
      "Early stopping, best iteration is:\n",
      "[704]\ttraining's auc: 0.540078\tvalid_1's auc: 0.54282\n",
      "var_67\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517942\tvalid_1's auc: 0.505162\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's auc: 0.509768\tvalid_1's auc: 0.508296\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516335\tvalid_1's auc: 0.506469\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttraining's auc: 0.512871\tvalid_1's auc: 0.507693\n",
      "var_68\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513903\tvalid_1's auc: 0.504866\n",
      "Early stopping, best iteration is:\n",
      "[118]\ttraining's auc: 0.5072\tvalid_1's auc: 0.513527\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510831\tvalid_1's auc: 0.498299\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.506424\tvalid_1's auc: 0.505456\n",
      "var_69\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.530406\tvalid_1's auc: 0.518189\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's auc: 0.52717\tvalid_1's auc: 0.521225\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527485\tvalid_1's auc: 0.519784\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttraining's auc: 0.525128\tvalid_1's auc: 0.522123\n",
      "var_70\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533698\tvalid_1's auc: 0.530118\n",
      "Early stopping, best iteration is:\n",
      "[279]\ttraining's auc: 0.533033\tvalid_1's auc: 0.530417\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53183\tvalid_1's auc: 0.528644\n",
      "Early stopping, best iteration is:\n",
      "[385]\ttraining's auc: 0.53048\tvalid_1's auc: 0.530599\n",
      "var_71\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517885\tvalid_1's auc: 0.507585\n",
      "Early stopping, best iteration is:\n",
      "[971]\ttraining's auc: 0.517885\tvalid_1's auc: 0.507585\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514722\tvalid_1's auc: 0.504566\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's auc: 0.509934\tvalid_1's auc: 0.506313\n",
      "var_72\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514624\tvalid_1's auc: 0.4998\n",
      "Early stopping, best iteration is:\n",
      "[160]\ttraining's auc: 0.508617\tvalid_1's auc: 0.504457\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510934\tvalid_1's auc: 0.498709\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.501112\tvalid_1's auc: 0.499108\n",
      "var_73\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.522791\tvalid_1's auc: 0.518275\n",
      "Early stopping, best iteration is:\n",
      "[261]\ttraining's auc: 0.521651\tvalid_1's auc: 0.519862\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.520791\tvalid_1's auc: 0.515786\n",
      "Early stopping, best iteration is:\n",
      "[896]\ttraining's auc: 0.52066\tvalid_1's auc: 0.516242\n",
      "var_74\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.539334\tvalid_1's auc: 0.529092\n",
      "Early stopping, best iteration is:\n",
      "[355]\ttraining's auc: 0.537378\tvalid_1's auc: 0.529788\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537056\tvalid_1's auc: 0.529314\n",
      "Early stopping, best iteration is:\n",
      "[510]\ttraining's auc: 0.535432\tvalid_1's auc: 0.529888\n",
      "var_75\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.558796\tvalid_1's auc: 0.552893\n",
      "Early stopping, best iteration is:\n",
      "[272]\ttraining's auc: 0.555707\tvalid_1's auc: 0.555189\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.554709\tvalid_1's auc: 0.548789\n",
      "[2000]\ttraining's auc: 0.555417\tvalid_1's auc: 0.549829\n",
      "[3000]\ttraining's auc: 0.55543\tvalid_1's auc: 0.549903\n",
      "[4000]\ttraining's auc: 0.555442\tvalid_1's auc: 0.549913\n",
      "Early stopping, best iteration is:\n",
      "[3083]\ttraining's auc: 0.555436\tvalid_1's auc: 0.549917\n",
      "var_76\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.519363\tvalid_1's auc: 0.519355\n",
      "Early stopping, best iteration is:\n",
      "[559]\ttraining's auc: 0.519316\tvalid_1's auc: 0.519412\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517684\tvalid_1's auc: 0.516328\n",
      "[2000]\ttraining's auc: 0.517921\tvalid_1's auc: 0.516604\n",
      "Early stopping, best iteration is:\n",
      "[1282]\ttraining's auc: 0.517804\tvalid_1's auc: 0.51785\n",
      "var_77\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.545501\tvalid_1's auc: 0.541607\n",
      "Early stopping, best iteration is:\n",
      "[119]\ttraining's auc: 0.543274\tvalid_1's auc: 0.542983\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543204\tvalid_1's auc: 0.541603\n",
      "[2000]\ttraining's auc: 0.543656\tvalid_1's auc: 0.542934\n",
      "[3000]\ttraining's auc: 0.543685\tvalid_1's auc: 0.543017\n",
      "[4000]\ttraining's auc: 0.543689\tvalid_1's auc: 0.543049\n",
      "Early stopping, best iteration is:\n",
      "[3658]\ttraining's auc: 0.543689\tvalid_1's auc: 0.543049\n",
      "var_78\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513226\tvalid_1's auc: 0.504062\n",
      "Early stopping, best iteration is:\n",
      "[196]\ttraining's auc: 0.507427\tvalid_1's auc: 0.509265\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510075\tvalid_1's auc: 0.507385\n",
      "Early stopping, best iteration is:\n",
      "[281]\ttraining's auc: 0.508488\tvalid_1's auc: 0.510872\n",
      "var_79\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.555069\tvalid_1's auc: 0.554989\n",
      "Early stopping, best iteration is:\n",
      "[309]\ttraining's auc: 0.552796\tvalid_1's auc: 0.556378\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.550078\tvalid_1's auc: 0.552868\n",
      "[2000]\ttraining's auc: 0.550632\tvalid_1's auc: 0.55264\n",
      "Early stopping, best iteration is:\n",
      "[1023]\ttraining's auc: 0.55037\tvalid_1's auc: 0.55334\n",
      "var_80\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.574749\tvalid_1's auc: 0.569642\n",
      "Early stopping, best iteration is:\n",
      "[427]\ttraining's auc: 0.572984\tvalid_1's auc: 0.571004\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.569815\tvalid_1's auc: 0.568204\n",
      "Early stopping, best iteration is:\n",
      "[161]\ttraining's auc: 0.568271\tvalid_1's auc: 0.56971\n",
      "var_81\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523246\tvalid_1's auc: 0.52386\n",
      "[2000]\ttraining's auc: 0.524284\tvalid_1's auc: 0.524173\n",
      "[3000]\ttraining's auc: 0.526042\tvalid_1's auc: 0.523171\n",
      "Early stopping, best iteration is:\n",
      "[2021]\ttraining's auc: 0.524522\tvalid_1's auc: 0.524359\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.522154\tvalid_1's auc: 0.523109\n",
      "Early stopping, best iteration is:\n",
      "[955]\ttraining's auc: 0.522154\tvalid_1's auc: 0.523109\n",
      "var_82\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.521999\tvalid_1's auc: 0.515625\n",
      "[2000]\ttraining's auc: 0.523841\tvalid_1's auc: 0.515564\n",
      "[3000]\ttraining's auc: 0.525444\tvalid_1's auc: 0.516507\n",
      "[4000]\ttraining's auc: 0.52613\tvalid_1's auc: 0.517105\n",
      "Early stopping, best iteration is:\n",
      "[3702]\ttraining's auc: 0.526032\tvalid_1's auc: 0.517232\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.520092\tvalid_1's auc: 0.511633\n",
      "Early stopping, best iteration is:\n",
      "[945]\ttraining's auc: 0.519725\tvalid_1's auc: 0.512077\n",
      "var_83\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515966\tvalid_1's auc: 0.51078\n",
      "Early stopping, best iteration is:\n",
      "[83]\ttraining's auc: 0.511838\tvalid_1's auc: 0.514838\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513238\tvalid_1's auc: 0.511656\n",
      "Early stopping, best iteration is:\n",
      "[894]\ttraining's auc: 0.513238\tvalid_1's auc: 0.511656\n",
      "var_84\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.525603\tvalid_1's auc: 0.517489\n",
      "Early stopping, best iteration is:\n",
      "[90]\ttraining's auc: 0.522309\tvalid_1's auc: 0.521805\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523757\tvalid_1's auc: 0.516084\n",
      "Early stopping, best iteration is:\n",
      "[164]\ttraining's auc: 0.521764\tvalid_1's auc: 0.519842\n",
      "var_85\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537487\tvalid_1's auc: 0.538737\n",
      "Early stopping, best iteration is:\n",
      "[366]\ttraining's auc: 0.535383\tvalid_1's auc: 0.540992\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533247\tvalid_1's auc: 0.53667\n",
      "Early stopping, best iteration is:\n",
      "[505]\ttraining's auc: 0.532897\tvalid_1's auc: 0.537531\n",
      "var_86\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53557\tvalid_1's auc: 0.539325\n",
      "Early stopping, best iteration is:\n",
      "[88]\ttraining's auc: 0.533608\tvalid_1's auc: 0.540618\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534514\tvalid_1's auc: 0.541081\n",
      "[2000]\ttraining's auc: 0.534669\tvalid_1's auc: 0.540595\n",
      "Early stopping, best iteration is:\n",
      "[1320]\ttraining's auc: 0.534608\tvalid_1's auc: 0.541242\n",
      "var_87\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.525891\tvalid_1's auc: 0.508689\n",
      "Early stopping, best iteration is:\n",
      "[359]\ttraining's auc: 0.524652\tvalid_1's auc: 0.509306\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523979\tvalid_1's auc: 0.511066\n",
      "Early stopping, best iteration is:\n",
      "[413]\ttraining's auc: 0.522962\tvalid_1's auc: 0.511883\n",
      "var_88\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535739\tvalid_1's auc: 0.531894\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's auc: 0.532414\tvalid_1's auc: 0.532976\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534296\tvalid_1's auc: 0.531781\n",
      "[2000]\ttraining's auc: 0.534382\tvalid_1's auc: 0.531808\n",
      "Early stopping, best iteration is:\n",
      "[1078]\ttraining's auc: 0.534318\tvalid_1's auc: 0.531961\n",
      "var_89\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.528871\tvalid_1's auc: 0.531705\n",
      "Early stopping, best iteration is:\n",
      "[280]\ttraining's auc: 0.527004\tvalid_1's auc: 0.532851\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526837\tvalid_1's auc: 0.532812\n",
      "[2000]\ttraining's auc: 0.527569\tvalid_1's auc: 0.532197\n",
      "Early stopping, best iteration is:\n",
      "[1027]\ttraining's auc: 0.526878\tvalid_1's auc: 0.533203\n",
      "var_90\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.539451\tvalid_1's auc: 0.534348\n",
      "Early stopping, best iteration is:\n",
      "[434]\ttraining's auc: 0.537981\tvalid_1's auc: 0.535603\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53652\tvalid_1's auc: 0.532527\n",
      "[2000]\ttraining's auc: 0.536842\tvalid_1's auc: 0.533183\n",
      "[3000]\ttraining's auc: 0.536871\tvalid_1's auc: 0.533579\n",
      "[4000]\ttraining's auc: 0.536889\tvalid_1's auc: 0.533715\n",
      "Early stopping, best iteration is:\n",
      "[3853]\ttraining's auc: 0.536889\tvalid_1's auc: 0.533715\n",
      "var_91\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.545743\tvalid_1's auc: 0.538092\n",
      "Early stopping, best iteration is:\n",
      "[78]\ttraining's auc: 0.543081\tvalid_1's auc: 0.539502\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543706\tvalid_1's auc: 0.537561\n",
      "[2000]\ttraining's auc: 0.544512\tvalid_1's auc: 0.537146\n",
      "Early stopping, best iteration is:\n",
      "[1451]\ttraining's auc: 0.544363\tvalid_1's auc: 0.538133\n",
      "var_92\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533958\tvalid_1's auc: 0.529953\n",
      "Early stopping, best iteration is:\n",
      "[559]\ttraining's auc: 0.533465\tvalid_1's auc: 0.530301\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532534\tvalid_1's auc: 0.528555\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's auc: 0.530457\tvalid_1's auc: 0.528642\n",
      "var_93\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.545681\tvalid_1's auc: 0.54374\n",
      "Early stopping, best iteration is:\n",
      "[330]\ttraining's auc: 0.542786\tvalid_1's auc: 0.546017\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540399\tvalid_1's auc: 0.542182\n",
      "[2000]\ttraining's auc: 0.540539\tvalid_1's auc: 0.544386\n",
      "Early stopping, best iteration is:\n",
      "[1834]\ttraining's auc: 0.540539\tvalid_1's auc: 0.544386\n",
      "var_94\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537513\tvalid_1's auc: 0.536318\n",
      "Early stopping, best iteration is:\n",
      "[326]\ttraining's auc: 0.536318\tvalid_1's auc: 0.536614\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534573\tvalid_1's auc: 0.536734\n",
      "[2000]\ttraining's auc: 0.534659\tvalid_1's auc: 0.536267\n",
      "Early stopping, best iteration is:\n",
      "[1123]\ttraining's auc: 0.534577\tvalid_1's auc: 0.536754\n",
      "var_95\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510894\tvalid_1's auc: 0.501663\n",
      "Early stopping, best iteration is:\n",
      "[360]\ttraining's auc: 0.509721\tvalid_1's auc: 0.50255\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.506896\tvalid_1's auc: 0.500157\n",
      "[2000]\ttraining's auc: 0.50694\tvalid_1's auc: 0.50015\n",
      "Early stopping, best iteration is:\n",
      "[1013]\ttraining's auc: 0.506912\tvalid_1's auc: 0.500216\n",
      "var_96\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.519675\tvalid_1's auc: 0.513062\n",
      "[2000]\ttraining's auc: 0.521462\tvalid_1's auc: 0.515002\n",
      "Early stopping, best iteration is:\n",
      "[1378]\ttraining's auc: 0.521217\tvalid_1's auc: 0.515243\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516139\tvalid_1's auc: 0.51514\n",
      "[2000]\ttraining's auc: 0.516653\tvalid_1's auc: 0.513902\n",
      "Early stopping, best iteration is:\n",
      "[1042]\ttraining's auc: 0.51614\tvalid_1's auc: 0.51514\n",
      "var_97\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513519\tvalid_1's auc: 0.506881\n",
      "Early stopping, best iteration is:\n",
      "[67]\ttraining's auc: 0.508185\tvalid_1's auc: 0.509035\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.508868\tvalid_1's auc: 0.509775\n",
      "Early stopping, best iteration is:\n",
      "[756]\ttraining's auc: 0.508867\tvalid_1's auc: 0.509781\n",
      "var_98\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.554286\tvalid_1's auc: 0.552153\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttraining's auc: 0.551761\tvalid_1's auc: 0.552733\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.551562\tvalid_1's auc: 0.55379\n",
      "Early stopping, best iteration is:\n",
      "[651]\ttraining's auc: 0.55143\tvalid_1's auc: 0.554169\n",
      "var_99\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513302\tvalid_1's auc: 0.501026\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's auc: 0.504714\tvalid_1's auc: 0.50271\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510551\tvalid_1's auc: 0.501386\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.502932\tvalid_1's auc: 0.503427\n",
      "var_100\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516101\tvalid_1's auc: 0.511763\n",
      "Early stopping, best iteration is:\n",
      "[343]\ttraining's auc: 0.514356\tvalid_1's auc: 0.513136\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513202\tvalid_1's auc: 0.508086\n",
      "Early stopping, best iteration is:\n",
      "[140]\ttraining's auc: 0.512518\tvalid_1's auc: 0.510266\n",
      "var_101\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.519563\tvalid_1's auc: 0.514275\n",
      "Early stopping, best iteration is:\n",
      "[523]\ttraining's auc: 0.51868\tvalid_1's auc: 0.514655\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517007\tvalid_1's auc: 0.511868\n",
      "Early stopping, best iteration is:\n",
      "[735]\ttraining's auc: 0.516805\tvalid_1's auc: 0.512\n",
      "var_102\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512506\tvalid_1's auc: 0.506379\n",
      "Early stopping, best iteration is:\n",
      "[186]\ttraining's auc: 0.509507\tvalid_1's auc: 0.507907\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.509167\tvalid_1's auc: 0.50459\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's auc: 0.506828\tvalid_1's auc: 0.506208\n",
      "var_103\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.530404\tvalid_1's auc: 0.52157\n",
      "Early stopping, best iteration is:\n",
      "[403]\ttraining's auc: 0.529528\tvalid_1's auc: 0.521961\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527748\tvalid_1's auc: 0.517847\n",
      "[2000]\ttraining's auc: 0.528199\tvalid_1's auc: 0.519491\n",
      "Early stopping, best iteration is:\n",
      "[1611]\ttraining's auc: 0.528173\tvalid_1's auc: 0.519587\n",
      "var_104\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526916\tvalid_1's auc: 0.522576\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttraining's auc: 0.524468\tvalid_1's auc: 0.524212\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.524831\tvalid_1's auc: 0.522511\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's auc: 0.520254\tvalid_1's auc: 0.523411\n",
      "var_105\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534225\tvalid_1's auc: 0.527922\n",
      "Early stopping, best iteration is:\n",
      "[821]\ttraining's auc: 0.533777\tvalid_1's auc: 0.528843\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532303\tvalid_1's auc: 0.527769\n",
      "[2000]\ttraining's auc: 0.5325\tvalid_1's auc: 0.527864\n",
      "[3000]\ttraining's auc: 0.53253\tvalid_1's auc: 0.528765\n",
      "[4000]\ttraining's auc: 0.532536\tvalid_1's auc: 0.528758\n",
      "Early stopping, best iteration is:\n",
      "[3344]\ttraining's auc: 0.532535\tvalid_1's auc: 0.528803\n",
      "var_106\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540848\tvalid_1's auc: 0.531787\n",
      "Early stopping, best iteration is:\n",
      "[168]\ttraining's auc: 0.539257\tvalid_1's auc: 0.532578\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538344\tvalid_1's auc: 0.530303\n",
      "Early stopping, best iteration is:\n",
      "[512]\ttraining's auc: 0.53602\tvalid_1's auc: 0.531058\n",
      "var_107\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546544\tvalid_1's auc: 0.541101\n",
      "[2000]\ttraining's auc: 0.547374\tvalid_1's auc: 0.541089\n",
      "[3000]\ttraining's auc: 0.548477\tvalid_1's auc: 0.542933\n",
      "[4000]\ttraining's auc: 0.550013\tvalid_1's auc: 0.542877\n",
      "Early stopping, best iteration is:\n",
      "[3220]\ttraining's auc: 0.548511\tvalid_1's auc: 0.543037\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538786\tvalid_1's auc: 0.537135\n",
      "[2000]\ttraining's auc: 0.539485\tvalid_1's auc: 0.537574\n",
      "Early stopping, best iteration is:\n",
      "[1733]\ttraining's auc: 0.539484\tvalid_1's auc: 0.537578\n",
      "var_108\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.545183\tvalid_1's auc: 0.545713\n",
      "[2000]\ttraining's auc: 0.548553\tvalid_1's auc: 0.544938\n",
      "Early stopping, best iteration is:\n",
      "[1233]\ttraining's auc: 0.546057\tvalid_1's auc: 0.546394\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543445\tvalid_1's auc: 0.543588\n",
      "Early stopping, best iteration is:\n",
      "[537]\ttraining's auc: 0.541865\tvalid_1's auc: 0.545727\n",
      "var_109\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.561344\tvalid_1's auc: 0.550068\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's auc: 0.556391\tvalid_1's auc: 0.553263\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.558473\tvalid_1's auc: 0.553533\n",
      "Early stopping, best iteration is:\n",
      "[741]\ttraining's auc: 0.55846\tvalid_1's auc: 0.553541\n",
      "var_110\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52783\tvalid_1's auc: 0.520137\n",
      "Early stopping, best iteration is:\n",
      "[303]\ttraining's auc: 0.527027\tvalid_1's auc: 0.520783\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.525426\tvalid_1's auc: 0.516384\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's auc: 0.521585\tvalid_1's auc: 0.520642\n",
      "var_111\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.531604\tvalid_1's auc: 0.526151\n",
      "[2000]\ttraining's auc: 0.533398\tvalid_1's auc: 0.526334\n",
      "Early stopping, best iteration is:\n",
      "[1070]\ttraining's auc: 0.531944\tvalid_1's auc: 0.526818\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52942\tvalid_1's auc: 0.527739\n",
      "[2000]\ttraining's auc: 0.529662\tvalid_1's auc: 0.526486\n",
      "Early stopping, best iteration is:\n",
      "[1257]\ttraining's auc: 0.529484\tvalid_1's auc: 0.527837\n",
      "var_112\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516411\tvalid_1's auc: 0.512573\n",
      "Early stopping, best iteration is:\n",
      "[49]\ttraining's auc: 0.511584\tvalid_1's auc: 0.515914\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513531\tvalid_1's auc: 0.512359\n",
      "Early stopping, best iteration is:\n",
      "[691]\ttraining's auc: 0.513424\tvalid_1's auc: 0.512927\n",
      "var_113\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526881\tvalid_1's auc: 0.527758\n",
      "Early stopping, best iteration is:\n",
      "[535]\ttraining's auc: 0.526189\tvalid_1's auc: 0.528993\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523239\tvalid_1's auc: 0.526602\n",
      "Early stopping, best iteration is:\n",
      "[539]\ttraining's auc: 0.522399\tvalid_1's auc: 0.528183\n",
      "var_114\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.549111\tvalid_1's auc: 0.542835\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's auc: 0.546258\tvalid_1's auc: 0.54647\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546988\tvalid_1's auc: 0.541003\n",
      "[2000]\ttraining's auc: 0.54721\tvalid_1's auc: 0.541579\n",
      "[3000]\ttraining's auc: 0.547224\tvalid_1's auc: 0.541634\n",
      "Early stopping, best iteration is:\n",
      "[2803]\ttraining's auc: 0.547224\tvalid_1's auc: 0.541634\n",
      "var_115\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.528534\tvalid_1's auc: 0.518749\n",
      "Early stopping, best iteration is:\n",
      "[912]\ttraining's auc: 0.528532\tvalid_1's auc: 0.518751\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52556\tvalid_1's auc: 0.515041\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttraining's auc: 0.522175\tvalid_1's auc: 0.5182\n",
      "var_116\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516574\tvalid_1's auc: 0.507245\n",
      "Early stopping, best iteration is:\n",
      "[675]\ttraining's auc: 0.514934\tvalid_1's auc: 0.508258\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512134\tvalid_1's auc: 0.505205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[116]\ttraining's auc: 0.510458\tvalid_1's auc: 0.507295\n",
      "var_117\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.541263\tvalid_1's auc: 0.532424\n",
      "Early stopping, best iteration is:\n",
      "[109]\ttraining's auc: 0.540183\tvalid_1's auc: 0.532799\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.539222\tvalid_1's auc: 0.529056\n",
      "[2000]\ttraining's auc: 0.53948\tvalid_1's auc: 0.529285\n",
      "Early stopping, best iteration is:\n",
      "[1421]\ttraining's auc: 0.53942\tvalid_1's auc: 0.529451\n",
      "var_118\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.530926\tvalid_1's auc: 0.52954\n",
      "Early stopping, best iteration is:\n",
      "[285]\ttraining's auc: 0.529281\tvalid_1's auc: 0.530952\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529201\tvalid_1's auc: 0.527151\n",
      "[2000]\ttraining's auc: 0.529339\tvalid_1's auc: 0.529061\n",
      "Early stopping, best iteration is:\n",
      "[1919]\ttraining's auc: 0.529339\tvalid_1's auc: 0.529061\n",
      "var_119\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517085\tvalid_1's auc: 0.513168\n",
      "Early stopping, best iteration is:\n",
      "[273]\ttraining's auc: 0.515374\tvalid_1's auc: 0.514217\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514505\tvalid_1's auc: 0.511388\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.511386\tvalid_1's auc: 0.511479\n",
      "var_120\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536854\tvalid_1's auc: 0.541553\n",
      "Early stopping, best iteration is:\n",
      "[95]\ttraining's auc: 0.534954\tvalid_1's auc: 0.543058\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535189\tvalid_1's auc: 0.54345\n",
      "[2000]\ttraining's auc: 0.535843\tvalid_1's auc: 0.545217\n",
      "[3000]\ttraining's auc: 0.535865\tvalid_1's auc: 0.545182\n",
      "Early stopping, best iteration is:\n",
      "[2454]\ttraining's auc: 0.535847\tvalid_1's auc: 0.545352\n",
      "var_121\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.542359\tvalid_1's auc: 0.532551\n",
      "Early stopping, best iteration is:\n",
      "[174]\ttraining's auc: 0.540717\tvalid_1's auc: 0.534194\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540544\tvalid_1's auc: 0.534049\n",
      "Early stopping, best iteration is:\n",
      "[282]\ttraining's auc: 0.538055\tvalid_1's auc: 0.535487\n",
      "var_122\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.541079\tvalid_1's auc: 0.545667\n",
      "[2000]\ttraining's auc: 0.541779\tvalid_1's auc: 0.545247\n",
      "Early stopping, best iteration is:\n",
      "[1264]\ttraining's auc: 0.541084\tvalid_1's auc: 0.545696\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538539\tvalid_1's auc: 0.546712\n",
      "[2000]\ttraining's auc: 0.538662\tvalid_1's auc: 0.546895\n",
      "Early stopping, best iteration is:\n",
      "[1501]\ttraining's auc: 0.538652\tvalid_1's auc: 0.547013\n",
      "var_123\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513054\tvalid_1's auc: 0.502913\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's auc: 0.508114\tvalid_1's auc: 0.505681\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.507476\tvalid_1's auc: 0.504595\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's auc: 0.506469\tvalid_1's auc: 0.506622\n",
      "var_124\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.5279\tvalid_1's auc: 0.520566\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttraining's auc: 0.525253\tvalid_1's auc: 0.524113\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52588\tvalid_1's auc: 0.521135\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttraining's auc: 0.521202\tvalid_1's auc: 0.526279\n",
      "var_125\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513706\tvalid_1's auc: 0.491717\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's auc: 0.507622\tvalid_1's auc: 0.493793\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510364\tvalid_1's auc: 0.490161\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.502859\tvalid_1's auc: 0.4981\n",
      "var_126\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538587\tvalid_1's auc: 0.540202\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's auc: 0.535396\tvalid_1's auc: 0.542857\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536571\tvalid_1's auc: 0.540738\n",
      "Early stopping, best iteration is:\n",
      "[603]\ttraining's auc: 0.535996\tvalid_1's auc: 0.543521\n",
      "var_127\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526773\tvalid_1's auc: 0.520945\n",
      "Early stopping, best iteration is:\n",
      "[87]\ttraining's auc: 0.52572\tvalid_1's auc: 0.521931\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.525069\tvalid_1's auc: 0.518601\n",
      "[2000]\ttraining's auc: 0.525121\tvalid_1's auc: 0.518065\n",
      "Early stopping, best iteration is:\n",
      "[1452]\ttraining's auc: 0.52509\tvalid_1's auc: 0.518745\n",
      "var_128\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51492\tvalid_1's auc: 0.494921\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttraining's auc: 0.506874\tvalid_1's auc: 0.503965\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510689\tvalid_1's auc: 0.493831\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's auc: 0.508341\tvalid_1's auc: 0.499762\n",
      "var_129\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.528731\tvalid_1's auc: 0.528461\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's auc: 0.526591\tvalid_1's auc: 0.529088\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526952\tvalid_1's auc: 0.527009\n",
      "[2000]\ttraining's auc: 0.52736\tvalid_1's auc: 0.527745\n",
      "Early stopping, best iteration is:\n",
      "[1118]\ttraining's auc: 0.527142\tvalid_1's auc: 0.527853\n",
      "var_130\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534073\tvalid_1's auc: 0.521015\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.528999\tvalid_1's auc: 0.52296\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532436\tvalid_1's auc: 0.520609\n",
      "Early stopping, best iteration is:\n",
      "[269]\ttraining's auc: 0.53094\tvalid_1's auc: 0.521735\n",
      "var_131\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.525155\tvalid_1's auc: 0.526477\n",
      "Early stopping, best iteration is:\n",
      "[517]\ttraining's auc: 0.524569\tvalid_1's auc: 0.526938\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523971\tvalid_1's auc: 0.525152\n",
      "[2000]\ttraining's auc: 0.524192\tvalid_1's auc: 0.525694\n",
      "Early stopping, best iteration is:\n",
      "[1266]\ttraining's auc: 0.52414\tvalid_1's auc: 0.525733\n",
      "var_132\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.550487\tvalid_1's auc: 0.545149\n",
      "Early stopping, best iteration is:\n",
      "[199]\ttraining's auc: 0.548822\tvalid_1's auc: 0.546114\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.547799\tvalid_1's auc: 0.543583\n",
      "[2000]\ttraining's auc: 0.548106\tvalid_1's auc: 0.544226\n",
      "[3000]\ttraining's auc: 0.548143\tvalid_1's auc: 0.54416\n",
      "Early stopping, best iteration is:\n",
      "[2174]\ttraining's auc: 0.548113\tvalid_1's auc: 0.544263\n",
      "var_133\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52042\tvalid_1's auc: 0.509541\n",
      "Early stopping, best iteration is:\n",
      "[661]\ttraining's auc: 0.519525\tvalid_1's auc: 0.509939\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517622\tvalid_1's auc: 0.506785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.511564\tvalid_1's auc: 0.509279\n",
      "var_134\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527682\tvalid_1's auc: 0.529292\n",
      "Early stopping, best iteration is:\n",
      "[824]\ttraining's auc: 0.52685\tvalid_1's auc: 0.529625\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526987\tvalid_1's auc: 0.523829\n",
      "[2000]\ttraining's auc: 0.527295\tvalid_1's auc: 0.525538\n",
      "Early stopping, best iteration is:\n",
      "[1053]\ttraining's auc: 0.527214\tvalid_1's auc: 0.525872\n",
      "var_135\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510909\tvalid_1's auc: 0.505671\n",
      "[2000]\ttraining's auc: 0.512982\tvalid_1's auc: 0.506279\n",
      "Early stopping, best iteration is:\n",
      "[1984]\ttraining's auc: 0.512997\tvalid_1's auc: 0.506279\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.508798\tvalid_1's auc: 0.507063\n",
      "Early stopping, best iteration is:\n",
      "[288]\ttraining's auc: 0.508246\tvalid_1's auc: 0.507493\n",
      "var_136\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529706\tvalid_1's auc: 0.525447\n",
      "Early stopping, best iteration is:\n",
      "[138]\ttraining's auc: 0.525929\tvalid_1's auc: 0.526138\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527506\tvalid_1's auc: 0.526317\n",
      "Early stopping, best iteration is:\n",
      "[943]\ttraining's auc: 0.527506\tvalid_1's auc: 0.526317\n",
      "var_137\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52072\tvalid_1's auc: 0.519917\n",
      "Early stopping, best iteration is:\n",
      "[118]\ttraining's auc: 0.517249\tvalid_1's auc: 0.522561\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517602\tvalid_1's auc: 0.516928\n",
      "[2000]\ttraining's auc: 0.517655\tvalid_1's auc: 0.517596\n",
      "Early stopping, best iteration is:\n",
      "[1309]\ttraining's auc: 0.517638\tvalid_1's auc: 0.517704\n",
      "var_138\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.571284\tvalid_1's auc: 0.566275\n",
      "Early stopping, best iteration is:\n",
      "[513]\ttraining's auc: 0.57061\tvalid_1's auc: 0.567199\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.568708\tvalid_1's auc: 0.564501\n",
      "[2000]\ttraining's auc: 0.568819\tvalid_1's auc: 0.564639\n",
      "Early stopping, best iteration is:\n",
      "[1723]\ttraining's auc: 0.568778\tvalid_1's auc: 0.564755\n",
      "var_139\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52075\tvalid_1's auc: 0.500269\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.510053\tvalid_1's auc: 0.50219\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517582\tvalid_1's auc: 0.501059\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's auc: 0.510917\tvalid_1's auc: 0.502492\n",
      "var_140\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52962\tvalid_1's auc: 0.530138\n",
      "[2000]\ttraining's auc: 0.530801\tvalid_1's auc: 0.533587\n",
      "Early stopping, best iteration is:\n",
      "[1188]\ttraining's auc: 0.53033\tvalid_1's auc: 0.533909\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526445\tvalid_1's auc: 0.530211\n",
      "Early stopping, best iteration is:\n",
      "[947]\ttraining's auc: 0.526242\tvalid_1's auc: 0.530523\n",
      "var_141\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.520987\tvalid_1's auc: 0.515406\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's auc: 0.518903\tvalid_1's auc: 0.516514\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.518919\tvalid_1's auc: 0.513852\n",
      "[2000]\ttraining's auc: 0.519091\tvalid_1's auc: 0.512857\n",
      "Early stopping, best iteration is:\n",
      "[1700]\ttraining's auc: 0.518971\tvalid_1's auc: 0.514317\n",
      "var_142\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514538\tvalid_1's auc: 0.509928\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttraining's auc: 0.510308\tvalid_1's auc: 0.512772\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51172\tvalid_1's auc: 0.510125\n",
      "Early stopping, best iteration is:\n",
      "[250]\ttraining's auc: 0.510743\tvalid_1's auc: 0.512269\n",
      "var_143\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523099\tvalid_1's auc: 0.527487\n",
      "Early stopping, best iteration is:\n",
      "[694]\ttraining's auc: 0.521347\tvalid_1's auc: 0.528554\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.520959\tvalid_1's auc: 0.527777\n",
      "[2000]\ttraining's auc: 0.521108\tvalid_1's auc: 0.527898\n",
      "Early stopping, best iteration is:\n",
      "[1233]\ttraining's auc: 0.521101\tvalid_1's auc: 0.5279\n",
      "var_144\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532364\tvalid_1's auc: 0.518524\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's auc: 0.528942\tvalid_1's auc: 0.520292\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529468\tvalid_1's auc: 0.518104\n",
      "Early stopping, best iteration is:\n",
      "[664]\ttraining's auc: 0.529037\tvalid_1's auc: 0.518531\n",
      "var_145\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.558803\tvalid_1's auc: 0.557634\n",
      "Early stopping, best iteration is:\n",
      "[500]\ttraining's auc: 0.557902\tvalid_1's auc: 0.558539\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.556052\tvalid_1's auc: 0.553868\n",
      "[2000]\ttraining's auc: 0.556441\tvalid_1's auc: 0.555152\n",
      "[3000]\ttraining's auc: 0.556512\tvalid_1's auc: 0.555537\n",
      "Early stopping, best iteration is:\n",
      "[2599]\ttraining's auc: 0.556512\tvalid_1's auc: 0.555537\n",
      "var_146\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540162\tvalid_1's auc: 0.533899\n",
      "Early stopping, best iteration is:\n",
      "[386]\ttraining's auc: 0.539714\tvalid_1's auc: 0.534454\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537384\tvalid_1's auc: 0.534568\n",
      "[2000]\ttraining's auc: 0.537846\tvalid_1's auc: 0.533013\n",
      "Early stopping, best iteration is:\n",
      "[1153]\ttraining's auc: 0.53746\tvalid_1's auc: 0.534615\n",
      "var_147\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.55396\tvalid_1's auc: 0.552349\n",
      "Early stopping, best iteration is:\n",
      "[67]\ttraining's auc: 0.548908\tvalid_1's auc: 0.554501\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.549377\tvalid_1's auc: 0.557642\n",
      "Early stopping, best iteration is:\n",
      "[412]\ttraining's auc: 0.548279\tvalid_1's auc: 0.55874\n",
      "var_148\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.547925\tvalid_1's auc: 0.550441\n",
      "Early stopping, best iteration is:\n",
      "[403]\ttraining's auc: 0.547233\tvalid_1's auc: 0.550927\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.544882\tvalid_1's auc: 0.548529\n",
      "[2000]\ttraining's auc: 0.545087\tvalid_1's auc: 0.549229\n",
      "[3000]\ttraining's auc: 0.545152\tvalid_1's auc: 0.549774\n",
      "[4000]\ttraining's auc: 0.545164\tvalid_1's auc: 0.550006\n",
      "[5000]\ttraining's auc: 0.545166\tvalid_1's auc: 0.550091\n",
      "Early stopping, best iteration is:\n",
      "[4395]\ttraining's auc: 0.545166\tvalid_1's auc: 0.550091\n",
      "var_149\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.530057\tvalid_1's auc: 0.526146\n",
      "Early stopping, best iteration is:\n",
      "[765]\ttraining's auc: 0.529431\tvalid_1's auc: 0.526803\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527452\tvalid_1's auc: 0.520879\n",
      "[2000]\ttraining's auc: 0.527583\tvalid_1's auc: 0.520965\n",
      "Early stopping, best iteration is:\n",
      "[1274]\ttraining's auc: 0.527535\tvalid_1's auc: 0.521269\n",
      "var_150\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's auc: 0.528602\tvalid_1's auc: 0.517081\n",
      "Early stopping, best iteration is:\n",
      "[426]\ttraining's auc: 0.528283\tvalid_1's auc: 0.517605\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52714\tvalid_1's auc: 0.519401\n",
      "Early stopping, best iteration is:\n",
      "[327]\ttraining's auc: 0.526077\tvalid_1's auc: 0.521191\n",
      "var_151\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51713\tvalid_1's auc: 0.49889\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.505982\tvalid_1's auc: 0.505615\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513978\tvalid_1's auc: 0.498268\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.502974\tvalid_1's auc: 0.502312\n",
      "var_152\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514485\tvalid_1's auc: 0.503736\n",
      "Early stopping, best iteration is:\n",
      "[140]\ttraining's auc: 0.510022\tvalid_1's auc: 0.508726\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511559\tvalid_1's auc: 0.502185\n",
      "[2000]\ttraining's auc: 0.511774\tvalid_1's auc: 0.501888\n",
      "Early stopping, best iteration is:\n",
      "[1213]\ttraining's auc: 0.511754\tvalid_1's auc: 0.502419\n",
      "var_153\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543817\tvalid_1's auc: 0.543058\n",
      "[2000]\ttraining's auc: 0.544712\tvalid_1's auc: 0.543739\n",
      "Early stopping, best iteration is:\n",
      "[1505]\ttraining's auc: 0.544591\tvalid_1's auc: 0.54382\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.542019\tvalid_1's auc: 0.543127\n",
      "[2000]\ttraining's auc: 0.542229\tvalid_1's auc: 0.543838\n",
      "[3000]\ttraining's auc: 0.542315\tvalid_1's auc: 0.54436\n",
      "[4000]\ttraining's auc: 0.542326\tvalid_1's auc: 0.544611\n",
      "Early stopping, best iteration is:\n",
      "[3456]\ttraining's auc: 0.542326\tvalid_1's auc: 0.544611\n",
      "var_154\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533116\tvalid_1's auc: 0.536464\n",
      "Early stopping, best iteration is:\n",
      "[169]\ttraining's auc: 0.531606\tvalid_1's auc: 0.538056\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529901\tvalid_1's auc: 0.537376\n",
      "[2000]\ttraining's auc: 0.5301\tvalid_1's auc: 0.538514\n",
      "[3000]\ttraining's auc: 0.53014\tvalid_1's auc: 0.538632\n",
      "[4000]\ttraining's auc: 0.530144\tvalid_1's auc: 0.538677\n",
      "Early stopping, best iteration is:\n",
      "[3018]\ttraining's auc: 0.530144\tvalid_1's auc: 0.538693\n",
      "var_155\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523115\tvalid_1's auc: 0.512085\n",
      "Early stopping, best iteration is:\n",
      "[153]\ttraining's auc: 0.521457\tvalid_1's auc: 0.51346\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.521567\tvalid_1's auc: 0.51328\n",
      "Early stopping, best iteration is:\n",
      "[153]\ttraining's auc: 0.519387\tvalid_1's auc: 0.514647\n",
      "var_156\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.531546\tvalid_1's auc: 0.533352\n",
      "Early stopping, best iteration is:\n",
      "[464]\ttraining's auc: 0.531122\tvalid_1's auc: 0.534343\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529311\tvalid_1's auc: 0.529954\n",
      "Early stopping, best iteration is:\n",
      "[451]\ttraining's auc: 0.529108\tvalid_1's auc: 0.531321\n",
      "var_157\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511443\tvalid_1's auc: 0.497779\n",
      "Early stopping, best iteration is:\n",
      "[227]\ttraining's auc: 0.507658\tvalid_1's auc: 0.503503\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.508119\tvalid_1's auc: 0.499693\n",
      "Early stopping, best iteration is:\n",
      "[366]\ttraining's auc: 0.507179\tvalid_1's auc: 0.503068\n",
      "var_158\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517697\tvalid_1's auc: 0.515289\n",
      "[2000]\ttraining's auc: 0.518376\tvalid_1's auc: 0.516079\n",
      "Early stopping, best iteration is:\n",
      "[1644]\ttraining's auc: 0.518321\tvalid_1's auc: 0.51613\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515493\tvalid_1's auc: 0.51705\n",
      "Early stopping, best iteration is:\n",
      "[204]\ttraining's auc: 0.513994\tvalid_1's auc: 0.518148\n",
      "var_159\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514176\tvalid_1's auc: 0.506989\n",
      "Early stopping, best iteration is:\n",
      "[43]\ttraining's auc: 0.505686\tvalid_1's auc: 0.511688\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513301\tvalid_1's auc: 0.502646\n",
      "Early stopping, best iteration is:\n",
      "[112]\ttraining's auc: 0.512161\tvalid_1's auc: 0.503592\n",
      "var_160\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514568\tvalid_1's auc: 0.497048\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttraining's auc: 0.505731\tvalid_1's auc: 0.503419\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510996\tvalid_1's auc: 0.500216\n",
      "Early stopping, best iteration is:\n",
      "[367]\ttraining's auc: 0.510499\tvalid_1's auc: 0.501477\n",
      "var_161\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532399\tvalid_1's auc: 0.533835\n",
      "Early stopping, best iteration is:\n",
      "[555]\ttraining's auc: 0.531844\tvalid_1's auc: 0.533957\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529693\tvalid_1's auc: 0.529864\n",
      "[2000]\ttraining's auc: 0.530074\tvalid_1's auc: 0.53045\n",
      "[3000]\ttraining's auc: 0.530098\tvalid_1's auc: 0.530726\n",
      "Early stopping, best iteration is:\n",
      "[2676]\ttraining's auc: 0.530096\tvalid_1's auc: 0.530785\n",
      "var_162\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533729\tvalid_1's auc: 0.529536\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.527934\tvalid_1's auc: 0.530855\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.531361\tvalid_1's auc: 0.529729\n",
      "Early stopping, best iteration is:\n",
      "[303]\ttraining's auc: 0.530094\tvalid_1's auc: 0.530751\n",
      "var_163\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540477\tvalid_1's auc: 0.543635\n",
      "Early stopping, best iteration is:\n",
      "[174]\ttraining's auc: 0.536094\tvalid_1's auc: 0.545329\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537735\tvalid_1's auc: 0.539883\n",
      "Early stopping, best iteration is:\n",
      "[126]\ttraining's auc: 0.535627\tvalid_1's auc: 0.542549\n",
      "var_164\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.556346\tvalid_1's auc: 0.540116\n",
      "Early stopping, best iteration is:\n",
      "[385]\ttraining's auc: 0.555662\tvalid_1's auc: 0.541746\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.554078\tvalid_1's auc: 0.539256\n",
      "[2000]\ttraining's auc: 0.554197\tvalid_1's auc: 0.539584\n",
      "Early stopping, best iteration is:\n",
      "[1766]\ttraining's auc: 0.554197\tvalid_1's auc: 0.539591\n",
      "var_165\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.556461\tvalid_1's auc: 0.541703\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's auc: 0.553401\tvalid_1's auc: 0.542265\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.553717\tvalid_1's auc: 0.543547\n",
      "Early stopping, best iteration is:\n",
      "[585]\ttraining's auc: 0.552893\tvalid_1's auc: 0.545395\n",
      "var_166\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.531491\tvalid_1's auc: 0.5256\n",
      "[2000]\ttraining's auc: 0.532936\tvalid_1's auc: 0.52541\n",
      "Early stopping, best iteration is:\n",
      "[1657]\ttraining's auc: 0.532377\tvalid_1's auc: 0.525787\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.530132\tvalid_1's auc: 0.51904\n",
      "[2000]\ttraining's auc: 0.530482\tvalid_1's auc: 0.521904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000]\ttraining's auc: 0.530555\tvalid_1's auc: 0.522044\n",
      "Early stopping, best iteration is:\n",
      "[2149]\ttraining's auc: 0.530555\tvalid_1's auc: 0.522044\n",
      "var_167\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.520552\tvalid_1's auc: 0.510327\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttraining's auc: 0.513076\tvalid_1's auc: 0.514925\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516004\tvalid_1's auc: 0.508506\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's auc: 0.511176\tvalid_1's auc: 0.511506\n",
      "var_168\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.549574\tvalid_1's auc: 0.547069\n",
      "Early stopping, best iteration is:\n",
      "[366]\ttraining's auc: 0.548282\tvalid_1's auc: 0.547289\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546642\tvalid_1's auc: 0.545506\n",
      "[2000]\ttraining's auc: 0.547025\tvalid_1's auc: 0.545684\n",
      "Early stopping, best iteration is:\n",
      "[1238]\ttraining's auc: 0.54688\tvalid_1's auc: 0.545744\n",
      "var_169\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54324\tvalid_1's auc: 0.538052\n",
      "Early stopping, best iteration is:\n",
      "[139]\ttraining's auc: 0.542244\tvalid_1's auc: 0.539173\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540588\tvalid_1's auc: 0.534955\n",
      "Early stopping, best iteration is:\n",
      "[285]\ttraining's auc: 0.537178\tvalid_1's auc: 0.536337\n",
      "var_170\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517708\tvalid_1's auc: 0.511547\n",
      "[2000]\ttraining's auc: 0.518758\tvalid_1's auc: 0.513368\n",
      "[3000]\ttraining's auc: 0.519687\tvalid_1's auc: 0.513515\n",
      "[4000]\ttraining's auc: 0.520627\tvalid_1's auc: 0.514877\n",
      "[5000]\ttraining's auc: 0.521121\tvalid_1's auc: 0.515727\n",
      "Early stopping, best iteration is:\n",
      "[4277]\ttraining's auc: 0.520987\tvalid_1's auc: 0.5158\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517011\tvalid_1's auc: 0.512892\n",
      "Early stopping, best iteration is:\n",
      "[318]\ttraining's auc: 0.516202\tvalid_1's auc: 0.514711\n",
      "var_171\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536357\tvalid_1's auc: 0.540633\n",
      "Early stopping, best iteration is:\n",
      "[256]\ttraining's auc: 0.534828\tvalid_1's auc: 0.543074\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534569\tvalid_1's auc: 0.538952\n",
      "Early stopping, best iteration is:\n",
      "[738]\ttraining's auc: 0.534359\tvalid_1's auc: 0.539398\n",
      "var_172\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.541768\tvalid_1's auc: 0.537419\n",
      "[2000]\ttraining's auc: 0.542333\tvalid_1's auc: 0.537396\n",
      "Early stopping, best iteration is:\n",
      "[1067]\ttraining's auc: 0.541984\tvalid_1's auc: 0.537616\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540899\tvalid_1's auc: 0.537843\n",
      "[2000]\ttraining's auc: 0.541401\tvalid_1's auc: 0.537556\n",
      "Early stopping, best iteration is:\n",
      "[1048]\ttraining's auc: 0.540977\tvalid_1's auc: 0.538037\n",
      "var_173\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.554986\tvalid_1's auc: 0.552115\n",
      "Early stopping, best iteration is:\n",
      "[134]\ttraining's auc: 0.553743\tvalid_1's auc: 0.552813\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.552697\tvalid_1's auc: 0.548071\n",
      "[2000]\ttraining's auc: 0.552984\tvalid_1's auc: 0.548887\n",
      "[3000]\ttraining's auc: 0.553098\tvalid_1's auc: 0.549441\n",
      "[4000]\ttraining's auc: 0.553104\tvalid_1's auc: 0.549455\n",
      "[5000]\ttraining's auc: 0.553104\tvalid_1's auc: 0.549462\n",
      "Early stopping, best iteration is:\n",
      "[4463]\ttraining's auc: 0.553104\tvalid_1's auc: 0.549462\n",
      "var_174\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526335\tvalid_1's auc: 0.514027\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's auc: 0.521758\tvalid_1's auc: 0.516838\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.522201\tvalid_1's auc: 0.515869\n",
      "Early stopping, best iteration is:\n",
      "[510]\ttraining's auc: 0.522094\tvalid_1's auc: 0.51628\n",
      "var_175\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514367\tvalid_1's auc: 0.497681\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's auc: 0.505594\tvalid_1's auc: 0.499953\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512267\tvalid_1's auc: 0.498\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.509377\tvalid_1's auc: 0.50229\n",
      "var_176\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538824\tvalid_1's auc: 0.532577\n",
      "[2000]\ttraining's auc: 0.541213\tvalid_1's auc: 0.53256\n",
      "Early stopping, best iteration is:\n",
      "[1665]\ttraining's auc: 0.539978\tvalid_1's auc: 0.534393\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537998\tvalid_1's auc: 0.529315\n",
      "Early stopping, best iteration is:\n",
      "[692]\ttraining's auc: 0.537653\tvalid_1's auc: 0.5296\n",
      "var_177\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523711\tvalid_1's auc: 0.520554\n",
      "Early stopping, best iteration is:\n",
      "[682]\ttraining's auc: 0.522108\tvalid_1's auc: 0.522749\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.520012\tvalid_1's auc: 0.516121\n",
      "[2000]\ttraining's auc: 0.520085\tvalid_1's auc: 0.516309\n",
      "[3000]\ttraining's auc: 0.520112\tvalid_1's auc: 0.516652\n",
      "[4000]\ttraining's auc: 0.520112\tvalid_1's auc: 0.516685\n",
      "Early stopping, best iteration is:\n",
      "[3566]\ttraining's auc: 0.520112\tvalid_1's auc: 0.516685\n",
      "var_178\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.545619\tvalid_1's auc: 0.543192\n",
      "Early stopping, best iteration is:\n",
      "[356]\ttraining's auc: 0.543319\tvalid_1's auc: 0.543648\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.542019\tvalid_1's auc: 0.540414\n",
      "Early stopping, best iteration is:\n",
      "[338]\ttraining's auc: 0.54091\tvalid_1's auc: 0.541733\n",
      "var_179\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52919\tvalid_1's auc: 0.533506\n",
      "Early stopping, best iteration is:\n",
      "[777]\ttraining's auc: 0.529189\tvalid_1's auc: 0.533507\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526498\tvalid_1's auc: 0.52986\n",
      "Early stopping, best iteration is:\n",
      "[670]\ttraining's auc: 0.526379\tvalid_1's auc: 0.529992\n",
      "var_180\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.5187\tvalid_1's auc: 0.513594\n",
      "[2000]\ttraining's auc: 0.51986\tvalid_1's auc: 0.513935\n",
      "[3000]\ttraining's auc: 0.520396\tvalid_1's auc: 0.513731\n",
      "Early stopping, best iteration is:\n",
      "[2098]\ttraining's auc: 0.519904\tvalid_1's auc: 0.513951\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513565\tvalid_1's auc: 0.514057\n",
      "Early stopping, best iteration is:\n",
      "[631]\ttraining's auc: 0.513525\tvalid_1's auc: 0.514175\n",
      "var_181\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51432\tvalid_1's auc: 0.500197\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's auc: 0.509352\tvalid_1's auc: 0.503864\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51242\tvalid_1's auc: 0.498526\n",
      "Early stopping, best iteration is:\n",
      "[627]\ttraining's auc: 0.512223\tvalid_1's auc: 0.499291\n",
      "var_182\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511924\tvalid_1's auc: 0.496719\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.504626\tvalid_1's auc: 0.502389\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.509051\tvalid_1's auc: 0.497927\n",
      "Early stopping, best iteration is:\n",
      "[32]\ttraining's auc: 0.506066\tvalid_1's auc: 0.501722\n",
      "var_183\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.547173\tvalid_1's auc: 0.541022\n",
      "Early stopping, best iteration is:\n",
      "[78]\ttraining's auc: 0.543714\tvalid_1's auc: 0.544221\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.544316\tvalid_1's auc: 0.54478\n",
      "Early stopping, best iteration is:\n",
      "[816]\ttraining's auc: 0.544023\tvalid_1's auc: 0.546125\n",
      "var_184\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511912\tvalid_1's auc: 0.500469\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttraining's auc: 0.506974\tvalid_1's auc: 0.502872\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.506618\tvalid_1's auc: 0.496931\n",
      "Early stopping, best iteration is:\n",
      "[4]\ttraining's auc: 0.503993\tvalid_1's auc: 0.498827\n",
      "var_185\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534625\tvalid_1's auc: 0.520497\n",
      "[2000]\ttraining's auc: 0.535489\tvalid_1's auc: 0.520572\n",
      "Early stopping, best iteration is:\n",
      "[1136]\ttraining's auc: 0.534645\tvalid_1's auc: 0.520646\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.531976\tvalid_1's auc: 0.521813\n",
      "Early stopping, best iteration is:\n",
      "[632]\ttraining's auc: 0.531289\tvalid_1's auc: 0.52233\n",
      "var_186\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.519272\tvalid_1's auc: 0.509734\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttraining's auc: 0.514613\tvalid_1's auc: 0.512049\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516259\tvalid_1's auc: 0.507221\n",
      "Early stopping, best iteration is:\n",
      "[454]\ttraining's auc: 0.515476\tvalid_1's auc: 0.511922\n",
      "var_187\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535221\tvalid_1's auc: 0.52328\n",
      "Early stopping, best iteration is:\n",
      "[294]\ttraining's auc: 0.534126\tvalid_1's auc: 0.523386\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53301\tvalid_1's auc: 0.52082\n",
      "Early stopping, best iteration is:\n",
      "[94]\ttraining's auc: 0.529371\tvalid_1's auc: 0.520861\n",
      "var_188\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515049\tvalid_1's auc: 0.505559\n",
      "[2000]\ttraining's auc: 0.516927\tvalid_1's auc: 0.507347\n",
      "[3000]\ttraining's auc: 0.517923\tvalid_1's auc: 0.507183\n",
      "Early stopping, best iteration is:\n",
      "[2540]\ttraining's auc: 0.517359\tvalid_1's auc: 0.507945\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512627\tvalid_1's auc: 0.505192\n",
      "[2000]\ttraining's auc: 0.512702\tvalid_1's auc: 0.505773\n",
      "[3000]\ttraining's auc: 0.512707\tvalid_1's auc: 0.505591\n",
      "Early stopping, best iteration is:\n",
      "[2629]\ttraining's auc: 0.512707\tvalid_1's auc: 0.505591\n",
      "var_189\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.553347\tvalid_1's auc: 0.54982\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttraining's auc: 0.549092\tvalid_1's auc: 0.552909\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.551056\tvalid_1's auc: 0.547801\n",
      "[2000]\ttraining's auc: 0.551578\tvalid_1's auc: 0.549485\n",
      "[3000]\ttraining's auc: 0.551612\tvalid_1's auc: 0.549553\n",
      "[4000]\ttraining's auc: 0.551626\tvalid_1's auc: 0.549832\n",
      "Early stopping, best iteration is:\n",
      "[3213]\ttraining's auc: 0.551626\tvalid_1's auc: 0.549832\n",
      "var_190\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.545898\tvalid_1's auc: 0.538676\n",
      "Early stopping, best iteration is:\n",
      "[94]\ttraining's auc: 0.542551\tvalid_1's auc: 0.538802\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54363\tvalid_1's auc: 0.536988\n",
      "[2000]\ttraining's auc: 0.544306\tvalid_1's auc: 0.538673\n",
      "Early stopping, best iteration is:\n",
      "[1703]\ttraining's auc: 0.544271\tvalid_1's auc: 0.538949\n",
      "var_191\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54637\tvalid_1's auc: 0.543052\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttraining's auc: 0.544043\tvalid_1's auc: 0.544003\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.544478\tvalid_1's auc: 0.539879\n",
      "Early stopping, best iteration is:\n",
      "[967]\ttraining's auc: 0.544466\tvalid_1's auc: 0.540004\n",
      "var_192\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.522414\tvalid_1's auc: 0.509898\n",
      "Early stopping, best iteration is:\n",
      "[122]\ttraining's auc: 0.517523\tvalid_1's auc: 0.513322\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517845\tvalid_1's auc: 0.507821\n",
      "Early stopping, best iteration is:\n",
      "[285]\ttraining's auc: 0.517243\tvalid_1's auc: 0.508536\n",
      "var_193\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.522514\tvalid_1's auc: 0.530479\n",
      "Early stopping, best iteration is:\n",
      "[734]\ttraining's auc: 0.522015\tvalid_1's auc: 0.531386\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.519362\tvalid_1's auc: 0.526629\n",
      "[2000]\ttraining's auc: 0.519611\tvalid_1's auc: 0.527931\n",
      "[3000]\ttraining's auc: 0.519621\tvalid_1's auc: 0.528003\n",
      "Early stopping, best iteration is:\n",
      "[2475]\ttraining's auc: 0.519618\tvalid_1's auc: 0.52801\n",
      "var_194\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529907\tvalid_1's auc: 0.525296\n",
      "[2000]\ttraining's auc: 0.530943\tvalid_1's auc: 0.526467\n",
      "Early stopping, best iteration is:\n",
      "[1038]\ttraining's auc: 0.530618\tvalid_1's auc: 0.526893\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526473\tvalid_1's auc: 0.523797\n",
      "Early stopping, best iteration is:\n",
      "[812]\ttraining's auc: 0.526381\tvalid_1's auc: 0.524111\n",
      "var_195\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526664\tvalid_1's auc: 0.514343\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's auc: 0.522442\tvalid_1's auc: 0.516172\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.522715\tvalid_1's auc: 0.517224\n",
      "Early stopping, best iteration is:\n",
      "[626]\ttraining's auc: 0.522485\tvalid_1's auc: 0.517432\n",
      "var_196\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532157\tvalid_1's auc: 0.532051\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's auc: 0.528403\tvalid_1's auc: 0.533429\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529942\tvalid_1's auc: 0.526205\n",
      "[2000]\ttraining's auc: 0.530808\tvalid_1's auc: 0.528295\n",
      "Early stopping, best iteration is:\n",
      "[1877]\ttraining's auc: 0.530808\tvalid_1's auc: 0.528295\n",
      "var_197\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.551167\tvalid_1's auc: 0.546833\n",
      "Early stopping, best iteration is:\n",
      "[297]\ttraining's auc: 0.549234\tvalid_1's auc: 0.548018\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.548544\tvalid_1's auc: 0.542707\n",
      "Early stopping, best iteration is:\n",
      "[422]\ttraining's auc: 0.548144\tvalid_1's auc: 0.543171\n",
      "var_198\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526164\tvalid_1's auc: 0.533232\n",
      "Early stopping, best iteration is:\n",
      "[282]\ttraining's auc: 0.523918\tvalid_1's auc: 0.534267\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523684\tvalid_1's auc: 0.532775\n",
      "[2000]\ttraining's auc: 0.523938\tvalid_1's auc: 0.532615\n",
      "Early stopping, best iteration is:\n",
      "[1343]\ttraining's auc: 0.523803\tvalid_1's auc: 0.533273\n",
      "var_199\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for col in list(X1.columns):\n",
    "    bins = np.linspace(-6.0,6.0,40)\n",
    "    X = pd.get_dummies(pd.cut(X1[col].values, bins))\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                     max_depth=-1,\n",
    "                     n_estimators=999999,\n",
    "                     learning_rate=0.02,\n",
    "                     colsample_bytree=0.3,\n",
    "                     num_leaves=2,\n",
    "                     metric='auc',\n",
    "    #                  random_seed = 42 + params,\n",
    "                     objective='binary', \n",
    "                     n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X1[col].reshape(-1,1), y, test_size=0.2, stratify=y)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "    np.random.seed(123)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "    print(col)\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_cols = ['var_4', 'var_7', 'var_8', 'var_12', 'var_17', 'var_27', 'var_28', 'var_29', 'var_33', 'var_35',\n",
    "'var_36', 'var_37', 'var_38', 'var_41', 'var_44', 'var_45', 'var_47', 'var_49', 'var_58', 'var_65',\n",
    "'var_70', 'var_71', 'var_75', 'var_78', 'var_87', 'var_88', 'var_90', 'var_98', 'var_99', 'var_100',\n",
    "'var_110', 'var_112', 'var_122', 'var_123', 'var_124', 'var_125', 'var_126', 'var_127', 'var_136',\n",
    "'var_137', 'var_140', 'var_147', 'var_148', 'var_151', 'var_154', 'var_155', 'var_156', 'var_159',\n",
    "'var_166', 'var_176', 'var_181', 'var_184', 'var_186', 'var_191', 'var_196']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_4\n",
      "-------------\n",
      "var_7\n",
      "-------------\n",
      "var_8\n",
      "-------------\n",
      "var_12\n",
      "-------------\n",
      "var_17\n",
      "-------------\n",
      "var_27\n",
      "-------------\n",
      "var_28\n",
      "-------------\n",
      "var_29\n",
      "-------------\n",
      "var_33\n",
      "-------------\n",
      "var_35\n",
      "-------------\n",
      "var_36\n",
      "-------------\n",
      "var_37\n",
      "-------------\n",
      "var_38\n",
      "-------------\n",
      "var_41\n",
      "-------------\n",
      "var_44\n",
      "-------------\n",
      "var_45\n",
      "-------------\n",
      "var_47\n",
      "-------------\n",
      "var_49\n",
      "-------------\n",
      "var_58\n",
      "-------------\n",
      "var_65\n",
      "-------------\n",
      "var_70\n",
      "-------------\n",
      "var_71\n",
      "-------------\n",
      "var_75\n",
      "-------------\n",
      "var_78\n",
      "-------------\n",
      "var_87\n",
      "-------------\n",
      "var_88\n",
      "-------------\n",
      "var_90\n",
      "-------------\n",
      "var_98\n",
      "-------------\n",
      "var_99\n",
      "-------------\n",
      "var_100\n",
      "-------------\n",
      "var_110\n",
      "-------------\n",
      "var_112\n",
      "-------------\n",
      "var_122\n",
      "-------------\n",
      "var_123\n",
      "-------------\n",
      "var_124\n",
      "-------------\n",
      "var_125\n",
      "-------------\n",
      "var_126\n",
      "-------------\n",
      "var_127\n",
      "-------------\n",
      "var_136\n",
      "-------------\n",
      "var_137\n",
      "-------------\n",
      "var_140\n",
      "-------------\n",
      "var_147\n",
      "-------------\n",
      "var_148\n",
      "-------------\n",
      "var_151\n",
      "-------------\n",
      "var_154\n",
      "-------------\n",
      "var_155\n",
      "-------------\n",
      "var_156\n",
      "-------------\n",
      "var_159\n",
      "-------------\n",
      "var_166\n",
      "-------------\n",
      "var_176\n",
      "-------------\n",
      "var_181\n",
      "-------------\n",
      "var_184\n",
      "-------------\n",
      "var_186\n",
      "-------------\n",
      "var_191\n",
      "-------------\n",
      "var_196\n",
      "-------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200000, 2290), (200000, 2290))"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "bins = np.linspace(-6.0,6.0,40)\n",
    "import gc\n",
    "# for col in tqdm(bin_cols):\n",
    "for col in bin_cols:\n",
    "    print(col)\n",
    "    X3_col_bins = pd.get_dummies(pd.cut(X3[col], bins))\n",
    "    X4_col_bins = pd.get_dummies(pd.cut(X4[col], bins))\n",
    "\n",
    "    X3_col_bins = pd.DataFrame(X3_col_bins)\n",
    "    cols = [col + '_' + str(i) for i in range(X3_col_bins.shape[1])]\n",
    "    X3_col_bins.columns = cols\n",
    "\n",
    "    X4_col_bins = pd.DataFrame(X4_col_bins)\n",
    "    X4_col_bins.columns = cols\n",
    " \n",
    "    X3 = pd.concat([X3, X3_col_bins], axis=1)\n",
    "    X4 = pd.concat([X4, X4_col_bins], axis=1)\n",
    "\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X4 = X4.drop(col, axis=1)\n",
    "    \n",
    "    del X3_col_bins, X4_col_bins\n",
    "    gc.collect()\n",
    "    print('-------------')\n",
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sat Mar 16 05:46:53 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.876615\n",
      "[2000]\tvalid_0's auc: 0.882975\n",
      "[3000]\tvalid_0's auc: 0.886965\n",
      "[4000]\tvalid_0's auc: 0.889546\n",
      "[5000]\tvalid_0's auc: 0.891961\n",
      "[6000]\tvalid_0's auc: 0.893547\n",
      "[7000]\tvalid_0's auc: 0.894764\n",
      "[8000]\tvalid_0's auc: 0.895712\n",
      "[9000]\tvalid_0's auc: 0.896495\n",
      "[10000]\tvalid_0's auc: 0.897342\n",
      "[11000]\tvalid_0's auc: 0.897604\n",
      "[12000]\tvalid_0's auc: 0.897787\n",
      "[13000]\tvalid_0's auc: 0.898102\n",
      "[14000]\tvalid_0's auc: 0.898382\n",
      "[15000]\tvalid_0's auc: 0.898466\n",
      "[16000]\tvalid_0's auc: 0.898747\n",
      "[17000]\tvalid_0's auc: 0.898905\n",
      "[18000]\tvalid_0's auc: 0.898858\n",
      "[19000]\tvalid_0's auc: 0.899045\n",
      "[20000]\tvalid_0's auc: 0.899014\n",
      "[21000]\tvalid_0's auc: 0.899082\n",
      "[22000]\tvalid_0's auc: 0.899269\n",
      "[23000]\tvalid_0's auc: 0.899207\n",
      "[24000]\tvalid_0's auc: 0.89922\n",
      "[25000]\tvalid_0's auc: 0.899048\n",
      "Early stopping, best iteration is:\n",
      "[22592]\tvalid_0's auc: 0.899298\n",
      "Fold 1 started at Sat Mar 16 05:51:33 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.872517\n",
      "[2000]\tvalid_0's auc: 0.879434\n",
      "[3000]\tvalid_0's auc: 0.883644\n",
      "[4000]\tvalid_0's auc: 0.886891\n",
      "[5000]\tvalid_0's auc: 0.888943\n",
      "[6000]\tvalid_0's auc: 0.891229\n",
      "[7000]\tvalid_0's auc: 0.892786\n",
      "[8000]\tvalid_0's auc: 0.893897\n",
      "[9000]\tvalid_0's auc: 0.894544\n",
      "[10000]\tvalid_0's auc: 0.895149\n",
      "[11000]\tvalid_0's auc: 0.895759\n",
      "[12000]\tvalid_0's auc: 0.89614\n",
      "[13000]\tvalid_0's auc: 0.896169\n",
      "[14000]\tvalid_0's auc: 0.896284\n",
      "[15000]\tvalid_0's auc: 0.896476\n",
      "[16000]\tvalid_0's auc: 0.896369\n",
      "[17000]\tvalid_0's auc: 0.89664\n",
      "[18000]\tvalid_0's auc: 0.896788\n",
      "[19000]\tvalid_0's auc: 0.896526\n",
      "[20000]\tvalid_0's auc: 0.89663\n",
      "Early stopping, best iteration is:\n",
      "[17859]\tvalid_0's auc: 0.896944\n",
      "Fold 2 started at Sat Mar 16 06:27:16 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.866827\n",
      "[2000]\tvalid_0's auc: 0.875616\n",
      "[3000]\tvalid_0's auc: 0.880111\n",
      "[4000]\tvalid_0's auc: 0.88352\n",
      "[5000]\tvalid_0's auc: 0.886166\n",
      "[6000]\tvalid_0's auc: 0.888663\n",
      "[7000]\tvalid_0's auc: 0.889952\n",
      "[8000]\tvalid_0's auc: 0.891115\n",
      "[9000]\tvalid_0's auc: 0.891768\n",
      "[10000]\tvalid_0's auc: 0.892514\n",
      "[11000]\tvalid_0's auc: 0.892964\n",
      "[12000]\tvalid_0's auc: 0.893286\n",
      "[13000]\tvalid_0's auc: 0.893648\n",
      "[14000]\tvalid_0's auc: 0.89391\n",
      "[15000]\tvalid_0's auc: 0.894015\n",
      "[16000]\tvalid_0's auc: 0.894032\n",
      "[17000]\tvalid_0's auc: 0.893989\n",
      "[18000]\tvalid_0's auc: 0.893995\n",
      "[19000]\tvalid_0's auc: 0.894078\n",
      "Early stopping, best iteration is:\n",
      "[16428]\tvalid_0's auc: 0.89414\n",
      "Fold 3 started at Sat Mar 16 07:00:35 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.875139\n",
      "[2000]\tvalid_0's auc: 0.881721\n",
      "[3000]\tvalid_0's auc: 0.885757\n",
      "[4000]\tvalid_0's auc: 0.888764\n",
      "[5000]\tvalid_0's auc: 0.890917\n",
      "[6000]\tvalid_0's auc: 0.892687\n",
      "[7000]\tvalid_0's auc: 0.893914\n",
      "[8000]\tvalid_0's auc: 0.894791\n",
      "[9000]\tvalid_0's auc: 0.895524\n",
      "[10000]\tvalid_0's auc: 0.895944\n",
      "[11000]\tvalid_0's auc: 0.89624\n",
      "[12000]\tvalid_0's auc: 0.896525\n",
      "[13000]\tvalid_0's auc: 0.896744\n",
      "[14000]\tvalid_0's auc: 0.896756\n",
      "[15000]\tvalid_0's auc: 0.897062\n",
      "[16000]\tvalid_0's auc: 0.897186\n",
      "[17000]\tvalid_0's auc: 0.897389\n",
      "[18000]\tvalid_0's auc: 0.897539\n",
      "[19000]\tvalid_0's auc: 0.897532\n",
      "[20000]\tvalid_0's auc: 0.8974\n",
      "[21000]\tvalid_0's auc: 0.897331\n",
      "Early stopping, best iteration is:\n",
      "[18275]\tvalid_0's auc: 0.897587\n",
      "Fold 4 started at Sat Mar 16 07:49:10 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.873515\n",
      "[2000]\tvalid_0's auc: 0.88109\n",
      "[3000]\tvalid_0's auc: 0.885187\n",
      "[4000]\tvalid_0's auc: 0.887576\n",
      "[5000]\tvalid_0's auc: 0.890472\n",
      "[6000]\tvalid_0's auc: 0.892221\n",
      "[7000]\tvalid_0's auc: 0.893629\n",
      "[8000]\tvalid_0's auc: 0.894518\n",
      "[9000]\tvalid_0's auc: 0.895418\n",
      "[10000]\tvalid_0's auc: 0.895921\n",
      "[11000]\tvalid_0's auc: 0.896185\n",
      "[12000]\tvalid_0's auc: 0.896591\n",
      "[13000]\tvalid_0's auc: 0.89697\n",
      "[14000]\tvalid_0's auc: 0.897024\n",
      "[15000]\tvalid_0's auc: 0.897245\n",
      "[16000]\tvalid_0's auc: 0.897482\n",
      "[17000]\tvalid_0's auc: 0.89745\n",
      "[18000]\tvalid_0's auc: 0.897398\n",
      "[19000]\tvalid_0's auc: 0.897527\n",
      "[20000]\tvalid_0's auc: 0.89762\n",
      "[21000]\tvalid_0's auc: 0.897578\n",
      "[22000]\tvalid_0's auc: 0.897509\n",
      "Early stopping, best iteration is:\n",
      "[19615]\tvalid_0's auc: 0.897698\n",
      "Fold 5 started at Sat Mar 16 08:24:33 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.874606\n",
      "[2000]\tvalid_0's auc: 0.88231\n",
      "[3000]\tvalid_0's auc: 0.885265\n",
      "[4000]\tvalid_0's auc: 0.88807\n",
      "[5000]\tvalid_0's auc: 0.890756\n",
      "[6000]\tvalid_0's auc: 0.892876\n",
      "[7000]\tvalid_0's auc: 0.894269\n",
      "[8000]\tvalid_0's auc: 0.895285\n",
      "[9000]\tvalid_0's auc: 0.89614\n",
      "[10000]\tvalid_0's auc: 0.896764\n",
      "[11000]\tvalid_0's auc: 0.897329\n",
      "[12000]\tvalid_0's auc: 0.897628\n",
      "[13000]\tvalid_0's auc: 0.897994\n",
      "[14000]\tvalid_0's auc: 0.898205\n",
      "[15000]\tvalid_0's auc: 0.898413\n",
      "[16000]\tvalid_0's auc: 0.898595\n",
      "[17000]\tvalid_0's auc: 0.898727\n",
      "[18000]\tvalid_0's auc: 0.89893\n",
      "[19000]\tvalid_0's auc: 0.899052\n",
      "[20000]\tvalid_0's auc: 0.898928\n",
      "[21000]\tvalid_0's auc: 0.899108\n",
      "[22000]\tvalid_0's auc: 0.899097\n",
      "[23000]\tvalid_0's auc: 0.899362\n",
      "[24000]\tvalid_0's auc: 0.899465\n",
      "[25000]\tvalid_0's auc: 0.899487\n",
      "[26000]\tvalid_0's auc: 0.899486\n",
      "[27000]\tvalid_0's auc: 0.899557\n",
      "[28000]\tvalid_0's auc: 0.899656\n",
      "[29000]\tvalid_0's auc: 0.899823\n",
      "[30000]\tvalid_0's auc: 0.899762\n",
      "[31000]\tvalid_0's auc: 0.899666\n",
      "[32000]\tvalid_0's auc: 0.899577\n",
      "Early stopping, best iteration is:\n",
      "[29261]\tvalid_0's auc: 0.899893\n",
      "Fold 6 started at Sat Mar 16 09:16:45 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883424\n",
      "[2000]\tvalid_0's auc: 0.89186\n",
      "[3000]\tvalid_0's auc: 0.896819\n",
      "[4000]\tvalid_0's auc: 0.900193\n",
      "[5000]\tvalid_0's auc: 0.902849\n",
      "[6000]\tvalid_0's auc: 0.905023\n",
      "[7000]\tvalid_0's auc: 0.906522\n",
      "[8000]\tvalid_0's auc: 0.907633\n",
      "[9000]\tvalid_0's auc: 0.90859\n",
      "[10000]\tvalid_0's auc: 0.909068\n",
      "[11000]\tvalid_0's auc: 0.909909\n",
      "[12000]\tvalid_0's auc: 0.910144\n",
      "[13000]\tvalid_0's auc: 0.910436\n",
      "[14000]\tvalid_0's auc: 0.910787\n",
      "[15000]\tvalid_0's auc: 0.91124\n",
      "[16000]\tvalid_0's auc: 0.911437\n",
      "[17000]\tvalid_0's auc: 0.911564\n",
      "[18000]\tvalid_0's auc: 0.911519\n",
      "[19000]\tvalid_0's auc: 0.911442\n",
      "[20000]\tvalid_0's auc: 0.911417\n",
      "Early stopping, best iteration is:\n",
      "[17280]\tvalid_0's auc: 0.911593\n",
      "Fold 7 started at Sat Mar 16 09:52:23 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.870277\n",
      "[2000]\tvalid_0's auc: 0.878182\n",
      "[3000]\tvalid_0's auc: 0.882321\n",
      "[4000]\tvalid_0's auc: 0.885785\n",
      "[5000]\tvalid_0's auc: 0.888082\n",
      "[6000]\tvalid_0's auc: 0.889671\n",
      "[7000]\tvalid_0's auc: 0.890763\n",
      "[8000]\tvalid_0's auc: 0.891679\n",
      "[9000]\tvalid_0's auc: 0.892533\n",
      "[10000]\tvalid_0's auc: 0.892978\n",
      "[11000]\tvalid_0's auc: 0.893394\n",
      "[12000]\tvalid_0's auc: 0.89381\n",
      "[13000]\tvalid_0's auc: 0.893994\n",
      "[14000]\tvalid_0's auc: 0.89416\n",
      "[15000]\tvalid_0's auc: 0.89439\n",
      "[16000]\tvalid_0's auc: 0.894563\n",
      "[17000]\tvalid_0's auc: 0.894619\n",
      "[18000]\tvalid_0's auc: 0.89464\n",
      "[19000]\tvalid_0's auc: 0.894672\n",
      "[20000]\tvalid_0's auc: 0.894608\n",
      "[21000]\tvalid_0's auc: 0.894566\n",
      "[22000]\tvalid_0's auc: 0.894645\n",
      "Early stopping, best iteration is:\n",
      "[19216]\tvalid_0's auc: 0.894794\n",
      "Fold 8 started at Sat Mar 16 10:37:21 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.874809\n",
      "[2000]\tvalid_0's auc: 0.882409\n",
      "[3000]\tvalid_0's auc: 0.887053\n",
      "[4000]\tvalid_0's auc: 0.890319\n",
      "[5000]\tvalid_0's auc: 0.893247\n",
      "[6000]\tvalid_0's auc: 0.895034\n",
      "[7000]\tvalid_0's auc: 0.896887\n",
      "[8000]\tvalid_0's auc: 0.898125\n",
      "[9000]\tvalid_0's auc: 0.898865\n",
      "[10000]\tvalid_0's auc: 0.89947\n",
      "[11000]\tvalid_0's auc: 0.900429\n",
      "[12000]\tvalid_0's auc: 0.900865\n",
      "[13000]\tvalid_0's auc: 0.901298\n",
      "[14000]\tvalid_0's auc: 0.901554\n",
      "[15000]\tvalid_0's auc: 0.901607\n",
      "[16000]\tvalid_0's auc: 0.90163\n",
      "[17000]\tvalid_0's auc: 0.901784\n",
      "[18000]\tvalid_0's auc: 0.90196\n",
      "[19000]\tvalid_0's auc: 0.902064\n",
      "[20000]\tvalid_0's auc: 0.902085\n",
      "[21000]\tvalid_0's auc: 0.902058\n",
      "[22000]\tvalid_0's auc: 0.902138\n",
      "[23000]\tvalid_0's auc: 0.902325\n",
      "[24000]\tvalid_0's auc: 0.90247\n",
      "[25000]\tvalid_0's auc: 0.902648\n",
      "[26000]\tvalid_0's auc: 0.902387\n",
      "[27000]\tvalid_0's auc: 0.902478\n",
      "Early stopping, best iteration is:\n",
      "[24872]\tvalid_0's auc: 0.902671\n",
      "Fold 9 started at Sat Mar 16 11:57:38 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_0's auc: 0.87357\n",
      "[2000]\tvalid_0's auc: 0.880993\n",
      "[3000]\tvalid_0's auc: 0.885384\n",
      "[4000]\tvalid_0's auc: 0.88914\n",
      "[5000]\tvalid_0's auc: 0.892049\n",
      "[6000]\tvalid_0's auc: 0.894319\n",
      "[7000]\tvalid_0's auc: 0.896081\n",
      "[8000]\tvalid_0's auc: 0.897214\n",
      "[9000]\tvalid_0's auc: 0.89812\n",
      "[10000]\tvalid_0's auc: 0.898867\n",
      "[11000]\tvalid_0's auc: 0.89963\n",
      "[12000]\tvalid_0's auc: 0.899938\n",
      "[13000]\tvalid_0's auc: 0.900396\n",
      "[14000]\tvalid_0's auc: 0.900476\n",
      "[15000]\tvalid_0's auc: 0.900617\n",
      "[16000]\tvalid_0's auc: 0.900706\n",
      "[17000]\tvalid_0's auc: 0.900723\n",
      "[18000]\tvalid_0's auc: 0.900725\n",
      "[19000]\tvalid_0's auc: 0.900775\n",
      "[20000]\tvalid_0's auc: 0.900722\n",
      "[21000]\tvalid_0's auc: 0.900845\n",
      "[22000]\tvalid_0's auc: 0.900785\n",
      "[23000]\tvalid_0's auc: 0.900729\n",
      "[24000]\tvalid_0's auc: 0.900812\n",
      "Early stopping, best iteration is:\n",
      "[21511]\tvalid_0's auc: 0.900906\n",
      "Fold 10 started at Sat Mar 16 12:53:54 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.869052\n",
      "[2000]\tvalid_0's auc: 0.876955\n",
      "[3000]\tvalid_0's auc: 0.8812\n",
      "[4000]\tvalid_0's auc: 0.88459\n",
      "[5000]\tvalid_0's auc: 0.887449\n",
      "[6000]\tvalid_0's auc: 0.889573\n",
      "[7000]\tvalid_0's auc: 0.891125\n",
      "[8000]\tvalid_0's auc: 0.892457\n",
      "[9000]\tvalid_0's auc: 0.893514\n",
      "[10000]\tvalid_0's auc: 0.894162\n",
      "[11000]\tvalid_0's auc: 0.89459\n",
      "[12000]\tvalid_0's auc: 0.894938\n",
      "[13000]\tvalid_0's auc: 0.894965\n",
      "[14000]\tvalid_0's auc: 0.894918\n",
      "[15000]\tvalid_0's auc: 0.895048\n",
      "[16000]\tvalid_0's auc: 0.895121\n",
      "[17000]\tvalid_0's auc: 0.895238\n",
      "[18000]\tvalid_0's auc: 0.895331\n",
      "[19000]\tvalid_0's auc: 0.895335\n",
      "[20000]\tvalid_0's auc: 0.895326\n",
      "[21000]\tvalid_0's auc: 0.895128\n",
      "[22000]\tvalid_0's auc: 0.895305\n",
      "Early stopping, best iteration is:\n",
      "[19515]\tvalid_0's auc: 0.895459\n",
      "Fold 11 started at Sat Mar 16 13:47:37 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.867091\n",
      "[2000]\tvalid_0's auc: 0.873977\n",
      "[3000]\tvalid_0's auc: 0.878383\n",
      "[4000]\tvalid_0's auc: 0.881631\n",
      "[5000]\tvalid_0's auc: 0.88409\n",
      "[6000]\tvalid_0's auc: 0.88613\n",
      "[7000]\tvalid_0's auc: 0.887371\n",
      "[8000]\tvalid_0's auc: 0.88851\n",
      "[9000]\tvalid_0's auc: 0.889692\n",
      "[10000]\tvalid_0's auc: 0.89029\n",
      "[11000]\tvalid_0's auc: 0.890731\n",
      "[12000]\tvalid_0's auc: 0.891227\n",
      "[13000]\tvalid_0's auc: 0.891361\n",
      "[14000]\tvalid_0's auc: 0.891588\n",
      "[15000]\tvalid_0's auc: 0.892004\n",
      "[16000]\tvalid_0's auc: 0.89219\n",
      "[17000]\tvalid_0's auc: 0.892307\n",
      "[18000]\tvalid_0's auc: 0.892431\n",
      "[19000]\tvalid_0's auc: 0.892627\n",
      "[20000]\tvalid_0's auc: 0.892644\n",
      "[21000]\tvalid_0's auc: 0.892656\n",
      "[22000]\tvalid_0's auc: 0.892791\n",
      "[23000]\tvalid_0's auc: 0.892878\n",
      "[24000]\tvalid_0's auc: 0.892901\n",
      "[25000]\tvalid_0's auc: 0.89285\n",
      "[26000]\tvalid_0's auc: 0.892471\n",
      "Early stopping, best iteration is:\n",
      "[23173]\tvalid_0's auc: 0.892972\n",
      "Fold 12 started at Sat Mar 16 15:12:25 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.861344\n",
      "[2000]\tvalid_0's auc: 0.868805\n",
      "[3000]\tvalid_0's auc: 0.873817\n",
      "[4000]\tvalid_0's auc: 0.877643\n",
      "[5000]\tvalid_0's auc: 0.880494\n",
      "[6000]\tvalid_0's auc: 0.882575\n",
      "[7000]\tvalid_0's auc: 0.884449\n",
      "[8000]\tvalid_0's auc: 0.885385\n",
      "[9000]\tvalid_0's auc: 0.886204\n",
      "[10000]\tvalid_0's auc: 0.88708\n",
      "[11000]\tvalid_0's auc: 0.887757\n",
      "[12000]\tvalid_0's auc: 0.888111\n",
      "[13000]\tvalid_0's auc: 0.888227\n",
      "[14000]\tvalid_0's auc: 0.888616\n",
      "[15000]\tvalid_0's auc: 0.888977\n",
      "[16000]\tvalid_0's auc: 0.889312\n",
      "[17000]\tvalid_0's auc: 0.889455\n",
      "[18000]\tvalid_0's auc: 0.88942\n",
      "[19000]\tvalid_0's auc: 0.889389\n",
      "[20000]\tvalid_0's auc: 0.889609\n",
      "[21000]\tvalid_0's auc: 0.88982\n",
      "[22000]\tvalid_0's auc: 0.889909\n",
      "[23000]\tvalid_0's auc: 0.889987\n",
      "[24000]\tvalid_0's auc: 0.890036\n",
      "[25000]\tvalid_0's auc: 0.889891\n",
      "[26000]\tvalid_0's auc: 0.889742\n",
      "Early stopping, best iteration is:\n",
      "[23969]\tvalid_0's auc: 0.890063\n",
      "Fold 13 started at Sat Mar 16 16:38:57 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.872631\n",
      "[2000]\tvalid_0's auc: 0.878702\n",
      "[3000]\tvalid_0's auc: 0.882817\n",
      "[4000]\tvalid_0's auc: 0.885948\n",
      "[5000]\tvalid_0's auc: 0.888433\n",
      "[6000]\tvalid_0's auc: 0.890211\n",
      "[7000]\tvalid_0's auc: 0.891219\n",
      "[8000]\tvalid_0's auc: 0.892324\n",
      "[9000]\tvalid_0's auc: 0.892859\n",
      "[10000]\tvalid_0's auc: 0.893241\n",
      "[11000]\tvalid_0's auc: 0.893747\n",
      "[12000]\tvalid_0's auc: 0.893914\n",
      "[13000]\tvalid_0's auc: 0.89406\n",
      "[14000]\tvalid_0's auc: 0.894201\n",
      "[15000]\tvalid_0's auc: 0.894245\n",
      "[16000]\tvalid_0's auc: 0.894173\n",
      "[17000]\tvalid_0's auc: 0.894086\n",
      "[18000]\tvalid_0's auc: 0.894416\n",
      "[19000]\tvalid_0's auc: 0.894352\n",
      "[20000]\tvalid_0's auc: 0.894376\n",
      "Early stopping, best iteration is:\n",
      "[17868]\tvalid_0's auc: 0.894459\n",
      "Fold 14 started at Sat Mar 16 17:44:08 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.868769\n",
      "[2000]\tvalid_0's auc: 0.876984\n",
      "[3000]\tvalid_0's auc: 0.880723\n",
      "[4000]\tvalid_0's auc: 0.883239\n",
      "[5000]\tvalid_0's auc: 0.885492\n",
      "[6000]\tvalid_0's auc: 0.88741\n",
      "[7000]\tvalid_0's auc: 0.88855\n",
      "[8000]\tvalid_0's auc: 0.889391\n",
      "[9000]\tvalid_0's auc: 0.89036\n",
      "[10000]\tvalid_0's auc: 0.890947\n",
      "[11000]\tvalid_0's auc: 0.891484\n",
      "[12000]\tvalid_0's auc: 0.891644\n",
      "[13000]\tvalid_0's auc: 0.891689\n",
      "[14000]\tvalid_0's auc: 0.891872\n",
      "[15000]\tvalid_0's auc: 0.891659\n",
      "[16000]\tvalid_0's auc: 0.891761\n",
      "[17000]\tvalid_0's auc: 0.89187\n",
      "[18000]\tvalid_0's auc: 0.891989\n",
      "[19000]\tvalid_0's auc: 0.891758\n",
      "[20000]\tvalid_0's auc: 0.891854\n",
      "Early stopping, best iteration is:\n",
      "[17391]\tvalid_0's auc: 0.89207\n",
      "CV mean score: 0.8974, std: 0.0051.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_all_bins_1_15_1', oof)\n",
    "np.save('../cache/preds_new_quant_all_bins_1_15_1', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.562771\tvalid_1's auc: 0.543174\n",
      "Early stopping, best iteration is:\n",
      "[62]\ttraining's auc: 0.553358\tvalid_1's auc: 0.54898\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546362\tvalid_1's auc: 0.542755\n",
      "Early stopping, best iteration is:\n",
      "[84]\ttraining's auc: 0.541943\tvalid_1's auc: 0.54547\n",
      "var_0\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.559618\tvalid_1's auc: 0.545395\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's auc: 0.549533\tvalid_1's auc: 0.553045\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543051\tvalid_1's auc: 0.550363\n",
      "Early stopping, best iteration is:\n",
      "[296]\ttraining's auc: 0.542851\tvalid_1's auc: 0.550904\n",
      "var_1\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.562907\tvalid_1's auc: 0.555084\n",
      "Early stopping, best iteration is:\n",
      "[216]\ttraining's auc: 0.557697\tvalid_1's auc: 0.559042\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.550602\tvalid_1's auc: 0.554183\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttraining's auc: 0.54455\tvalid_1's auc: 0.559132\n",
      "var_2\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537146\tvalid_1's auc: 0.506958\n",
      "Early stopping, best iteration is:\n",
      "[161]\ttraining's auc: 0.529075\tvalid_1's auc: 0.51142\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513817\tvalid_1's auc: 0.507962\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's auc: 0.513565\tvalid_1's auc: 0.511107\n",
      "var_3\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533948\tvalid_1's auc: 0.504645\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.516084\tvalid_1's auc: 0.512595\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514134\tvalid_1's auc: 0.507855\n",
      "Early stopping, best iteration is:\n",
      "[138]\ttraining's auc: 0.512968\tvalid_1's auc: 0.509004\n",
      "var_4\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.550278\tvalid_1's auc: 0.518376\n",
      "Early stopping, best iteration is:\n",
      "[94]\ttraining's auc: 0.539881\tvalid_1's auc: 0.524727\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.528815\tvalid_1's auc: 0.513907\n",
      "Early stopping, best iteration is:\n",
      "[345]\ttraining's auc: 0.528242\tvalid_1's auc: 0.519365\n",
      "var_5\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.571493\tvalid_1's auc: 0.560488\n",
      "Early stopping, best iteration is:\n",
      "[360]\ttraining's auc: 0.568663\tvalid_1's auc: 0.56303\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.559122\tvalid_1's auc: 0.556601\n",
      "Early stopping, best iteration is:\n",
      "[229]\ttraining's auc: 0.558115\tvalid_1's auc: 0.559199\n",
      "var_6\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532944\tvalid_1's auc: 0.495738\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's auc: 0.520844\tvalid_1's auc: 0.503677\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510646\tvalid_1's auc: 0.497909\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's auc: 0.503598\tvalid_1's auc: 0.509029\n",
      "var_7\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.539211\tvalid_1's auc: 0.513101\n",
      "Early stopping, best iteration is:\n",
      "[95]\ttraining's auc: 0.529292\tvalid_1's auc: 0.521756\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.521853\tvalid_1's auc: 0.519647\n",
      "Early stopping, best iteration is:\n",
      "[271]\ttraining's auc: 0.522105\tvalid_1's auc: 0.519146\n",
      "var_8\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.558252\tvalid_1's auc: 0.536101\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's auc: 0.547103\tvalid_1's auc: 0.543211\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.544281\tvalid_1's auc: 0.539782\n",
      "Early stopping, best iteration is:\n",
      "[361]\ttraining's auc: 0.543866\tvalid_1's auc: 0.541494\n",
      "var_9\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532276\tvalid_1's auc: 0.500012\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.508072\tvalid_1's auc: 0.510475\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.508615\tvalid_1's auc: 0.501609\n",
      "Early stopping, best iteration is:\n",
      "[91]\ttraining's auc: 0.506999\tvalid_1's auc: 0.503723\n",
      "var_10\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.541091\tvalid_1's auc: 0.514452\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's auc: 0.521063\tvalid_1's auc: 0.52381\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.519297\tvalid_1's auc: 0.522068\n",
      "Early stopping, best iteration is:\n",
      "[491]\ttraining's auc: 0.519321\tvalid_1's auc: 0.523\n",
      "var_11\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.572754\tvalid_1's auc: 0.558878\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttraining's auc: 0.566487\tvalid_1's auc: 0.563352\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.560925\tvalid_1's auc: 0.560019\n",
      "Early stopping, best iteration is:\n",
      "[386]\ttraining's auc: 0.56076\tvalid_1's auc: 0.563472\n",
      "var_12\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.568458\tvalid_1's auc: 0.540571\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's auc: 0.55719\tvalid_1's auc: 0.545121\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.55428\tvalid_1's auc: 0.542418\n",
      "Early stopping, best iteration is:\n",
      "[169]\ttraining's auc: 0.553268\tvalid_1's auc: 0.543769\n",
      "var_13\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534851\tvalid_1's auc: 0.50206\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's auc: 0.51413\tvalid_1's auc: 0.509074\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510313\tvalid_1's auc: 0.506743\n",
      "Early stopping, best iteration is:\n",
      "[175]\ttraining's auc: 0.510446\tvalid_1's auc: 0.505038\n",
      "var_14\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535494\tvalid_1's auc: 0.513897\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.52003\tvalid_1's auc: 0.514727\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.518721\tvalid_1's auc: 0.508384\n",
      "Early stopping, best iteration is:\n",
      "[176]\ttraining's auc: 0.518319\tvalid_1's auc: 0.510198\n",
      "var_15\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535688\tvalid_1's auc: 0.503804\n",
      "Early stopping, best iteration is:\n",
      "[224]\ttraining's auc: 0.528202\tvalid_1's auc: 0.506747\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513111\tvalid_1's auc: 0.504544\n",
      "Early stopping, best iteration is:\n",
      "[552]\ttraining's auc: 0.512872\tvalid_1's auc: 0.505095\n",
      "var_16\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532004\tvalid_1's auc: 0.493976\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's auc: 0.511678\tvalid_1's auc: 0.497693\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.508045\tvalid_1's auc: 0.490544\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.500512\tvalid_1's auc: 0.498705\n",
      "var_17\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.553501\tvalid_1's auc: 0.534159\n",
      "Early stopping, best iteration is:\n",
      "[148]\ttraining's auc: 0.546091\tvalid_1's auc: 0.540577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538178\tvalid_1's auc: 0.538905\n",
      "[2000]\ttraining's auc: 0.538212\tvalid_1's auc: 0.538289\n",
      "Early stopping, best iteration is:\n",
      "[1457]\ttraining's auc: 0.538176\tvalid_1's auc: 0.538916\n",
      "var_18\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536966\tvalid_1's auc: 0.50449\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's auc: 0.521403\tvalid_1's auc: 0.509643\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515743\tvalid_1's auc: 0.506437\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's auc: 0.504434\tvalid_1's auc: 0.509294\n",
      "var_19\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537833\tvalid_1's auc: 0.513814\n",
      "Early stopping, best iteration is:\n",
      "[230]\ttraining's auc: 0.530993\tvalid_1's auc: 0.517729\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.522125\tvalid_1's auc: 0.51576\n",
      "[2000]\ttraining's auc: 0.522161\tvalid_1's auc: 0.515669\n",
      "Early stopping, best iteration is:\n",
      "[1378]\ttraining's auc: 0.522034\tvalid_1's auc: 0.516499\n",
      "var_20\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.568023\tvalid_1's auc: 0.542762\n",
      "Early stopping, best iteration is:\n",
      "[521]\ttraining's auc: 0.565446\tvalid_1's auc: 0.544467\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.556476\tvalid_1's auc: 0.540886\n",
      "Early stopping, best iteration is:\n",
      "[213]\ttraining's auc: 0.555223\tvalid_1's auc: 0.543069\n",
      "var_21\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.565034\tvalid_1's auc: 0.546439\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's auc: 0.558487\tvalid_1's auc: 0.551358\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.551636\tvalid_1's auc: 0.550312\n",
      "Early stopping, best iteration is:\n",
      "[530]\ttraining's auc: 0.552159\tvalid_1's auc: 0.550076\n",
      "var_22\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.541376\tvalid_1's auc: 0.515691\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's auc: 0.523666\tvalid_1's auc: 0.521392\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523926\tvalid_1's auc: 0.519106\n",
      "Early stopping, best iteration is:\n",
      "[353]\ttraining's auc: 0.524086\tvalid_1's auc: 0.521255\n",
      "var_23\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546195\tvalid_1's auc: 0.521669\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttraining's auc: 0.536977\tvalid_1's auc: 0.526873\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529309\tvalid_1's auc: 0.523917\n",
      "Early stopping, best iteration is:\n",
      "[152]\ttraining's auc: 0.528654\tvalid_1's auc: 0.524628\n",
      "var_24\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536241\tvalid_1's auc: 0.507398\n",
      "Early stopping, best iteration is:\n",
      "[220]\ttraining's auc: 0.528382\tvalid_1's auc: 0.514477\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51306\tvalid_1's auc: 0.512467\n",
      "Early stopping, best iteration is:\n",
      "[78]\ttraining's auc: 0.512462\tvalid_1's auc: 0.513928\n",
      "var_25\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.565799\tvalid_1's auc: 0.549651\n",
      "Early stopping, best iteration is:\n",
      "[49]\ttraining's auc: 0.556622\tvalid_1's auc: 0.55811\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.553066\tvalid_1's auc: 0.551439\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttraining's auc: 0.550472\tvalid_1's auc: 0.556678\n",
      "var_26\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534149\tvalid_1's auc: 0.500059\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.513054\tvalid_1's auc: 0.50208\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.509109\tvalid_1's auc: 0.500215\n",
      "Early stopping, best iteration is:\n",
      "[4]\ttraining's auc: 0.501312\tvalid_1's auc: 0.506487\n",
      "var_27\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543606\tvalid_1's auc: 0.512279\n",
      "Early stopping, best iteration is:\n",
      "[183]\ttraining's auc: 0.536064\tvalid_1's auc: 0.515212\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527963\tvalid_1's auc: 0.517559\n",
      "[2000]\ttraining's auc: 0.527892\tvalid_1's auc: 0.517855\n",
      "Early stopping, best iteration is:\n",
      "[1022]\ttraining's auc: 0.527749\tvalid_1's auc: 0.519005\n",
      "var_28\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535787\tvalid_1's auc: 0.491068\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.508383\tvalid_1's auc: 0.497\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510603\tvalid_1's auc: 0.488985\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.505042\tvalid_1's auc: 0.500911\n",
      "var_29\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533132\tvalid_1's auc: 0.506862\n",
      "Early stopping, best iteration is:\n",
      "[458]\ttraining's auc: 0.529393\tvalid_1's auc: 0.509195\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.508528\tvalid_1's auc: 0.505475\n",
      "Early stopping, best iteration is:\n",
      "[166]\ttraining's auc: 0.507906\tvalid_1's auc: 0.510149\n",
      "var_30\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.544274\tvalid_1's auc: 0.530243\n",
      "Early stopping, best iteration is:\n",
      "[197]\ttraining's auc: 0.536628\tvalid_1's auc: 0.535496\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.525615\tvalid_1's auc: 0.533548\n",
      "Early stopping, best iteration is:\n",
      "[292]\ttraining's auc: 0.524847\tvalid_1's auc: 0.534908\n",
      "var_31\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.548275\tvalid_1's auc: 0.51813\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's auc: 0.529363\tvalid_1's auc: 0.52934\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529621\tvalid_1's auc: 0.523379\n",
      "Early stopping, best iteration is:\n",
      "[344]\ttraining's auc: 0.529417\tvalid_1's auc: 0.524348\n",
      "var_32\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.554091\tvalid_1's auc: 0.540852\n",
      "Early stopping, best iteration is:\n",
      "[95]\ttraining's auc: 0.545762\tvalid_1's auc: 0.543652\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538915\tvalid_1's auc: 0.544615\n",
      "[2000]\ttraining's auc: 0.53886\tvalid_1's auc: 0.544659\n",
      "Early stopping, best iteration is:\n",
      "[1029]\ttraining's auc: 0.538919\tvalid_1's auc: 0.544719\n",
      "var_33\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.561908\tvalid_1's auc: 0.537102\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttraining's auc: 0.551342\tvalid_1's auc: 0.543728\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.547269\tvalid_1's auc: 0.543352\n",
      "Early stopping, best iteration is:\n",
      "[163]\ttraining's auc: 0.546429\tvalid_1's auc: 0.544839\n",
      "var_34\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.548046\tvalid_1's auc: 0.537812\n",
      "Early stopping, best iteration is:\n",
      "[311]\ttraining's auc: 0.543097\tvalid_1's auc: 0.542103\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533232\tvalid_1's auc: 0.543238\n",
      "Early stopping, best iteration is:\n",
      "[179]\ttraining's auc: 0.531605\tvalid_1's auc: 0.543977\n",
      "var_35\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.551072\tvalid_1's auc: 0.530455\n",
      "Early stopping, best iteration is:\n",
      "[155]\ttraining's auc: 0.545019\tvalid_1's auc: 0.533016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538698\tvalid_1's auc: 0.535159\n",
      "Early stopping, best iteration is:\n",
      "[444]\ttraining's auc: 0.53887\tvalid_1's auc: 0.532769\n",
      "var_36\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537535\tvalid_1's auc: 0.503553\n",
      "Early stopping, best iteration is:\n",
      "[295]\ttraining's auc: 0.530687\tvalid_1's auc: 0.507629\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513183\tvalid_1's auc: 0.509344\n",
      "Early stopping, best iteration is:\n",
      "[117]\ttraining's auc: 0.513147\tvalid_1's auc: 0.510245\n",
      "var_37\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532861\tvalid_1's auc: 0.506325\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's auc: 0.519011\tvalid_1's auc: 0.508553\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.509487\tvalid_1's auc: 0.508867\n",
      "Early stopping, best iteration is:\n",
      "[327]\ttraining's auc: 0.509616\tvalid_1's auc: 0.508198\n",
      "var_38\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536588\tvalid_1's auc: 0.496919\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's auc: 0.521381\tvalid_1's auc: 0.502413\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512994\tvalid_1's auc: 0.49667\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.502898\tvalid_1's auc: 0.50417\n",
      "var_39\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.558368\tvalid_1's auc: 0.540272\n",
      "Early stopping, best iteration is:\n",
      "[341]\ttraining's auc: 0.554475\tvalid_1's auc: 0.542106\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54494\tvalid_1's auc: 0.543102\n",
      "Early stopping, best iteration is:\n",
      "[565]\ttraining's auc: 0.544676\tvalid_1's auc: 0.544218\n",
      "var_40\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534077\tvalid_1's auc: 0.506768\n",
      "Early stopping, best iteration is:\n",
      "[871]\ttraining's auc: 0.533527\tvalid_1's auc: 0.507422\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.508781\tvalid_1's auc: 0.50245\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttraining's auc: 0.506775\tvalid_1's auc: 0.505504\n",
      "var_41\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532885\tvalid_1's auc: 0.497044\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.513057\tvalid_1's auc: 0.510504\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510844\tvalid_1's auc: 0.506329\n",
      "Early stopping, best iteration is:\n",
      "[195]\ttraining's auc: 0.509211\tvalid_1's auc: 0.508981\n",
      "var_42\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.545497\tvalid_1's auc: 0.51285\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttraining's auc: 0.531914\tvalid_1's auc: 0.520608\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527498\tvalid_1's auc: 0.519938\n",
      "Early stopping, best iteration is:\n",
      "[90]\ttraining's auc: 0.52506\tvalid_1's auc: 0.522936\n",
      "var_43\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.5594\tvalid_1's auc: 0.538949\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's auc: 0.545728\tvalid_1's auc: 0.542539\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54209\tvalid_1's auc: 0.542274\n",
      "Early stopping, best iteration is:\n",
      "[441]\ttraining's auc: 0.541644\tvalid_1's auc: 0.547289\n",
      "var_44\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.541172\tvalid_1's auc: 0.509818\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.524847\tvalid_1's auc: 0.517455\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.520512\tvalid_1's auc: 0.520274\n",
      "Early stopping, best iteration is:\n",
      "[723]\ttraining's auc: 0.520626\tvalid_1's auc: 0.520751\n",
      "var_45\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534891\tvalid_1's auc: 0.495889\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttraining's auc: 0.521731\tvalid_1's auc: 0.503935\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510094\tvalid_1's auc: 0.502145\n",
      "Early stopping, best iteration is:\n",
      "[80]\ttraining's auc: 0.507344\tvalid_1's auc: 0.507856\n",
      "var_46\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53848\tvalid_1's auc: 0.499772\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.51685\tvalid_1's auc: 0.503538\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510232\tvalid_1's auc: 0.496764\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's auc: 0.504597\tvalid_1's auc: 0.507534\n",
      "var_47\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54506\tvalid_1's auc: 0.519867\n",
      "Early stopping, best iteration is:\n",
      "[69]\ttraining's auc: 0.536589\tvalid_1's auc: 0.531147\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.531923\tvalid_1's auc: 0.530958\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttraining's auc: 0.530923\tvalid_1's auc: 0.532033\n",
      "var_48\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.545723\tvalid_1's auc: 0.520574\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's auc: 0.529312\tvalid_1's auc: 0.52628\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529108\tvalid_1's auc: 0.526321\n",
      "Early stopping, best iteration is:\n",
      "[100]\ttraining's auc: 0.527467\tvalid_1's auc: 0.527975\n",
      "var_49\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537068\tvalid_1's auc: 0.510361\n",
      "Early stopping, best iteration is:\n",
      "[270]\ttraining's auc: 0.530191\tvalid_1's auc: 0.514163\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517707\tvalid_1's auc: 0.510071\n",
      "[2000]\ttraining's auc: 0.517704\tvalid_1's auc: 0.51101\n",
      "Early stopping, best iteration is:\n",
      "[1349]\ttraining's auc: 0.517782\tvalid_1's auc: 0.509971\n",
      "var_50\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54572\tvalid_1's auc: 0.517617\n",
      "Early stopping, best iteration is:\n",
      "[207]\ttraining's auc: 0.538044\tvalid_1's auc: 0.519784\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.528261\tvalid_1's auc: 0.51425\n",
      "Early stopping, best iteration is:\n",
      "[543]\ttraining's auc: 0.52827\tvalid_1's auc: 0.514466\n",
      "var_51\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54353\tvalid_1's auc: 0.523226\n",
      "Early stopping, best iteration is:\n",
      "[739]\ttraining's auc: 0.542201\tvalid_1's auc: 0.523974\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529008\tvalid_1's auc: 0.520122\n",
      "Early stopping, best iteration is:\n",
      "[282]\ttraining's auc: 0.527508\tvalid_1's auc: 0.522685\n",
      "var_52\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.570322\tvalid_1's auc: 0.542948\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.55841\tvalid_1's auc: 0.552224\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.556792\tvalid_1's auc: 0.551108\n",
      "Early stopping, best iteration is:\n",
      "[528]\ttraining's auc: 0.556845\tvalid_1's auc: 0.551844\n",
      "var_53\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537246\tvalid_1's auc: 0.511851\n",
      "Early stopping, best iteration is:\n",
      "[100]\ttraining's auc: 0.527113\tvalid_1's auc: 0.516605\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.519007\tvalid_1's auc: 0.514553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[390]\ttraining's auc: 0.519108\tvalid_1's auc: 0.515019\n",
      "var_54\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537707\tvalid_1's auc: 0.508864\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's auc: 0.523779\tvalid_1's auc: 0.510609\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.518159\tvalid_1's auc: 0.508909\n",
      "Early stopping, best iteration is:\n",
      "[99]\ttraining's auc: 0.51735\tvalid_1's auc: 0.510207\n",
      "var_55\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546159\tvalid_1's auc: 0.530289\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.530257\tvalid_1's auc: 0.537511\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527897\tvalid_1's auc: 0.533014\n",
      "Early stopping, best iteration is:\n",
      "[377]\ttraining's auc: 0.528124\tvalid_1's auc: 0.534126\n",
      "var_56\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535647\tvalid_1's auc: 0.508842\n",
      "Early stopping, best iteration is:\n",
      "[105]\ttraining's auc: 0.525022\tvalid_1's auc: 0.514125\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517797\tvalid_1's auc: 0.509594\n",
      "Early stopping, best iteration is:\n",
      "[173]\ttraining's auc: 0.516862\tvalid_1's auc: 0.51112\n",
      "var_57\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540703\tvalid_1's auc: 0.521913\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttraining's auc: 0.530448\tvalid_1's auc: 0.526732\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526103\tvalid_1's auc: 0.528251\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's auc: 0.522973\tvalid_1's auc: 0.531759\n",
      "var_58\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53488\tvalid_1's auc: 0.50003\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttraining's auc: 0.523324\tvalid_1's auc: 0.508162\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510897\tvalid_1's auc: 0.501762\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's auc: 0.502764\tvalid_1's auc: 0.512897\n",
      "var_59\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533149\tvalid_1's auc: 0.509028\n",
      "Early stopping, best iteration is:\n",
      "[279]\ttraining's auc: 0.526521\tvalid_1's auc: 0.518434\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511464\tvalid_1's auc: 0.505919\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttraining's auc: 0.508882\tvalid_1's auc: 0.510072\n",
      "var_60\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534575\tvalid_1's auc: 0.501987\n",
      "Early stopping, best iteration is:\n",
      "[375]\ttraining's auc: 0.52965\tvalid_1's auc: 0.505919\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512179\tvalid_1's auc: 0.500681\n",
      "Early stopping, best iteration is:\n",
      "[488]\ttraining's auc: 0.512436\tvalid_1's auc: 0.501065\n",
      "var_61\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53747\tvalid_1's auc: 0.505651\n",
      "Early stopping, best iteration is:\n",
      "[225]\ttraining's auc: 0.529765\tvalid_1's auc: 0.507954\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516838\tvalid_1's auc: 0.505154\n",
      "Early stopping, best iteration is:\n",
      "[313]\ttraining's auc: 0.517069\tvalid_1's auc: 0.504676\n",
      "var_62\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.539075\tvalid_1's auc: 0.504065\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's auc: 0.520158\tvalid_1's auc: 0.512177\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515627\tvalid_1's auc: 0.509179\n",
      "Early stopping, best iteration is:\n",
      "[95]\ttraining's auc: 0.513822\tvalid_1's auc: 0.512497\n",
      "var_63\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537542\tvalid_1's auc: 0.504053\n",
      "Early stopping, best iteration is:\n",
      "[501]\ttraining's auc: 0.533793\tvalid_1's auc: 0.506745\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516378\tvalid_1's auc: 0.5021\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.501881\tvalid_1's auc: 0.504595\n",
      "var_64\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535759\tvalid_1's auc: 0.502497\n",
      "Early stopping, best iteration is:\n",
      "[320]\ttraining's auc: 0.52873\tvalid_1's auc: 0.510538\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514277\tvalid_1's auc: 0.51003\n",
      "Early stopping, best iteration is:\n",
      "[792]\ttraining's auc: 0.514305\tvalid_1's auc: 0.509558\n",
      "var_65\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.539332\tvalid_1's auc: 0.518648\n",
      "Early stopping, best iteration is:\n",
      "[280]\ttraining's auc: 0.531545\tvalid_1's auc: 0.526477\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.5217\tvalid_1's auc: 0.525711\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's auc: 0.520966\tvalid_1's auc: 0.527726\n",
      "var_66\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.553853\tvalid_1's auc: 0.538746\n",
      "Early stopping, best iteration is:\n",
      "[276]\ttraining's auc: 0.548714\tvalid_1's auc: 0.542903\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540562\tvalid_1's auc: 0.543616\n",
      "Early stopping, best iteration is:\n",
      "[279]\ttraining's auc: 0.538823\tvalid_1's auc: 0.545147\n",
      "var_67\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536087\tvalid_1's auc: 0.506257\n",
      "Early stopping, best iteration is:\n",
      "[479]\ttraining's auc: 0.531995\tvalid_1's auc: 0.508372\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51566\tvalid_1's auc: 0.507158\n",
      "Early stopping, best iteration is:\n",
      "[645]\ttraining's auc: 0.515759\tvalid_1's auc: 0.509178\n",
      "var_68\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533973\tvalid_1's auc: 0.501471\n",
      "Early stopping, best iteration is:\n",
      "[590]\ttraining's auc: 0.531156\tvalid_1's auc: 0.506275\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510821\tvalid_1's auc: 0.496864\n",
      "Early stopping, best iteration is:\n",
      "[88]\ttraining's auc: 0.509842\tvalid_1's auc: 0.50513\n",
      "var_69\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54306\tvalid_1's auc: 0.520724\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's auc: 0.526437\tvalid_1's auc: 0.525495\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527872\tvalid_1's auc: 0.519299\n",
      "Early stopping, best iteration is:\n",
      "[307]\ttraining's auc: 0.528035\tvalid_1's auc: 0.520419\n",
      "var_70\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.547025\tvalid_1's auc: 0.526533\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttraining's auc: 0.53493\tvalid_1's auc: 0.52797\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532015\tvalid_1's auc: 0.52917\n",
      "Early stopping, best iteration is:\n",
      "[275]\ttraining's auc: 0.531467\tvalid_1's auc: 0.531612\n",
      "var_71\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538071\tvalid_1's auc: 0.492927\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.516773\tvalid_1's auc: 0.504762\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514453\tvalid_1's auc: 0.504899\n",
      "Early stopping, best iteration is:\n",
      "[343]\ttraining's auc: 0.512948\tvalid_1's auc: 0.509027\n",
      "var_72\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's auc: 0.537193\tvalid_1's auc: 0.49521\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's auc: 0.508061\tvalid_1's auc: 0.503773\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51077\tvalid_1's auc: 0.500368\n",
      "Early stopping, best iteration is:\n",
      "[388]\ttraining's auc: 0.509932\tvalid_1's auc: 0.50443\n",
      "var_73\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.539957\tvalid_1's auc: 0.512249\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's auc: 0.52699\tvalid_1's auc: 0.518266\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.520206\tvalid_1's auc: 0.513962\n",
      "Early stopping, best iteration is:\n",
      "[255]\ttraining's auc: 0.520645\tvalid_1's auc: 0.516702\n",
      "var_74\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.552077\tvalid_1's auc: 0.523126\n",
      "Early stopping, best iteration is:\n",
      "[28]\ttraining's auc: 0.540506\tvalid_1's auc: 0.528494\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537105\tvalid_1's auc: 0.52877\n",
      "Early stopping, best iteration is:\n",
      "[348]\ttraining's auc: 0.537198\tvalid_1's auc: 0.528774\n",
      "var_75\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.56927\tvalid_1's auc: 0.548309\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttraining's auc: 0.558916\tvalid_1's auc: 0.556103\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.55508\tvalid_1's auc: 0.549877\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's auc: 0.552741\tvalid_1's auc: 0.553311\n",
      "var_76\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540142\tvalid_1's auc: 0.511588\n",
      "Early stopping, best iteration is:\n",
      "[95]\ttraining's auc: 0.529149\tvalid_1's auc: 0.519416\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517444\tvalid_1's auc: 0.51565\n",
      "Early stopping, best iteration is:\n",
      "[331]\ttraining's auc: 0.517573\tvalid_1's auc: 0.518653\n",
      "var_77\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.558256\tvalid_1's auc: 0.543785\n",
      "Early stopping, best iteration is:\n",
      "[648]\ttraining's auc: 0.556625\tvalid_1's auc: 0.545166\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543324\tvalid_1's auc: 0.542645\n",
      "Early stopping, best iteration is:\n",
      "[530]\ttraining's auc: 0.543414\tvalid_1's auc: 0.543203\n",
      "var_78\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534745\tvalid_1's auc: 0.506397\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.507667\tvalid_1's auc: 0.511578\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.509926\tvalid_1's auc: 0.506747\n",
      "Early stopping, best iteration is:\n",
      "[389]\ttraining's auc: 0.50938\tvalid_1's auc: 0.509929\n",
      "var_79\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.568086\tvalid_1's auc: 0.5519\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's auc: 0.555786\tvalid_1's auc: 0.561321\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.550556\tvalid_1's auc: 0.553365\n",
      "Early stopping, best iteration is:\n",
      "[224]\ttraining's auc: 0.549568\tvalid_1's auc: 0.555633\n",
      "var_80\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.585923\tvalid_1's auc: 0.569747\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.568571\tvalid_1's auc: 0.57282\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.570159\tvalid_1's auc: 0.568894\n",
      "Early stopping, best iteration is:\n",
      "[444]\ttraining's auc: 0.569767\tvalid_1's auc: 0.569428\n",
      "var_81\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540828\tvalid_1's auc: 0.514313\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's auc: 0.5263\tvalid_1's auc: 0.523658\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.521878\tvalid_1's auc: 0.523697\n",
      "Early stopping, best iteration is:\n",
      "[282]\ttraining's auc: 0.521657\tvalid_1's auc: 0.524235\n",
      "var_82\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.541348\tvalid_1's auc: 0.514499\n",
      "Early stopping, best iteration is:\n",
      "[315]\ttraining's auc: 0.535745\tvalid_1's auc: 0.51826\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.520066\tvalid_1's auc: 0.507716\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttraining's auc: 0.517274\tvalid_1's auc: 0.515155\n",
      "var_83\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535468\tvalid_1's auc: 0.506815\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.514295\tvalid_1's auc: 0.511137\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513078\tvalid_1's auc: 0.508387\n",
      "Early stopping, best iteration is:\n",
      "[124]\ttraining's auc: 0.511778\tvalid_1's auc: 0.515763\n",
      "var_84\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.539479\tvalid_1's auc: 0.513983\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's auc: 0.527028\tvalid_1's auc: 0.522626\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52388\tvalid_1's auc: 0.515036\n",
      "Early stopping, best iteration is:\n",
      "[391]\ttraining's auc: 0.523472\tvalid_1's auc: 0.517126\n",
      "var_85\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.550915\tvalid_1's auc: 0.537174\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttraining's auc: 0.543104\tvalid_1's auc: 0.540163\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533506\tvalid_1's auc: 0.536309\n",
      "Early stopping, best iteration is:\n",
      "[234]\ttraining's auc: 0.532455\tvalid_1's auc: 0.538384\n",
      "var_86\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.549715\tvalid_1's auc: 0.530452\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's auc: 0.536892\tvalid_1's auc: 0.538363\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534332\tvalid_1's auc: 0.540122\n",
      "[2000]\ttraining's auc: 0.534163\tvalid_1's auc: 0.541732\n",
      "Early stopping, best iteration is:\n",
      "[1053]\ttraining's auc: 0.534351\tvalid_1's auc: 0.540194\n",
      "var_87\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.542558\tvalid_1's auc: 0.509221\n",
      "[2000]\ttraining's auc: 0.544616\tvalid_1's auc: 0.509141\n",
      "[3000]\ttraining's auc: 0.54498\tvalid_1's auc: 0.50866\n",
      "Early stopping, best iteration is:\n",
      "[2074]\ttraining's auc: 0.544678\tvalid_1's auc: 0.510077\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523864\tvalid_1's auc: 0.510103\n",
      "Early stopping, best iteration is:\n",
      "[127]\ttraining's auc: 0.522412\tvalid_1's auc: 0.511248\n",
      "var_88\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.549975\tvalid_1's auc: 0.525893\n",
      "Early stopping, best iteration is:\n",
      "[140]\ttraining's auc: 0.541299\tvalid_1's auc: 0.531407\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534324\tvalid_1's auc: 0.531929\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's auc: 0.532285\tvalid_1's auc: 0.534227\n",
      "var_89\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54187\tvalid_1's auc: 0.528838\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's auc: 0.532509\tvalid_1's auc: 0.537292\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527149\tvalid_1's auc: 0.53089\n",
      "Early stopping, best iteration is:\n",
      "[279]\ttraining's auc: 0.527179\tvalid_1's auc: 0.530675\n",
      "var_90\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's auc: 0.553118\tvalid_1's auc: 0.526292\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's auc: 0.540509\tvalid_1's auc: 0.534949\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536381\tvalid_1's auc: 0.533781\n",
      "Early stopping, best iteration is:\n",
      "[184]\ttraining's auc: 0.536604\tvalid_1's auc: 0.5338\n",
      "var_91\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.558372\tvalid_1's auc: 0.539953\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's auc: 0.548717\tvalid_1's auc: 0.541861\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.544213\tvalid_1's auc: 0.537305\n",
      "Early stopping, best iteration is:\n",
      "[111]\ttraining's auc: 0.542469\tvalid_1's auc: 0.540901\n",
      "var_92\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.550072\tvalid_1's auc: 0.528142\n",
      "Early stopping, best iteration is:\n",
      "[294]\ttraining's auc: 0.543947\tvalid_1's auc: 0.530221\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532494\tvalid_1's auc: 0.528507\n",
      "Early stopping, best iteration is:\n",
      "[745]\ttraining's auc: 0.532401\tvalid_1's auc: 0.530175\n",
      "var_93\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.558428\tvalid_1's auc: 0.5403\n",
      "Early stopping, best iteration is:\n",
      "[225]\ttraining's auc: 0.55203\tvalid_1's auc: 0.544951\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540504\tvalid_1's auc: 0.542524\n",
      "Early stopping, best iteration is:\n",
      "[823]\ttraining's auc: 0.540584\tvalid_1's auc: 0.544108\n",
      "var_94\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.551477\tvalid_1's auc: 0.536882\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's auc: 0.540607\tvalid_1's auc: 0.540201\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534379\tvalid_1's auc: 0.535586\n",
      "Early stopping, best iteration is:\n",
      "[313]\ttraining's auc: 0.534623\tvalid_1's auc: 0.536203\n",
      "var_95\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.531675\tvalid_1's auc: 0.492138\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.513479\tvalid_1's auc: 0.506232\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.506549\tvalid_1's auc: 0.502424\n",
      "Early stopping, best iteration is:\n",
      "[126]\ttraining's auc: 0.506045\tvalid_1's auc: 0.504761\n",
      "var_96\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536674\tvalid_1's auc: 0.512796\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's auc: 0.524878\tvalid_1's auc: 0.517324\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51645\tvalid_1's auc: 0.514856\n",
      "Early stopping, best iteration is:\n",
      "[122]\ttraining's auc: 0.516254\tvalid_1's auc: 0.516166\n",
      "var_97\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535597\tvalid_1's auc: 0.5066\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's auc: 0.515641\tvalid_1's auc: 0.511163\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.508768\tvalid_1's auc: 0.508627\n",
      "Early stopping, best iteration is:\n",
      "[295]\ttraining's auc: 0.508472\tvalid_1's auc: 0.510987\n",
      "var_98\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.564927\tvalid_1's auc: 0.552022\n",
      "Early stopping, best iteration is:\n",
      "[146]\ttraining's auc: 0.559688\tvalid_1's auc: 0.553176\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.552097\tvalid_1's auc: 0.551632\n",
      "Early stopping, best iteration is:\n",
      "[241]\ttraining's auc: 0.551005\tvalid_1's auc: 0.552788\n",
      "var_99\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532239\tvalid_1's auc: 0.505489\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttraining's auc: 0.509214\tvalid_1's auc: 0.506056\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510461\tvalid_1's auc: 0.49992\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.50156\tvalid_1's auc: 0.505195\n",
      "var_100\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534137\tvalid_1's auc: 0.514585\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.51651\tvalid_1's auc: 0.519173\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513227\tvalid_1's auc: 0.505762\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's auc: 0.509856\tvalid_1's auc: 0.515843\n",
      "var_101\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538708\tvalid_1's auc: 0.510411\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's auc: 0.528245\tvalid_1's auc: 0.513374\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.516455\tvalid_1's auc: 0.510316\n",
      "Early stopping, best iteration is:\n",
      "[176]\ttraining's auc: 0.516084\tvalid_1's auc: 0.513627\n",
      "var_102\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532447\tvalid_1's auc: 0.504104\n",
      "[2000]\ttraining's auc: 0.534177\tvalid_1's auc: 0.502205\n",
      "Early stopping, best iteration is:\n",
      "[1130]\ttraining's auc: 0.532699\tvalid_1's auc: 0.504867\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.508702\tvalid_1's auc: 0.504912\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's auc: 0.505229\tvalid_1's auc: 0.507905\n",
      "var_103\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.5427\tvalid_1's auc: 0.519665\n",
      "Early stopping, best iteration is:\n",
      "[336]\ttraining's auc: 0.537183\tvalid_1's auc: 0.520856\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527879\tvalid_1's auc: 0.520064\n",
      "Early stopping, best iteration is:\n",
      "[199]\ttraining's auc: 0.527324\tvalid_1's auc: 0.520853\n",
      "var_104\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.542217\tvalid_1's auc: 0.523549\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's auc: 0.525403\tvalid_1's auc: 0.527629\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.524817\tvalid_1's auc: 0.523444\n",
      "Early stopping, best iteration is:\n",
      "[558]\ttraining's auc: 0.524853\tvalid_1's auc: 0.522695\n",
      "var_105\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546459\tvalid_1's auc: 0.521815\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's auc: 0.536692\tvalid_1's auc: 0.528855\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532408\tvalid_1's auc: 0.528248\n",
      "Early stopping, best iteration is:\n",
      "[305]\ttraining's auc: 0.532502\tvalid_1's auc: 0.528525\n",
      "var_106\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.551259\tvalid_1's auc: 0.532291\n",
      "Early stopping, best iteration is:\n",
      "[165]\ttraining's auc: 0.545152\tvalid_1's auc: 0.533676\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538618\tvalid_1's auc: 0.529858\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's auc: 0.536361\tvalid_1's auc: 0.533197\n",
      "var_107\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.562247\tvalid_1's auc: 0.541726\n",
      "Early stopping, best iteration is:\n",
      "[672]\ttraining's auc: 0.560362\tvalid_1's auc: 0.545711\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53939\tvalid_1's auc: 0.536951\n",
      "[2000]\ttraining's auc: 0.539268\tvalid_1's auc: 0.538696\n",
      "Early stopping, best iteration is:\n",
      "[1266]\ttraining's auc: 0.539425\tvalid_1's auc: 0.53765\n",
      "var_108\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.561221\tvalid_1's auc: 0.539768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.545445\tvalid_1's auc: 0.547517\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543495\tvalid_1's auc: 0.546129\n",
      "Early stopping, best iteration is:\n",
      "[90]\ttraining's auc: 0.542383\tvalid_1's auc: 0.548439\n",
      "var_109\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.570957\tvalid_1's auc: 0.549019\n",
      "Early stopping, best iteration is:\n",
      "[176]\ttraining's auc: 0.565072\tvalid_1's auc: 0.553334\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.558925\tvalid_1's auc: 0.55324\n",
      "Early stopping, best iteration is:\n",
      "[605]\ttraining's auc: 0.558735\tvalid_1's auc: 0.554013\n",
      "var_110\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.545113\tvalid_1's auc: 0.519199\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's auc: 0.531767\tvalid_1's auc: 0.523723\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.525247\tvalid_1's auc: 0.517613\n",
      "Early stopping, best iteration is:\n",
      "[172]\ttraining's auc: 0.52451\tvalid_1's auc: 0.521072\n",
      "var_111\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.547737\tvalid_1's auc: 0.522459\n",
      "Early stopping, best iteration is:\n",
      "[130]\ttraining's auc: 0.538762\tvalid_1's auc: 0.526803\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529587\tvalid_1's auc: 0.527395\n",
      "Early stopping, best iteration is:\n",
      "[442]\ttraining's auc: 0.529502\tvalid_1's auc: 0.52819\n",
      "var_112\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535223\tvalid_1's auc: 0.513838\n",
      "Early stopping, best iteration is:\n",
      "[471]\ttraining's auc: 0.532057\tvalid_1's auc: 0.516411\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513647\tvalid_1's auc: 0.511179\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's auc: 0.512211\tvalid_1's auc: 0.513445\n",
      "var_113\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.541154\tvalid_1's auc: 0.514697\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttraining's auc: 0.531753\tvalid_1's auc: 0.525456\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523318\tvalid_1's auc: 0.526283\n",
      "Early stopping, best iteration is:\n",
      "[436]\ttraining's auc: 0.523386\tvalid_1's auc: 0.525658\n",
      "var_114\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.559125\tvalid_1's auc: 0.540079\n",
      "Early stopping, best iteration is:\n",
      "[69]\ttraining's auc: 0.552807\tvalid_1's auc: 0.543242\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54661\tvalid_1's auc: 0.541182\n",
      "Early stopping, best iteration is:\n",
      "[308]\ttraining's auc: 0.546941\tvalid_1's auc: 0.541439\n",
      "var_115\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.542212\tvalid_1's auc: 0.512978\n",
      "Early stopping, best iteration is:\n",
      "[240]\ttraining's auc: 0.535474\tvalid_1's auc: 0.516091\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52562\tvalid_1's auc: 0.516109\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's auc: 0.524854\tvalid_1's auc: 0.51732\n",
      "var_116\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535886\tvalid_1's auc: 0.501926\n",
      "Early stopping, best iteration is:\n",
      "[95]\ttraining's auc: 0.526626\tvalid_1's auc: 0.507826\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511821\tvalid_1's auc: 0.505678\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's auc: 0.508074\tvalid_1's auc: 0.508181\n",
      "var_117\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.553637\tvalid_1's auc: 0.532902\n",
      "Early stopping, best iteration is:\n",
      "[585]\ttraining's auc: 0.550949\tvalid_1's auc: 0.535058\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.539425\tvalid_1's auc: 0.528882\n",
      "[2000]\ttraining's auc: 0.539439\tvalid_1's auc: 0.529005\n",
      "Early stopping, best iteration is:\n",
      "[1100]\ttraining's auc: 0.539473\tvalid_1's auc: 0.528914\n",
      "var_118\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.544634\tvalid_1's auc: 0.521624\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's auc: 0.537076\tvalid_1's auc: 0.526761\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529218\tvalid_1's auc: 0.527789\n",
      "Early stopping, best iteration is:\n",
      "[360]\ttraining's auc: 0.52902\tvalid_1's auc: 0.528809\n",
      "var_119\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534536\tvalid_1's auc: 0.508488\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's auc: 0.519205\tvalid_1's auc: 0.513525\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514279\tvalid_1's auc: 0.510868\n",
      "Early stopping, best iteration is:\n",
      "[187]\ttraining's auc: 0.51404\tvalid_1's auc: 0.513457\n",
      "var_120\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.551768\tvalid_1's auc: 0.53653\n",
      "Early stopping, best iteration is:\n",
      "[32]\ttraining's auc: 0.540158\tvalid_1's auc: 0.545579\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535741\tvalid_1's auc: 0.544503\n",
      "Early stopping, best iteration is:\n",
      "[463]\ttraining's auc: 0.535839\tvalid_1's auc: 0.545453\n",
      "var_121\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.550634\tvalid_1's auc: 0.528915\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's auc: 0.542728\tvalid_1's auc: 0.531532\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540274\tvalid_1's auc: 0.531941\n",
      "Early stopping, best iteration is:\n",
      "[200]\ttraining's auc: 0.539932\tvalid_1's auc: 0.534444\n",
      "var_122\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.551833\tvalid_1's auc: 0.542224\n",
      "Early stopping, best iteration is:\n",
      "[205]\ttraining's auc: 0.545227\tvalid_1's auc: 0.546702\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53861\tvalid_1's auc: 0.547049\n",
      "Early stopping, best iteration is:\n",
      "[846]\ttraining's auc: 0.53865\tvalid_1's auc: 0.546786\n",
      "var_123\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535329\tvalid_1's auc: 0.501916\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.507035\tvalid_1's auc: 0.5085\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.507091\tvalid_1's auc: 0.50557\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttraining's auc: 0.506905\tvalid_1's auc: 0.509963\n",
      "var_124\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.542387\tvalid_1's auc: 0.523133\n",
      "Early stopping, best iteration is:\n",
      "[970]\ttraining's auc: 0.542216\tvalid_1's auc: 0.523939\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.525729\tvalid_1's auc: 0.522698\n",
      "Early stopping, best iteration is:\n",
      "[83]\ttraining's auc: 0.524076\tvalid_1's auc: 0.525444\n",
      "var_125\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534826\tvalid_1's auc: 0.490958\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.508832\tvalid_1's auc: 0.499111\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510045\tvalid_1's auc: 0.489502\n",
      "Early stopping, best iteration is:\n",
      "[4]\ttraining's auc: 0.501091\tvalid_1's auc: 0.502423\n",
      "var_126\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.553397\tvalid_1's auc: 0.536952\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's auc: 0.53991\tvalid_1's auc: 0.545876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536551\tvalid_1's auc: 0.54367\n",
      "Early stopping, best iteration is:\n",
      "[650]\ttraining's auc: 0.536677\tvalid_1's auc: 0.541469\n",
      "var_127\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543552\tvalid_1's auc: 0.511887\n",
      "Early stopping, best iteration is:\n",
      "[233]\ttraining's auc: 0.535779\tvalid_1's auc: 0.51853\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.524929\tvalid_1's auc: 0.517343\n",
      "Early stopping, best iteration is:\n",
      "[408]\ttraining's auc: 0.524668\tvalid_1's auc: 0.520853\n",
      "var_128\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.5367\tvalid_1's auc: 0.500664\n",
      "[2000]\ttraining's auc: 0.538509\tvalid_1's auc: 0.502772\n",
      "[3000]\ttraining's auc: 0.538962\tvalid_1's auc: 0.501872\n",
      "Early stopping, best iteration is:\n",
      "[2519]\ttraining's auc: 0.538816\tvalid_1's auc: 0.503318\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510151\tvalid_1's auc: 0.493656\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's auc: 0.503181\tvalid_1's auc: 0.500332\n",
      "var_129\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.544972\tvalid_1's auc: 0.528801\n",
      "Early stopping, best iteration is:\n",
      "[790]\ttraining's auc: 0.544241\tvalid_1's auc: 0.529208\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526936\tvalid_1's auc: 0.526963\n",
      "Early stopping, best iteration is:\n",
      "[888]\ttraining's auc: 0.527074\tvalid_1's auc: 0.52863\n",
      "var_130\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.545077\tvalid_1's auc: 0.524989\n",
      "Early stopping, best iteration is:\n",
      "[925]\ttraining's auc: 0.544709\tvalid_1's auc: 0.525233\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532653\tvalid_1's auc: 0.520982\n",
      "Early stopping, best iteration is:\n",
      "[378]\ttraining's auc: 0.532093\tvalid_1's auc: 0.523004\n",
      "var_131\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.544451\tvalid_1's auc: 0.513453\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's auc: 0.526915\tvalid_1's auc: 0.527778\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523944\tvalid_1's auc: 0.524063\n",
      "Early stopping, best iteration is:\n",
      "[359]\ttraining's auc: 0.523662\tvalid_1's auc: 0.525637\n",
      "var_132\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.560231\tvalid_1's auc: 0.546822\n",
      "Early stopping, best iteration is:\n",
      "[814]\ttraining's auc: 0.559413\tvalid_1's auc: 0.5479\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.547853\tvalid_1's auc: 0.543595\n",
      "Early stopping, best iteration is:\n",
      "[531]\ttraining's auc: 0.548006\tvalid_1's auc: 0.544205\n",
      "var_133\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538142\tvalid_1's auc: 0.50912\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's auc: 0.523342\tvalid_1's auc: 0.515068\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517702\tvalid_1's auc: 0.510288\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's auc: 0.516322\tvalid_1's auc: 0.510817\n",
      "var_134\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.542068\tvalid_1's auc: 0.520684\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.526173\tvalid_1's auc: 0.53144\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527263\tvalid_1's auc: 0.525443\n",
      "Early stopping, best iteration is:\n",
      "[467]\ttraining's auc: 0.526895\tvalid_1's auc: 0.528152\n",
      "var_135\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.532922\tvalid_1's auc: 0.502687\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's auc: 0.511511\tvalid_1's auc: 0.506935\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.508491\tvalid_1's auc: 0.507231\n",
      "Early stopping, best iteration is:\n",
      "[319]\ttraining's auc: 0.50713\tvalid_1's auc: 0.508022\n",
      "var_136\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54358\tvalid_1's auc: 0.519534\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttraining's auc: 0.534507\tvalid_1's auc: 0.527343\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527017\tvalid_1's auc: 0.524817\n",
      "Early stopping, best iteration is:\n",
      "[217]\ttraining's auc: 0.527274\tvalid_1's auc: 0.524726\n",
      "var_137\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537412\tvalid_1's auc: 0.517499\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's auc: 0.518737\tvalid_1's auc: 0.520376\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517295\tvalid_1's auc: 0.51693\n",
      "Early stopping, best iteration is:\n",
      "[123]\ttraining's auc: 0.516478\tvalid_1's auc: 0.518274\n",
      "var_138\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.582817\tvalid_1's auc: 0.560334\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's auc: 0.569466\tvalid_1's auc: 0.565272\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.5686\tvalid_1's auc: 0.565258\n",
      "Early stopping, best iteration is:\n",
      "[90]\ttraining's auc: 0.565441\tvalid_1's auc: 0.566439\n",
      "var_139\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538604\tvalid_1's auc: 0.492602\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.513827\tvalid_1's auc: 0.505563\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517621\tvalid_1's auc: 0.501346\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttraining's auc: 0.514413\tvalid_1's auc: 0.508459\n",
      "var_140\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54323\tvalid_1's auc: 0.522527\n",
      "Early stopping, best iteration is:\n",
      "[153]\ttraining's auc: 0.53571\tvalid_1's auc: 0.531601\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526439\tvalid_1's auc: 0.5289\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's auc: 0.524609\tvalid_1's auc: 0.533383\n",
      "var_141\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537482\tvalid_1's auc: 0.511005\n",
      "Early stopping, best iteration is:\n",
      "[145]\ttraining's auc: 0.52823\tvalid_1's auc: 0.516443\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.518783\tvalid_1's auc: 0.5118\n",
      "Early stopping, best iteration is:\n",
      "[81]\ttraining's auc: 0.517276\tvalid_1's auc: 0.515404\n",
      "var_142\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536555\tvalid_1's auc: 0.498617\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.51094\tvalid_1's auc: 0.503278\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511605\tvalid_1's auc: 0.511169\n",
      "Early stopping, best iteration is:\n",
      "[502]\ttraining's auc: 0.511703\tvalid_1's auc: 0.508734\n",
      "var_143\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538583\tvalid_1's auc: 0.519658\n",
      "Early stopping, best iteration is:\n",
      "[132]\ttraining's auc: 0.530321\tvalid_1's auc: 0.525639\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.521105\tvalid_1's auc: 0.527434\n",
      "Early stopping, best iteration is:\n",
      "[463]\ttraining's auc: 0.521003\tvalid_1's auc: 0.527737\n",
      "var_144\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54897\tvalid_1's auc: 0.515344\n",
      "Early stopping, best iteration is:\n",
      "[290]\ttraining's auc: 0.541463\tvalid_1's auc: 0.519016\n",
      "Training until validation scores don't improve for 1000 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's auc: 0.529428\tvalid_1's auc: 0.516495\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's auc: 0.526848\tvalid_1's auc: 0.520831\n",
      "var_145\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.568328\tvalid_1's auc: 0.553091\n",
      "Early stopping, best iteration is:\n",
      "[130]\ttraining's auc: 0.561288\tvalid_1's auc: 0.55806\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.556151\tvalid_1's auc: 0.555458\n",
      "Early stopping, best iteration is:\n",
      "[168]\ttraining's auc: 0.55485\tvalid_1's auc: 0.558337\n",
      "var_146\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.551969\tvalid_1's auc: 0.526274\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's auc: 0.541142\tvalid_1's auc: 0.53413\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.5374\tvalid_1's auc: 0.535252\n",
      "Early stopping, best iteration is:\n",
      "[148]\ttraining's auc: 0.536643\tvalid_1's auc: 0.536652\n",
      "var_147\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.564569\tvalid_1's auc: 0.549829\n",
      "Early stopping, best iteration is:\n",
      "[212]\ttraining's auc: 0.558981\tvalid_1's auc: 0.554597\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.549545\tvalid_1's auc: 0.55468\n",
      "Early stopping, best iteration is:\n",
      "[250]\ttraining's auc: 0.548577\tvalid_1's auc: 0.556351\n",
      "var_148\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.556279\tvalid_1's auc: 0.548816\n",
      "Early stopping, best iteration is:\n",
      "[534]\ttraining's auc: 0.554529\tvalid_1's auc: 0.550292\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.544724\tvalid_1's auc: 0.549079\n",
      "[2000]\ttraining's auc: 0.544884\tvalid_1's auc: 0.549938\n",
      "Early stopping, best iteration is:\n",
      "[1350]\ttraining's auc: 0.544731\tvalid_1's auc: 0.55052\n",
      "var_149\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.545637\tvalid_1's auc: 0.5209\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's auc: 0.534781\tvalid_1's auc: 0.524976\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527419\tvalid_1's auc: 0.52152\n",
      "Early stopping, best iteration is:\n",
      "[453]\ttraining's auc: 0.527445\tvalid_1's auc: 0.521283\n",
      "var_150\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543889\tvalid_1's auc: 0.515246\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's auc: 0.530078\tvalid_1's auc: 0.519915\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.527095\tvalid_1's auc: 0.520175\n",
      "Early stopping, best iteration is:\n",
      "[585]\ttraining's auc: 0.527251\tvalid_1's auc: 0.518645\n",
      "var_151\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53326\tvalid_1's auc: 0.504649\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's auc: 0.51729\tvalid_1's auc: 0.504703\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513649\tvalid_1's auc: 0.497881\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's auc: 0.50903\tvalid_1's auc: 0.5006\n",
      "var_152\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534509\tvalid_1's auc: 0.504447\n",
      "Early stopping, best iteration is:\n",
      "[100]\ttraining's auc: 0.524848\tvalid_1's auc: 0.508633\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.511654\tvalid_1's auc: 0.503197\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's auc: 0.501981\tvalid_1's auc: 0.510116\n",
      "var_153\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.555147\tvalid_1's auc: 0.5409\n",
      "Early stopping, best iteration is:\n",
      "[293]\ttraining's auc: 0.550343\tvalid_1's auc: 0.545377\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.541823\tvalid_1's auc: 0.544338\n",
      "Early stopping, best iteration is:\n",
      "[354]\ttraining's auc: 0.54167\tvalid_1's auc: 0.545634\n",
      "var_154\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.548781\tvalid_1's auc: 0.524484\n",
      "Early stopping, best iteration is:\n",
      "[190]\ttraining's auc: 0.540784\tvalid_1's auc: 0.534488\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529544\tvalid_1's auc: 0.536\n",
      "Early stopping, best iteration is:\n",
      "[537]\ttraining's auc: 0.529854\tvalid_1's auc: 0.537615\n",
      "var_155\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.539992\tvalid_1's auc: 0.504279\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's auc: 0.528342\tvalid_1's auc: 0.511979\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52155\tvalid_1's auc: 0.513566\n",
      "Early stopping, best iteration is:\n",
      "[510]\ttraining's auc: 0.521406\tvalid_1's auc: 0.514148\n",
      "var_156\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.547518\tvalid_1's auc: 0.523678\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.528029\tvalid_1's auc: 0.532543\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.529183\tvalid_1's auc: 0.530425\n",
      "Early stopping, best iteration is:\n",
      "[124]\ttraining's auc: 0.52778\tvalid_1's auc: 0.536287\n",
      "var_157\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533759\tvalid_1's auc: 0.498659\n",
      "Early stopping, best iteration is:\n",
      "[607]\ttraining's auc: 0.531556\tvalid_1's auc: 0.502246\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.507854\tvalid_1's auc: 0.501078\n",
      "Early stopping, best iteration is:\n",
      "[582]\ttraining's auc: 0.507571\tvalid_1's auc: 0.503826\n",
      "var_158\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534688\tvalid_1's auc: 0.513157\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's auc: 0.525643\tvalid_1's auc: 0.516636\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515327\tvalid_1's auc: 0.518319\n",
      "Early stopping, best iteration is:\n",
      "[572]\ttraining's auc: 0.515448\tvalid_1's auc: 0.517178\n",
      "var_159\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535285\tvalid_1's auc: 0.503428\n",
      "Early stopping, best iteration is:\n",
      "[241]\ttraining's auc: 0.527661\tvalid_1's auc: 0.508481\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513066\tvalid_1's auc: 0.505972\n",
      "Early stopping, best iteration is:\n",
      "[32]\ttraining's auc: 0.504666\tvalid_1's auc: 0.508551\n",
      "var_160\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536662\tvalid_1's auc: 0.501038\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's auc: 0.521961\tvalid_1's auc: 0.506267\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.510909\tvalid_1's auc: 0.499541\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's auc: 0.504825\tvalid_1's auc: 0.507504\n",
      "var_161\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.547872\tvalid_1's auc: 0.524291\n",
      "Early stopping, best iteration is:\n",
      "[205]\ttraining's auc: 0.539184\tvalid_1's auc: 0.531346\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52992\tvalid_1's auc: 0.528979\n",
      "Early stopping, best iteration is:\n",
      "[626]\ttraining's auc: 0.529971\tvalid_1's auc: 0.529389\n",
      "var_162\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.548335\tvalid_1's auc: 0.532149\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttraining's auc: 0.541295\tvalid_1's auc: 0.535446\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.530919\tvalid_1's auc: 0.529927\n",
      "Early stopping, best iteration is:\n",
      "[600]\ttraining's auc: 0.530878\tvalid_1's auc: 0.531922\n",
      "var_163\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.554323\tvalid_1's auc: 0.535126\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's auc: 0.541527\tvalid_1's auc: 0.544804\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537633\tvalid_1's auc: 0.539666\n",
      "Early stopping, best iteration is:\n",
      "[318]\ttraining's auc: 0.537007\tvalid_1's auc: 0.540321\n",
      "var_164\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.567565\tvalid_1's auc: 0.538825\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's auc: 0.554511\tvalid_1's auc: 0.542324\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.553998\tvalid_1's auc: 0.537863\n",
      "Early stopping, best iteration is:\n",
      "[467]\ttraining's auc: 0.553921\tvalid_1's auc: 0.54092\n",
      "var_165\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.568931\tvalid_1's auc: 0.544823\n",
      "Early stopping, best iteration is:\n",
      "[499]\ttraining's auc: 0.564662\tvalid_1's auc: 0.545985\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.554164\tvalid_1's auc: 0.543498\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttraining's auc: 0.551513\tvalid_1's auc: 0.544019\n",
      "var_166\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546941\tvalid_1's auc: 0.517976\n",
      "Early stopping, best iteration is:\n",
      "[125]\ttraining's auc: 0.53956\tvalid_1's auc: 0.527167\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.530438\tvalid_1's auc: 0.520111\n",
      "Early stopping, best iteration is:\n",
      "[96]\ttraining's auc: 0.528892\tvalid_1's auc: 0.522699\n",
      "var_167\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540203\tvalid_1's auc: 0.513364\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.513017\tvalid_1's auc: 0.515701\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515786\tvalid_1's auc: 0.509859\n",
      "Early stopping, best iteration is:\n",
      "[434]\ttraining's auc: 0.515992\tvalid_1's auc: 0.509606\n",
      "var_168\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.560629\tvalid_1's auc: 0.542329\n",
      "Early stopping, best iteration is:\n",
      "[146]\ttraining's auc: 0.553506\tvalid_1's auc: 0.548886\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546785\tvalid_1's auc: 0.545948\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttraining's auc: 0.545955\tvalid_1's auc: 0.546938\n",
      "var_169\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.556823\tvalid_1's auc: 0.536044\n",
      "Early stopping, best iteration is:\n",
      "[330]\ttraining's auc: 0.549693\tvalid_1's auc: 0.539263\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.540538\tvalid_1's auc: 0.538385\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's auc: 0.538217\tvalid_1's auc: 0.53946\n",
      "var_170\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536463\tvalid_1's auc: 0.510607\n",
      "Early stopping, best iteration is:\n",
      "[330]\ttraining's auc: 0.530138\tvalid_1's auc: 0.514987\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51671\tvalid_1's auc: 0.512171\n",
      "Early stopping, best iteration is:\n",
      "[93]\ttraining's auc: 0.515441\tvalid_1's auc: 0.515206\n",
      "var_171\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.548431\tvalid_1's auc: 0.537057\n",
      "Early stopping, best iteration is:\n",
      "[85]\ttraining's auc: 0.538398\tvalid_1's auc: 0.542626\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.534707\tvalid_1's auc: 0.537748\n",
      "Early stopping, best iteration is:\n",
      "[168]\ttraining's auc: 0.534051\tvalid_1's auc: 0.5401\n",
      "var_172\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.552884\tvalid_1's auc: 0.533248\n",
      "Early stopping, best iteration is:\n",
      "[196]\ttraining's auc: 0.54691\tvalid_1's auc: 0.537584\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.541101\tvalid_1's auc: 0.537748\n",
      "Early stopping, best iteration is:\n",
      "[126]\ttraining's auc: 0.539221\tvalid_1's auc: 0.538363\n",
      "var_173\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.56486\tvalid_1's auc: 0.552473\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's auc: 0.556717\tvalid_1's auc: 0.554544\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.552877\tvalid_1's auc: 0.548723\n",
      "Early stopping, best iteration is:\n",
      "[201]\ttraining's auc: 0.552398\tvalid_1's auc: 0.549449\n",
      "var_174\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543264\tvalid_1's auc: 0.511816\n",
      "[2000]\ttraining's auc: 0.545702\tvalid_1's auc: 0.510853\n",
      "Early stopping, best iteration is:\n",
      "[1351]\ttraining's auc: 0.54485\tvalid_1's auc: 0.513143\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.522327\tvalid_1's auc: 0.515216\n",
      "Early stopping, best iteration is:\n",
      "[149]\ttraining's auc: 0.52093\tvalid_1's auc: 0.517793\n",
      "var_175\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53525\tvalid_1's auc: 0.50313\n",
      "[2000]\ttraining's auc: 0.537531\tvalid_1's auc: 0.503613\n",
      "[3000]\ttraining's auc: 0.538198\tvalid_1's auc: 0.504387\n",
      "[4000]\ttraining's auc: 0.538281\tvalid_1's auc: 0.503329\n",
      "Early stopping, best iteration is:\n",
      "[3246]\ttraining's auc: 0.538148\tvalid_1's auc: 0.505533\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.51218\tvalid_1's auc: 0.493292\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's auc: 0.500066\tvalid_1's auc: 0.506141\n",
      "var_176\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.553654\tvalid_1's auc: 0.532045\n",
      "Early stopping, best iteration is:\n",
      "[115]\ttraining's auc: 0.543851\tvalid_1's auc: 0.535612\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538109\tvalid_1's auc: 0.530436\n",
      "Early stopping, best iteration is:\n",
      "[420]\ttraining's auc: 0.537548\tvalid_1's auc: 0.531733\n",
      "var_177\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538919\tvalid_1's auc: 0.512723\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's auc: 0.529404\tvalid_1's auc: 0.520668\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.519856\tvalid_1's auc: 0.516159\n",
      "Early stopping, best iteration is:\n",
      "[631]\ttraining's auc: 0.520015\tvalid_1's auc: 0.51586\n",
      "var_178\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.558578\tvalid_1's auc: 0.53295\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's auc: 0.550119\tvalid_1's auc: 0.540798\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.542126\tvalid_1's auc: 0.541063\n",
      "Early stopping, best iteration is:\n",
      "[889]\ttraining's auc: 0.542111\tvalid_1's auc: 0.541817\n",
      "var_179\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546757\tvalid_1's auc: 0.52805\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's auc: 0.533521\tvalid_1's auc: 0.535667\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526594\tvalid_1's auc: 0.529321\n",
      "Early stopping, best iteration is:\n",
      "[641]\ttraining's auc: 0.526241\tvalid_1's auc: 0.531422\n",
      "var_180\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54004\tvalid_1's auc: 0.509971\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.518748\tvalid_1's auc: 0.516157\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513636\tvalid_1's auc: 0.513861\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttraining's auc: 0.510843\tvalid_1's auc: 0.516293\n",
      "var_181\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538276\tvalid_1's auc: 0.497836\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's auc: 0.513183\tvalid_1's auc: 0.501619\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512112\tvalid_1's auc: 0.500184\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's auc: 0.503255\tvalid_1's auc: 0.501643\n",
      "var_182\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533274\tvalid_1's auc: 0.497453\n",
      "Early stopping, best iteration is:\n",
      "[65]\ttraining's auc: 0.519389\tvalid_1's auc: 0.502913\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.509144\tvalid_1's auc: 0.496661\n",
      "Early stopping, best iteration is:\n",
      "[52]\ttraining's auc: 0.505221\tvalid_1's auc: 0.499079\n",
      "var_183\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.557775\tvalid_1's auc: 0.545266\n",
      "Early stopping, best iteration is:\n",
      "[73]\ttraining's auc: 0.550602\tvalid_1's auc: 0.547019\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543873\tvalid_1's auc: 0.543903\n",
      "Early stopping, best iteration is:\n",
      "[555]\ttraining's auc: 0.544074\tvalid_1's auc: 0.543897\n",
      "var_184\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.531221\tvalid_1's auc: 0.502917\n",
      "[2000]\ttraining's auc: 0.532864\tvalid_1's auc: 0.503669\n",
      "[3000]\ttraining's auc: 0.53332\tvalid_1's auc: 0.50297\n",
      "Early stopping, best iteration is:\n",
      "[2504]\ttraining's auc: 0.533075\tvalid_1's auc: 0.505061\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.506311\tvalid_1's auc: 0.496127\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's auc: 0.502482\tvalid_1's auc: 0.504335\n",
      "var_185\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546524\tvalid_1's auc: 0.521629\n",
      "Early stopping, best iteration is:\n",
      "[219]\ttraining's auc: 0.539798\tvalid_1's auc: 0.522476\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.531928\tvalid_1's auc: 0.521688\n",
      "Early stopping, best iteration is:\n",
      "[233]\ttraining's auc: 0.531019\tvalid_1's auc: 0.522317\n",
      "var_186\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536759\tvalid_1's auc: 0.509619\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's auc: 0.516757\tvalid_1's auc: 0.513105\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.515982\tvalid_1's auc: 0.508548\n",
      "[2000]\ttraining's auc: 0.516054\tvalid_1's auc: 0.506737\n",
      "Early stopping, best iteration is:\n",
      "[1012]\ttraining's auc: 0.515938\tvalid_1's auc: 0.509056\n",
      "var_187\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.550626\tvalid_1's auc: 0.516589\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttraining's auc: 0.538044\tvalid_1's auc: 0.523271\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533381\tvalid_1's auc: 0.522893\n",
      "Early stopping, best iteration is:\n",
      "[297]\ttraining's auc: 0.533423\tvalid_1's auc: 0.523644\n",
      "var_188\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.535832\tvalid_1's auc: 0.499791\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's auc: 0.518534\tvalid_1's auc: 0.506324\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.512536\tvalid_1's auc: 0.504612\n",
      "Early stopping, best iteration is:\n",
      "[538]\ttraining's auc: 0.512623\tvalid_1's auc: 0.506177\n",
      "var_189\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.563368\tvalid_1's auc: 0.543009\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's auc: 0.550286\tvalid_1's auc: 0.551523\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.551267\tvalid_1's auc: 0.549891\n",
      "Early stopping, best iteration is:\n",
      "[266]\ttraining's auc: 0.548424\tvalid_1's auc: 0.553416\n",
      "var_190\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.559146\tvalid_1's auc: 0.529966\n",
      "Early stopping, best iteration is:\n",
      "[201]\ttraining's auc: 0.551547\tvalid_1's auc: 0.536457\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.54411\tvalid_1's auc: 0.539033\n",
      "Early stopping, best iteration is:\n",
      "[77]\ttraining's auc: 0.542529\tvalid_1's auc: 0.540711\n",
      "var_191\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.554406\tvalid_1's auc: 0.538829\n",
      "Early stopping, best iteration is:\n",
      "[264]\ttraining's auc: 0.550529\tvalid_1's auc: 0.541619\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.544484\tvalid_1's auc: 0.541683\n",
      "Early stopping, best iteration is:\n",
      "[502]\ttraining's auc: 0.543955\tvalid_1's auc: 0.542876\n",
      "var_192\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.538829\tvalid_1's auc: 0.508951\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttraining's auc: 0.530085\tvalid_1's auc: 0.512623\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.517805\tvalid_1's auc: 0.508104\n",
      "Early stopping, best iteration is:\n",
      "[492]\ttraining's auc: 0.51771\tvalid_1's auc: 0.510681\n",
      "var_193\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.536734\tvalid_1's auc: 0.526199\n",
      "Early stopping, best iteration is:\n",
      "[520]\ttraining's auc: 0.533924\tvalid_1's auc: 0.531597\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.519286\tvalid_1's auc: 0.527526\n",
      "Early stopping, best iteration is:\n",
      "[707]\ttraining's auc: 0.51904\tvalid_1's auc: 0.528687\n",
      "var_194\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543504\tvalid_1's auc: 0.52241\n",
      "Early stopping, best iteration is:\n",
      "[517]\ttraining's auc: 0.540235\tvalid_1's auc: 0.526654\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.526687\tvalid_1's auc: 0.523801\n",
      "Early stopping, best iteration is:\n",
      "[232]\ttraining's auc: 0.526719\tvalid_1's auc: 0.524161\n",
      "var_195\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.542245\tvalid_1's auc: 0.51343\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's auc: 0.530907\tvalid_1's auc: 0.51676\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.52315\tvalid_1's auc: 0.513862\n",
      "Early stopping, best iteration is:\n",
      "[300]\ttraining's auc: 0.523103\tvalid_1's auc: 0.514857\n",
      "var_196\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.547061\tvalid_1's auc: 0.525308\n",
      "Early stopping, best iteration is:\n",
      "[179]\ttraining's auc: 0.538994\tvalid_1's auc: 0.531171\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.53031\tvalid_1's auc: 0.528685\n",
      "Early stopping, best iteration is:\n",
      "[235]\ttraining's auc: 0.52991\tvalid_1's auc: 0.530666\n",
      "var_197\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.560828\tvalid_1's auc: 0.546256\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.546575\tvalid_1's auc: 0.547266\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.548339\tvalid_1's auc: 0.542056\n",
      "Early stopping, best iteration is:\n",
      "[176]\ttraining's auc: 0.546024\tvalid_1's auc: 0.546568\n",
      "var_198\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.541699\tvalid_1's auc: 0.525607\n",
      "Early stopping, best iteration is:\n",
      "[88]\ttraining's auc: 0.53199\tvalid_1's auc: 0.532526\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.523592\tvalid_1's auc: 0.531252\n",
      "Early stopping, best iteration is:\n",
      "[295]\ttraining's auc: 0.523238\tvalid_1's auc: 0.534019\n",
      "var_199\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for col in list(X1.columns):\n",
    "    bins = np.linspace(-6.0,6.0,40)\n",
    "    X = pd.get_dummies(pd.cut(X1[col].values, bins))\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X1[col].reshape(-1,1), y, test_size=0.2, stratify=y)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "    np.random.seed(123)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "    print(col)\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_cols1 = ['var_2', 'var_7', 'var_12', 'var_17', 'var_27', 'var_28', 'var_29', 'var_30', 'var_33', 'var_34', 'var_35',\n",
    "'var_37', 'var_39', 'var_40', 'var_43', 'var_44', 'var_45', 'var_46', 'var_47', 'var_48', 'var_49', 'var_58', \n",
    "'var_59', 'var_63', 'var_66', 'var_67', 'var_68', \n",
    "'var_71', 'var_72', 'var_73', 'var_82', 'var_84', 'var_87', 'var_88', 'var_89', 'var_102', 'var_103', \n",
    "'var_110', 'var_112', 'var_114', 'var_116', 'var_117', 'var_119', 'var_122', 'var_123', 'var_124', 'var_125', 'var_126',  \n",
    "'var_128',  'var_136', 'var_139', 'var_140', 'var_141', 'var_143', 'var_144', 'var_145',\n",
    "'var_146', 'var_147', 'var_148', 'var_149', 'var_153', 'var_154', 'var_155', 'var_156', 'var_157', 'var_158', 'var_159',\n",
    "'var_160', 'var_161', 'var_170', 'var_171', 'var_173', 'var_175', 'var_176', 'var_179', 'var_181', 'var_182', 'var_188',\n",
    "'var_190', 'var_191', 'var_192']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_2\n",
      "-------------\n",
      "var_7\n",
      "-------------\n",
      "var_12\n",
      "-------------\n",
      "var_17\n",
      "-------------\n",
      "var_27\n",
      "-------------\n",
      "var_28\n",
      "-------------\n",
      "var_29\n",
      "-------------\n",
      "var_30\n",
      "-------------\n",
      "var_33\n",
      "-------------\n",
      "var_34\n",
      "-------------\n",
      "var_35\n",
      "-------------\n",
      "var_37\n",
      "-------------\n",
      "var_39\n",
      "-------------\n",
      "var_40\n",
      "-------------\n",
      "var_43\n",
      "-------------\n",
      "var_44\n",
      "-------------\n",
      "var_45\n",
      "-------------\n",
      "var_46\n",
      "-------------\n",
      "var_47\n",
      "-------------\n",
      "var_48\n",
      "-------------\n",
      "var_49\n",
      "-------------\n",
      "var_58\n",
      "-------------\n",
      "var_59\n",
      "-------------\n",
      "var_63\n",
      "-------------\n",
      "var_66\n",
      "-------------\n",
      "var_67\n",
      "-------------\n",
      "var_68\n",
      "-------------\n",
      "var_71\n",
      "-------------\n",
      "var_72\n",
      "-------------\n",
      "var_73\n",
      "-------------\n",
      "var_82\n",
      "-------------\n",
      "var_84\n",
      "-------------\n",
      "var_87\n",
      "-------------\n",
      "var_88\n",
      "-------------\n",
      "var_89\n",
      "-------------\n",
      "var_102\n",
      "-------------\n",
      "var_103\n",
      "-------------\n",
      "var_110\n",
      "-------------\n",
      "var_112\n",
      "-------------\n",
      "var_114\n",
      "-------------\n",
      "var_116\n",
      "-------------\n",
      "var_117\n",
      "-------------\n",
      "var_119\n",
      "-------------\n",
      "var_122\n",
      "-------------\n",
      "var_123\n",
      "-------------\n",
      "var_124\n",
      "-------------\n",
      "var_125\n",
      "-------------\n",
      "var_126\n",
      "-------------\n",
      "var_128\n",
      "-------------\n",
      "var_136\n",
      "-------------\n",
      "var_139\n",
      "-------------\n",
      "var_140\n",
      "-------------\n",
      "var_141\n",
      "-------------\n",
      "var_143\n",
      "-------------\n",
      "var_144\n",
      "-------------\n",
      "var_145\n",
      "-------------\n",
      "var_146\n",
      "-------------\n",
      "var_147\n",
      "-------------\n",
      "var_148\n",
      "-------------\n",
      "var_149\n",
      "-------------\n",
      "var_153\n",
      "-------------\n",
      "var_154\n",
      "-------------\n",
      "var_155\n",
      "-------------\n",
      "var_156\n",
      "-------------\n",
      "var_157\n",
      "-------------\n",
      "var_158\n",
      "-------------\n",
      "var_159\n",
      "-------------\n",
      "var_160\n",
      "-------------\n",
      "var_161\n",
      "-------------\n",
      "var_170\n",
      "-------------\n",
      "var_171\n",
      "-------------\n",
      "var_173\n",
      "-------------\n",
      "var_175\n",
      "-------------\n",
      "var_176\n",
      "-------------\n",
      "var_179\n",
      "-------------\n",
      "var_181\n",
      "-------------\n",
      "var_182\n",
      "-------------\n",
      "var_188\n",
      "-------------\n",
      "var_190\n",
      "-------------\n",
      "var_191\n",
      "-------------\n",
      "var_192\n",
      "-------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200000, 3278), (200000, 3278))"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "bins = np.linspace(-6.0,6.0,40)\n",
    "import gc\n",
    "for col in bin_cols1:\n",
    "    print(col)\n",
    "    X3_col_bins = pd.get_dummies(pd.cut(X3[col], bins))\n",
    "    X4_col_bins = pd.get_dummies(pd.cut(X4[col], bins))\n",
    "\n",
    "    X3_col_bins = pd.DataFrame(X3_col_bins)\n",
    "    cols = [col + '_' + str(i) for i in range(X3_col_bins.shape[1])]\n",
    "    X3_col_bins.columns = cols\n",
    "\n",
    "    X4_col_bins = pd.DataFrame(X4_col_bins)\n",
    "    X4_col_bins.columns = cols\n",
    " \n",
    "    X3 = pd.concat([X3, X3_col_bins], axis=1)\n",
    "    X4 = pd.concat([X4, X4_col_bins], axis=1)\n",
    "\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X4 = X4.drop(col, axis=1)\n",
    "    \n",
    "    del X3_col_bins, X4_col_bins\n",
    "    gc.collect()\n",
    "    print('-------------')\n",
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sun Mar 17 12:33:19 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.873384\n",
      "[2000]\tvalid_0's auc: 0.880848\n",
      "[3000]\tvalid_0's auc: 0.884705\n",
      "[4000]\tvalid_0's auc: 0.888064\n",
      "[5000]\tvalid_0's auc: 0.889981\n",
      "[6000]\tvalid_0's auc: 0.891921\n",
      "[7000]\tvalid_0's auc: 0.893468\n",
      "[8000]\tvalid_0's auc: 0.894663\n",
      "[9000]\tvalid_0's auc: 0.895793\n",
      "[10000]\tvalid_0's auc: 0.896435\n",
      "[11000]\tvalid_0's auc: 0.897049\n",
      "[12000]\tvalid_0's auc: 0.89733\n",
      "[13000]\tvalid_0's auc: 0.89776\n",
      "[14000]\tvalid_0's auc: 0.898134\n",
      "[15000]\tvalid_0's auc: 0.898375\n",
      "[16000]\tvalid_0's auc: 0.898539\n",
      "[17000]\tvalid_0's auc: 0.898652\n",
      "[18000]\tvalid_0's auc: 0.898596\n",
      "[19000]\tvalid_0's auc: 0.898537\n",
      "[20000]\tvalid_0's auc: 0.898708\n",
      "[21000]\tvalid_0's auc: 0.898839\n",
      "[22000]\tvalid_0's auc: 0.898928\n",
      "[23000]\tvalid_0's auc: 0.898741\n",
      "[24000]\tvalid_0's auc: 0.898555\n",
      "Early stopping, best iteration is:\n",
      "[21459]\tvalid_0's auc: 0.89897\n",
      "Fold 1 started at Sun Mar 17 12:38:03 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.867108\n",
      "[2000]\tvalid_0's auc: 0.875021\n",
      "[3000]\tvalid_0's auc: 0.879458\n",
      "[4000]\tvalid_0's auc: 0.882305\n",
      "[5000]\tvalid_0's auc: 0.885083\n",
      "[6000]\tvalid_0's auc: 0.887325\n",
      "[7000]\tvalid_0's auc: 0.888988\n",
      "[8000]\tvalid_0's auc: 0.890116\n",
      "[9000]\tvalid_0's auc: 0.891154\n",
      "[10000]\tvalid_0's auc: 0.891887\n",
      "[11000]\tvalid_0's auc: 0.892529\n",
      "[12000]\tvalid_0's auc: 0.893178\n",
      "[13000]\tvalid_0's auc: 0.893778\n",
      "[14000]\tvalid_0's auc: 0.894021\n",
      "[15000]\tvalid_0's auc: 0.894398\n",
      "[16000]\tvalid_0's auc: 0.894524\n",
      "[17000]\tvalid_0's auc: 0.894672\n",
      "[18000]\tvalid_0's auc: 0.894727\n",
      "[19000]\tvalid_0's auc: 0.894893\n",
      "[20000]\tvalid_0's auc: 0.894876\n",
      "[21000]\tvalid_0's auc: 0.894968\n",
      "[22000]\tvalid_0's auc: 0.894894\n",
      "[23000]\tvalid_0's auc: 0.894794\n",
      "Early stopping, best iteration is:\n",
      "[20734]\tvalid_0's auc: 0.895066\n",
      "Fold 2 started at Sun Mar 17 13:17:59 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.865497\n",
      "[2000]\tvalid_0's auc: 0.871219\n",
      "[3000]\tvalid_0's auc: 0.875421\n",
      "[4000]\tvalid_0's auc: 0.879083\n",
      "[5000]\tvalid_0's auc: 0.88186\n",
      "[6000]\tvalid_0's auc: 0.883937\n",
      "[7000]\tvalid_0's auc: 0.885672\n",
      "[8000]\tvalid_0's auc: 0.887122\n",
      "[9000]\tvalid_0's auc: 0.888129\n",
      "[10000]\tvalid_0's auc: 0.888768\n",
      "[11000]\tvalid_0's auc: 0.889344\n",
      "[12000]\tvalid_0's auc: 0.89008\n",
      "[13000]\tvalid_0's auc: 0.89047\n",
      "[14000]\tvalid_0's auc: 0.890833\n",
      "[15000]\tvalid_0's auc: 0.891164\n",
      "[16000]\tvalid_0's auc: 0.891394\n",
      "[17000]\tvalid_0's auc: 0.891671\n",
      "[18000]\tvalid_0's auc: 0.891672\n",
      "[19000]\tvalid_0's auc: 0.891642\n",
      "[20000]\tvalid_0's auc: 0.891635\n",
      "[21000]\tvalid_0's auc: 0.891893\n",
      "[22000]\tvalid_0's auc: 0.891855\n",
      "[23000]\tvalid_0's auc: 0.891938\n",
      "[24000]\tvalid_0's auc: 0.89205\n",
      "[25000]\tvalid_0's auc: 0.892032\n",
      "[26000]\tvalid_0's auc: 0.892031\n",
      "[27000]\tvalid_0's auc: 0.891913\n",
      "Early stopping, best iteration is:\n",
      "[24268]\tvalid_0's auc: 0.89218\n",
      "Fold 3 started at Sun Mar 17 14:10:40 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.874995\n",
      "[2000]\tvalid_0's auc: 0.880543\n",
      "[3000]\tvalid_0's auc: 0.884774\n",
      "[4000]\tvalid_0's auc: 0.887895\n",
      "[5000]\tvalid_0's auc: 0.890442\n",
      "[6000]\tvalid_0's auc: 0.892199\n",
      "[7000]\tvalid_0's auc: 0.893726\n",
      "[8000]\tvalid_0's auc: 0.894757\n",
      "[9000]\tvalid_0's auc: 0.895764\n",
      "[10000]\tvalid_0's auc: 0.896134\n",
      "[11000]\tvalid_0's auc: 0.896452\n",
      "[12000]\tvalid_0's auc: 0.896731\n",
      "[13000]\tvalid_0's auc: 0.897147\n",
      "[14000]\tvalid_0's auc: 0.897586\n",
      "[15000]\tvalid_0's auc: 0.897902\n",
      "[16000]\tvalid_0's auc: 0.898037\n",
      "[17000]\tvalid_0's auc: 0.898121\n",
      "[18000]\tvalid_0's auc: 0.898219\n",
      "[19000]\tvalid_0's auc: 0.898101\n",
      "[20000]\tvalid_0's auc: 0.898214\n",
      "[21000]\tvalid_0's auc: 0.898183\n",
      "[22000]\tvalid_0's auc: 0.898065\n",
      "[23000]\tvalid_0's auc: 0.898024\n",
      "Early stopping, best iteration is:\n",
      "[20132]\tvalid_0's auc: 0.898305\n",
      "Fold 4 started at Sun Mar 17 15:04:23 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.872431\n",
      "[2000]\tvalid_0's auc: 0.877732\n",
      "[3000]\tvalid_0's auc: 0.881493\n",
      "[4000]\tvalid_0's auc: 0.883826\n",
      "[5000]\tvalid_0's auc: 0.88613\n",
      "[6000]\tvalid_0's auc: 0.887774\n",
      "[7000]\tvalid_0's auc: 0.889392\n",
      "[8000]\tvalid_0's auc: 0.890541\n",
      "[9000]\tvalid_0's auc: 0.891295\n",
      "[10000]\tvalid_0's auc: 0.891963\n",
      "[11000]\tvalid_0's auc: 0.892406\n",
      "[12000]\tvalid_0's auc: 0.892804\n",
      "[13000]\tvalid_0's auc: 0.893128\n",
      "[14000]\tvalid_0's auc: 0.893398\n",
      "[15000]\tvalid_0's auc: 0.893509\n",
      "[16000]\tvalid_0's auc: 0.893513\n",
      "[17000]\tvalid_0's auc: 0.893647\n",
      "[18000]\tvalid_0's auc: 0.893706\n",
      "[19000]\tvalid_0's auc: 0.893919\n",
      "[20000]\tvalid_0's auc: 0.893925\n",
      "[21000]\tvalid_0's auc: 0.893847\n",
      "[22000]\tvalid_0's auc: 0.893997\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-457-c39f5a9d0a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n\u001b[0;32m----> 5\u001b[0;31m                                          model_type='lgb', plot_feature_importance=False)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-423-c2d9f4412d9d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, averaging, model)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 early_stopping_rounds=3000)\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;31m#             y_pred_valid = model.predict(X_valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m#             y_pred = model.predict(X_test, num_iteration=model.best_iteration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_cols2 = list(set(bin_cols1).intersection(bin_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['var_155', 'var_136', 'var_191', 'var_49', 'var_181', 'var_148', 'var_124', 'var_45', 'var_27', 'var_126', 'var_12', 'var_33', 'var_159', 'var_123', 'var_156', 'var_112', 'var_110', 'var_35', 'var_29', 'var_47', 'var_125', 'var_122', 'var_7', 'var_17', 'var_44', 'var_154', 'var_37', 'var_58', 'var_28', 'var_71', 'var_176', 'var_88', 'var_147', 'var_140', 'var_87']"
      ],
      "text/plain": [
       "['var_155',\n",
       " 'var_136',\n",
       " 'var_191',\n",
       " 'var_49',\n",
       " 'var_181',\n",
       " 'var_148',\n",
       " 'var_124',\n",
       " 'var_45',\n",
       " 'var_27',\n",
       " 'var_126',\n",
       " 'var_12',\n",
       " 'var_33',\n",
       " 'var_159',\n",
       " 'var_123',\n",
       " 'var_156',\n",
       " 'var_112',\n",
       " 'var_110',\n",
       " 'var_35',\n",
       " 'var_29',\n",
       " 'var_47',\n",
       " 'var_125',\n",
       " 'var_122',\n",
       " 'var_7',\n",
       " 'var_17',\n",
       " 'var_44',\n",
       " 'var_154',\n",
       " 'var_37',\n",
       " 'var_58',\n",
       " 'var_28',\n",
       " 'var_71',\n",
       " 'var_176',\n",
       " 'var_88',\n",
       " 'var_147',\n",
       " 'var_140',\n",
       " 'var_87']"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_cols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_155\n",
      "-------------\n",
      "var_136\n",
      "-------------\n",
      "var_191\n",
      "-------------\n",
      "var_49\n",
      "-------------\n",
      "var_181\n",
      "-------------\n",
      "var_148\n",
      "-------------\n",
      "var_124\n",
      "-------------\n",
      "var_45\n",
      "-------------\n",
      "var_27\n",
      "-------------\n",
      "var_126\n",
      "-------------\n",
      "var_12\n",
      "-------------\n",
      "var_33\n",
      "-------------\n",
      "var_159\n",
      "-------------\n",
      "var_123\n",
      "-------------\n",
      "var_156\n",
      "-------------\n",
      "var_112\n",
      "-------------\n",
      "var_110\n",
      "-------------\n",
      "var_35\n",
      "-------------\n",
      "var_29\n",
      "-------------\n",
      "var_47\n",
      "-------------\n",
      "var_125\n",
      "-------------\n",
      "var_122\n",
      "-------------\n",
      "var_7\n",
      "-------------\n",
      "var_17\n",
      "-------------\n",
      "var_44\n",
      "-------------\n",
      "var_154\n",
      "-------------\n",
      "var_37\n",
      "-------------\n",
      "var_58\n",
      "-------------\n",
      "var_28\n",
      "-------------\n",
      "var_71\n",
      "-------------\n",
      "var_176\n",
      "-------------\n",
      "var_88\n",
      "-------------\n",
      "var_147\n",
      "-------------\n",
      "var_140\n",
      "-------------\n",
      "var_87\n",
      "-------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200000, 1530), (200000, 1530))"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "bins = np.linspace(-6.0,6.0,40)\n",
    "import gc\n",
    "for col in bin_cols2:\n",
    "    print(col)\n",
    "    X3_col_bins = pd.get_dummies(pd.cut(X3[col], bins))\n",
    "    X4_col_bins = pd.get_dummies(pd.cut(X4[col], bins))\n",
    "\n",
    "    X3_col_bins = pd.DataFrame(X3_col_bins)\n",
    "    cols = [col + '_' + str(i) for i in range(X3_col_bins.shape[1])]\n",
    "    X3_col_bins.columns = cols\n",
    "\n",
    "    X4_col_bins = pd.DataFrame(X4_col_bins)\n",
    "    X4_col_bins.columns = cols\n",
    " \n",
    "    X3 = pd.concat([X3, X3_col_bins], axis=1)\n",
    "    X4 = pd.concat([X4, X4_col_bins], axis=1)\n",
    "\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X4 = X4.drop(col, axis=1)\n",
    "    \n",
    "    del X3_col_bins, X4_col_bins\n",
    "    gc.collect()\n",
    "    print('-------------')\n",
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sun Mar 17 15:49:21 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.880615\n",
      "[2000]\tvalid_0's auc: 0.888462\n",
      "[3000]\tvalid_0's auc: 0.892053\n",
      "[4000]\tvalid_0's auc: 0.894747\n",
      "[5000]\tvalid_0's auc: 0.896547\n",
      "[6000]\tvalid_0's auc: 0.898042\n",
      "[7000]\tvalid_0's auc: 0.898933\n",
      "[8000]\tvalid_0's auc: 0.899638\n",
      "[9000]\tvalid_0's auc: 0.900195\n",
      "[10000]\tvalid_0's auc: 0.900479\n",
      "[11000]\tvalid_0's auc: 0.900819\n",
      "[12000]\tvalid_0's auc: 0.900811\n",
      "[13000]\tvalid_0's auc: 0.900953\n",
      "[14000]\tvalid_0's auc: 0.901041\n",
      "[15000]\tvalid_0's auc: 0.901067\n",
      "[16000]\tvalid_0's auc: 0.901055\n",
      "[17000]\tvalid_0's auc: 0.901151\n",
      "[18000]\tvalid_0's auc: 0.901279\n",
      "[19000]\tvalid_0's auc: 0.901247\n",
      "[20000]\tvalid_0's auc: 0.901179\n",
      "Early stopping, best iteration is:\n",
      "[17819]\tvalid_0's auc: 0.90133\n",
      "Fold 1 started at Sun Mar 17 16:43:59 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.875479\n",
      "[2000]\tvalid_0's auc: 0.881772\n",
      "[3000]\tvalid_0's auc: 0.886429\n",
      "[4000]\tvalid_0's auc: 0.889398\n",
      "[5000]\tvalid_0's auc: 0.891898\n",
      "[6000]\tvalid_0's auc: 0.893195\n",
      "[7000]\tvalid_0's auc: 0.89441\n",
      "[8000]\tvalid_0's auc: 0.895145\n",
      "[9000]\tvalid_0's auc: 0.895988\n",
      "[10000]\tvalid_0's auc: 0.896512\n",
      "[11000]\tvalid_0's auc: 0.896854\n",
      "[12000]\tvalid_0's auc: 0.897039\n",
      "[13000]\tvalid_0's auc: 0.897332\n",
      "[14000]\tvalid_0's auc: 0.897187\n",
      "[15000]\tvalid_0's auc: 0.897248\n",
      "[16000]\tvalid_0's auc: 0.897348\n",
      "[17000]\tvalid_0's auc: 0.897357\n",
      "[18000]\tvalid_0's auc: 0.897354\n",
      "Early stopping, best iteration is:\n",
      "[15864]\tvalid_0's auc: 0.897431\n",
      "Fold 2 started at Sun Mar 17 17:52:41 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.872037\n",
      "[2000]\tvalid_0's auc: 0.880458\n",
      "[3000]\tvalid_0's auc: 0.885055\n",
      "[4000]\tvalid_0's auc: 0.887257\n",
      "[5000]\tvalid_0's auc: 0.88898\n",
      "[6000]\tvalid_0's auc: 0.890425\n",
      "[7000]\tvalid_0's auc: 0.891797\n",
      "[8000]\tvalid_0's auc: 0.892282\n",
      "[9000]\tvalid_0's auc: 0.892746\n",
      "[10000]\tvalid_0's auc: 0.893248\n",
      "[11000]\tvalid_0's auc: 0.893538\n",
      "[12000]\tvalid_0's auc: 0.89394\n",
      "[13000]\tvalid_0's auc: 0.894207\n",
      "[14000]\tvalid_0's auc: 0.894423\n",
      "[15000]\tvalid_0's auc: 0.894497\n",
      "[16000]\tvalid_0's auc: 0.894405\n",
      "[17000]\tvalid_0's auc: 0.894497\n",
      "[18000]\tvalid_0's auc: 0.894768\n",
      "[19000]\tvalid_0's auc: 0.894778\n",
      "[20000]\tvalid_0's auc: 0.894701\n",
      "[21000]\tvalid_0's auc: 0.894625\n",
      "[22000]\tvalid_0's auc: 0.894509\n",
      "Early stopping, best iteration is:\n",
      "[19326]\tvalid_0's auc: 0.894879\n",
      "Fold 3 started at Sun Mar 17 19:13:01 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.880251\n",
      "[2000]\tvalid_0's auc: 0.888005\n",
      "[3000]\tvalid_0's auc: 0.891352\n",
      "[4000]\tvalid_0's auc: 0.893607\n",
      "[5000]\tvalid_0's auc: 0.895151\n",
      "[6000]\tvalid_0's auc: 0.896348\n",
      "[7000]\tvalid_0's auc: 0.89713\n",
      "[8000]\tvalid_0's auc: 0.897636\n",
      "[9000]\tvalid_0's auc: 0.898114\n",
      "[10000]\tvalid_0's auc: 0.898429\n",
      "[11000]\tvalid_0's auc: 0.898681\n",
      "[12000]\tvalid_0's auc: 0.898763\n",
      "[13000]\tvalid_0's auc: 0.898813\n",
      "[14000]\tvalid_0's auc: 0.898853\n",
      "[15000]\tvalid_0's auc: 0.898985\n",
      "[16000]\tvalid_0's auc: 0.898849\n",
      "[17000]\tvalid_0's auc: 0.898665\n",
      "[18000]\tvalid_0's auc: 0.898822\n",
      "Early stopping, best iteration is:\n",
      "[15093]\tvalid_0's auc: 0.899027\n",
      "Fold 4 started at Sun Mar 17 20:20:42 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.879775\n",
      "[2000]\tvalid_0's auc: 0.886181\n",
      "[3000]\tvalid_0's auc: 0.890231\n",
      "[4000]\tvalid_0's auc: 0.893168\n",
      "[5000]\tvalid_0's auc: 0.894453\n",
      "[6000]\tvalid_0's auc: 0.895833\n",
      "[7000]\tvalid_0's auc: 0.896372\n",
      "[8000]\tvalid_0's auc: 0.897169\n",
      "[9000]\tvalid_0's auc: 0.897525\n",
      "[10000]\tvalid_0's auc: 0.89797\n",
      "[11000]\tvalid_0's auc: 0.898347\n",
      "[12000]\tvalid_0's auc: 0.89838\n",
      "[13000]\tvalid_0's auc: 0.898492\n",
      "[14000]\tvalid_0's auc: 0.898446\n",
      "[15000]\tvalid_0's auc: 0.898346\n",
      "[16000]\tvalid_0's auc: 0.89837\n",
      "Early stopping, best iteration is:\n",
      "[13277]\tvalid_0's auc: 0.898522\n",
      "Fold 5 started at Sun Mar 17 21:17:01 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.880824\n",
      "[2000]\tvalid_0's auc: 0.885439\n",
      "[3000]\tvalid_0's auc: 0.889502\n",
      "[4000]\tvalid_0's auc: 0.892645\n",
      "[5000]\tvalid_0's auc: 0.894209\n",
      "[6000]\tvalid_0's auc: 0.895505\n",
      "[7000]\tvalid_0's auc: 0.896713\n",
      "[8000]\tvalid_0's auc: 0.897596\n",
      "[9000]\tvalid_0's auc: 0.898245\n",
      "[10000]\tvalid_0's auc: 0.898837\n",
      "[11000]\tvalid_0's auc: 0.899184\n",
      "[12000]\tvalid_0's auc: 0.899435\n",
      "[13000]\tvalid_0's auc: 0.899762\n",
      "[14000]\tvalid_0's auc: 0.900013\n",
      "[15000]\tvalid_0's auc: 0.900234\n",
      "[16000]\tvalid_0's auc: 0.900476\n",
      "[17000]\tvalid_0's auc: 0.900479\n",
      "[18000]\tvalid_0's auc: 0.900534\n",
      "[19000]\tvalid_0's auc: 0.900589\n",
      "[20000]\tvalid_0's auc: 0.900669\n",
      "[21000]\tvalid_0's auc: 0.900771\n",
      "[22000]\tvalid_0's auc: 0.900655\n",
      "[23000]\tvalid_0's auc: 0.90062\n",
      "Early stopping, best iteration is:\n",
      "[20756]\tvalid_0's auc: 0.900831\n",
      "Fold 6 started at Sun Mar 17 22:23:14 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.892197\n",
      "[2000]\tvalid_0's auc: 0.899781\n",
      "[3000]\tvalid_0's auc: 0.904221\n",
      "[4000]\tvalid_0's auc: 0.907161\n",
      "[5000]\tvalid_0's auc: 0.90871\n",
      "[6000]\tvalid_0's auc: 0.910255\n",
      "[7000]\tvalid_0's auc: 0.911325\n",
      "[8000]\tvalid_0's auc: 0.911981\n",
      "[9000]\tvalid_0's auc: 0.912672\n",
      "[10000]\tvalid_0's auc: 0.91325\n",
      "[11000]\tvalid_0's auc: 0.913445\n",
      "[12000]\tvalid_0's auc: 0.913695\n",
      "[13000]\tvalid_0's auc: 0.913815\n",
      "[14000]\tvalid_0's auc: 0.913897\n",
      "[15000]\tvalid_0's auc: 0.913897\n",
      "[16000]\tvalid_0's auc: 0.913706\n",
      "[17000]\tvalid_0's auc: 0.913644\n",
      "Early stopping, best iteration is:\n",
      "[14229]\tvalid_0's auc: 0.913975\n",
      "Fold 7 started at Sun Mar 17 23:18:42 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.87602\n",
      "[2000]\tvalid_0's auc: 0.883791\n",
      "[3000]\tvalid_0's auc: 0.887245\n",
      "[4000]\tvalid_0's auc: 0.889506\n",
      "[5000]\tvalid_0's auc: 0.891327\n",
      "[6000]\tvalid_0's auc: 0.89274\n",
      "[7000]\tvalid_0's auc: 0.893504\n",
      "[8000]\tvalid_0's auc: 0.894271\n",
      "[9000]\tvalid_0's auc: 0.894623\n",
      "[10000]\tvalid_0's auc: 0.895079\n",
      "[11000]\tvalid_0's auc: 0.895388\n",
      "[12000]\tvalid_0's auc: 0.895483\n",
      "[13000]\tvalid_0's auc: 0.895677\n",
      "[14000]\tvalid_0's auc: 0.895555\n",
      "[15000]\tvalid_0's auc: 0.895567\n",
      "Early stopping, best iteration is:\n",
      "[12733]\tvalid_0's auc: 0.895768\n",
      "Fold 8 started at Mon Mar 18 00:11:33 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.879655\n",
      "[2000]\tvalid_0's auc: 0.887511\n",
      "[3000]\tvalid_0's auc: 0.891846\n",
      "[4000]\tvalid_0's auc: 0.895023\n",
      "[5000]\tvalid_0's auc: 0.897016\n",
      "[6000]\tvalid_0's auc: 0.898461\n",
      "[7000]\tvalid_0's auc: 0.899865\n",
      "[8000]\tvalid_0's auc: 0.900856\n",
      "[9000]\tvalid_0's auc: 0.901522\n",
      "[10000]\tvalid_0's auc: 0.901859\n",
      "[11000]\tvalid_0's auc: 0.902233\n",
      "[12000]\tvalid_0's auc: 0.902555\n",
      "[13000]\tvalid_0's auc: 0.902828\n",
      "[14000]\tvalid_0's auc: 0.903041\n",
      "[15000]\tvalid_0's auc: 0.903136\n",
      "[16000]\tvalid_0's auc: 0.903239\n",
      "[17000]\tvalid_0's auc: 0.903225\n",
      "[18000]\tvalid_0's auc: 0.903168\n",
      "[19000]\tvalid_0's auc: 0.903201\n",
      "Early stopping, best iteration is:\n",
      "[16323]\tvalid_0's auc: 0.903342\n",
      "Fold 9 started at Mon Mar 18 01:09:45 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88184\n",
      "[2000]\tvalid_0's auc: 0.887606\n",
      "[3000]\tvalid_0's auc: 0.891208\n",
      "[4000]\tvalid_0's auc: 0.894058\n",
      "[5000]\tvalid_0's auc: 0.896504\n",
      "[6000]\tvalid_0's auc: 0.897906\n",
      "[7000]\tvalid_0's auc: 0.899318\n",
      "[8000]\tvalid_0's auc: 0.900226\n",
      "[9000]\tvalid_0's auc: 0.900667\n",
      "[10000]\tvalid_0's auc: 0.901022\n",
      "[11000]\tvalid_0's auc: 0.901361\n",
      "[12000]\tvalid_0's auc: 0.901586\n",
      "[13000]\tvalid_0's auc: 0.901674\n",
      "[14000]\tvalid_0's auc: 0.901829\n",
      "[15000]\tvalid_0's auc: 0.90184\n",
      "[16000]\tvalid_0's auc: 0.901764\n",
      "[17000]\tvalid_0's auc: 0.901637\n",
      "Early stopping, best iteration is:\n",
      "[14891]\tvalid_0's auc: 0.901898\n",
      "Fold 10 started at Mon Mar 18 02:12:19 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.873809\n",
      "[2000]\tvalid_0's auc: 0.881458\n",
      "[3000]\tvalid_0's auc: 0.885282\n",
      "[4000]\tvalid_0's auc: 0.888845\n",
      "[5000]\tvalid_0's auc: 0.890673\n",
      "[6000]\tvalid_0's auc: 0.89212\n",
      "[7000]\tvalid_0's auc: 0.893127\n",
      "[8000]\tvalid_0's auc: 0.893719\n",
      "[9000]\tvalid_0's auc: 0.894326\n",
      "[10000]\tvalid_0's auc: 0.894852\n",
      "[11000]\tvalid_0's auc: 0.895294\n",
      "[12000]\tvalid_0's auc: 0.895712\n",
      "[13000]\tvalid_0's auc: 0.896048\n",
      "[14000]\tvalid_0's auc: 0.896308\n",
      "[15000]\tvalid_0's auc: 0.896618\n",
      "[16000]\tvalid_0's auc: 0.896767\n",
      "[17000]\tvalid_0's auc: 0.896756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18000]\tvalid_0's auc: 0.89686\n",
      "[19000]\tvalid_0's auc: 0.896949\n",
      "[20000]\tvalid_0's auc: 0.896736\n",
      "[21000]\tvalid_0's auc: 0.89645\n",
      "Early stopping, best iteration is:\n",
      "[18957]\tvalid_0's auc: 0.896992\n",
      "Fold 11 started at Mon Mar 18 03:18:51 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.871349\n",
      "[2000]\tvalid_0's auc: 0.879584\n",
      "[3000]\tvalid_0's auc: 0.884354\n",
      "[4000]\tvalid_0's auc: 0.887283\n",
      "[5000]\tvalid_0's auc: 0.889177\n",
      "[6000]\tvalid_0's auc: 0.890487\n",
      "[7000]\tvalid_0's auc: 0.891546\n",
      "[8000]\tvalid_0's auc: 0.892413\n",
      "[9000]\tvalid_0's auc: 0.89309\n",
      "[10000]\tvalid_0's auc: 0.893829\n",
      "[11000]\tvalid_0's auc: 0.894241\n",
      "[12000]\tvalid_0's auc: 0.894391\n",
      "[13000]\tvalid_0's auc: 0.894553\n",
      "[14000]\tvalid_0's auc: 0.894759\n",
      "[15000]\tvalid_0's auc: 0.894907\n",
      "[16000]\tvalid_0's auc: 0.894987\n",
      "[17000]\tvalid_0's auc: 0.895183\n",
      "[18000]\tvalid_0's auc: 0.895196\n",
      "[19000]\tvalid_0's auc: 0.895208\n",
      "[20000]\tvalid_0's auc: 0.895406\n",
      "[21000]\tvalid_0's auc: 0.895358\n",
      "[22000]\tvalid_0's auc: 0.895386\n",
      "[23000]\tvalid_0's auc: 0.895363\n",
      "Early stopping, best iteration is:\n",
      "[20742]\tvalid_0's auc: 0.895528\n",
      "Fold 12 started at Mon Mar 18 04:26:13 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.868597\n",
      "[2000]\tvalid_0's auc: 0.875439\n",
      "[3000]\tvalid_0's auc: 0.879915\n",
      "[4000]\tvalid_0's auc: 0.883257\n",
      "[5000]\tvalid_0's auc: 0.885273\n",
      "[6000]\tvalid_0's auc: 0.886822\n",
      "[7000]\tvalid_0's auc: 0.887648\n",
      "[8000]\tvalid_0's auc: 0.888452\n",
      "[9000]\tvalid_0's auc: 0.889108\n",
      "[10000]\tvalid_0's auc: 0.889688\n",
      "[11000]\tvalid_0's auc: 0.890158\n",
      "[12000]\tvalid_0's auc: 0.890494\n",
      "[13000]\tvalid_0's auc: 0.890618\n",
      "[14000]\tvalid_0's auc: 0.890792\n",
      "[15000]\tvalid_0's auc: 0.89061\n",
      "[16000]\tvalid_0's auc: 0.890608\n",
      "Early stopping, best iteration is:\n",
      "[13911]\tvalid_0's auc: 0.890845\n",
      "Fold 13 started at Mon Mar 18 05:13:37 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.876553\n",
      "[2000]\tvalid_0's auc: 0.88309\n",
      "[3000]\tvalid_0's auc: 0.88713\n",
      "[4000]\tvalid_0's auc: 0.889131\n",
      "[5000]\tvalid_0's auc: 0.890916\n",
      "[6000]\tvalid_0's auc: 0.8922\n",
      "[7000]\tvalid_0's auc: 0.893338\n",
      "[8000]\tvalid_0's auc: 0.894115\n",
      "[9000]\tvalid_0's auc: 0.894496\n",
      "[10000]\tvalid_0's auc: 0.894833\n",
      "[11000]\tvalid_0's auc: 0.895191\n",
      "[12000]\tvalid_0's auc: 0.895478\n",
      "[13000]\tvalid_0's auc: 0.895443\n",
      "[14000]\tvalid_0's auc: 0.895424\n",
      "[15000]\tvalid_0's auc: 0.895502\n",
      "[16000]\tvalid_0's auc: 0.895521\n",
      "[17000]\tvalid_0's auc: 0.895344\n",
      "[18000]\tvalid_0's auc: 0.895384\n",
      "Early stopping, best iteration is:\n",
      "[15570]\tvalid_0's auc: 0.895589\n",
      "Fold 14 started at Mon Mar 18 06:13:21 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.873165\n",
      "[2000]\tvalid_0's auc: 0.882141\n",
      "[3000]\tvalid_0's auc: 0.886204\n",
      "[4000]\tvalid_0's auc: 0.888994\n",
      "[5000]\tvalid_0's auc: 0.89083\n",
      "[6000]\tvalid_0's auc: 0.891973\n",
      "[7000]\tvalid_0's auc: 0.892853\n",
      "[8000]\tvalid_0's auc: 0.893151\n",
      "[9000]\tvalid_0's auc: 0.893789\n",
      "[10000]\tvalid_0's auc: 0.89414\n",
      "[11000]\tvalid_0's auc: 0.894373\n",
      "[12000]\tvalid_0's auc: 0.894353\n",
      "[13000]\tvalid_0's auc: 0.894389\n",
      "[14000]\tvalid_0's auc: 0.894482\n",
      "[15000]\tvalid_0's auc: 0.894407\n",
      "[16000]\tvalid_0's auc: 0.894175\n",
      "[17000]\tvalid_0's auc: 0.894172\n",
      "Early stopping, best iteration is:\n",
      "[14344]\tvalid_0's auc: 0.894545\n",
      "CV mean score: 0.8987, std: 0.0052.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_new_quant_all_bins_1_15_2', oof)\n",
    "np.save('../cache/preds_new_quant_all_bins_1_15_2', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = prediction_lgb\n",
    "sub.to_csv('../submissions/sub12l6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.565138\tvalid_1's auc: 0.542124\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.54683\tvalid_1's auc: 0.543532\n",
      "var_0\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.562427\tvalid_1's auc: 0.540825\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[27]\ttraining's auc: 0.54324\tvalid_1's auc: 0.550605\n",
      "var_1\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.565371\tvalid_1's auc: 0.553036\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[89]\ttraining's auc: 0.551104\tvalid_1's auc: 0.553558\n",
      "var_2\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.540395\tvalid_1's auc: 0.507727\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\ttraining's auc: 0.515086\tvalid_1's auc: 0.509595\n",
      "var_3\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.536887\tvalid_1's auc: 0.504874\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[66]\ttraining's auc: 0.514632\tvalid_1's auc: 0.507833\n",
      "var_4\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.553357\tvalid_1's auc: 0.517757\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.529057\tvalid_1's auc: 0.516539\n",
      "var_5\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.573733\tvalid_1's auc: 0.559242\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.559991\tvalid_1's auc: 0.556554\n",
      "var_6\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.53616\tvalid_1's auc: 0.495171\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[84]\ttraining's auc: 0.511109\tvalid_1's auc: 0.499534\n",
      "var_7\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.54199\tvalid_1's auc: 0.510936\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\ttraining's auc: 0.522417\tvalid_1's auc: 0.51973\n",
      "var_8\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.560742\tvalid_1's auc: 0.532816\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.544609\tvalid_1's auc: 0.540036\n",
      "var_9\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.535003\tvalid_1's auc: 0.501998\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.50919\tvalid_1's auc: 0.4991\n",
      "var_10\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.544482\tvalid_1's auc: 0.515288\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12]\ttraining's auc: 0.519642\tvalid_1's auc: 0.522482\n",
      "var_11\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.575086\tvalid_1's auc: 0.556376\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.561551\tvalid_1's auc: 0.560092\n",
      "var_12\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.570589\tvalid_1's auc: 0.537704\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[43]\ttraining's auc: 0.554491\tvalid_1's auc: 0.542207\n",
      "var_13\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.537962\tvalid_1's auc: 0.500596\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.510862\tvalid_1's auc: 0.505353\n",
      "var_14\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.538417\tvalid_1's auc: 0.513681\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\ttraining's auc: 0.518869\tvalid_1's auc: 0.507638\n",
      "var_15\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.539023\tvalid_1's auc: 0.500276\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[93]\ttraining's auc: 0.513432\tvalid_1's auc: 0.503678\n",
      "var_16\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.534405\tvalid_1's auc: 0.49201\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.508171\tvalid_1's auc: 0.491218\n",
      "var_17\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.556202\tvalid_1's auc: 0.532139\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.538495\tvalid_1's auc: 0.53851\n",
      "var_18\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.540437\tvalid_1's auc: 0.506373\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[71]\ttraining's auc: 0.516385\tvalid_1's auc: 0.505883\n",
      "var_19\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.540906\tvalid_1's auc: 0.513529\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[90]\ttraining's auc: 0.522211\tvalid_1's auc: 0.516178\n",
      "var_20\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.569991\tvalid_1's auc: 0.543226\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[82]\ttraining's auc: 0.556851\tvalid_1's auc: 0.541176\n",
      "var_21\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.567314\tvalid_1's auc: 0.546116\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.552535\tvalid_1's auc: 0.55127\n",
      "var_22\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.544358\tvalid_1's auc: 0.512971\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[27]\ttraining's auc: 0.524521\tvalid_1's auc: 0.520045\n",
      "var_23\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.549106\tvalid_1's auc: 0.518995\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\ttraining's auc: 0.529629\tvalid_1's auc: 0.523188\n",
      "var_24\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.539078\tvalid_1's auc: 0.506962\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[93]\ttraining's auc: 0.513356\tvalid_1's auc: 0.512309\n",
      "var_25\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.568658\tvalid_1's auc: 0.546659\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[81]\ttraining's auc: 0.553634\tvalid_1's auc: 0.551057\n",
      "var_26\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.537245\tvalid_1's auc: 0.503155\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.509783\tvalid_1's auc: 0.500795\n",
      "var_27\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.546382\tvalid_1's auc: 0.511602\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.528027\tvalid_1's auc: 0.516876\n",
      "var_28\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.538748\tvalid_1's auc: 0.492657\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[83]\ttraining's auc: 0.510922\tvalid_1's auc: 0.491108\n",
      "var_29\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.536474\tvalid_1's auc: 0.50587\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[93]\ttraining's auc: 0.509027\tvalid_1's auc: 0.508936\n",
      "var_30\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.547009\tvalid_1's auc: 0.528318\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[76]\ttraining's auc: 0.525757\tvalid_1's auc: 0.534656\n",
      "var_31\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.551433\tvalid_1's auc: 0.516209\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[84]\ttraining's auc: 0.52988\tvalid_1's auc: 0.522631\n",
      "var_32\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.557529\tvalid_1's auc: 0.540579\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.539379\tvalid_1's auc: 0.544971\n",
      "var_33\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.564575\tvalid_1's auc: 0.533266\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.547758\tvalid_1's auc: 0.543849\n",
      "var_34\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.551024\tvalid_1's auc: 0.534687\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[77]\ttraining's auc: 0.533515\tvalid_1's auc: 0.542079\n",
      "var_35\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.553542\tvalid_1's auc: 0.52796\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.539115\tvalid_1's auc: 0.532355\n",
      "var_36\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.540918\tvalid_1's auc: 0.504653\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[8]\ttraining's auc: 0.513422\tvalid_1's auc: 0.509705\n",
      "var_37\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.535772\tvalid_1's auc: 0.504947\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.50989\tvalid_1's auc: 0.508134\n",
      "var_38\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.539922\tvalid_1's auc: 0.496873\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.513246\tvalid_1's auc: 0.496509\n",
      "var_39\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.561279\tvalid_1's auc: 0.538581\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.545244\tvalid_1's auc: 0.543005\n",
      "var_40\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.536961\tvalid_1's auc: 0.506909\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.509107\tvalid_1's auc: 0.501935\n",
      "var_41\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.536404\tvalid_1's auc: 0.497057\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[76]\ttraining's auc: 0.511582\tvalid_1's auc: 0.500015\n",
      "var_42\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.548561\tvalid_1's auc: 0.509876\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.52771\tvalid_1's auc: 0.520083\n",
      "var_43\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.561907\tvalid_1's auc: 0.538777\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[88]\ttraining's auc: 0.542515\tvalid_1's auc: 0.544004\n",
      "var_44\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.544242\tvalid_1's auc: 0.506872\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[98]\ttraining's auc: 0.520883\tvalid_1's auc: 0.520759\n",
      "var_45\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.538261\tvalid_1's auc: 0.496175\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[84]\ttraining's auc: 0.510407\tvalid_1's auc: 0.502862\n",
      "var_46\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.541744\tvalid_1's auc: 0.498357\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\ttraining's auc: 0.510611\tvalid_1's auc: 0.497973\n",
      "var_47\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.547547\tvalid_1's auc: 0.517923\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\ttraining's auc: 0.532098\tvalid_1's auc: 0.530644\n",
      "var_48\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.548729\tvalid_1's auc: 0.517083\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[80]\ttraining's auc: 0.529366\tvalid_1's auc: 0.526111\n",
      "var_49\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.54075\tvalid_1's auc: 0.509749\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.517991\tvalid_1's auc: 0.510354\n",
      "var_50\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.548725\tvalid_1's auc: 0.51912\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[92]\ttraining's auc: 0.528497\tvalid_1's auc: 0.514418\n",
      "var_51\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.546974\tvalid_1's auc: 0.523163\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[92]\ttraining's auc: 0.529343\tvalid_1's auc: 0.519542\n",
      "var_52\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.572824\tvalid_1's auc: 0.538871\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.556915\tvalid_1's auc: 0.552308\n",
      "var_53\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.540322\tvalid_1's auc: 0.509479\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[78]\ttraining's auc: 0.519495\tvalid_1's auc: 0.515052\n",
      "var_54\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.540867\tvalid_1's auc: 0.507452\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.518524\tvalid_1's auc: 0.508486\n",
      "var_55\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.549128\tvalid_1's auc: 0.527743\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[82]\ttraining's auc: 0.528333\tvalid_1's auc: 0.533521\n",
      "var_56\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.538446\tvalid_1's auc: 0.508125\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[91]\ttraining's auc: 0.518053\tvalid_1's auc: 0.508475\n",
      "var_57\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.543614\tvalid_1's auc: 0.518522\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\ttraining's auc: 0.526494\tvalid_1's auc: 0.528827\n",
      "var_58\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.537808\tvalid_1's auc: 0.499398\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\ttraining's auc: 0.5113\tvalid_1's auc: 0.500757\n",
      "var_59\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.536043\tvalid_1's auc: 0.508446\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.511805\tvalid_1's auc: 0.508729\n",
      "var_60\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's auc: 0.537492\tvalid_1's auc: 0.506083\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[87]\ttraining's auc: 0.512653\tvalid_1's auc: 0.501456\n",
      "var_61\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.540101\tvalid_1's auc: 0.505808\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.517231\tvalid_1's auc: 0.504227\n",
      "var_62\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.542227\tvalid_1's auc: 0.504314\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[83]\ttraining's auc: 0.515802\tvalid_1's auc: 0.509419\n",
      "var_63\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.540639\tvalid_1's auc: 0.503819\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's auc: 0.517065\tvalid_1's auc: 0.50325\n",
      "var_64\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.538603\tvalid_1's auc: 0.504061\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.514372\tvalid_1's auc: 0.509469\n",
      "var_65\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.542058\tvalid_1's auc: 0.517012\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[38]\ttraining's auc: 0.521881\tvalid_1's auc: 0.525534\n",
      "var_66\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.556784\tvalid_1's auc: 0.535121\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's auc: 0.541004\tvalid_1's auc: 0.542446\n",
      "var_67\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.539118\tvalid_1's auc: 0.504712\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.516387\tvalid_1's auc: 0.506504\n",
      "var_68\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.537325\tvalid_1's auc: 0.501584\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[71]\ttraining's auc: 0.511\tvalid_1's auc: 0.497137\n",
      "var_69\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.545994\tvalid_1's auc: 0.517973\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.528133\tvalid_1's auc: 0.520843\n",
      "var_70\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.549669\tvalid_1's auc: 0.524103\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\ttraining's auc: 0.532385\tvalid_1's auc: 0.529412\n",
      "var_71\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.540362\tvalid_1's auc: 0.492629\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[85]\ttraining's auc: 0.514828\tvalid_1's auc: 0.505111\n",
      "var_72\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.540217\tvalid_1's auc: 0.49777\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[93]\ttraining's auc: 0.511075\tvalid_1's auc: 0.499005\n",
      "var_73\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.543105\tvalid_1's auc: 0.51265\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[66]\ttraining's auc: 0.520946\tvalid_1's auc: 0.516443\n",
      "var_74\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.554975\tvalid_1's auc: 0.522464\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[87]\ttraining's auc: 0.537278\tvalid_1's auc: 0.528987\n",
      "var_75\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.571088\tvalid_1's auc: 0.545597\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.555491\tvalid_1's auc: 0.549787\n",
      "var_76\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.54334\tvalid_1's auc: 0.510101\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.517955\tvalid_1's auc: 0.517124\n",
      "var_77\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.561299\tvalid_1's auc: 0.542306\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.543712\tvalid_1's auc: 0.542858\n",
      "var_78\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.537853\tvalid_1's auc: 0.507527\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.510264\tvalid_1's auc: 0.506938\n",
      "var_79\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.570539\tvalid_1's auc: 0.548163\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.550693\tvalid_1's auc: 0.552618\n",
      "var_80\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.588099\tvalid_1's auc: 0.568735\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[68]\ttraining's auc: 0.570382\tvalid_1's auc: 0.569003\n",
      "var_81\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.544293\tvalid_1's auc: 0.512918\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[90]\ttraining's auc: 0.522423\tvalid_1's auc: 0.522514\n",
      "var_82\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.543935\tvalid_1's auc: 0.515357\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[74]\ttraining's auc: 0.520585\tvalid_1's auc: 0.509507\n",
      "var_83\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.5387\tvalid_1's auc: 0.504462\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[70]\ttraining's auc: 0.513482\tvalid_1's auc: 0.510774\n",
      "var_84\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.542089\tvalid_1's auc: 0.514197\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.524034\tvalid_1's auc: 0.515863\n",
      "var_85\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.553682\tvalid_1's auc: 0.532059\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[60]\ttraining's auc: 0.533808\tvalid_1's auc: 0.535579\n",
      "var_86\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.552886\tvalid_1's auc: 0.528784\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[71]\ttraining's auc: 0.534764\tvalid_1's auc: 0.540443\n",
      "var_87\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.545301\tvalid_1's auc: 0.508104\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[92]\ttraining's auc: 0.524275\tvalid_1's auc: 0.510241\n",
      "var_88\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.552705\tvalid_1's auc: 0.523715\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\ttraining's auc: 0.534449\tvalid_1's auc: 0.531889\n",
      "var_89\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.544875\tvalid_1's auc: 0.527383\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.527682\tvalid_1's auc: 0.53122\n",
      "var_90\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.55556\tvalid_1's auc: 0.521725\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[45]\ttraining's auc: 0.536923\tvalid_1's auc: 0.533743\n",
      "var_91\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.561636\tvalid_1's auc: 0.538584\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.544583\tvalid_1's auc: 0.537459\n",
      "var_92\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.552954\tvalid_1's auc: 0.526939\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[91]\ttraining's auc: 0.532887\tvalid_1's auc: 0.529878\n",
      "var_93\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.561304\tvalid_1's auc: 0.536437\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[81]\ttraining's auc: 0.54069\tvalid_1's auc: 0.543745\n",
      "var_94\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.554073\tvalid_1's auc: 0.533694\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.534746\tvalid_1's auc: 0.535881\n",
      "var_95\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.535616\tvalid_1's auc: 0.494438\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[80]\ttraining's auc: 0.506963\tvalid_1's auc: 0.500822\n",
      "var_96\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.539929\tvalid_1's auc: 0.513366\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[77]\ttraining's auc: 0.516691\tvalid_1's auc: 0.513764\n",
      "var_97\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.538301\tvalid_1's auc: 0.506052\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[91]\ttraining's auc: 0.509027\tvalid_1's auc: 0.508995\n",
      "var_98\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.567647\tvalid_1's auc: 0.550557\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.552457\tvalid_1's auc: 0.551256\n",
      "var_99\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.535553\tvalid_1's auc: 0.504152\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[87]\ttraining's auc: 0.510638\tvalid_1's auc: 0.501993\n",
      "var_100\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.53748\tvalid_1's auc: 0.512017\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.513305\tvalid_1's auc: 0.507153\n",
      "var_101\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.541554\tvalid_1's auc: 0.509885\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\ttraining's auc: 0.517136\tvalid_1's auc: 0.511679\n",
      "var_102\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.535206\tvalid_1's auc: 0.499933\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[77]\ttraining's auc: 0.509194\tvalid_1's auc: 0.504622\n",
      "var_103\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.545258\tvalid_1's auc: 0.51823\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[58]\ttraining's auc: 0.528274\tvalid_1's auc: 0.519485\n",
      "var_104\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.544729\tvalid_1's auc: 0.523778\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[72]\ttraining's auc: 0.525031\tvalid_1's auc: 0.523394\n",
      "var_105\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.549033\tvalid_1's auc: 0.520631\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[68]\ttraining's auc: 0.532545\tvalid_1's auc: 0.528985\n",
      "var_106\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.554112\tvalid_1's auc: 0.53122\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[90]\ttraining's auc: 0.538836\tvalid_1's auc: 0.530159\n",
      "var_107\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.564935\tvalid_1's auc: 0.538909\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[71]\ttraining's auc: 0.539555\tvalid_1's auc: 0.537487\n",
      "var_108\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.56395\tvalid_1's auc: 0.53654\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's auc: 0.544342\tvalid_1's auc: 0.545863\n",
      "var_109\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.573387\tvalid_1's auc: 0.548028\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.559566\tvalid_1's auc: 0.552998\n",
      "var_110\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.548072\tvalid_1's auc: 0.519651\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[53]\ttraining's auc: 0.525693\tvalid_1's auc: 0.516392\n",
      "var_111\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.550386\tvalid_1's auc: 0.521273\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[77]\ttraining's auc: 0.529707\tvalid_1's auc: 0.527053\n",
      "var_112\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.53803\tvalid_1's auc: 0.512268\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.513911\tvalid_1's auc: 0.510906\n",
      "var_113\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.543491\tvalid_1's auc: 0.514073\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\ttraining's auc: 0.523539\tvalid_1's auc: 0.52587\n",
      "var_114\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.561397\tvalid_1's auc: 0.538155\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's auc: 0.54723\tvalid_1's auc: 0.541551\n",
      "var_115\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.544521\tvalid_1's auc: 0.510988\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[69]\ttraining's auc: 0.525729\tvalid_1's auc: 0.515699\n",
      "var_116\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.538973\tvalid_1's auc: 0.503552\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.512318\tvalid_1's auc: 0.50597\n",
      "var_117\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.556029\tvalid_1's auc: 0.532154\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\ttraining's auc: 0.539616\tvalid_1's auc: 0.52868\n",
      "var_118\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.547525\tvalid_1's auc: 0.519895\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.529359\tvalid_1's auc: 0.529074\n",
      "var_119\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.537989\tvalid_1's auc: 0.508596\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\ttraining's auc: 0.514656\tvalid_1's auc: 0.510778\n",
      "var_120\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.554551\tvalid_1's auc: 0.533887\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.535892\tvalid_1's auc: 0.545342\n",
      "var_121\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.552988\tvalid_1's auc: 0.529022\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.540964\tvalid_1's auc: 0.531187\n",
      "var_122\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.554194\tvalid_1's auc: 0.539259\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.538733\tvalid_1's auc: 0.546967\n",
      "var_123\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.538587\tvalid_1's auc: 0.504141\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.50758\tvalid_1's auc: 0.507159\n",
      "var_124\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.545822\tvalid_1's auc: 0.520777\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.52637\tvalid_1's auc: 0.520907\n",
      "var_125\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.537635\tvalid_1's auc: 0.489597\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[88]\ttraining's auc: 0.510431\tvalid_1's auc: 0.491167\n",
      "var_126\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.556337\tvalid_1's auc: 0.534438\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[93]\ttraining's auc: 0.536814\tvalid_1's auc: 0.540983\n",
      "var_127\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.546555\tvalid_1's auc: 0.508444\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.525163\tvalid_1's auc: 0.519352\n",
      "var_128\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.539319\tvalid_1's auc: 0.50213\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.51083\tvalid_1's auc: 0.495472\n",
      "var_129\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.547779\tvalid_1's auc: 0.524886\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[81]\ttraining's auc: 0.527418\tvalid_1's auc: 0.526311\n",
      "var_130\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.54794\tvalid_1's auc: 0.524123\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.532987\tvalid_1's auc: 0.521126\n",
      "var_131\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.547436\tvalid_1's auc: 0.511378\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.524199\tvalid_1's auc: 0.52572\n",
      "var_132\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.562462\tvalid_1's auc: 0.54606\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[89]\ttraining's auc: 0.54815\tvalid_1's auc: 0.544207\n",
      "var_133\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.541941\tvalid_1's auc: 0.512499\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[72]\ttraining's auc: 0.517827\tvalid_1's auc: 0.509517\n",
      "var_134\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.545566\tvalid_1's auc: 0.517681\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\ttraining's auc: 0.527362\tvalid_1's auc: 0.525502\n",
      "var_135\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's auc: 0.535785\tvalid_1's auc: 0.502162\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\ttraining's auc: 0.508897\tvalid_1's auc: 0.506401\n",
      "var_136\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.546322\tvalid_1's auc: 0.515904\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\ttraining's auc: 0.527736\tvalid_1's auc: 0.52494\n",
      "var_137\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.540175\tvalid_1's auc: 0.515926\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.51773\tvalid_1's auc: 0.516486\n",
      "var_138\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.584799\tvalid_1's auc: 0.559867\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\ttraining's auc: 0.568873\tvalid_1's auc: 0.565266\n",
      "var_139\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.54131\tvalid_1's auc: 0.494133\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.517852\tvalid_1's auc: 0.499555\n",
      "var_140\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.546357\tvalid_1's auc: 0.519627\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[76]\ttraining's auc: 0.526701\tvalid_1's auc: 0.529825\n",
      "var_141\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.540114\tvalid_1's auc: 0.508531\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[67]\ttraining's auc: 0.519099\tvalid_1's auc: 0.51451\n",
      "var_142\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.539425\tvalid_1's auc: 0.501995\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[86]\ttraining's auc: 0.511852\tvalid_1's auc: 0.509424\n",
      "var_143\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.542097\tvalid_1's auc: 0.51777\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[76]\ttraining's auc: 0.521136\tvalid_1's auc: 0.527311\n",
      "var_144\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.551687\tvalid_1's auc: 0.514413\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.529831\tvalid_1's auc: 0.516364\n",
      "var_145\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.570901\tvalid_1's auc: 0.551474\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.556564\tvalid_1's auc: 0.555707\n",
      "var_146\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.554375\tvalid_1's auc: 0.523082\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.537959\tvalid_1's auc: 0.53347\n",
      "var_147\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.567106\tvalid_1's auc: 0.548631\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\ttraining's auc: 0.550292\tvalid_1's auc: 0.550371\n",
      "var_148\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.558276\tvalid_1's auc: 0.547218\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.545177\tvalid_1's auc: 0.550139\n",
      "var_149\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.548856\tvalid_1's auc: 0.522804\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.5276\tvalid_1's auc: 0.521179\n",
      "var_150\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.547078\tvalid_1's auc: 0.516068\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[72]\ttraining's auc: 0.52755\tvalid_1's auc: 0.517439\n",
      "var_151\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.536858\tvalid_1's auc: 0.50612\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[86]\ttraining's auc: 0.514119\tvalid_1's auc: 0.498531\n",
      "var_152\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.537777\tvalid_1's auc: 0.504758\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\ttraining's auc: 0.511778\tvalid_1's auc: 0.502032\n",
      "var_153\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.557802\tvalid_1's auc: 0.538908\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[49]\ttraining's auc: 0.542328\tvalid_1's auc: 0.544612\n",
      "var_154\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.551725\tvalid_1's auc: 0.522881\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[88]\ttraining's auc: 0.530144\tvalid_1's auc: 0.538677\n",
      "var_155\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.542772\tvalid_1's auc: 0.501676\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[88]\ttraining's auc: 0.521705\tvalid_1's auc: 0.513325\n",
      "var_156\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.55033\tvalid_1's auc: 0.519404\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[65]\ttraining's auc: 0.529867\tvalid_1's auc: 0.529428\n",
      "var_157\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.536519\tvalid_1's auc: 0.497368\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.508317\tvalid_1's auc: 0.499685\n",
      "var_158\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.538193\tvalid_1's auc: 0.509827\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.515613\tvalid_1's auc: 0.518118\n",
      "var_159\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.538417\tvalid_1's auc: 0.50374\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.513392\tvalid_1's auc: 0.504379\n",
      "var_160\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.539386\tvalid_1's auc: 0.504334\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.51113\tvalid_1's auc: 0.499476\n",
      "var_161\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.551141\tvalid_1's auc: 0.520647\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[62]\ttraining's auc: 0.530102\tvalid_1's auc: 0.530663\n",
      "var_162\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.55153\tvalid_1's auc: 0.528326\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[84]\ttraining's auc: 0.531547\tvalid_1's auc: 0.530289\n",
      "var_163\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.557699\tvalid_1's auc: 0.533948\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.538059\tvalid_1's auc: 0.539538\n",
      "var_164\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.570219\tvalid_1's auc: 0.537612\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.554241\tvalid_1's auc: 0.539272\n",
      "var_165\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.571821\tvalid_1's auc: 0.540807\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[77]\ttraining's auc: 0.554299\tvalid_1's auc: 0.543108\n",
      "var_166\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.549884\tvalid_1's auc: 0.519278\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\ttraining's auc: 0.530583\tvalid_1's auc: 0.522022\n",
      "var_167\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.543398\tvalid_1's auc: 0.512306\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.516184\tvalid_1's auc: 0.510778\n",
      "var_168\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.563338\tvalid_1's auc: 0.538661\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[24]\ttraining's auc: 0.547137\tvalid_1's auc: 0.545275\n",
      "var_169\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.55933\tvalid_1's auc: 0.535817\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.541236\tvalid_1's auc: 0.537039\n",
      "var_170\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.538848\tvalid_1's auc: 0.506577\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[70]\ttraining's auc: 0.517311\tvalid_1's auc: 0.511941\n",
      "var_171\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.550855\tvalid_1's auc: 0.533776\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[89]\ttraining's auc: 0.534853\tvalid_1's auc: 0.538994\n",
      "var_172\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.555695\tvalid_1's auc: 0.532108\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[60]\ttraining's auc: 0.541415\tvalid_1's auc: 0.53733\n",
      "var_173\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.567175\tvalid_1's auc: 0.552647\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[69]\ttraining's auc: 0.553126\tvalid_1's auc: 0.549486\n",
      "var_174\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.546905\tvalid_1's auc: 0.509906\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[87]\ttraining's auc: 0.522546\tvalid_1's auc: 0.514331\n",
      "var_175\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.538552\tvalid_1's auc: 0.505226\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.512609\tvalid_1's auc: 0.495174\n",
      "var_176\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.556737\tvalid_1's auc: 0.529938\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[91]\ttraining's auc: 0.538326\tvalid_1's auc: 0.527622\n",
      "var_177\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.541383\tvalid_1's auc: 0.510153\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[87]\ttraining's auc: 0.520117\tvalid_1's auc: 0.516705\n",
      "var_178\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.561301\tvalid_1's auc: 0.532707\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.54233\tvalid_1's auc: 0.540717\n",
      "var_179\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.550221\tvalid_1's auc: 0.525398\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.527007\tvalid_1's auc: 0.529419\n",
      "var_180\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.542994\tvalid_1's auc: 0.510829\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[61]\ttraining's auc: 0.513749\tvalid_1's auc: 0.513738\n",
      "var_181\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.541301\tvalid_1's auc: 0.496418\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.512649\tvalid_1's auc: 0.500408\n",
      "var_182\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.536241\tvalid_1's auc: 0.498323\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[63]\ttraining's auc: 0.50948\tvalid_1's auc: 0.497152\n",
      "var_183\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.5603\tvalid_1's auc: 0.543533\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.544568\tvalid_1's auc: 0.544016\n",
      "var_184\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.534095\tvalid_1's auc: 0.502676\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[54]\ttraining's auc: 0.506718\tvalid_1's auc: 0.496822\n",
      "var_185\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.548944\tvalid_1's auc: 0.520564\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.532181\tvalid_1's auc: 0.521848\n",
      "var_186\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.540319\tvalid_1's auc: 0.510229\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.516507\tvalid_1's auc: 0.507156\n",
      "var_187\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.553053\tvalid_1's auc: 0.517341\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.533666\tvalid_1's auc: 0.522097\n",
      "var_188\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.539156\tvalid_1's auc: 0.498309\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[87]\ttraining's auc: 0.512724\tvalid_1's auc: 0.505635\n",
      "var_189\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.565652\tvalid_1's auc: 0.541059\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.551667\tvalid_1's auc: 0.549927\n",
      "var_190\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.562062\tvalid_1's auc: 0.528382\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.544358\tvalid_1's auc: 0.538632\n",
      "var_191\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.556549\tvalid_1's auc: 0.535048\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[28]\ttraining's auc: 0.544924\tvalid_1's auc: 0.541225\n",
      "var_192\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.541958\tvalid_1's auc: 0.505895\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's auc: 0.518078\tvalid_1's auc: 0.509117\n",
      "var_193\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.539934\tvalid_1's auc: 0.522976\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[46]\ttraining's auc: 0.519673\tvalid_1's auc: 0.528079\n",
      "var_194\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.547028\tvalid_1's auc: 0.52427\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\ttraining's auc: 0.526801\tvalid_1's auc: 0.524223\n",
      "var_195\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\ttraining's auc: 0.545577\tvalid_1's auc: 0.5094\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[37]\ttraining's auc: 0.523387\tvalid_1's auc: 0.514507\n",
      "var_196\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.549768\tvalid_1's auc: 0.522953\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[31]\ttraining's auc: 0.530811\tvalid_1's auc: 0.528382\n",
      "var_197\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.562963\tvalid_1's auc: 0.544066\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[86]\ttraining's auc: 0.54888\tvalid_1's auc: 0.542765\n",
      "var_198\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's auc: 0.544211\tvalid_1's auc: 0.52627\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[87]\ttraining's auc: 0.524018\tvalid_1's auc: 0.533093\n",
      "var_199\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for col in list(X1.columns):\n",
    "    bins = np.linspace(-6.0,6.0,40)\n",
    "    X = pd.get_dummies(pd.cut(X1[col].values, bins))\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X1[col].reshape(-1,1), y, test_size=0.2, stratify=y)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "    np.random.seed(123)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "    print(col)\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_cols3 = ['var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_7', 'var_8', 'var_9', 'var_11', \n",
    "            'var_12', 'var_13', 'var_14', 'var_16', 'var_18', 'var_20', 'var_22', 'var_23', \n",
    "            'var_24', 'var_25', 'var_26', 'var_28', 'var_30', 'var_31', 'var_32', 'var_33',\n",
    "            'var_34', 'var_35', 'var_36', 'var37', 'var_38', 'var_40', 'var_42', 'var_43',\n",
    "            'var_44', 'var_45', 'var_46', 'var_48', 'var_49', 'var_50', 'var_53', 'var_54', 'var_55',\n",
    "            'var_56', 'var_57', 'var_58', 'var_59', 'var_60', 'var_63', 'var_65', 'var_66', 'var_67',\n",
    "            'var_68', 'var_70', 'var_71', 'var_72', 'var_74', 'var_75', 'var_76', 'var_77', 'var_78',\n",
    "            'var_80', 'var_81', 'var_82', 'var_84', 'var_85', 'var_87', 'var_88', 'var_89', 'var_90',\n",
    "            'var_91', 'var_93', 'var_94', 'var_95', 'var_96', 'var_97', 'var_98', 'var_99', 'var_102',\n",
    "            'var_103', 'var_104', 'var_106', 'var_109', 'var_110', 'var_112', 'var_114', 'var_115',\n",
    "            'var_116', 'var_117', 'var_119', 'var_120', 'var_121', 'var_122', 'var_123', 'var_124',\n",
    "            'var_125', 'var_127', 'var_128', 'var_130', 'var_132', 'var_135', 'var_136', 'var_137',\n",
    "            'var_138', 'var_139', 'var_141', 'var_142', 'var_143', 'var_144', 'var_145', 'var_146',\n",
    "            'var_147', 'var_148', 'var_149', 'var_151', 'var_154', 'var_155', 'var_156', 'var_157',\n",
    "            'var_159', 'var_160', 'var_162', 'var_163', 'var_164', 'var_165', 'var_166', 'var_167',\n",
    "            'var_169', 'var_170', 'var_171', 'var_172', 'var_173', 'var_175', 'var_178', 'var_179',\n",
    "            'var_180', 'var_181', 'var_184', 'var_186', 'var_188', 'var_190', 'var_191', 'var_192',\n",
    "            'var_193', 'var_194', 'var_195', 'var_196', 'var_197', 'var_199']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.562771\tvalid_1's auc: 0.543174\n",
      "Early stopping, best iteration is:\n",
      "[62]\ttraining's auc: 0.553358\tvalid_1's auc: 0.54898\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.546362\tvalid_1's auc: 0.542755\n",
      "Early stopping, best iteration is:\n",
      "[84]\ttraining's auc: 0.541943\tvalid_1's auc: 0.54547\n",
      "var_0\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.559618\tvalid_1's auc: 0.545395\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's auc: 0.549533\tvalid_1's auc: 0.553045\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.543051\tvalid_1's auc: 0.550363\n",
      "Early stopping, best iteration is:\n",
      "[296]\ttraining's auc: 0.542851\tvalid_1's auc: 0.550904\n",
      "var_1\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.562907\tvalid_1's auc: 0.555084\n",
      "Early stopping, best iteration is:\n",
      "[216]\ttraining's auc: 0.557697\tvalid_1's auc: 0.559042\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.550602\tvalid_1's auc: 0.554183\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttraining's auc: 0.54455\tvalid_1's auc: 0.559132\n",
      "var_2\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.537146\tvalid_1's auc: 0.506958\n",
      "Early stopping, best iteration is:\n",
      "[161]\ttraining's auc: 0.529075\tvalid_1's auc: 0.51142\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.513817\tvalid_1's auc: 0.507962\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's auc: 0.513565\tvalid_1's auc: 0.511107\n",
      "var_3\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.533948\tvalid_1's auc: 0.504645\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.516084\tvalid_1's auc: 0.512595\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.514134\tvalid_1's auc: 0.507855\n",
      "Early stopping, best iteration is:\n",
      "[138]\ttraining's auc: 0.512968\tvalid_1's auc: 0.509004\n",
      "var_4\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-469-b8b393b757cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mX3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     model = lgb.LGBMClassifier(\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for col in list(X1.columns):\n",
    "    X3 = X1.copy()\n",
    "    \n",
    "    bins = np.linspace(-6.0,6.0,40)\n",
    "    X = pd.get_dummies(pd.cut(X1[col].values, bins))\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    X3 = pd.concat([X3, X], axis=1)\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X1[col].reshape(-1,1), y, test_size=0.2, stratify=y)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "    np.random.seed(123)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "    print(col)\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(\n",
    "             boost=\"gbdt\",\n",
    "             metric=\"auc\",\n",
    "             boost_from_average=\"false\",\n",
    "             n_estimators=999999,\n",
    "             learning_rate = 0.0083,\n",
    "             num_leaves = 13,\n",
    "             max_depth=-1,\n",
    "             tree_learner = \"serial\",\n",
    "             feature_fraction = 0.041,\n",
    "             bagging_freq = 5,\n",
    "             bagging_fraction = 0.335,\n",
    "             min_data_in_leaf = 80,\n",
    "             min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "             objective='binary', \n",
    "             n_jobs=-1)\n",
    "np.random.seed(123)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X1, y, test_size=0.2, stratify=y, random_state=123)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901887\tvalid_1's auc: 0.884927\n",
      "[2000]\ttraining's auc: 0.911484\tvalid_1's auc: 0.892885\n",
      "[3000]\ttraining's auc: 0.917615\tvalid_1's auc: 0.896233\n",
      "[4000]\ttraining's auc: 0.922687\tvalid_1's auc: 0.898475\n",
      "[5000]\ttraining's auc: 0.927171\tvalid_1's auc: 0.899852\n",
      "[6000]\ttraining's auc: 0.931165\tvalid_1's auc: 0.900667\n",
      "[7000]\ttraining's auc: 0.934662\tvalid_1's auc: 0.901146\n",
      "[8000]\ttraining's auc: 0.9381\tvalid_1's auc: 0.901417\n",
      "[9000]\ttraining's auc: 0.941327\tvalid_1's auc: 0.901516\n",
      "[10000]\ttraining's auc: 0.944418\tvalid_1's auc: 0.901434\n",
      "Early stopping, best iteration is:\n",
      "[9001]\ttraining's auc: 0.94133\tvalid_1's auc: 0.901518\n",
      "var_0\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902046\tvalid_1's auc: 0.884504\n",
      "[2000]\ttraining's auc: 0.911574\tvalid_1's auc: 0.892368\n",
      "[3000]\ttraining's auc: 0.917634\tvalid_1's auc: 0.895863\n",
      "[4000]\ttraining's auc: 0.922681\tvalid_1's auc: 0.898198\n",
      "[5000]\ttraining's auc: 0.927135\tvalid_1's auc: 0.899413\n",
      "[6000]\ttraining's auc: 0.931126\tvalid_1's auc: 0.900232\n",
      "[7000]\ttraining's auc: 0.934622\tvalid_1's auc: 0.900668\n",
      "[8000]\ttraining's auc: 0.938042\tvalid_1's auc: 0.900937\n",
      "[9000]\ttraining's auc: 0.941258\tvalid_1's auc: 0.901061\n",
      "Early stopping, best iteration is:\n",
      "[8867]\ttraining's auc: 0.940841\tvalid_1's auc: 0.901071\n",
      "var_1\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901899\tvalid_1's auc: 0.884514\n",
      "[2000]\ttraining's auc: 0.911558\tvalid_1's auc: 0.892414\n",
      "[3000]\ttraining's auc: 0.917684\tvalid_1's auc: 0.895946\n",
      "[4000]\ttraining's auc: 0.922758\tvalid_1's auc: 0.898259\n",
      "[5000]\ttraining's auc: 0.927214\tvalid_1's auc: 0.899587\n",
      "[6000]\ttraining's auc: 0.931272\tvalid_1's auc: 0.900375\n",
      "[7000]\ttraining's auc: 0.934772\tvalid_1's auc: 0.900799\n",
      "[8000]\ttraining's auc: 0.93819\tvalid_1's auc: 0.901148\n",
      "[9000]\ttraining's auc: 0.941396\tvalid_1's auc: 0.901288\n",
      "Early stopping, best iteration is:\n",
      "[8929]\ttraining's auc: 0.94117\tvalid_1's auc: 0.901292\n",
      "var_2\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.90246\tvalid_1's auc: 0.885594\n",
      "[2000]\ttraining's auc: 0.912081\tvalid_1's auc: 0.893379\n",
      "[3000]\ttraining's auc: 0.918087\tvalid_1's auc: 0.896737\n",
      "[4000]\ttraining's auc: 0.923102\tvalid_1's auc: 0.89882\n",
      "[5000]\ttraining's auc: 0.92753\tvalid_1's auc: 0.899992\n",
      "[6000]\ttraining's auc: 0.931507\tvalid_1's auc: 0.900684\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-472-5074fb934f27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mX3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for col in list(X1.columns):\n",
    "    X3 = X1.copy()\n",
    "    \n",
    "    bins = np.linspace(-6.0,6.0,40)\n",
    "    X = pd.get_dummies(pd.cut(X1[col].values, bins))\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    X3 = pd.concat([X3, X], axis=1)\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X3, y, test_size=0.2, stratify=y)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "   \n",
    "    print(col)\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.90246\tvalid_1's auc: 0.885594\n",
      "[2000]\ttraining's auc: 0.912081\tvalid_1's auc: 0.893379\n",
      "[3000]\ttraining's auc: 0.918087\tvalid_1's auc: 0.896737\n",
      "[4000]\ttraining's auc: 0.923102\tvalid_1's auc: 0.89882\n",
      "[5000]\ttraining's auc: 0.92753\tvalid_1's auc: 0.899992\n",
      "[6000]\ttraining's auc: 0.931507\tvalid_1's auc: 0.900684\n",
      "[7000]\ttraining's auc: 0.934975\tvalid_1's auc: 0.901184\n",
      "[8000]\ttraining's auc: 0.938356\tvalid_1's auc: 0.901403\n",
      "[9000]\ttraining's auc: 0.941554\tvalid_1's auc: 0.901568\n",
      "[10000]\ttraining's auc: 0.944604\tvalid_1's auc: 0.901506\n",
      "Early stopping, best iteration is:\n",
      "[9001]\ttraining's auc: 0.941557\tvalid_1's auc: 0.901568\n",
      "var_3\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902469\tvalid_1's auc: 0.885641\n",
      "[2000]\ttraining's auc: 0.912073\tvalid_1's auc: 0.893378\n",
      "[3000]\ttraining's auc: 0.918047\tvalid_1's auc: 0.896741\n",
      "[4000]\ttraining's auc: 0.923098\tvalid_1's auc: 0.898899\n",
      "[5000]\ttraining's auc: 0.927548\tvalid_1's auc: 0.900031\n",
      "[6000]\ttraining's auc: 0.931515\tvalid_1's auc: 0.900714\n",
      "[7000]\ttraining's auc: 0.934997\tvalid_1's auc: 0.901184\n",
      "[8000]\ttraining's auc: 0.938352\tvalid_1's auc: 0.901381\n",
      "[9000]\ttraining's auc: 0.941542\tvalid_1's auc: 0.901478\n",
      "Early stopping, best iteration is:\n",
      "[8244]\ttraining's auc: 0.93915\tvalid_1's auc: 0.9015\n",
      "var_4\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902163\tvalid_1's auc: 0.885247\n",
      "[2000]\ttraining's auc: 0.911815\tvalid_1's auc: 0.893118\n",
      "[3000]\ttraining's auc: 0.917858\tvalid_1's auc: 0.896417\n",
      "[4000]\ttraining's auc: 0.922896\tvalid_1's auc: 0.898652\n",
      "[5000]\ttraining's auc: 0.927364\tvalid_1's auc: 0.899866\n",
      "[6000]\ttraining's auc: 0.931301\tvalid_1's auc: 0.900661\n",
      "[7000]\ttraining's auc: 0.934793\tvalid_1's auc: 0.901135\n",
      "[8000]\ttraining's auc: 0.938193\tvalid_1's auc: 0.90139\n",
      "[9000]\ttraining's auc: 0.94143\tvalid_1's auc: 0.901476\n",
      "Early stopping, best iteration is:\n",
      "[8878]\ttraining's auc: 0.941031\tvalid_1's auc: 0.901508\n",
      "var_5\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901986\tvalid_1's auc: 0.884432\n",
      "[2000]\ttraining's auc: 0.911536\tvalid_1's auc: 0.892263\n",
      "[3000]\ttraining's auc: 0.917626\tvalid_1's auc: 0.895699\n",
      "[4000]\ttraining's auc: 0.922694\tvalid_1's auc: 0.897989\n",
      "[5000]\ttraining's auc: 0.927222\tvalid_1's auc: 0.899353\n",
      "[6000]\ttraining's auc: 0.931226\tvalid_1's auc: 0.900182\n",
      "[7000]\ttraining's auc: 0.934715\tvalid_1's auc: 0.900677\n",
      "[8000]\ttraining's auc: 0.938123\tvalid_1's auc: 0.901038\n",
      "[9000]\ttraining's auc: 0.94134\tvalid_1's auc: 0.90119\n",
      "Early stopping, best iteration is:\n",
      "[8866]\ttraining's auc: 0.940919\tvalid_1's auc: 0.901221\n",
      "var_6\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902555\tvalid_1's auc: 0.885949\n",
      "[2000]\ttraining's auc: 0.912131\tvalid_1's auc: 0.893571\n",
      "[3000]\ttraining's auc: 0.918169\tvalid_1's auc: 0.896874\n",
      "[4000]\ttraining's auc: 0.923191\tvalid_1's auc: 0.89909\n",
      "[5000]\ttraining's auc: 0.927564\tvalid_1's auc: 0.900303\n",
      "[6000]\ttraining's auc: 0.931515\tvalid_1's auc: 0.901077\n",
      "[7000]\ttraining's auc: 0.934996\tvalid_1's auc: 0.901499\n",
      "[8000]\ttraining's auc: 0.938373\tvalid_1's auc: 0.901873\n",
      "[9000]\ttraining's auc: 0.941555\tvalid_1's auc: 0.901979\n",
      "Early stopping, best iteration is:\n",
      "[8926]\ttraining's auc: 0.941326\tvalid_1's auc: 0.901994\n",
      "var_7\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902482\tvalid_1's auc: 0.885981\n",
      "[2000]\ttraining's auc: 0.912056\tvalid_1's auc: 0.89357\n",
      "[3000]\ttraining's auc: 0.918092\tvalid_1's auc: 0.896876\n",
      "[4000]\ttraining's auc: 0.923101\tvalid_1's auc: 0.899017\n",
      "[5000]\ttraining's auc: 0.927454\tvalid_1's auc: 0.900202\n",
      "[6000]\ttraining's auc: 0.931399\tvalid_1's auc: 0.901029\n",
      "[7000]\ttraining's auc: 0.934927\tvalid_1's auc: 0.901546\n",
      "[8000]\ttraining's auc: 0.938337\tvalid_1's auc: 0.901901\n",
      "[9000]\ttraining's auc: 0.941528\tvalid_1's auc: 0.902008\n",
      "Early stopping, best iteration is:\n",
      "[8926]\ttraining's auc: 0.941305\tvalid_1's auc: 0.902035\n",
      "var_8\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902106\tvalid_1's auc: 0.885237\n",
      "[2000]\ttraining's auc: 0.911705\tvalid_1's auc: 0.892964\n",
      "[3000]\ttraining's auc: 0.917837\tvalid_1's auc: 0.896359\n",
      "[4000]\ttraining's auc: 0.922881\tvalid_1's auc: 0.898626\n",
      "[5000]\ttraining's auc: 0.92731\tvalid_1's auc: 0.899895\n",
      "[6000]\ttraining's auc: 0.931274\tvalid_1's auc: 0.900704\n",
      "[7000]\ttraining's auc: 0.934797\tvalid_1's auc: 0.901219\n",
      "[8000]\ttraining's auc: 0.938232\tvalid_1's auc: 0.901563\n",
      "[9000]\ttraining's auc: 0.941453\tvalid_1's auc: 0.901675\n",
      "[10000]\ttraining's auc: 0.94449\tvalid_1's auc: 0.901744\n",
      "[11000]\ttraining's auc: 0.947414\tvalid_1's auc: 0.901805\n",
      "[12000]\ttraining's auc: 0.950227\tvalid_1's auc: 0.901899\n",
      "[13000]\ttraining's auc: 0.952899\tvalid_1's auc: 0.901754\n",
      "Early stopping, best iteration is:\n",
      "[12217]\ttraining's auc: 0.950807\tvalid_1's auc: 0.901948\n",
      "var_9\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902523\tvalid_1's auc: 0.885973\n",
      "[2000]\ttraining's auc: 0.912159\tvalid_1's auc: 0.893687\n",
      "[3000]\ttraining's auc: 0.918216\tvalid_1's auc: 0.897081\n",
      "[4000]\ttraining's auc: 0.923211\tvalid_1's auc: 0.899178\n",
      "[5000]\ttraining's auc: 0.927553\tvalid_1's auc: 0.90036\n",
      "[6000]\ttraining's auc: 0.931504\tvalid_1's auc: 0.901163\n",
      "[7000]\ttraining's auc: 0.934971\tvalid_1's auc: 0.901574\n",
      "[8000]\ttraining's auc: 0.938397\tvalid_1's auc: 0.901929\n",
      "[9000]\ttraining's auc: 0.941537\tvalid_1's auc: 0.902037\n",
      "[10000]\ttraining's auc: 0.944542\tvalid_1's auc: 0.902129\n",
      "[11000]\ttraining's auc: 0.947419\tvalid_1's auc: 0.90213\n",
      "Early stopping, best iteration is:\n",
      "[10928]\ttraining's auc: 0.947222\tvalid_1's auc: 0.90216\n",
      "var_10\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902465\tvalid_1's auc: 0.885893\n",
      "[2000]\ttraining's auc: 0.912012\tvalid_1's auc: 0.893686\n",
      "[3000]\ttraining's auc: 0.918073\tvalid_1's auc: 0.897077\n",
      "[4000]\ttraining's auc: 0.923074\tvalid_1's auc: 0.899205\n",
      "[5000]\ttraining's auc: 0.927405\tvalid_1's auc: 0.900331\n",
      "[6000]\ttraining's auc: 0.931386\tvalid_1's auc: 0.901064\n",
      "[7000]\ttraining's auc: 0.934881\tvalid_1's auc: 0.901524\n",
      "[8000]\ttraining's auc: 0.938315\tvalid_1's auc: 0.901823\n",
      "[9000]\ttraining's auc: 0.941504\tvalid_1's auc: 0.901911\n",
      "[10000]\ttraining's auc: 0.944534\tvalid_1's auc: 0.90198\n",
      "[11000]\ttraining's auc: 0.947445\tvalid_1's auc: 0.901994\n",
      "[12000]\ttraining's auc: 0.950263\tvalid_1's auc: 0.90203\n",
      "Early stopping, best iteration is:\n",
      "[11338]\ttraining's auc: 0.948423\tvalid_1's auc: 0.902076\n",
      "var_11\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902218\tvalid_1's auc: 0.885805\n",
      "[2000]\ttraining's auc: 0.911548\tvalid_1's auc: 0.893247\n",
      "[3000]\ttraining's auc: 0.917703\tvalid_1's auc: 0.896813\n",
      "[4000]\ttraining's auc: 0.922779\tvalid_1's auc: 0.899082\n",
      "[5000]\ttraining's auc: 0.927152\tvalid_1's auc: 0.900321\n",
      "[6000]\ttraining's auc: 0.931178\tvalid_1's auc: 0.901113\n",
      "[7000]\ttraining's auc: 0.934701\tvalid_1's auc: 0.901588\n",
      "[8000]\ttraining's auc: 0.93812\tvalid_1's auc: 0.901924\n",
      "[9000]\ttraining's auc: 0.941301\tvalid_1's auc: 0.902085\n",
      "Early stopping, best iteration is:\n",
      "[8867]\ttraining's auc: 0.940878\tvalid_1's auc: 0.902129\n",
      "var_12\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901962\tvalid_1's auc: 0.88545\n",
      "[2000]\ttraining's auc: 0.911437\tvalid_1's auc: 0.893037\n",
      "[3000]\ttraining's auc: 0.917775\tvalid_1's auc: 0.896644\n",
      "[4000]\ttraining's auc: 0.922817\tvalid_1's auc: 0.898855\n",
      "[5000]\ttraining's auc: 0.927185\tvalid_1's auc: 0.900104\n",
      "[6000]\ttraining's auc: 0.931159\tvalid_1's auc: 0.900825\n",
      "[7000]\ttraining's auc: 0.934696\tvalid_1's auc: 0.901345\n",
      "[8000]\ttraining's auc: 0.938162\tvalid_1's auc: 0.901695\n",
      "[9000]\ttraining's auc: 0.941293\tvalid_1's auc: 0.901895\n",
      "Early stopping, best iteration is:\n",
      "[8867]\ttraining's auc: 0.940875\tvalid_1's auc: 0.901923\n",
      "var_13\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902361\tvalid_1's auc: 0.885257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\ttraining's auc: 0.911948\tvalid_1's auc: 0.893129\n",
      "[3000]\ttraining's auc: 0.918233\tvalid_1's auc: 0.896807\n",
      "[4000]\ttraining's auc: 0.923195\tvalid_1's auc: 0.899016\n",
      "[5000]\ttraining's auc: 0.927499\tvalid_1's auc: 0.900189\n",
      "[6000]\ttraining's auc: 0.931458\tvalid_1's auc: 0.900938\n",
      "[7000]\ttraining's auc: 0.934974\tvalid_1's auc: 0.901352\n",
      "[8000]\ttraining's auc: 0.938395\tvalid_1's auc: 0.901674\n",
      "[9000]\ttraining's auc: 0.941541\tvalid_1's auc: 0.901781\n",
      "[10000]\ttraining's auc: 0.944598\tvalid_1's auc: 0.901858\n",
      "[11000]\ttraining's auc: 0.947507\tvalid_1's auc: 0.901876\n",
      "[12000]\ttraining's auc: 0.950282\tvalid_1's auc: 0.901902\n",
      "Early stopping, best iteration is:\n",
      "[11475]\ttraining's auc: 0.948854\tvalid_1's auc: 0.901957\n",
      "var_14\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902316\tvalid_1's auc: 0.885172\n",
      "[2000]\ttraining's auc: 0.911856\tvalid_1's auc: 0.893001\n",
      "[3000]\ttraining's auc: 0.918097\tvalid_1's auc: 0.896691\n",
      "[4000]\ttraining's auc: 0.923086\tvalid_1's auc: 0.89894\n",
      "[5000]\ttraining's auc: 0.927434\tvalid_1's auc: 0.900177\n",
      "[6000]\ttraining's auc: 0.931404\tvalid_1's auc: 0.900941\n",
      "[7000]\ttraining's auc: 0.934892\tvalid_1's auc: 0.901395\n",
      "[8000]\ttraining's auc: 0.938327\tvalid_1's auc: 0.901716\n",
      "[9000]\ttraining's auc: 0.94151\tvalid_1's auc: 0.901841\n",
      "Early stopping, best iteration is:\n",
      "[8838]\ttraining's auc: 0.940987\tvalid_1's auc: 0.901882\n",
      "var_15\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902351\tvalid_1's auc: 0.885156\n",
      "[2000]\ttraining's auc: 0.91192\tvalid_1's auc: 0.893075\n",
      "[3000]\ttraining's auc: 0.91821\tvalid_1's auc: 0.896771\n",
      "[4000]\ttraining's auc: 0.923216\tvalid_1's auc: 0.898967\n",
      "[5000]\ttraining's auc: 0.927527\tvalid_1's auc: 0.900198\n",
      "[6000]\ttraining's auc: 0.931506\tvalid_1's auc: 0.900881\n",
      "[7000]\ttraining's auc: 0.935023\tvalid_1's auc: 0.90135\n",
      "[8000]\ttraining's auc: 0.93842\tvalid_1's auc: 0.901626\n",
      "[9000]\ttraining's auc: 0.941594\tvalid_1's auc: 0.90172\n",
      "Early stopping, best iteration is:\n",
      "[8804]\ttraining's auc: 0.940988\tvalid_1's auc: 0.90178\n",
      "var_16\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902332\tvalid_1's auc: 0.88519\n",
      "[2000]\ttraining's auc: 0.911945\tvalid_1's auc: 0.893108\n",
      "[3000]\ttraining's auc: 0.918228\tvalid_1's auc: 0.896822\n",
      "[4000]\ttraining's auc: 0.923209\tvalid_1's auc: 0.899036\n",
      "[5000]\ttraining's auc: 0.927548\tvalid_1's auc: 0.900227\n",
      "[6000]\ttraining's auc: 0.931515\tvalid_1's auc: 0.900922\n",
      "[7000]\ttraining's auc: 0.935021\tvalid_1's auc: 0.901344\n",
      "[8000]\ttraining's auc: 0.938414\tvalid_1's auc: 0.901651\n",
      "[9000]\ttraining's auc: 0.941579\tvalid_1's auc: 0.901739\n",
      "[10000]\ttraining's auc: 0.944596\tvalid_1's auc: 0.901803\n",
      "[11000]\ttraining's auc: 0.947492\tvalid_1's auc: 0.901834\n",
      "[12000]\ttraining's auc: 0.950293\tvalid_1's auc: 0.901836\n",
      "Early stopping, best iteration is:\n",
      "[11402]\ttraining's auc: 0.948643\tvalid_1's auc: 0.901915\n",
      "var_17\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901881\tvalid_1's auc: 0.884584\n",
      "[2000]\ttraining's auc: 0.911458\tvalid_1's auc: 0.892599\n",
      "[3000]\ttraining's auc: 0.917785\tvalid_1's auc: 0.896343\n",
      "[4000]\ttraining's auc: 0.922834\tvalid_1's auc: 0.898692\n",
      "[5000]\ttraining's auc: 0.927184\tvalid_1's auc: 0.899866\n",
      "[6000]\ttraining's auc: 0.9312\tvalid_1's auc: 0.900707\n",
      "[7000]\ttraining's auc: 0.934749\tvalid_1's auc: 0.901289\n",
      "[8000]\ttraining's auc: 0.938174\tvalid_1's auc: 0.901525\n",
      "[9000]\ttraining's auc: 0.941346\tvalid_1's auc: 0.901696\n",
      "Early stopping, best iteration is:\n",
      "[8800]\ttraining's auc: 0.94073\tvalid_1's auc: 0.901752\n",
      "var_18\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902328\tvalid_1's auc: 0.885095\n",
      "[2000]\ttraining's auc: 0.911882\tvalid_1's auc: 0.893005\n",
      "[3000]\ttraining's auc: 0.918145\tvalid_1's auc: 0.896684\n",
      "[4000]\ttraining's auc: 0.923113\tvalid_1's auc: 0.898918\n",
      "[5000]\ttraining's auc: 0.927455\tvalid_1's auc: 0.900151\n",
      "[6000]\ttraining's auc: 0.931423\tvalid_1's auc: 0.900936\n",
      "[7000]\ttraining's auc: 0.934963\tvalid_1's auc: 0.901403\n",
      "[8000]\ttraining's auc: 0.938357\tvalid_1's auc: 0.901646\n",
      "[9000]\ttraining's auc: 0.941506\tvalid_1's auc: 0.901735\n",
      "Early stopping, best iteration is:\n",
      "[8867]\ttraining's auc: 0.941091\tvalid_1's auc: 0.90179\n",
      "var_19\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902313\tvalid_1's auc: 0.885228\n",
      "[2000]\ttraining's auc: 0.911848\tvalid_1's auc: 0.892977\n",
      "[3000]\ttraining's auc: 0.91811\tvalid_1's auc: 0.896716\n",
      "[4000]\ttraining's auc: 0.923114\tvalid_1's auc: 0.898936\n",
      "[5000]\ttraining's auc: 0.927441\tvalid_1's auc: 0.900145\n",
      "[6000]\ttraining's auc: 0.931379\tvalid_1's auc: 0.900874\n",
      "[7000]\ttraining's auc: 0.9349\tvalid_1's auc: 0.901355\n",
      "[8000]\ttraining's auc: 0.938301\tvalid_1's auc: 0.901684\n",
      "[9000]\ttraining's auc: 0.941506\tvalid_1's auc: 0.901708\n",
      "Early stopping, best iteration is:\n",
      "[8757]\ttraining's auc: 0.940728\tvalid_1's auc: 0.9018\n",
      "var_20\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902191\tvalid_1's auc: 0.885528\n",
      "[2000]\ttraining's auc: 0.911525\tvalid_1's auc: 0.893075\n",
      "[3000]\ttraining's auc: 0.917795\tvalid_1's auc: 0.89669\n",
      "[4000]\ttraining's auc: 0.922904\tvalid_1's auc: 0.898907\n",
      "[5000]\ttraining's auc: 0.927279\tvalid_1's auc: 0.900076\n",
      "[6000]\ttraining's auc: 0.931253\tvalid_1's auc: 0.90073\n",
      "[7000]\ttraining's auc: 0.934811\tvalid_1's auc: 0.901242\n",
      "[8000]\ttraining's auc: 0.938226\tvalid_1's auc: 0.901576\n",
      "[9000]\ttraining's auc: 0.941366\tvalid_1's auc: 0.90158\n",
      "Early stopping, best iteration is:\n",
      "[8800]\ttraining's auc: 0.940746\tvalid_1's auc: 0.901637\n",
      "var_21\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902525\tvalid_1's auc: 0.884811\n",
      "[2000]\ttraining's auc: 0.911659\tvalid_1's auc: 0.892301\n",
      "[3000]\ttraining's auc: 0.917824\tvalid_1's auc: 0.896007\n",
      "[4000]\ttraining's auc: 0.922941\tvalid_1's auc: 0.898338\n",
      "[5000]\ttraining's auc: 0.927265\tvalid_1's auc: 0.899605\n",
      "[6000]\ttraining's auc: 0.931246\tvalid_1's auc: 0.900424\n",
      "[7000]\ttraining's auc: 0.934772\tvalid_1's auc: 0.900911\n",
      "[8000]\ttraining's auc: 0.938203\tvalid_1's auc: 0.901146\n",
      "[9000]\ttraining's auc: 0.941394\tvalid_1's auc: 0.9012\n",
      "Early stopping, best iteration is:\n",
      "[8802]\ttraining's auc: 0.940759\tvalid_1's auc: 0.901248\n",
      "var_22\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902974\tvalid_1's auc: 0.885719\n",
      "[2000]\ttraining's auc: 0.911839\tvalid_1's auc: 0.893082\n",
      "[3000]\ttraining's auc: 0.917975\tvalid_1's auc: 0.896687\n",
      "[4000]\ttraining's auc: 0.922984\tvalid_1's auc: 0.898923\n",
      "[5000]\ttraining's auc: 0.927314\tvalid_1's auc: 0.900009\n",
      "[6000]\ttraining's auc: 0.931293\tvalid_1's auc: 0.900745\n",
      "[7000]\ttraining's auc: 0.934849\tvalid_1's auc: 0.901155\n",
      "[8000]\ttraining's auc: 0.938281\tvalid_1's auc: 0.901404\n",
      "[9000]\ttraining's auc: 0.94141\tvalid_1's auc: 0.90156\n",
      "[10000]\ttraining's auc: 0.944474\tvalid_1's auc: 0.90169\n",
      "[11000]\ttraining's auc: 0.947406\tvalid_1's auc: 0.901725\n",
      "[12000]\ttraining's auc: 0.950226\tvalid_1's auc: 0.901743\n",
      "Early stopping, best iteration is:\n",
      "[11340]\ttraining's auc: 0.948394\tvalid_1's auc: 0.901795\n",
      "var_23\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902874\tvalid_1's auc: 0.885636\n",
      "[2000]\ttraining's auc: 0.911844\tvalid_1's auc: 0.893256\n",
      "[3000]\ttraining's auc: 0.918052\tvalid_1's auc: 0.896644\n",
      "[4000]\ttraining's auc: 0.923091\tvalid_1's auc: 0.89887\n",
      "[5000]\ttraining's auc: 0.927438\tvalid_1's auc: 0.90011\n",
      "[6000]\ttraining's auc: 0.931403\tvalid_1's auc: 0.900829\n",
      "[7000]\ttraining's auc: 0.934888\tvalid_1's auc: 0.901278\n",
      "[8000]\ttraining's auc: 0.93827\tvalid_1's auc: 0.901512\n",
      "[9000]\ttraining's auc: 0.941426\tvalid_1's auc: 0.90164\n",
      "[10000]\ttraining's auc: 0.944478\tvalid_1's auc: 0.901758\n",
      "[11000]\ttraining's auc: 0.947405\tvalid_1's auc: 0.901823\n",
      "[12000]\ttraining's auc: 0.950247\tvalid_1's auc: 0.901825\n",
      "Early stopping, best iteration is:\n",
      "[11340]\ttraining's auc: 0.948408\tvalid_1's auc: 0.901874\n",
      "var_24\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903032\tvalid_1's auc: 0.88573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\ttraining's auc: 0.912033\tvalid_1's auc: 0.893275\n",
      "[3000]\ttraining's auc: 0.918162\tvalid_1's auc: 0.896809\n",
      "[4000]\ttraining's auc: 0.923188\tvalid_1's auc: 0.899129\n",
      "[5000]\ttraining's auc: 0.927496\tvalid_1's auc: 0.900287\n",
      "[6000]\ttraining's auc: 0.931432\tvalid_1's auc: 0.901052\n",
      "[7000]\ttraining's auc: 0.934936\tvalid_1's auc: 0.901443\n",
      "[8000]\ttraining's auc: 0.938328\tvalid_1's auc: 0.901726\n",
      "[9000]\ttraining's auc: 0.941491\tvalid_1's auc: 0.901833\n",
      "Early stopping, best iteration is:\n",
      "[8802]\ttraining's auc: 0.940869\tvalid_1's auc: 0.901914\n",
      "var_25\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902517\tvalid_1's auc: 0.885311\n",
      "[2000]\ttraining's auc: 0.911612\tvalid_1's auc: 0.893053\n",
      "[3000]\ttraining's auc: 0.917823\tvalid_1's auc: 0.8966\n",
      "[4000]\ttraining's auc: 0.922895\tvalid_1's auc: 0.898871\n",
      "[5000]\ttraining's auc: 0.927223\tvalid_1's auc: 0.900025\n",
      "[6000]\ttraining's auc: 0.931202\tvalid_1's auc: 0.900717\n",
      "[7000]\ttraining's auc: 0.934716\tvalid_1's auc: 0.901081\n",
      "[8000]\ttraining's auc: 0.938129\tvalid_1's auc: 0.901413\n",
      "[9000]\ttraining's auc: 0.941299\tvalid_1's auc: 0.901587\n",
      "Early stopping, best iteration is:\n",
      "[8844]\ttraining's auc: 0.940812\tvalid_1's auc: 0.901655\n",
      "var_26\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902942\tvalid_1's auc: 0.885877\n",
      "[2000]\ttraining's auc: 0.912148\tvalid_1's auc: 0.893523\n",
      "[3000]\ttraining's auc: 0.918284\tvalid_1's auc: 0.896884\n",
      "[4000]\ttraining's auc: 0.92327\tvalid_1's auc: 0.899173\n",
      "[5000]\ttraining's auc: 0.927543\tvalid_1's auc: 0.900282\n",
      "[6000]\ttraining's auc: 0.931473\tvalid_1's auc: 0.90099\n",
      "[7000]\ttraining's auc: 0.934971\tvalid_1's auc: 0.901382\n",
      "[8000]\ttraining's auc: 0.938337\tvalid_1's auc: 0.901648\n",
      "[9000]\ttraining's auc: 0.941481\tvalid_1's auc: 0.901715\n",
      "Early stopping, best iteration is:\n",
      "[8810]\ttraining's auc: 0.940899\tvalid_1's auc: 0.901789\n",
      "var_27\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.90269\tvalid_1's auc: 0.885782\n",
      "[2000]\ttraining's auc: 0.911902\tvalid_1's auc: 0.893518\n",
      "[3000]\ttraining's auc: 0.918055\tvalid_1's auc: 0.896883\n",
      "[4000]\ttraining's auc: 0.923102\tvalid_1's auc: 0.899124\n",
      "[5000]\ttraining's auc: 0.927404\tvalid_1's auc: 0.900219\n",
      "[6000]\ttraining's auc: 0.93136\tvalid_1's auc: 0.900942\n",
      "[7000]\ttraining's auc: 0.934909\tvalid_1's auc: 0.901393\n",
      "[8000]\ttraining's auc: 0.938286\tvalid_1's auc: 0.901684\n",
      "[9000]\ttraining's auc: 0.941437\tvalid_1's auc: 0.901799\n",
      "Early stopping, best iteration is:\n",
      "[8755]\ttraining's auc: 0.940682\tvalid_1's auc: 0.901856\n",
      "var_28\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902834\tvalid_1's auc: 0.88596\n",
      "[2000]\ttraining's auc: 0.912046\tvalid_1's auc: 0.893631\n",
      "[3000]\ttraining's auc: 0.918195\tvalid_1's auc: 0.897036\n",
      "[4000]\ttraining's auc: 0.923205\tvalid_1's auc: 0.899279\n",
      "[5000]\ttraining's auc: 0.927493\tvalid_1's auc: 0.900424\n",
      "[6000]\ttraining's auc: 0.931433\tvalid_1's auc: 0.901158\n",
      "[7000]\ttraining's auc: 0.934931\tvalid_1's auc: 0.901583\n",
      "[8000]\ttraining's auc: 0.93831\tvalid_1's auc: 0.901845\n",
      "[9000]\ttraining's auc: 0.941449\tvalid_1's auc: 0.901906\n",
      "Early stopping, best iteration is:\n",
      "[8738]\ttraining's auc: 0.940638\tvalid_1's auc: 0.901979\n",
      "var_29\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902835\tvalid_1's auc: 0.885931\n",
      "[2000]\ttraining's auc: 0.912063\tvalid_1's auc: 0.893675\n",
      "[3000]\ttraining's auc: 0.918224\tvalid_1's auc: 0.897071\n",
      "[4000]\ttraining's auc: 0.923223\tvalid_1's auc: 0.899257\n",
      "[5000]\ttraining's auc: 0.927513\tvalid_1's auc: 0.900343\n",
      "[6000]\ttraining's auc: 0.931471\tvalid_1's auc: 0.901075\n",
      "[7000]\ttraining's auc: 0.93495\tvalid_1's auc: 0.901494\n",
      "[8000]\ttraining's auc: 0.938349\tvalid_1's auc: 0.901744\n",
      "[9000]\ttraining's auc: 0.941495\tvalid_1's auc: 0.901782\n",
      "Early stopping, best iteration is:\n",
      "[8741]\ttraining's auc: 0.940686\tvalid_1's auc: 0.901889\n",
      "var_30\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902712\tvalid_1's auc: 0.885709\n",
      "[2000]\ttraining's auc: 0.911923\tvalid_1's auc: 0.893527\n",
      "[3000]\ttraining's auc: 0.918028\tvalid_1's auc: 0.896909\n",
      "[4000]\ttraining's auc: 0.923056\tvalid_1's auc: 0.899107\n",
      "[5000]\ttraining's auc: 0.927339\tvalid_1's auc: 0.900071\n",
      "[6000]\ttraining's auc: 0.93131\tvalid_1's auc: 0.900823\n",
      "[7000]\ttraining's auc: 0.9348\tvalid_1's auc: 0.901179\n",
      "[8000]\ttraining's auc: 0.938176\tvalid_1's auc: 0.901449\n",
      "[9000]\ttraining's auc: 0.941373\tvalid_1's auc: 0.901461\n",
      "Early stopping, best iteration is:\n",
      "[8740]\ttraining's auc: 0.940554\tvalid_1's auc: 0.901554\n",
      "var_31\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902587\tvalid_1's auc: 0.88566\n",
      "[2000]\ttraining's auc: 0.911755\tvalid_1's auc: 0.893366\n",
      "[3000]\ttraining's auc: 0.917961\tvalid_1's auc: 0.896883\n",
      "[4000]\ttraining's auc: 0.923036\tvalid_1's auc: 0.899132\n",
      "[5000]\ttraining's auc: 0.927355\tvalid_1's auc: 0.900204\n",
      "[6000]\ttraining's auc: 0.931353\tvalid_1's auc: 0.900898\n",
      "[7000]\ttraining's auc: 0.934878\tvalid_1's auc: 0.901366\n",
      "[8000]\ttraining's auc: 0.93822\tvalid_1's auc: 0.9016\n",
      "[9000]\ttraining's auc: 0.941388\tvalid_1's auc: 0.901616\n",
      "Early stopping, best iteration is:\n",
      "[8739]\ttraining's auc: 0.94057\tvalid_1's auc: 0.901692\n",
      "var_32\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902339\tvalid_1's auc: 0.885069\n",
      "[2000]\ttraining's auc: 0.91149\tvalid_1's auc: 0.892939\n",
      "[3000]\ttraining's auc: 0.917693\tvalid_1's auc: 0.896495\n",
      "[4000]\ttraining's auc: 0.922831\tvalid_1's auc: 0.898851\n",
      "[5000]\ttraining's auc: 0.9272\tvalid_1's auc: 0.899975\n",
      "[6000]\ttraining's auc: 0.931228\tvalid_1's auc: 0.90076\n",
      "[7000]\ttraining's auc: 0.934802\tvalid_1's auc: 0.9013\n",
      "[8000]\ttraining's auc: 0.938194\tvalid_1's auc: 0.901523\n",
      "[9000]\ttraining's auc: 0.941349\tvalid_1's auc: 0.901588\n",
      "Early stopping, best iteration is:\n",
      "[8752]\ttraining's auc: 0.940584\tvalid_1's auc: 0.901668\n",
      "var_33\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902125\tvalid_1's auc: 0.885548\n",
      "[2000]\ttraining's auc: 0.911483\tvalid_1's auc: 0.893143\n",
      "[3000]\ttraining's auc: 0.917774\tvalid_1's auc: 0.896666\n",
      "[4000]\ttraining's auc: 0.922926\tvalid_1's auc: 0.899021\n",
      "[5000]\ttraining's auc: 0.927303\tvalid_1's auc: 0.900217\n",
      "[6000]\ttraining's auc: 0.931297\tvalid_1's auc: 0.900818\n",
      "[7000]\ttraining's auc: 0.934825\tvalid_1's auc: 0.901324\n",
      "[8000]\ttraining's auc: 0.938207\tvalid_1's auc: 0.901649\n",
      "[9000]\ttraining's auc: 0.941396\tvalid_1's auc: 0.901713\n",
      "[10000]\ttraining's auc: 0.944453\tvalid_1's auc: 0.901774\n",
      "[11000]\ttraining's auc: 0.947343\tvalid_1's auc: 0.901834\n",
      "[12000]\ttraining's auc: 0.950157\tvalid_1's auc: 0.901812\n",
      "Early stopping, best iteration is:\n",
      "[11340]\ttraining's auc: 0.948321\tvalid_1's auc: 0.901924\n",
      "var_34\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902645\tvalid_1's auc: 0.885166\n",
      "[2000]\ttraining's auc: 0.911864\tvalid_1's auc: 0.893087\n",
      "[3000]\ttraining's auc: 0.918034\tvalid_1's auc: 0.896566\n",
      "[4000]\ttraining's auc: 0.923112\tvalid_1's auc: 0.898823\n",
      "[5000]\ttraining's auc: 0.927441\tvalid_1's auc: 0.899967\n",
      "[6000]\ttraining's auc: 0.931431\tvalid_1's auc: 0.900649\n",
      "[7000]\ttraining's auc: 0.934955\tvalid_1's auc: 0.901127\n",
      "[8000]\ttraining's auc: 0.938275\tvalid_1's auc: 0.901405\n",
      "[9000]\ttraining's auc: 0.941442\tvalid_1's auc: 0.901444\n",
      "Early stopping, best iteration is:\n",
      "[8837]\ttraining's auc: 0.940944\tvalid_1's auc: 0.901484\n",
      "var_35\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.90249\tvalid_1's auc: 0.885264\n",
      "[2000]\ttraining's auc: 0.911692\tvalid_1's auc: 0.893343\n",
      "[3000]\ttraining's auc: 0.917892\tvalid_1's auc: 0.896745\n",
      "[4000]\ttraining's auc: 0.92298\tvalid_1's auc: 0.899013\n",
      "[5000]\ttraining's auc: 0.927292\tvalid_1's auc: 0.900101\n",
      "[6000]\ttraining's auc: 0.931306\tvalid_1's auc: 0.900737\n",
      "[7000]\ttraining's auc: 0.934827\tvalid_1's auc: 0.90121\n",
      "[8000]\ttraining's auc: 0.938185\tvalid_1's auc: 0.901472\n",
      "[9000]\ttraining's auc: 0.941348\tvalid_1's auc: 0.901519\n",
      "Early stopping, best iteration is:\n",
      "[8789]\ttraining's auc: 0.940696\tvalid_1's auc: 0.901596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_36\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902794\tvalid_1's auc: 0.885655\n",
      "[2000]\ttraining's auc: 0.911975\tvalid_1's auc: 0.893661\n",
      "[3000]\ttraining's auc: 0.918163\tvalid_1's auc: 0.897099\n",
      "[4000]\ttraining's auc: 0.923236\tvalid_1's auc: 0.899372\n",
      "[5000]\ttraining's auc: 0.92753\tvalid_1's auc: 0.900437\n",
      "[6000]\ttraining's auc: 0.931512\tvalid_1's auc: 0.901105\n",
      "[7000]\ttraining's auc: 0.935001\tvalid_1's auc: 0.901571\n",
      "[8000]\ttraining's auc: 0.938351\tvalid_1's auc: 0.901834\n",
      "[9000]\ttraining's auc: 0.941491\tvalid_1's auc: 0.901885\n",
      "Early stopping, best iteration is:\n",
      "[8836]\ttraining's auc: 0.94099\tvalid_1's auc: 0.901912\n",
      "var_37\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902824\tvalid_1's auc: 0.885686\n",
      "[2000]\ttraining's auc: 0.912\tvalid_1's auc: 0.893654\n",
      "[3000]\ttraining's auc: 0.918182\tvalid_1's auc: 0.897068\n",
      "[4000]\ttraining's auc: 0.923285\tvalid_1's auc: 0.899344\n",
      "[5000]\ttraining's auc: 0.927539\tvalid_1's auc: 0.900415\n",
      "[6000]\ttraining's auc: 0.931534\tvalid_1's auc: 0.901065\n",
      "[7000]\ttraining's auc: 0.935034\tvalid_1's auc: 0.901465\n",
      "[8000]\ttraining's auc: 0.938376\tvalid_1's auc: 0.901732\n",
      "[9000]\ttraining's auc: 0.941506\tvalid_1's auc: 0.901843\n",
      "Early stopping, best iteration is:\n",
      "[8837]\ttraining's auc: 0.941011\tvalid_1's auc: 0.901893\n",
      "var_38\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902778\tvalid_1's auc: 0.885664\n",
      "[2000]\ttraining's auc: 0.911978\tvalid_1's auc: 0.893645\n",
      "[3000]\ttraining's auc: 0.918179\tvalid_1's auc: 0.897101\n",
      "[4000]\ttraining's auc: 0.92327\tvalid_1's auc: 0.899409\n",
      "[5000]\ttraining's auc: 0.927554\tvalid_1's auc: 0.900514\n",
      "[6000]\ttraining's auc: 0.931547\tvalid_1's auc: 0.901201\n",
      "[7000]\ttraining's auc: 0.935046\tvalid_1's auc: 0.901606\n",
      "[8000]\ttraining's auc: 0.938379\tvalid_1's auc: 0.901859\n",
      "[9000]\ttraining's auc: 0.941521\tvalid_1's auc: 0.901901\n",
      "Early stopping, best iteration is:\n",
      "[8739]\ttraining's auc: 0.940725\tvalid_1's auc: 0.901946\n",
      "var_39\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902357\tvalid_1's auc: 0.884883\n",
      "[2000]\ttraining's auc: 0.911556\tvalid_1's auc: 0.892932\n",
      "[3000]\ttraining's auc: 0.917782\tvalid_1's auc: 0.896456\n",
      "[4000]\ttraining's auc: 0.922926\tvalid_1's auc: 0.898713\n",
      "[5000]\ttraining's auc: 0.927269\tvalid_1's auc: 0.899976\n",
      "[6000]\ttraining's auc: 0.931321\tvalid_1's auc: 0.900679\n",
      "[7000]\ttraining's auc: 0.934819\tvalid_1's auc: 0.901128\n",
      "[8000]\ttraining's auc: 0.938178\tvalid_1's auc: 0.901359\n",
      "[9000]\ttraining's auc: 0.941367\tvalid_1's auc: 0.901475\n",
      "Early stopping, best iteration is:\n",
      "[8839]\ttraining's auc: 0.940862\tvalid_1's auc: 0.901514\n",
      "var_40\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902808\tvalid_1's auc: 0.885494\n",
      "[2000]\ttraining's auc: 0.91212\tvalid_1's auc: 0.893519\n",
      "[3000]\ttraining's auc: 0.918276\tvalid_1's auc: 0.896989\n",
      "[4000]\ttraining's auc: 0.923299\tvalid_1's auc: 0.899251\n",
      "[5000]\ttraining's auc: 0.927573\tvalid_1's auc: 0.900355\n",
      "[6000]\ttraining's auc: 0.931573\tvalid_1's auc: 0.901041\n",
      "[7000]\ttraining's auc: 0.935067\tvalid_1's auc: 0.901468\n",
      "[8000]\ttraining's auc: 0.938423\tvalid_1's auc: 0.901619\n",
      "[9000]\ttraining's auc: 0.941581\tvalid_1's auc: 0.901673\n",
      "Early stopping, best iteration is:\n",
      "[8836]\ttraining's auc: 0.941075\tvalid_1's auc: 0.901719\n",
      "var_41\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902817\tvalid_1's auc: 0.885484\n",
      "[2000]\ttraining's auc: 0.912126\tvalid_1's auc: 0.893543\n",
      "[3000]\ttraining's auc: 0.918292\tvalid_1's auc: 0.896983\n",
      "[4000]\ttraining's auc: 0.923328\tvalid_1's auc: 0.899241\n",
      "[5000]\ttraining's auc: 0.927601\tvalid_1's auc: 0.900347\n",
      "[6000]\ttraining's auc: 0.931606\tvalid_1's auc: 0.901082\n",
      "[7000]\ttraining's auc: 0.935091\tvalid_1's auc: 0.901529\n",
      "[8000]\ttraining's auc: 0.938425\tvalid_1's auc: 0.901717\n",
      "[9000]\ttraining's auc: 0.941574\tvalid_1's auc: 0.901767\n",
      "Early stopping, best iteration is:\n",
      "[8837]\ttraining's auc: 0.941078\tvalid_1's auc: 0.90181\n",
      "var_42\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.90247\tvalid_1's auc: 0.885479\n",
      "[2000]\ttraining's auc: 0.91187\tvalid_1's auc: 0.893569\n",
      "[3000]\ttraining's auc: 0.918053\tvalid_1's auc: 0.896935\n",
      "[4000]\ttraining's auc: 0.923107\tvalid_1's auc: 0.899172\n",
      "[5000]\ttraining's auc: 0.92747\tvalid_1's auc: 0.900319\n",
      "[6000]\ttraining's auc: 0.931446\tvalid_1's auc: 0.901057\n",
      "[7000]\ttraining's auc: 0.93492\tvalid_1's auc: 0.901512\n",
      "[8000]\ttraining's auc: 0.93823\tvalid_1's auc: 0.901736\n",
      "[9000]\ttraining's auc: 0.941428\tvalid_1's auc: 0.901791\n",
      "Early stopping, best iteration is:\n",
      "[8836]\ttraining's auc: 0.940921\tvalid_1's auc: 0.901854\n",
      "var_43\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902178\tvalid_1's auc: 0.884525\n",
      "[2000]\ttraining's auc: 0.911549\tvalid_1's auc: 0.892766\n",
      "[3000]\ttraining's auc: 0.917968\tvalid_1's auc: 0.896216\n",
      "[4000]\ttraining's auc: 0.923099\tvalid_1's auc: 0.898554\n",
      "[5000]\ttraining's auc: 0.927472\tvalid_1's auc: 0.899719\n",
      "[6000]\ttraining's auc: 0.931469\tvalid_1's auc: 0.900468\n",
      "[7000]\ttraining's auc: 0.934998\tvalid_1's auc: 0.900916\n",
      "[8000]\ttraining's auc: 0.938354\tvalid_1's auc: 0.901184\n",
      "[9000]\ttraining's auc: 0.941553\tvalid_1's auc: 0.901214\n",
      "Early stopping, best iteration is:\n",
      "[8837]\ttraining's auc: 0.941044\tvalid_1's auc: 0.901258\n",
      "var_44\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902547\tvalid_1's auc: 0.885353\n",
      "[2000]\ttraining's auc: 0.911843\tvalid_1's auc: 0.893552\n",
      "[3000]\ttraining's auc: 0.918124\tvalid_1's auc: 0.896941\n",
      "[4000]\ttraining's auc: 0.923146\tvalid_1's auc: 0.899071\n",
      "[5000]\ttraining's auc: 0.927452\tvalid_1's auc: 0.900088\n",
      "[6000]\ttraining's auc: 0.93148\tvalid_1's auc: 0.900831\n",
      "[7000]\ttraining's auc: 0.934983\tvalid_1's auc: 0.901328\n",
      "[8000]\ttraining's auc: 0.938321\tvalid_1's auc: 0.901559\n",
      "[9000]\ttraining's auc: 0.941534\tvalid_1's auc: 0.901589\n",
      "Early stopping, best iteration is:\n",
      "[8836]\ttraining's auc: 0.941019\tvalid_1's auc: 0.901634\n",
      "var_45\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902669\tvalid_1's auc: 0.885363\n",
      "[2000]\ttraining's auc: 0.911978\tvalid_1's auc: 0.893567\n",
      "[3000]\ttraining's auc: 0.918266\tvalid_1's auc: 0.897006\n",
      "[4000]\ttraining's auc: 0.923322\tvalid_1's auc: 0.899224\n",
      "[5000]\ttraining's auc: 0.927631\tvalid_1's auc: 0.900314\n",
      "[6000]\ttraining's auc: 0.931609\tvalid_1's auc: 0.901046\n",
      "[7000]\ttraining's auc: 0.935074\tvalid_1's auc: 0.901541\n",
      "[8000]\ttraining's auc: 0.938404\tvalid_1's auc: 0.90174\n",
      "[9000]\ttraining's auc: 0.941603\tvalid_1's auc: 0.901747\n",
      "Early stopping, best iteration is:\n",
      "[8361]\ttraining's auc: 0.939565\tvalid_1's auc: 0.901836\n",
      "var_46\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902698\tvalid_1's auc: 0.885354\n",
      "[2000]\ttraining's auc: 0.911974\tvalid_1's auc: 0.893517\n",
      "[3000]\ttraining's auc: 0.918245\tvalid_1's auc: 0.896921\n",
      "[4000]\ttraining's auc: 0.923306\tvalid_1's auc: 0.899094\n",
      "[5000]\ttraining's auc: 0.927634\tvalid_1's auc: 0.900154\n",
      "[6000]\ttraining's auc: 0.931646\tvalid_1's auc: 0.900932\n",
      "[7000]\ttraining's auc: 0.935116\tvalid_1's auc: 0.901408\n",
      "[8000]\ttraining's auc: 0.938467\tvalid_1's auc: 0.901607\n",
      "[9000]\ttraining's auc: 0.941674\tvalid_1's auc: 0.901637\n",
      "Early stopping, best iteration is:\n",
      "[8836]\ttraining's auc: 0.941164\tvalid_1's auc: 0.901688\n",
      "var_47\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902362\tvalid_1's auc: 0.885039\n",
      "[2000]\ttraining's auc: 0.911632\tvalid_1's auc: 0.893208\n",
      "[3000]\ttraining's auc: 0.91793\tvalid_1's auc: 0.896638\n",
      "[4000]\ttraining's auc: 0.92301\tvalid_1's auc: 0.898859\n",
      "[5000]\ttraining's auc: 0.927387\tvalid_1's auc: 0.899991\n",
      "[6000]\ttraining's auc: 0.931401\tvalid_1's auc: 0.900765\n",
      "[7000]\ttraining's auc: 0.934865\tvalid_1's auc: 0.901304\n",
      "[8000]\ttraining's auc: 0.938227\tvalid_1's auc: 0.901538\n",
      "[9000]\ttraining's auc: 0.941443\tvalid_1's auc: 0.901576\n",
      "[10000]\ttraining's auc: 0.944512\tvalid_1's auc: 0.901567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9335]\ttraining's auc: 0.942495\tvalid_1's auc: 0.901619\n",
      "var_48\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902286\tvalid_1's auc: 0.885079\n",
      "[2000]\ttraining's auc: 0.911636\tvalid_1's auc: 0.893309\n",
      "[3000]\ttraining's auc: 0.917973\tvalid_1's auc: 0.89673\n",
      "[4000]\ttraining's auc: 0.923037\tvalid_1's auc: 0.898898\n",
      "[5000]\ttraining's auc: 0.927442\tvalid_1's auc: 0.900005\n",
      "[6000]\ttraining's auc: 0.931448\tvalid_1's auc: 0.900786\n",
      "[7000]\ttraining's auc: 0.934967\tvalid_1's auc: 0.901228\n",
      "[8000]\ttraining's auc: 0.938298\tvalid_1's auc: 0.901464\n",
      "[9000]\ttraining's auc: 0.941465\tvalid_1's auc: 0.901537\n",
      "[10000]\ttraining's auc: 0.944558\tvalid_1's auc: 0.901654\n",
      "[11000]\ttraining's auc: 0.947418\tvalid_1's auc: 0.901674\n",
      "[12000]\ttraining's auc: 0.950233\tvalid_1's auc: 0.901667\n",
      "Early stopping, best iteration is:\n",
      "[11344]\ttraining's auc: 0.948396\tvalid_1's auc: 0.901764\n",
      "var_49\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902582\tvalid_1's auc: 0.885387\n",
      "[2000]\ttraining's auc: 0.911942\tvalid_1's auc: 0.893473\n",
      "[3000]\ttraining's auc: 0.918224\tvalid_1's auc: 0.896823\n",
      "[4000]\ttraining's auc: 0.923281\tvalid_1's auc: 0.899092\n",
      "[5000]\ttraining's auc: 0.927603\tvalid_1's auc: 0.900113\n",
      "[6000]\ttraining's auc: 0.931619\tvalid_1's auc: 0.900843\n",
      "[7000]\ttraining's auc: 0.935103\tvalid_1's auc: 0.90138\n",
      "[8000]\ttraining's auc: 0.938415\tvalid_1's auc: 0.90162\n",
      "[9000]\ttraining's auc: 0.94161\tvalid_1's auc: 0.901566\n",
      "Early stopping, best iteration is:\n",
      "[8230]\ttraining's auc: 0.939173\tvalid_1's auc: 0.901653\n",
      "var_50\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902237\tvalid_1's auc: 0.885481\n",
      "[2000]\ttraining's auc: 0.911683\tvalid_1's auc: 0.893542\n",
      "[3000]\ttraining's auc: 0.918035\tvalid_1's auc: 0.896826\n",
      "[4000]\ttraining's auc: 0.923161\tvalid_1's auc: 0.899044\n",
      "[5000]\ttraining's auc: 0.927494\tvalid_1's auc: 0.900018\n",
      "[6000]\ttraining's auc: 0.931508\tvalid_1's auc: 0.900761\n",
      "[7000]\ttraining's auc: 0.935025\tvalid_1's auc: 0.901314\n",
      "[8000]\ttraining's auc: 0.938334\tvalid_1's auc: 0.901471\n",
      "[9000]\ttraining's auc: 0.941537\tvalid_1's auc: 0.901516\n",
      "Early stopping, best iteration is:\n",
      "[8230]\ttraining's auc: 0.9391\tvalid_1's auc: 0.901546\n",
      "var_51\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902333\tvalid_1's auc: 0.885107\n",
      "[2000]\ttraining's auc: 0.911714\tvalid_1's auc: 0.893422\n",
      "[3000]\ttraining's auc: 0.91806\tvalid_1's auc: 0.896624\n",
      "[4000]\ttraining's auc: 0.92314\tvalid_1's auc: 0.898883\n",
      "[5000]\ttraining's auc: 0.927508\tvalid_1's auc: 0.899951\n",
      "[6000]\ttraining's auc: 0.931522\tvalid_1's auc: 0.900669\n",
      "[7000]\ttraining's auc: 0.935059\tvalid_1's auc: 0.901225\n",
      "[8000]\ttraining's auc: 0.938406\tvalid_1's auc: 0.901447\n",
      "[9000]\ttraining's auc: 0.941627\tvalid_1's auc: 0.901426\n",
      "Early stopping, best iteration is:\n",
      "[8334]\ttraining's auc: 0.939506\tvalid_1's auc: 0.901498\n",
      "var_52\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901965\tvalid_1's auc: 0.885103\n",
      "[2000]\ttraining's auc: 0.911311\tvalid_1's auc: 0.893222\n",
      "[3000]\ttraining's auc: 0.917712\tvalid_1's auc: 0.896675\n",
      "[4000]\ttraining's auc: 0.922837\tvalid_1's auc: 0.898991\n",
      "[5000]\ttraining's auc: 0.927206\tvalid_1's auc: 0.899988\n",
      "[6000]\ttraining's auc: 0.931243\tvalid_1's auc: 0.900762\n",
      "[7000]\ttraining's auc: 0.934788\tvalid_1's auc: 0.901394\n",
      "[8000]\ttraining's auc: 0.938131\tvalid_1's auc: 0.901715\n",
      "[9000]\ttraining's auc: 0.941363\tvalid_1's auc: 0.901767\n",
      "Early stopping, best iteration is:\n",
      "[8839]\ttraining's auc: 0.940868\tvalid_1's auc: 0.901819\n",
      "var_53\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902533\tvalid_1's auc: 0.885384\n",
      "[2000]\ttraining's auc: 0.911861\tvalid_1's auc: 0.893568\n",
      "[3000]\ttraining's auc: 0.91821\tvalid_1's auc: 0.896913\n",
      "[4000]\ttraining's auc: 0.923264\tvalid_1's auc: 0.899158\n",
      "[5000]\ttraining's auc: 0.927555\tvalid_1's auc: 0.900169\n",
      "[6000]\ttraining's auc: 0.931593\tvalid_1's auc: 0.900924\n",
      "[7000]\ttraining's auc: 0.935077\tvalid_1's auc: 0.901462\n",
      "[8000]\ttraining's auc: 0.938387\tvalid_1's auc: 0.901679\n",
      "[9000]\ttraining's auc: 0.941586\tvalid_1's auc: 0.901676\n",
      "Early stopping, best iteration is:\n",
      "[8113]\ttraining's auc: 0.938771\tvalid_1's auc: 0.901717\n",
      "var_54\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902463\tvalid_1's auc: 0.885371\n",
      "[2000]\ttraining's auc: 0.911781\tvalid_1's auc: 0.893618\n",
      "[3000]\ttraining's auc: 0.9181\tvalid_1's auc: 0.896902\n",
      "[4000]\ttraining's auc: 0.923171\tvalid_1's auc: 0.899053\n",
      "[5000]\ttraining's auc: 0.927493\tvalid_1's auc: 0.900111\n",
      "[6000]\ttraining's auc: 0.931544\tvalid_1's auc: 0.900847\n",
      "[7000]\ttraining's auc: 0.935025\tvalid_1's auc: 0.901374\n",
      "[8000]\ttraining's auc: 0.93837\tvalid_1's auc: 0.901602\n",
      "[9000]\ttraining's auc: 0.941591\tvalid_1's auc: 0.901608\n",
      "Early stopping, best iteration is:\n",
      "[8239]\ttraining's auc: 0.939174\tvalid_1's auc: 0.901669\n",
      "var_55\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902135\tvalid_1's auc: 0.88462\n",
      "[2000]\ttraining's auc: 0.911598\tvalid_1's auc: 0.892945\n",
      "[3000]\ttraining's auc: 0.917962\tvalid_1's auc: 0.896256\n",
      "[4000]\ttraining's auc: 0.923045\tvalid_1's auc: 0.898591\n",
      "[5000]\ttraining's auc: 0.927379\tvalid_1's auc: 0.899617\n",
      "[6000]\ttraining's auc: 0.931407\tvalid_1's auc: 0.900433\n",
      "[7000]\ttraining's auc: 0.934945\tvalid_1's auc: 0.901001\n",
      "[8000]\ttraining's auc: 0.93827\tvalid_1's auc: 0.90126\n",
      "[9000]\ttraining's auc: 0.941455\tvalid_1's auc: 0.901213\n",
      "Early stopping, best iteration is:\n",
      "[8060]\ttraining's auc: 0.938473\tvalid_1's auc: 0.901297\n",
      "var_56\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902566\tvalid_1's auc: 0.885256\n",
      "[2000]\ttraining's auc: 0.911941\tvalid_1's auc: 0.893456\n",
      "[3000]\ttraining's auc: 0.918243\tvalid_1's auc: 0.8967\n",
      "[4000]\ttraining's auc: 0.923276\tvalid_1's auc: 0.898931\n",
      "[5000]\ttraining's auc: 0.927629\tvalid_1's auc: 0.900087\n",
      "[6000]\ttraining's auc: 0.931605\tvalid_1's auc: 0.900814\n",
      "[7000]\ttraining's auc: 0.93509\tvalid_1's auc: 0.901279\n",
      "[8000]\ttraining's auc: 0.938394\tvalid_1's auc: 0.901557\n",
      "[9000]\ttraining's auc: 0.941567\tvalid_1's auc: 0.901556\n",
      "Early stopping, best iteration is:\n",
      "[8066]\ttraining's auc: 0.938612\tvalid_1's auc: 0.901609\n",
      "var_57\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902534\tvalid_1's auc: 0.885154\n",
      "[2000]\ttraining's auc: 0.911866\tvalid_1's auc: 0.893289\n",
      "[3000]\ttraining's auc: 0.918176\tvalid_1's auc: 0.89657\n",
      "[4000]\ttraining's auc: 0.92322\tvalid_1's auc: 0.898827\n",
      "[5000]\ttraining's auc: 0.927576\tvalid_1's auc: 0.899909\n",
      "[6000]\ttraining's auc: 0.931547\tvalid_1's auc: 0.900615\n",
      "[7000]\ttraining's auc: 0.935056\tvalid_1's auc: 0.901046\n",
      "[8000]\ttraining's auc: 0.938386\tvalid_1's auc: 0.901319\n",
      "[9000]\ttraining's auc: 0.941565\tvalid_1's auc: 0.901315\n",
      "Early stopping, best iteration is:\n",
      "[8239]\ttraining's auc: 0.939177\tvalid_1's auc: 0.90138\n",
      "var_58\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902664\tvalid_1's auc: 0.885326\n",
      "[2000]\ttraining's auc: 0.912038\tvalid_1's auc: 0.893465\n",
      "[3000]\ttraining's auc: 0.918295\tvalid_1's auc: 0.896764\n",
      "[4000]\ttraining's auc: 0.92333\tvalid_1's auc: 0.899047\n",
      "[5000]\ttraining's auc: 0.927635\tvalid_1's auc: 0.900195\n",
      "[6000]\ttraining's auc: 0.931611\tvalid_1's auc: 0.900902\n",
      "[7000]\ttraining's auc: 0.93511\tvalid_1's auc: 0.901391\n",
      "[8000]\ttraining's auc: 0.938451\tvalid_1's auc: 0.901649\n",
      "[9000]\ttraining's auc: 0.941627\tvalid_1's auc: 0.901628\n",
      "Early stopping, best iteration is:\n",
      "[8059]\ttraining's auc: 0.938644\tvalid_1's auc: 0.901682\n",
      "var_59\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902662\tvalid_1's auc: 0.885263\n",
      "[2000]\ttraining's auc: 0.911986\tvalid_1's auc: 0.893478\n",
      "[3000]\ttraining's auc: 0.918286\tvalid_1's auc: 0.896798\n",
      "[4000]\ttraining's auc: 0.923318\tvalid_1's auc: 0.899051\n",
      "[5000]\ttraining's auc: 0.927646\tvalid_1's auc: 0.900196\n",
      "[6000]\ttraining's auc: 0.931642\tvalid_1's auc: 0.900943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7000]\ttraining's auc: 0.93511\tvalid_1's auc: 0.90145\n",
      "[8000]\ttraining's auc: 0.938426\tvalid_1's auc: 0.90162\n",
      "Early stopping, best iteration is:\n",
      "[7694]\ttraining's auc: 0.937428\tvalid_1's auc: 0.901668\n",
      "var_60\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902631\tvalid_1's auc: 0.885272\n",
      "[2000]\ttraining's auc: 0.912001\tvalid_1's auc: 0.893462\n",
      "[3000]\ttraining's auc: 0.918303\tvalid_1's auc: 0.896769\n",
      "[4000]\ttraining's auc: 0.923335\tvalid_1's auc: 0.899019\n",
      "[5000]\ttraining's auc: 0.927705\tvalid_1's auc: 0.90014\n",
      "[6000]\ttraining's auc: 0.931665\tvalid_1's auc: 0.900901\n",
      "[7000]\ttraining's auc: 0.935109\tvalid_1's auc: 0.901403\n",
      "[8000]\ttraining's auc: 0.938453\tvalid_1's auc: 0.901634\n",
      "[9000]\ttraining's auc: 0.941623\tvalid_1's auc: 0.901613\n",
      "Early stopping, best iteration is:\n",
      "[8219]\ttraining's auc: 0.939171\tvalid_1's auc: 0.901677\n",
      "var_61\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902673\tvalid_1's auc: 0.885235\n",
      "[2000]\ttraining's auc: 0.911996\tvalid_1's auc: 0.893341\n",
      "[3000]\ttraining's auc: 0.918281\tvalid_1's auc: 0.896681\n",
      "[4000]\ttraining's auc: 0.923342\tvalid_1's auc: 0.898886\n",
      "[5000]\ttraining's auc: 0.927694\tvalid_1's auc: 0.90005\n",
      "[6000]\ttraining's auc: 0.931688\tvalid_1's auc: 0.90084\n",
      "[7000]\ttraining's auc: 0.935175\tvalid_1's auc: 0.901403\n",
      "[8000]\ttraining's auc: 0.938514\tvalid_1's auc: 0.901589\n",
      "[9000]\ttraining's auc: 0.941666\tvalid_1's auc: 0.901554\n",
      "Early stopping, best iteration is:\n",
      "[8066]\ttraining's auc: 0.938733\tvalid_1's auc: 0.901644\n",
      "var_62\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902687\tvalid_1's auc: 0.885336\n",
      "[2000]\ttraining's auc: 0.911995\tvalid_1's auc: 0.893398\n",
      "[3000]\ttraining's auc: 0.91827\tvalid_1's auc: 0.896746\n",
      "[4000]\ttraining's auc: 0.923279\tvalid_1's auc: 0.898986\n",
      "[5000]\ttraining's auc: 0.927634\tvalid_1's auc: 0.900088\n",
      "[6000]\ttraining's auc: 0.931646\tvalid_1's auc: 0.900789\n",
      "[7000]\ttraining's auc: 0.935115\tvalid_1's auc: 0.901308\n",
      "[8000]\ttraining's auc: 0.938453\tvalid_1's auc: 0.901452\n",
      "[9000]\ttraining's auc: 0.941637\tvalid_1's auc: 0.901451\n",
      "Early stopping, best iteration is:\n",
      "[8099]\ttraining's auc: 0.938776\tvalid_1's auc: 0.90153\n",
      "var_63\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902693\tvalid_1's auc: 0.885391\n",
      "[2000]\ttraining's auc: 0.911991\tvalid_1's auc: 0.893526\n",
      "[3000]\ttraining's auc: 0.918265\tvalid_1's auc: 0.896841\n",
      "[4000]\ttraining's auc: 0.923301\tvalid_1's auc: 0.899083\n",
      "[5000]\ttraining's auc: 0.927629\tvalid_1's auc: 0.900151\n",
      "[6000]\ttraining's auc: 0.931629\tvalid_1's auc: 0.900887\n",
      "[7000]\ttraining's auc: 0.935126\tvalid_1's auc: 0.901449\n",
      "[8000]\ttraining's auc: 0.938435\tvalid_1's auc: 0.90169\n",
      "[9000]\ttraining's auc: 0.94161\tvalid_1's auc: 0.901666\n",
      "Early stopping, best iteration is:\n",
      "[8061]\ttraining's auc: 0.938628\tvalid_1's auc: 0.901758\n",
      "var_64\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902737\tvalid_1's auc: 0.88519\n",
      "[2000]\ttraining's auc: 0.912029\tvalid_1's auc: 0.893413\n",
      "[3000]\ttraining's auc: 0.918275\tvalid_1's auc: 0.896788\n",
      "[4000]\ttraining's auc: 0.923343\tvalid_1's auc: 0.899022\n",
      "[5000]\ttraining's auc: 0.927666\tvalid_1's auc: 0.900137\n",
      "[6000]\ttraining's auc: 0.931677\tvalid_1's auc: 0.900785\n",
      "[7000]\ttraining's auc: 0.93514\tvalid_1's auc: 0.901346\n",
      "[8000]\ttraining's auc: 0.938452\tvalid_1's auc: 0.901557\n",
      "[9000]\ttraining's auc: 0.941601\tvalid_1's auc: 0.901544\n",
      "Early stopping, best iteration is:\n",
      "[8061]\ttraining's auc: 0.938655\tvalid_1's auc: 0.901628\n",
      "var_65\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902608\tvalid_1's auc: 0.88524\n",
      "[2000]\ttraining's auc: 0.911911\tvalid_1's auc: 0.89336\n",
      "[3000]\ttraining's auc: 0.91818\tvalid_1's auc: 0.896662\n",
      "[4000]\ttraining's auc: 0.923223\tvalid_1's auc: 0.898938\n",
      "[5000]\ttraining's auc: 0.927553\tvalid_1's auc: 0.89998\n",
      "[6000]\ttraining's auc: 0.931582\tvalid_1's auc: 0.900726\n",
      "[7000]\ttraining's auc: 0.935075\tvalid_1's auc: 0.901253\n",
      "[8000]\ttraining's auc: 0.938417\tvalid_1's auc: 0.901463\n",
      "[9000]\ttraining's auc: 0.941607\tvalid_1's auc: 0.901432\n",
      "Early stopping, best iteration is:\n",
      "[8061]\ttraining's auc: 0.938608\tvalid_1's auc: 0.901512\n",
      "var_66\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902388\tvalid_1's auc: 0.885004\n",
      "[2000]\ttraining's auc: 0.911672\tvalid_1's auc: 0.89317\n",
      "[3000]\ttraining's auc: 0.91796\tvalid_1's auc: 0.896535\n",
      "[4000]\ttraining's auc: 0.923051\tvalid_1's auc: 0.898779\n",
      "[5000]\ttraining's auc: 0.927443\tvalid_1's auc: 0.899953\n",
      "[6000]\ttraining's auc: 0.931472\tvalid_1's auc: 0.90063\n",
      "[7000]\ttraining's auc: 0.934986\tvalid_1's auc: 0.901171\n",
      "[8000]\ttraining's auc: 0.938328\tvalid_1's auc: 0.901416\n",
      "[9000]\ttraining's auc: 0.94149\tvalid_1's auc: 0.901441\n",
      "[10000]\ttraining's auc: 0.944564\tvalid_1's auc: 0.901506\n",
      "[11000]\ttraining's auc: 0.947463\tvalid_1's auc: 0.901598\n",
      "[12000]\ttraining's auc: 0.950268\tvalid_1's auc: 0.901549\n",
      "Early stopping, best iteration is:\n",
      "[11340]\ttraining's auc: 0.948413\tvalid_1's auc: 0.901638\n",
      "var_67\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902733\tvalid_1's auc: 0.88541\n",
      "[2000]\ttraining's auc: 0.91199\tvalid_1's auc: 0.893392\n",
      "[3000]\ttraining's auc: 0.918233\tvalid_1's auc: 0.896634\n",
      "[4000]\ttraining's auc: 0.923231\tvalid_1's auc: 0.898868\n",
      "[5000]\ttraining's auc: 0.927589\tvalid_1's auc: 0.899899\n",
      "[6000]\ttraining's auc: 0.931606\tvalid_1's auc: 0.900575\n",
      "[7000]\ttraining's auc: 0.935082\tvalid_1's auc: 0.90115\n",
      "[8000]\ttraining's auc: 0.938419\tvalid_1's auc: 0.901371\n",
      "[9000]\ttraining's auc: 0.941571\tvalid_1's auc: 0.901394\n",
      "[10000]\ttraining's auc: 0.944647\tvalid_1's auc: 0.901409\n",
      "Early stopping, best iteration is:\n",
      "[9345]\ttraining's auc: 0.942675\tvalid_1's auc: 0.901438\n",
      "var_68\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902698\tvalid_1's auc: 0.885351\n",
      "[2000]\ttraining's auc: 0.911953\tvalid_1's auc: 0.893312\n",
      "[3000]\ttraining's auc: 0.918201\tvalid_1's auc: 0.896628\n",
      "[4000]\ttraining's auc: 0.923211\tvalid_1's auc: 0.898836\n",
      "[5000]\ttraining's auc: 0.927535\tvalid_1's auc: 0.899914\n",
      "[6000]\ttraining's auc: 0.93156\tvalid_1's auc: 0.900601\n",
      "[7000]\ttraining's auc: 0.935041\tvalid_1's auc: 0.901174\n",
      "[8000]\ttraining's auc: 0.938399\tvalid_1's auc: 0.901438\n",
      "[9000]\ttraining's auc: 0.941569\tvalid_1's auc: 0.901425\n",
      "Early stopping, best iteration is:\n",
      "[8219]\ttraining's auc: 0.939132\tvalid_1's auc: 0.901527\n",
      "var_69\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902504\tvalid_1's auc: 0.885295\n",
      "[2000]\ttraining's auc: 0.911748\tvalid_1's auc: 0.893358\n",
      "[3000]\ttraining's auc: 0.918005\tvalid_1's auc: 0.896699\n",
      "[4000]\ttraining's auc: 0.923082\tvalid_1's auc: 0.899062\n",
      "[5000]\ttraining's auc: 0.927476\tvalid_1's auc: 0.900091\n",
      "[6000]\ttraining's auc: 0.931495\tvalid_1's auc: 0.900739\n",
      "[7000]\ttraining's auc: 0.93499\tvalid_1's auc: 0.901337\n",
      "[8000]\ttraining's auc: 0.938327\tvalid_1's auc: 0.901639\n",
      "[9000]\ttraining's auc: 0.941501\tvalid_1's auc: 0.901669\n",
      "Early stopping, best iteration is:\n",
      "[8975]\ttraining's auc: 0.94143\tvalid_1's auc: 0.901692\n",
      "var_70\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902305\tvalid_1's auc: 0.885151\n",
      "[2000]\ttraining's auc: 0.911612\tvalid_1's auc: 0.893268\n",
      "[3000]\ttraining's auc: 0.917911\tvalid_1's auc: 0.89656\n",
      "[4000]\ttraining's auc: 0.922928\tvalid_1's auc: 0.898885\n",
      "[5000]\ttraining's auc: 0.927323\tvalid_1's auc: 0.899863\n",
      "[6000]\ttraining's auc: 0.931356\tvalid_1's auc: 0.900512\n",
      "[7000]\ttraining's auc: 0.934911\tvalid_1's auc: 0.901003\n",
      "[8000]\ttraining's auc: 0.938284\tvalid_1's auc: 0.901285\n",
      "[9000]\ttraining's auc: 0.94148\tvalid_1's auc: 0.901302\n",
      "Early stopping, best iteration is:\n",
      "[8060]\ttraining's auc: 0.938482\tvalid_1's auc: 0.901348\n",
      "var_71\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902669\tvalid_1's auc: 0.885514\n",
      "[2000]\ttraining's auc: 0.911986\tvalid_1's auc: 0.89364\n",
      "[3000]\ttraining's auc: 0.918229\tvalid_1's auc: 0.896966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000]\ttraining's auc: 0.923251\tvalid_1's auc: 0.899217\n",
      "[5000]\ttraining's auc: 0.927601\tvalid_1's auc: 0.900126\n",
      "[6000]\ttraining's auc: 0.931606\tvalid_1's auc: 0.900798\n",
      "[7000]\ttraining's auc: 0.935103\tvalid_1's auc: 0.901303\n",
      "[8000]\ttraining's auc: 0.938419\tvalid_1's auc: 0.901527\n",
      "[9000]\ttraining's auc: 0.941581\tvalid_1's auc: 0.901579\n",
      "Early stopping, best iteration is:\n",
      "[8102]\ttraining's auc: 0.938762\tvalid_1's auc: 0.901601\n",
      "var_72\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902704\tvalid_1's auc: 0.885466\n",
      "[2000]\ttraining's auc: 0.912026\tvalid_1's auc: 0.893557\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-474-77dd631b1473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mX3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for col in list(X1.columns):\n",
    "    if col in ['var_0', 'var_1', 'var_2']:\n",
    "        continue\n",
    "    X3 = X1.copy()\n",
    "    \n",
    "    bins = np.linspace(-6.0,6.0,40)\n",
    "    X = pd.get_dummies(pd.cut(X1[col].values, bins))\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    X3 = pd.concat([X3, X], axis=1)\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X3, y, test_size=0.2, stratify=y, random_state=123)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "   \n",
    "    print(col)\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "f_cats = list(X1.columns)\n",
    "for f in f_cats:\n",
    "    X3[f + \"_avg\"], X4[f + \"_avg\"] = target_encode(trn_series=X3[f],\n",
    "                                         tst_series=X4[f],\n",
    "                                         target=y,\n",
    "                                         min_samples_leaf=200,\n",
    "                                         smoothing=10,\n",
    "                                         noise_level=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Wed Mar 20 13:03:31 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.8652\n",
      "Fold 1 started at Wed Mar 20 13:04:17 2019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-484-c39f5a9d0a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n\u001b[0;32m----> 5\u001b[0;31m                                          model_type='lgb', plot_feature_importance=False)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-465-b3e7949fa4da>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, averaging, model)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 early_stopping_rounds=3000)\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;31m#             y_pred_valid = model.predict(X_valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m#             y_pred = model.predict(X_test, num_iteration=model.best_iteration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# construct booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[0;32m-> 1552\u001b[0;31m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m                 ctypes.byref(self.handle)))\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    999\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[1;32m   1002\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_np2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_list_np2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init_from_np2d\u001b[0;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0mref_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m             ctypes.byref(self.handle)))\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_encode(train_data, test_data, columns, target_col, reg_method=None,\n",
    "                alpha=0, add_random=False, rmean=0, rstd=0.1, folds=1, myfolds=folds):\n",
    "    '''Returns a DataFrame with encoded columns'''\n",
    "    encoded_cols = []\n",
    "    target_mean_global = train_data[target_col].mean()\n",
    "    for col in columns:\n",
    "        # Getting means for test data\n",
    "        nrows_cat = train_data.groupby(col)[target_col].count()\n",
    "        target_means_cats = train_data.groupby(col)[target_col].mean()\n",
    "        target_means_cats_adj = (target_means_cats*nrows_cat + \n",
    "                                 target_mean_global*alpha)/(nrows_cat+alpha)\n",
    "        # Mapping means to test data\n",
    "        encoded_col_test = test_data[col].map(target_means_cats_adj)\n",
    "        # Getting a train encodings\n",
    "        if reg_method == 'expanding_mean':\n",
    "            train_data_shuffled = train_data.sample(frac=1, random_state=1)\n",
    "            cumsum = train_data_shuffled.groupby(col)[target_col].cumsum() - train_data_shuffled[target_col]\n",
    "            cumcnt = train_data_shuffled.groupby(col).cumcount()\n",
    "            encoded_col_train = cumsum/(cumcnt)\n",
    "            encoded_col_train.fillna(target_mean_global, inplace=True)\n",
    "            if add_random:\n",
    "                encoded_col_train = encoded_col_train + normal(loc=rmean, scale=rstd, \n",
    "                                                               size=(encoded_col_train.shape[0]))\n",
    "        elif (reg_method == 'k_fold') and (folds > 1):\n",
    "#             kfold = StratifiedKFold(train_data[target_col].values, folds, shuffle=True, random_state=1)\n",
    "            kfold = myfolds\n",
    "            parts = []\n",
    "            for tr_in, val_ind in kfold:\n",
    "                # divide data\n",
    "                df_for_estimation, df_estimated = train_data.iloc[tr_in], train_data.iloc[val_ind]\n",
    "                # getting means on data for estimation (all folds except estimated)\n",
    "                nrows_cat = df_for_estimation.groupby(col)[target_col].count()\n",
    "                target_means_cats = df_for_estimation.groupby(col)[target_col].mean()\n",
    "                target_means_cats_adj = (target_means_cats*nrows_cat + \n",
    "                                         target_mean_global*alpha)/(nrows_cat+alpha)\n",
    "                # Mapping means to estimated fold\n",
    "                encoded_col_train_part = df_estimated[col].map(target_means_cats_adj)\n",
    "                if add_random:\n",
    "                    encoded_col_train_part = encoded_col_train_part + normal(loc=rmean, scale=rstd, \n",
    "                                                                             size=(encoded_col_train_part.shape[0]))\n",
    "                # Saving estimated encodings for a fold\n",
    "                parts.append(encoded_col_train_part)\n",
    "            encoded_col_train = pd.concat(parts, axis=0)\n",
    "            encoded_col_train.fillna(target_mean_global, inplace=True)\n",
    "        else:\n",
    "            encoded_col_train = train_data[col].map(target_means_cats_adj)\n",
    "            if add_random:\n",
    "                encoded_col_train = encoded_col_train + normal(loc=rmean, scale=rstd, \n",
    "                                                               size=(encoded_col_train.shape[0]))\n",
    "\n",
    "        # Saving the column with means\n",
    "        encoded_col = pd.concat([encoded_col_train, encoded_col_test], axis=0)\n",
    "        encoded_col[encoded_col.isnull()] = target_mean_global\n",
    "        encoded_cols.append(pd.DataFrame({'mean_'+target_col+'_'+col:encoded_col}))\n",
    "    all_encoded = pd.concat(encoded_cols, axis=1)\n",
    "    return (all_encoded.loc[train_data.index,:], \n",
    "            all_encoded.loc[test_data.index,:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-516-14058984d98f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m X3, X4 = mean_encode(X3, X4, f_cats, 'target', reg_method=None,\n\u001b[0;32m----> 6\u001b[0;31m                 alpha=2., add_random=True, rmean=0, rstd=0.1, folds=10, myfolds=folds)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-485-a7a41ca7e7c4>\u001b[0m in \u001b[0;36mmean_encode\u001b[0;34m(train_data, test_data, columns, target_col, reg_method, alpha, add_random, rmean, rstd, folds, myfolds)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mencoded_col_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_means_cats_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0madd_random\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 encoded_col_train = encoded_col_train + normal(loc=rmean, scale=rstd, \n\u001b[0m\u001b[1;32m     49\u001b[0m                                                                size=(encoded_col_train.shape[0]))\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normal' is not defined"
     ]
    }
   ],
   "source": [
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "X4.index = 200000 + X4.index\n",
    "X3['target'] = y\n",
    "X3, X4 = mean_encode(X3, X4, f_cats, 'target', reg_method=None,\n",
    "                alpha=2., add_random=False, rmean=0, rstd=0.1, folds=10, myfolds=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 201), (200000, 200))"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=200000, step=1)"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=200000, stop=400000, step=1)"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X4.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_target_var_0</th>\n",
       "      <th>mean_target_var_1</th>\n",
       "      <th>mean_target_var_2</th>\n",
       "      <th>mean_target_var_3</th>\n",
       "      <th>mean_target_var_4</th>\n",
       "      <th>mean_target_var_5</th>\n",
       "      <th>mean_target_var_6</th>\n",
       "      <th>mean_target_var_7</th>\n",
       "      <th>mean_target_var_8</th>\n",
       "      <th>mean_target_var_9</th>\n",
       "      <th>mean_target_var_10</th>\n",
       "      <th>mean_target_var_11</th>\n",
       "      <th>mean_target_var_12</th>\n",
       "      <th>mean_target_var_13</th>\n",
       "      <th>mean_target_var_14</th>\n",
       "      <th>mean_target_var_15</th>\n",
       "      <th>mean_target_var_16</th>\n",
       "      <th>mean_target_var_17</th>\n",
       "      <th>mean_target_var_18</th>\n",
       "      <th>mean_target_var_19</th>\n",
       "      <th>mean_target_var_20</th>\n",
       "      <th>mean_target_var_21</th>\n",
       "      <th>mean_target_var_22</th>\n",
       "      <th>mean_target_var_23</th>\n",
       "      <th>mean_target_var_24</th>\n",
       "      <th>mean_target_var_25</th>\n",
       "      <th>mean_target_var_26</th>\n",
       "      <th>mean_target_var_27</th>\n",
       "      <th>mean_target_var_28</th>\n",
       "      <th>mean_target_var_29</th>\n",
       "      <th>mean_target_var_30</th>\n",
       "      <th>mean_target_var_31</th>\n",
       "      <th>mean_target_var_32</th>\n",
       "      <th>mean_target_var_33</th>\n",
       "      <th>mean_target_var_34</th>\n",
       "      <th>mean_target_var_35</th>\n",
       "      <th>mean_target_var_36</th>\n",
       "      <th>mean_target_var_37</th>\n",
       "      <th>mean_target_var_38</th>\n",
       "      <th>mean_target_var_39</th>\n",
       "      <th>mean_target_var_40</th>\n",
       "      <th>mean_target_var_41</th>\n",
       "      <th>mean_target_var_42</th>\n",
       "      <th>mean_target_var_43</th>\n",
       "      <th>mean_target_var_44</th>\n",
       "      <th>mean_target_var_45</th>\n",
       "      <th>mean_target_var_46</th>\n",
       "      <th>mean_target_var_47</th>\n",
       "      <th>mean_target_var_48</th>\n",
       "      <th>mean_target_var_49</th>\n",
       "      <th>mean_target_var_50</th>\n",
       "      <th>mean_target_var_51</th>\n",
       "      <th>mean_target_var_52</th>\n",
       "      <th>mean_target_var_53</th>\n",
       "      <th>mean_target_var_54</th>\n",
       "      <th>mean_target_var_55</th>\n",
       "      <th>mean_target_var_56</th>\n",
       "      <th>mean_target_var_57</th>\n",
       "      <th>mean_target_var_58</th>\n",
       "      <th>mean_target_var_59</th>\n",
       "      <th>mean_target_var_60</th>\n",
       "      <th>mean_target_var_61</th>\n",
       "      <th>mean_target_var_62</th>\n",
       "      <th>mean_target_var_63</th>\n",
       "      <th>mean_target_var_64</th>\n",
       "      <th>mean_target_var_65</th>\n",
       "      <th>mean_target_var_66</th>\n",
       "      <th>mean_target_var_67</th>\n",
       "      <th>mean_target_var_68</th>\n",
       "      <th>mean_target_var_69</th>\n",
       "      <th>mean_target_var_70</th>\n",
       "      <th>mean_target_var_71</th>\n",
       "      <th>mean_target_var_72</th>\n",
       "      <th>mean_target_var_73</th>\n",
       "      <th>mean_target_var_74</th>\n",
       "      <th>mean_target_var_75</th>\n",
       "      <th>mean_target_var_76</th>\n",
       "      <th>mean_target_var_77</th>\n",
       "      <th>mean_target_var_78</th>\n",
       "      <th>mean_target_var_79</th>\n",
       "      <th>mean_target_var_80</th>\n",
       "      <th>mean_target_var_81</th>\n",
       "      <th>mean_target_var_82</th>\n",
       "      <th>mean_target_var_83</th>\n",
       "      <th>mean_target_var_84</th>\n",
       "      <th>mean_target_var_85</th>\n",
       "      <th>mean_target_var_86</th>\n",
       "      <th>mean_target_var_87</th>\n",
       "      <th>mean_target_var_88</th>\n",
       "      <th>mean_target_var_89</th>\n",
       "      <th>mean_target_var_90</th>\n",
       "      <th>mean_target_var_91</th>\n",
       "      <th>mean_target_var_92</th>\n",
       "      <th>mean_target_var_93</th>\n",
       "      <th>mean_target_var_94</th>\n",
       "      <th>mean_target_var_95</th>\n",
       "      <th>mean_target_var_96</th>\n",
       "      <th>mean_target_var_97</th>\n",
       "      <th>mean_target_var_98</th>\n",
       "      <th>mean_target_var_99</th>\n",
       "      <th>mean_target_var_100</th>\n",
       "      <th>mean_target_var_101</th>\n",
       "      <th>mean_target_var_102</th>\n",
       "      <th>mean_target_var_103</th>\n",
       "      <th>mean_target_var_104</th>\n",
       "      <th>mean_target_var_105</th>\n",
       "      <th>mean_target_var_106</th>\n",
       "      <th>mean_target_var_107</th>\n",
       "      <th>mean_target_var_108</th>\n",
       "      <th>mean_target_var_109</th>\n",
       "      <th>mean_target_var_110</th>\n",
       "      <th>mean_target_var_111</th>\n",
       "      <th>mean_target_var_112</th>\n",
       "      <th>mean_target_var_113</th>\n",
       "      <th>mean_target_var_114</th>\n",
       "      <th>mean_target_var_115</th>\n",
       "      <th>mean_target_var_116</th>\n",
       "      <th>mean_target_var_117</th>\n",
       "      <th>mean_target_var_118</th>\n",
       "      <th>mean_target_var_119</th>\n",
       "      <th>mean_target_var_120</th>\n",
       "      <th>mean_target_var_121</th>\n",
       "      <th>mean_target_var_122</th>\n",
       "      <th>mean_target_var_123</th>\n",
       "      <th>mean_target_var_124</th>\n",
       "      <th>mean_target_var_125</th>\n",
       "      <th>mean_target_var_126</th>\n",
       "      <th>mean_target_var_127</th>\n",
       "      <th>mean_target_var_128</th>\n",
       "      <th>mean_target_var_129</th>\n",
       "      <th>mean_target_var_130</th>\n",
       "      <th>mean_target_var_131</th>\n",
       "      <th>mean_target_var_132</th>\n",
       "      <th>mean_target_var_133</th>\n",
       "      <th>mean_target_var_134</th>\n",
       "      <th>mean_target_var_135</th>\n",
       "      <th>mean_target_var_136</th>\n",
       "      <th>mean_target_var_137</th>\n",
       "      <th>mean_target_var_138</th>\n",
       "      <th>mean_target_var_139</th>\n",
       "      <th>mean_target_var_140</th>\n",
       "      <th>mean_target_var_141</th>\n",
       "      <th>mean_target_var_142</th>\n",
       "      <th>mean_target_var_143</th>\n",
       "      <th>mean_target_var_144</th>\n",
       "      <th>mean_target_var_145</th>\n",
       "      <th>mean_target_var_146</th>\n",
       "      <th>mean_target_var_147</th>\n",
       "      <th>mean_target_var_148</th>\n",
       "      <th>mean_target_var_149</th>\n",
       "      <th>mean_target_var_150</th>\n",
       "      <th>mean_target_var_151</th>\n",
       "      <th>mean_target_var_152</th>\n",
       "      <th>mean_target_var_153</th>\n",
       "      <th>mean_target_var_154</th>\n",
       "      <th>mean_target_var_155</th>\n",
       "      <th>mean_target_var_156</th>\n",
       "      <th>mean_target_var_157</th>\n",
       "      <th>mean_target_var_158</th>\n",
       "      <th>mean_target_var_159</th>\n",
       "      <th>mean_target_var_160</th>\n",
       "      <th>mean_target_var_161</th>\n",
       "      <th>mean_target_var_162</th>\n",
       "      <th>mean_target_var_163</th>\n",
       "      <th>mean_target_var_164</th>\n",
       "      <th>mean_target_var_165</th>\n",
       "      <th>mean_target_var_166</th>\n",
       "      <th>mean_target_var_167</th>\n",
       "      <th>mean_target_var_168</th>\n",
       "      <th>mean_target_var_169</th>\n",
       "      <th>mean_target_var_170</th>\n",
       "      <th>mean_target_var_171</th>\n",
       "      <th>mean_target_var_172</th>\n",
       "      <th>mean_target_var_173</th>\n",
       "      <th>mean_target_var_174</th>\n",
       "      <th>mean_target_var_175</th>\n",
       "      <th>mean_target_var_176</th>\n",
       "      <th>mean_target_var_177</th>\n",
       "      <th>mean_target_var_178</th>\n",
       "      <th>mean_target_var_179</th>\n",
       "      <th>mean_target_var_180</th>\n",
       "      <th>mean_target_var_181</th>\n",
       "      <th>mean_target_var_182</th>\n",
       "      <th>mean_target_var_183</th>\n",
       "      <th>mean_target_var_184</th>\n",
       "      <th>mean_target_var_185</th>\n",
       "      <th>mean_target_var_186</th>\n",
       "      <th>mean_target_var_187</th>\n",
       "      <th>mean_target_var_188</th>\n",
       "      <th>mean_target_var_189</th>\n",
       "      <th>mean_target_var_190</th>\n",
       "      <th>mean_target_var_191</th>\n",
       "      <th>mean_target_var_192</th>\n",
       "      <th>mean_target_var_193</th>\n",
       "      <th>mean_target_var_194</th>\n",
       "      <th>mean_target_var_195</th>\n",
       "      <th>mean_target_var_196</th>\n",
       "      <th>mean_target_var_197</th>\n",
       "      <th>mean_target_var_198</th>\n",
       "      <th>mean_target_var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.059486</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.128039</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.010049</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.221464</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.120098</td>\n",
       "      <td>0.070646</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.100082</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.133442</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.016748</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.092383</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.244553</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.087306</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.070999</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.133442</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.064735</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.075061</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.137561</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.015460</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.103257</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.150020</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.120098</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.366830</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.228641</td>\n",
       "      <td>0.120098</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.120098</td>\n",
       "      <td>0.075061</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.080065</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.110552</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.120098</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.052217</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.070646</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.095695</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.022331</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.022331</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.080065</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.050245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.087520</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.022331</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.155592</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.100082</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.109180</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.057190</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.022331</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.013399</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.016748</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.092468</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.100045</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.080788</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.152428</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.120098</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.096999</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.100082</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.120098</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.103257</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.092383</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.157213</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.060049</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.110379</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.057190</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.063209</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.080065</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.366830</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040033</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.070646</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.063209</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.013399</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.075061</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.133442</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.133442</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.200061</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.080065</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.016748</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.275122</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.074604</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.033361</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.024020</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.010578</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.247116</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.078606</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.355664</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.037531</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.100082</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.100082</td>\n",
       "      <td>0.275122</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.128039</td>\n",
       "      <td>0.016748</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.157213</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.114321</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.068106</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.022331</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.100045</td>\n",
       "      <td>0.133442</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.213399</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.085784</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.022331</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.029292</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.100045</td>\n",
       "      <td>0.075061</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.120098</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.173366</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.200089</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.440196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.022331</td>\n",
       "      <td>0.078606</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.070646</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.133442</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.120098</td>\n",
       "      <td>0.244553</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.092383</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.016748</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.092468</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.016748</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.120098</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.126551</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.169306</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.440196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.127302</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.092383</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.048911</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.109180</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.109180</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.054590</td>\n",
       "      <td>0.012561</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.012561</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.188293</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.120098</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.016748</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.073366</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.244553</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.355664</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.275122</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.183415</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.091707</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.160049</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.070646</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.145499</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.013399</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.075061</td>\n",
       "      <td>0.064735</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.080065</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.290998</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.244553</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.089658</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.006090</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.022331</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.126551</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.109180</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.015460</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.109180</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.120028</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.290998</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.128039</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.016748</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.109180</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.085784</td>\n",
       "      <td>0.109180</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.016748</td>\n",
       "      <td>0.110049</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.100045</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.200061</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.200089</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.096999</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.152428</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.200163</td>\n",
       "      <td>0.081518</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.109180</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.050245</td>\n",
       "      <td>0.050245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_target_var_0  mean_target_var_1  mean_target_var_2  mean_target_var_3  \\\n",
       "0           0.025122           0.050245           0.040196           0.028711   \n",
       "1           0.050245           0.050245           0.050245           0.240196   \n",
       "2           0.033497           0.066993           0.028711           0.040196   \n",
       "3           0.040196           0.150122           0.025122           0.025122   \n",
       "4           0.040196           0.040196           0.050245           0.033497   \n",
       "\n",
       "   mean_target_var_4  mean_target_var_5  mean_target_var_6  mean_target_var_7  \\\n",
       "0           0.040196           0.066993           0.020098           0.040196   \n",
       "1           0.240196           0.066993           0.025122           0.200163   \n",
       "2           0.366830           0.050245           0.050245           0.200163   \n",
       "3           0.022331           0.050245           0.171569           0.033497   \n",
       "4           0.275122           0.066993           0.025122           0.066993   \n",
       "\n",
       "   mean_target_var_8  mean_target_var_9  mean_target_var_10  \\\n",
       "0           0.033497           0.033497            0.050245   \n",
       "1           0.040196           0.150122            0.050245   \n",
       "2           0.066993           0.150122            0.066993   \n",
       "3           0.050245           0.171569            0.033497   \n",
       "4           0.066993           0.183415            0.066993   \n",
       "\n",
       "   mean_target_var_11  mean_target_var_12  mean_target_var_13  \\\n",
       "0            0.050245            0.059486            0.066993   \n",
       "1            0.066993            0.087520            0.050245   \n",
       "2            0.066993            0.040033            0.050245   \n",
       "3            0.050245            0.029292            0.050245   \n",
       "4            0.066993            0.091707            0.066993   \n",
       "\n",
       "   mean_target_var_14  mean_target_var_15  mean_target_var_16  \\\n",
       "0            0.040196            0.128039            0.066993   \n",
       "1            0.033497            0.050245            0.050245   \n",
       "2            0.050245            0.070646            0.171569   \n",
       "3            0.171569            0.100045            0.075061   \n",
       "4            0.040196            0.160049            0.171569   \n",
       "\n",
       "   mean_target_var_17  mean_target_var_18  mean_target_var_19  \\\n",
       "0            0.050245            0.066993            0.066993   \n",
       "1            0.066993            0.066993            0.066993   \n",
       "2            0.066993            0.300245            0.040196   \n",
       "3            0.033497            0.066993            0.050245   \n",
       "4            0.050245            0.066993            0.033497   \n",
       "\n",
       "   mean_target_var_20  mean_target_var_21  mean_target_var_22  \\\n",
       "0            0.033497            0.040196            0.040196   \n",
       "1            0.066993            0.066993            0.066993   \n",
       "2            0.050245            0.066993            0.300245   \n",
       "3            0.040196            0.033497            0.040196   \n",
       "4            0.040196            0.050245            0.040196   \n",
       "\n",
       "   mean_target_var_23  mean_target_var_24  mean_target_var_25  \\\n",
       "0            0.010049            0.066993            0.221464   \n",
       "1            0.022331            0.033497            0.155592   \n",
       "2            0.018271            0.066993            0.063209   \n",
       "3            0.120098            0.066993            0.173366   \n",
       "4            0.070646            0.300245            0.145499   \n",
       "\n",
       "   mean_target_var_26  mean_target_var_27  mean_target_var_28  \\\n",
       "0            0.050245            0.120098            0.070646   \n",
       "1            0.066993            0.025122            0.100082   \n",
       "2            0.300245            0.240196            0.013399   \n",
       "3            0.040196            0.171569            0.200089   \n",
       "4            0.066993            0.033497            0.013399   \n",
       "\n",
       "   mean_target_var_29  mean_target_var_30  mean_target_var_31  \\\n",
       "0            0.028711            0.066993            0.150122   \n",
       "1            0.066993            0.050245            0.171569   \n",
       "2            0.066993            0.050245            0.050245   \n",
       "3            0.050245            0.066993            0.033497   \n",
       "4            0.066993            0.066993            0.171569   \n",
       "\n",
       "   mean_target_var_32  mean_target_var_33  mean_target_var_34  \\\n",
       "0            0.040196            0.040196            0.100082   \n",
       "1            0.240196            0.300245            0.109180   \n",
       "2            0.033497            0.040196            0.075061   \n",
       "3            0.066993            0.066993            0.200163   \n",
       "4            0.171569            0.040196            0.028711   \n",
       "\n",
       "   mean_target_var_35  mean_target_var_36  mean_target_var_37  \\\n",
       "0            0.300245            0.050245            0.133442   \n",
       "1            0.033497            0.040196            0.028711   \n",
       "2            0.240196            0.050245            0.040196   \n",
       "3            0.066993            0.050245            0.440196   \n",
       "4            0.040196            0.033497            0.025122   \n",
       "\n",
       "   mean_target_var_38  mean_target_var_39  mean_target_var_40  \\\n",
       "0            0.066993            0.066993            0.050245   \n",
       "1            0.066993            0.040196            0.040196   \n",
       "2            0.033497            0.200163            0.066993   \n",
       "3            0.050245            0.050245            0.066993   \n",
       "4            0.066993            0.040196            0.066993   \n",
       "\n",
       "   mean_target_var_41  mean_target_var_42  mean_target_var_43  \\\n",
       "0            0.050245            0.020098            0.018271   \n",
       "1            0.050245            0.171569            0.057190   \n",
       "2            0.040196            0.133442            0.020098   \n",
       "3            0.066993            0.022331            0.078606   \n",
       "4            0.050245            0.075061            0.064735   \n",
       "\n",
       "   mean_target_var_44  mean_target_var_45  mean_target_var_46  \\\n",
       "0            0.066993            0.066993            0.066993   \n",
       "1            0.050245            0.066993            0.066993   \n",
       "2            0.066993            0.050245            0.066993   \n",
       "3            0.066993            0.066993            0.150122   \n",
       "4            0.050245            0.066993            0.040196   \n",
       "\n",
       "   mean_target_var_47  mean_target_var_48  mean_target_var_49  \\\n",
       "0            0.066993            0.040196            0.040196   \n",
       "1            0.050245            0.066993            0.050245   \n",
       "2            0.066993            0.040196            0.050245   \n",
       "3            0.066993            0.050245            0.200163   \n",
       "4            0.066993            0.050245            0.066993   \n",
       "\n",
       "   mean_target_var_50  mean_target_var_51  mean_target_var_52  \\\n",
       "0            0.016748            0.066993            0.040196   \n",
       "1            0.022331            0.066993            0.040196   \n",
       "2            0.133442            0.050245            0.050245   \n",
       "3            0.070646            0.050245            0.040196   \n",
       "4            0.020098            0.040196            0.033497   \n",
       "\n",
       "   mean_target_var_53  mean_target_var_54  mean_target_var_55  \\\n",
       "0            0.018271            0.066993            0.300245   \n",
       "1            0.013399            0.066993            0.066993   \n",
       "2            0.200061            0.066993            0.066993   \n",
       "3            0.133442            0.066993            0.050245   \n",
       "4            0.080065            0.066993            0.066993   \n",
       "\n",
       "   mean_target_var_56  mean_target_var_57  mean_target_var_58  \\\n",
       "0            0.066993            0.092383            0.028711   \n",
       "1            0.040196            0.171569            0.033497   \n",
       "2            0.300245            0.080065            0.050245   \n",
       "3            0.120098            0.244553            0.066993   \n",
       "4            0.150122            0.171569            0.240196   \n",
       "\n",
       "   mean_target_var_59  mean_target_var_60  mean_target_var_61  \\\n",
       "0            0.028711            0.240196            0.066993   \n",
       "1            0.016748            0.040196            0.066993   \n",
       "2            0.016748            0.050245            0.066993   \n",
       "3            0.092383            0.066993            0.300245   \n",
       "4            0.290998            0.050245            0.300245   \n",
       "\n",
       "   mean_target_var_62  mean_target_var_63  mean_target_var_64  \\\n",
       "0            0.171569            0.050245            0.200163   \n",
       "1            0.200163            0.033497            0.020098   \n",
       "2            0.275122            0.240196            0.150122   \n",
       "3            0.040196            0.050245            0.050245   \n",
       "4            0.033497            0.033497            0.066993   \n",
       "\n",
       "   mean_target_var_65  mean_target_var_66  mean_target_var_67  \\\n",
       "0            0.066993            0.244553            0.066993   \n",
       "1            0.050245            0.025122            0.050245   \n",
       "2            0.028711            0.240196            0.066993   \n",
       "3            0.066993            0.016748            0.066993   \n",
       "4            0.050245            0.244553            0.066993   \n",
       "\n",
       "   mean_target_var_68  mean_target_var_69  mean_target_var_70  \\\n",
       "0            0.087306            0.040196            0.050245   \n",
       "1            0.092468            0.066993            0.066993   \n",
       "2            0.074604            0.240196            0.066993   \n",
       "3            0.092468            0.066993            0.066993   \n",
       "4            0.089658            0.050245            0.050245   \n",
       "\n",
       "   mean_target_var_71  mean_target_var_72  mean_target_var_73  \\\n",
       "0            0.070999            0.066993            0.066993   \n",
       "1            0.100045            0.040196            0.066993   \n",
       "2            0.033361            0.040196            0.066993   \n",
       "3            0.016748            0.066993            0.066993   \n",
       "4            0.006090            0.033497            0.066993   \n",
       "\n",
       "   mean_target_var_74  mean_target_var_75  mean_target_var_76  \\\n",
       "0            0.066993            0.050245            0.066993   \n",
       "1            0.050245            0.066993            0.066993   \n",
       "2            0.066993            0.066993            0.040196   \n",
       "3            0.040196            0.050245            0.066993   \n",
       "4            0.050245            0.066993            0.050245   \n",
       "\n",
       "   mean_target_var_77  mean_target_var_78  mean_target_var_79  \\\n",
       "0            0.040196            0.133442            0.050245   \n",
       "1            0.066993            0.025122            0.040196   \n",
       "2            0.040196            0.025122            0.050245   \n",
       "3            0.120098            0.040196            0.033497   \n",
       "4            0.066993            0.025122            0.022331   \n",
       "\n",
       "   mean_target_var_80  mean_target_var_81  mean_target_var_82  \\\n",
       "0            0.066993            0.200163            0.066993   \n",
       "1            0.040196            0.066993            0.066993   \n",
       "2            0.050245            0.040196            0.050245   \n",
       "3            0.066993            0.066993            0.066993   \n",
       "4            0.050245            0.066993            0.050245   \n",
       "\n",
       "   mean_target_var_83  mean_target_var_84  mean_target_var_85  \\\n",
       "0            0.050245            0.033497            0.200163   \n",
       "1            0.050245            0.240196            0.040196   \n",
       "2            0.050245            0.066993            0.040196   \n",
       "3            0.066993            0.050245            0.050245   \n",
       "4            0.066993            0.066993            0.050245   \n",
       "\n",
       "   mean_target_var_86  mean_target_var_87  mean_target_var_88  \\\n",
       "0            0.040196            0.040196            0.033497   \n",
       "1            0.066993            0.040196            0.200163   \n",
       "2            0.066993            0.050245            0.028711   \n",
       "3            0.050245            0.033497            0.040196   \n",
       "4            0.040196            0.050245            0.040196   \n",
       "\n",
       "   mean_target_var_89  mean_target_var_90  mean_target_var_91  \\\n",
       "0            0.066993            0.066993            0.064735   \n",
       "1            0.066993            0.066993            0.080788   \n",
       "2            0.050245            0.066993            0.024020   \n",
       "3            0.066993            0.066993            0.126551   \n",
       "4            0.050245            0.066993            0.126551   \n",
       "\n",
       "   mean_target_var_92  mean_target_var_93  mean_target_var_94  \\\n",
       "0            0.040196            0.075061            0.040196   \n",
       "1            0.066993            0.152428            0.200163   \n",
       "2            0.025122            0.010578            0.033497   \n",
       "3            0.050245            0.169306            0.066993   \n",
       "4            0.050245            0.109180            0.300245   \n",
       "\n",
       "   mean_target_var_95  mean_target_var_96  mean_target_var_97  \\\n",
       "0            0.137561            0.066993            0.066993   \n",
       "1            0.050245            0.066993            0.066993   \n",
       "2            0.014356            0.300245            0.066993   \n",
       "3            0.040196            0.300245            0.066993   \n",
       "4            0.015460            0.066993            0.066993   \n",
       "\n",
       "   mean_target_var_98  mean_target_var_99  mean_target_var_100  \\\n",
       "0            0.015460            0.240196             0.066993   \n",
       "1            0.120098            0.040196             0.066993   \n",
       "2            0.247116            0.150122             0.033497   \n",
       "3            0.150122            0.440196             0.066993   \n",
       "4            0.109180            0.050245             0.066993   \n",
       "\n",
       "   mean_target_var_101  mean_target_var_102  mean_target_var_103  \\\n",
       "0             0.050245             0.025122             0.103257   \n",
       "1             0.240196             0.050245             0.096999   \n",
       "2             0.300245             0.050245             0.078606   \n",
       "3             0.050245             0.040196             0.127302   \n",
       "4             0.040196             0.050245             0.120028   \n",
       "\n",
       "   mean_target_var_104  mean_target_var_105  mean_target_var_106  \\\n",
       "0             0.050245             0.066993             0.025122   \n",
       "1             0.066993             0.050245             0.028711   \n",
       "2             0.050245             0.355664             0.050245   \n",
       "3             0.033497             0.092383             0.050245   \n",
       "4             0.066993             0.290998             0.033497   \n",
       "\n",
       "   mean_target_var_107  mean_target_var_108  mean_target_var_109  \\\n",
       "0             0.066993             0.150020             0.050245   \n",
       "1             0.240196             0.100082             0.066993   \n",
       "2             0.066993             0.037531             0.050245   \n",
       "3             0.066993             0.048911             0.040196   \n",
       "4             0.050245             0.128039             0.066993   \n",
       "\n",
       "   mean_target_var_110  mean_target_var_111  mean_target_var_112  \\\n",
       "0             0.050245             0.120098             0.033497   \n",
       "1             0.300245             0.040196             0.050245   \n",
       "2             0.050245             0.171569             0.100082   \n",
       "3             0.050245             0.109180             0.050245   \n",
       "4             0.050245             0.240196             0.150122   \n",
       "\n",
       "   mean_target_var_113  mean_target_var_114  mean_target_var_115  \\\n",
       "0             0.066993             0.014356             0.366830   \n",
       "1             0.066993             0.120098             0.040196   \n",
       "2             0.050245             0.100082             0.275122   \n",
       "3             0.066993             0.109180             0.066993   \n",
       "4             0.066993             0.016748             0.066993   \n",
       "\n",
       "   mean_target_var_116  mean_target_var_117  mean_target_var_118  \\\n",
       "0             0.028711             0.066993             0.040196   \n",
       "1             0.040196             0.066993             0.066993   \n",
       "2             0.028711             0.050245             0.050245   \n",
       "3             0.066993             0.050245             0.066993   \n",
       "4             0.109180             0.066993             0.066993   \n",
       "\n",
       "   mean_target_var_119  mean_target_var_120  mean_target_var_121  \\\n",
       "0             0.066993             0.066993             0.033497   \n",
       "1             0.066993             0.050245             0.033497   \n",
       "2             0.028711             0.066993             0.066993   \n",
       "3             0.066993             0.066993             0.040196   \n",
       "4             0.066993             0.066993             0.171569   \n",
       "\n",
       "   mean_target_var_122  mean_target_var_123  mean_target_var_124  \\\n",
       "0             0.050245             0.066993             0.028711   \n",
       "1             0.050245             0.050245             0.066993   \n",
       "2             0.050245             0.066993             0.040196   \n",
       "3             0.050245             0.066993             0.050245   \n",
       "4             0.040196             0.066993             0.200163   \n",
       "\n",
       "   mean_target_var_125  mean_target_var_126  mean_target_var_127  \\\n",
       "0             0.228641             0.120098             0.040196   \n",
       "1             0.103257             0.200163             0.300245   \n",
       "2             0.128039             0.016748             0.033497   \n",
       "3             0.054590             0.012561             0.033497   \n",
       "4             0.085784             0.109180             0.240196   \n",
       "\n",
       "   mean_target_var_128  mean_target_var_129  mean_target_var_130  \\\n",
       "0             0.040196             0.040196             0.120098   \n",
       "1             0.050245             0.066993             0.092383   \n",
       "2             0.050245             0.066993             0.157213   \n",
       "3             0.050245             0.040196             0.050245   \n",
       "4             0.066993             0.040196             0.016748   \n",
       "\n",
       "   mean_target_var_131  mean_target_var_132  mean_target_var_133  \\\n",
       "0             0.075061             0.028711             0.080065   \n",
       "1             0.040196             0.033497             0.157213   \n",
       "2             0.014356             0.150122             0.114321   \n",
       "3             0.014356             0.028711             0.012561   \n",
       "4             0.110049             0.040196             0.100045   \n",
       "\n",
       "   mean_target_var_134  mean_target_var_135  mean_target_var_136  \\\n",
       "0             0.050245             0.050245             0.050245   \n",
       "1             0.040196             0.050245             0.066993   \n",
       "2             0.050245             0.066993             0.050245   \n",
       "3             0.040196             0.066993             0.050245   \n",
       "4             0.050245             0.050245             0.066993   \n",
       "\n",
       "   mean_target_var_137  mean_target_var_138  mean_target_var_139  \\\n",
       "0             0.066993             0.040196             0.066993   \n",
       "1             0.200163             0.040196             0.066993   \n",
       "2             0.050245             0.028711             0.028711   \n",
       "3             0.300245             0.033497             0.300245   \n",
       "4             0.050245             0.050245             0.066993   \n",
       "\n",
       "   mean_target_var_140  mean_target_var_141  mean_target_var_142  \\\n",
       "0             0.028711             0.050245             0.300245   \n",
       "1             0.028711             0.050245             0.066993   \n",
       "2             0.040196             0.050245             0.240196   \n",
       "3             0.040196             0.066993             0.040196   \n",
       "4             0.066993             0.066993             0.050245   \n",
       "\n",
       "   mean_target_var_143  mean_target_var_144  mean_target_var_145  \\\n",
       "0             0.033497             0.018271             0.050245   \n",
       "1             0.200163             0.033497             0.050245   \n",
       "2             0.066993             0.020098             0.040196   \n",
       "3             0.040196             0.033497             0.066993   \n",
       "4             0.066993             0.171569             0.066993   \n",
       "\n",
       "   mean_target_var_146  mean_target_var_147  mean_target_var_148  \\\n",
       "0             0.050245             0.040196             0.110552   \n",
       "1             0.040196             0.300245             0.060049   \n",
       "2             0.050245             0.171569             0.068106   \n",
       "3             0.014356             0.066993             0.188293   \n",
       "4             0.033497             0.066993             0.200061   \n",
       "\n",
       "   mean_target_var_149  mean_target_var_150  mean_target_var_151  \\\n",
       "0             0.066993             0.020098             0.050245   \n",
       "1             0.066993             0.066993             0.066993   \n",
       "2             0.040196             0.050245             0.040196   \n",
       "3             0.066993             0.028711             0.066993   \n",
       "4             0.066993             0.033497             0.066993   \n",
       "\n",
       "   mean_target_var_152  mean_target_var_153  mean_target_var_154  \\\n",
       "0             0.033497             0.028711             0.240196   \n",
       "1             0.033497             0.025122             0.066993   \n",
       "2             0.050245             0.040196             0.066993   \n",
       "3             0.240196             0.033497             0.050245   \n",
       "4             0.050245             0.050245             0.300245   \n",
       "\n",
       "   mean_target_var_155  mean_target_var_156  mean_target_var_157  \\\n",
       "0             0.066993             0.120098             0.050245   \n",
       "1             0.050245             0.028711             0.050245   \n",
       "2             0.040196             0.022331             0.050245   \n",
       "3             0.066993             0.018271             0.066993   \n",
       "4             0.066993             0.200089             0.066993   \n",
       "\n",
       "   mean_target_var_158  mean_target_var_159  mean_target_var_160  \\\n",
       "0             0.066993             0.028711             0.066993   \n",
       "1             0.050245             0.040196             0.050245   \n",
       "2             0.066993             0.200163             0.050245   \n",
       "3             0.040196             0.066993             0.066993   \n",
       "4             0.066993             0.066993             0.050245   \n",
       "\n",
       "   mean_target_var_161  mean_target_var_162  mean_target_var_163  \\\n",
       "0             0.052217             0.040196             0.240196   \n",
       "1             0.110379             0.200163             0.066993   \n",
       "2             0.100045             0.133442             0.066993   \n",
       "3             0.120098             0.050245             0.050245   \n",
       "4             0.096999             0.020098             0.066993   \n",
       "\n",
       "   mean_target_var_164  mean_target_var_165  mean_target_var_166  \\\n",
       "0             0.066993             0.050245             0.070646   \n",
       "1             0.066993             0.050245             0.057190   \n",
       "2             0.040196             0.040196             0.213399   \n",
       "3             0.066993             0.040196             0.016748   \n",
       "4             0.066993             0.066993             0.152428   \n",
       "\n",
       "   mean_target_var_167  mean_target_var_168  mean_target_var_169  \\\n",
       "0             0.066993             0.240196             0.095695   \n",
       "1             0.066993             0.020098             0.063209   \n",
       "2             0.050245             0.040196             0.085784   \n",
       "3             0.066993             0.240196             0.073366   \n",
       "4             0.066993             0.200163             0.081518   \n",
       "\n",
       "   mean_target_var_170  mean_target_var_171  mean_target_var_172  \\\n",
       "0             0.171569             0.050245             0.050245   \n",
       "1             0.066993             0.066993             0.066993   \n",
       "2             0.066993             0.040196             0.066993   \n",
       "3             0.066993             0.066993             0.066993   \n",
       "4             0.066993             0.050245             0.066993   \n",
       "\n",
       "   mean_target_var_173  mean_target_var_174  mean_target_var_175  \\\n",
       "0             0.066993             0.050245             0.022331   \n",
       "1             0.066993             0.066993             0.033497   \n",
       "2             0.040196             0.066993             0.028711   \n",
       "3             0.066993             0.050245             0.050245   \n",
       "4             0.240196             0.050245             0.040196   \n",
       "\n",
       "   mean_target_var_176  mean_target_var_177  mean_target_var_178  \\\n",
       "0             0.040196             0.066993             0.040196   \n",
       "1             0.066993             0.040196             0.066993   \n",
       "2             0.050245             0.050245             0.050245   \n",
       "3             0.050245             0.050245             0.066993   \n",
       "4             0.240196             0.066993             0.050245   \n",
       "\n",
       "   mean_target_var_179  mean_target_var_180  mean_target_var_181  \\\n",
       "0             0.033497             0.050245             0.025122   \n",
       "1             0.066993             0.040196             0.033497   \n",
       "2             0.028711             0.040196             0.033497   \n",
       "3             0.028711             0.066993             0.020098   \n",
       "4             0.033497             0.040196             0.171569   \n",
       "\n",
       "   mean_target_var_182  mean_target_var_183  mean_target_var_184  \\\n",
       "0             0.066993             0.033497             0.050245   \n",
       "1             0.066993             0.040196             0.066993   \n",
       "2             0.300245             0.066993             0.040196   \n",
       "3             0.150122             0.050245             0.066993   \n",
       "4             0.066993             0.066993             0.050245   \n",
       "\n",
       "   mean_target_var_185  mean_target_var_186  mean_target_var_187  \\\n",
       "0             0.066993             0.050245             0.050245   \n",
       "1             0.050245             0.040196             0.066993   \n",
       "2             0.040196             0.040196             0.066993   \n",
       "3             0.240196             0.028711             0.066993   \n",
       "4             0.066993             0.040196             0.066993   \n",
       "\n",
       "   mean_target_var_188  mean_target_var_189  mean_target_var_190  \\\n",
       "0             0.050245             0.171569             0.240196   \n",
       "1             0.028711             0.020098             0.028711   \n",
       "2             0.066993             0.240196             0.040196   \n",
       "3             0.244553             0.200163             0.066993   \n",
       "4             0.050245             0.109180             0.040196   \n",
       "\n",
       "   mean_target_var_191  mean_target_var_192  mean_target_var_193  \\\n",
       "0             0.150122             0.022331             0.240196   \n",
       "1             0.200163             0.150122             0.066993   \n",
       "2             0.033497             0.040196             0.066993   \n",
       "3             0.300245             0.200163             0.033497   \n",
       "4             0.033497             0.066993             0.066993   \n",
       "\n",
       "   mean_target_var_194  mean_target_var_195  mean_target_var_196  \\\n",
       "0             0.033497             0.033497             0.040196   \n",
       "1             0.066993             0.050245             0.050245   \n",
       "2             0.050245             0.050245             0.040196   \n",
       "3             0.040196             0.355664             0.033497   \n",
       "4             0.066993             0.028711             0.040196   \n",
       "\n",
       "   mean_target_var_197  mean_target_var_198  mean_target_var_199  \n",
       "0             0.080065             0.171569             0.050245  \n",
       "1             0.080065             0.050245             0.066993  \n",
       "2             0.020098             0.050245             0.050245  \n",
       "3             0.200163             0.050245             0.050245  \n",
       "4             0.150122             0.050245             0.050245  "
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X3 = pd.concat([X1, X3], axis=1)\n",
    "X4 = pd.concat([X2, X4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Wed Mar 20 13:34:12 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[17]\tvalid_0's auc: 1\n",
      "Fold 1 started at Wed Mar 20 13:34:31 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-515-c39f5a9d0a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n\u001b[0;32m----> 5\u001b[0;31m                                          model_type='lgb', plot_feature_importance=False)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-465-b3e7949fa4da>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, averaging, model)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 early_stopping_rounds=3000)\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;31m#             y_pred_valid = model.predict(X_valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m#             y_pred = model.predict(X_test, num_iteration=model.best_iteration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.335,\n",
    "    'boost_from_average':'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 0.041,\n",
    "    'learning_rate': 0.0083,\n",
    "    'max_depth': -1,\n",
    "    'metric':'auc',\n",
    "    'min_data_in_leaf': 80,\n",
    "    'min_sum_hessian_in_leaf': 10.0,\n",
    "    'num_leaves': 13,\n",
    "    'num_threads': 20,\n",
    "    'n_jobs': -1,\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary', \n",
    "    'verbosity': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment(x,y,t=2):\n",
    "    xs,xn = [],[]\n",
    "    for i in range(t):\n",
    "        mask = y>0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xs.append(x1)\n",
    "\n",
    "    for i in range(t//2):\n",
    "        mask = y==0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xn.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "    xn = np.vstack(xn)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "    yn = np.zeros(xn.shape[0])\n",
    "    x = np.vstack([x,xs,xn])\n",
    "    y = np.concatenate([y,ys,yn])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light GBM Model\n",
      "Fold idx:1\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912013\tvalid_1's auc: 0.899625\n",
      "[10000]\ttraining's auc: 0.921726\tvalid_1's auc: 0.903\n",
      "[15000]\ttraining's auc: 0.929102\tvalid_1's auc: 0.903258\n",
      "Early stopping, best iteration is:\n",
      "[12209]\ttraining's auc: 0.9251\tvalid_1's auc: 0.90333\n",
      "Fold idx:2\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911462\tvalid_1's auc: 0.89748\n",
      "[10000]\ttraining's auc: 0.921234\tvalid_1's auc: 0.900338\n",
      "[15000]\ttraining's auc: 0.928636\tvalid_1's auc: 0.900366\n",
      "Early stopping, best iteration is:\n",
      "[12322]\ttraining's auc: 0.924768\tvalid_1's auc: 0.900459\n",
      "Fold idx:3\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912401\tvalid_1's auc: 0.894125\n",
      "[10000]\ttraining's auc: 0.922024\tvalid_1's auc: 0.897065\n",
      "[15000]\ttraining's auc: 0.929311\tvalid_1's auc: 0.897221\n",
      "Early stopping, best iteration is:\n",
      "[14343]\ttraining's auc: 0.928402\tvalid_1's auc: 0.897381\n",
      "Fold idx:4\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911823\tvalid_1's auc: 0.898036\n",
      "[10000]\ttraining's auc: 0.921773\tvalid_1's auc: 0.900192\n",
      "[15000]\ttraining's auc: 0.929243\tvalid_1's auc: 0.899914\n",
      "Early stopping, best iteration is:\n",
      "[11752]\ttraining's auc: 0.924485\tvalid_1's auc: 0.90032\n",
      "Fold idx:5\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.91215\tvalid_1's auc: 0.897774\n",
      "[10000]\ttraining's auc: 0.921903\tvalid_1's auc: 0.900055\n",
      "[15000]\ttraining's auc: 0.929341\tvalid_1's auc: 0.900025\n",
      "Early stopping, best iteration is:\n",
      "[12012]\ttraining's auc: 0.925037\tvalid_1's auc: 0.900209\n",
      "Fold idx:6\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912139\tvalid_1's auc: 0.89956\n",
      "[10000]\ttraining's auc: 0.921961\tvalid_1's auc: 0.902369\n",
      "[15000]\ttraining's auc: 0.929364\tvalid_1's auc: 0.902887\n",
      "[20000]\ttraining's auc: 0.936047\tvalid_1's auc: 0.902755\n",
      "Early stopping, best iteration is:\n",
      "[17917]\ttraining's auc: 0.933319\tvalid_1's auc: 0.902991\n",
      "Fold idx:7\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.91116\tvalid_1's auc: 0.91178\n",
      "[10000]\ttraining's auc: 0.921006\tvalid_1's auc: 0.915351\n",
      "[15000]\ttraining's auc: 0.928478\tvalid_1's auc: 0.916023\n",
      "[20000]\ttraining's auc: 0.935302\tvalid_1's auc: 0.915809\n",
      "Early stopping, best iteration is:\n",
      "[16132]\ttraining's auc: 0.930066\tvalid_1's auc: 0.916188\n",
      "Fold idx:8\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911549\tvalid_1's auc: 0.89596\n",
      "[10000]\ttraining's auc: 0.921438\tvalid_1's auc: 0.898485\n",
      "[15000]\ttraining's auc: 0.928906\tvalid_1's auc: 0.898539\n",
      "Early stopping, best iteration is:\n",
      "[11590]\ttraining's auc: 0.923954\tvalid_1's auc: 0.898668\n",
      "Fold idx:9\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.91183\tvalid_1's auc: 0.901782\n",
      "[10000]\ttraining's auc: 0.921691\tvalid_1's auc: 0.905537\n",
      "[15000]\ttraining's auc: 0.929092\tvalid_1's auc: 0.906297\n",
      "[20000]\ttraining's auc: 0.935819\tvalid_1's auc: 0.906277\n",
      "Early stopping, best iteration is:\n",
      "[18821]\ttraining's auc: 0.934269\tvalid_1's auc: 0.9064\n",
      "Fold idx:10\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911934\tvalid_1's auc: 0.90217\n",
      "[10000]\ttraining's auc: 0.921804\tvalid_1's auc: 0.905625\n",
      "[15000]\ttraining's auc: 0.929215\tvalid_1's auc: 0.905894\n",
      "Early stopping, best iteration is:\n",
      "[15289]\ttraining's auc: 0.92961\tvalid_1's auc: 0.905968\n",
      "Fold idx:11\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911749\tvalid_1's auc: 0.895659\n",
      "[10000]\ttraining's auc: 0.921412\tvalid_1's auc: 0.899029\n",
      "[15000]\ttraining's auc: 0.928771\tvalid_1's auc: 0.899564\n",
      "Early stopping, best iteration is:\n",
      "[15332]\ttraining's auc: 0.929229\tvalid_1's auc: 0.899634\n",
      "Fold idx:12\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911468\tvalid_1's auc: 0.891032\n",
      "[10000]\ttraining's auc: 0.921174\tvalid_1's auc: 0.89462\n",
      "[15000]\ttraining's auc: 0.928649\tvalid_1's auc: 0.895528\n",
      "Early stopping, best iteration is:\n",
      "[14835]\ttraining's auc: 0.928411\tvalid_1's auc: 0.89561\n",
      "Fold idx:13\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912491\tvalid_1's auc: 0.890182\n",
      "[10000]\ttraining's auc: 0.922261\tvalid_1's auc: 0.894412\n",
      "[15000]\ttraining's auc: 0.92964\tvalid_1's auc: 0.894783\n",
      "[20000]\ttraining's auc: 0.936317\tvalid_1's auc: 0.894735\n",
      "Early stopping, best iteration is:\n",
      "[16180]\ttraining's auc: 0.931263\tvalid_1's auc: 0.894985\n",
      "Fold idx:14\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911973\tvalid_1's auc: 0.897211\n",
      "[10000]\ttraining's auc: 0.921718\tvalid_1's auc: 0.900431\n",
      "Early stopping, best iteration is:\n",
      "[9648]\ttraining's auc: 0.921169\tvalid_1's auc: 0.900532\n",
      "Fold idx:15\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.91227\tvalid_1's auc: 0.895376\n",
      "[10000]\ttraining's auc: 0.922146\tvalid_1's auc: 0.898378\n",
      "Early stopping, best iteration is:\n",
      "[10661]\ttraining's auc: 0.9232\tvalid_1's auc: 0.898539\n",
      "CV score: 0.90131 \n"
     ]
    }
   ],
   "source": [
    "train = X1.copy()\n",
    "test = X2.copy()\n",
    "target = y.copy()\n",
    "\n",
    "# num_folds = 15\n",
    "features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "\n",
    "# folds = KFold(n_splits=num_folds, random_state=2319)\n",
    "oof = np.zeros(len(train))\n",
    "getVal = np.zeros(len(train))\n",
    "predictions = np.zeros(len(target))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "n_fold = 15\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)\n",
    "\n",
    "f_cats = list(X1.columns)\n",
    "\n",
    "for f in f_cats:\n",
    "    train[f + \"_avg\"], test[f + \"_avg\"] = target_encode(trn_series=train[f],\n",
    "                                     tst_series=test[f],\n",
    "                                     target=y,\n",
    "                                     min_samples_leaf=200,\n",
    "                                     smoothing=10,\n",
    "                                     noise_level=0)\n",
    "\n",
    "print('Light GBM Model')\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    \n",
    "    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]\n",
    "    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]\n",
    "    \n",
    "    X_tr, y_tr = augment(X_train.values, y_train.values)\n",
    "    X_tr = pd.DataFrame(X_tr)\n",
    "    \n",
    "    print(\"Fold idx:{}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "    \n",
    "    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    getVal[val_idx]+= clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    \n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"feature\"] = features\n",
    "#     fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#     fold_importance_df[\"fold\"] = fold_ + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_lgb_me_aug_1_15_1', oof)\n",
    "np.save('../cache/preds_lgb_me_aug_1_15_1', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14848849, 0.26907124, 0.22618725, ..., 0.00623895, 0.14036473,\n",
       "       0.10482042])"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12l50.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0.148488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>0.269071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0.226187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.306754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0.061679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    target\n",
       "0  test_0  0.148488\n",
       "1  test_1  0.269071\n",
       "2  test_2  0.226187\n",
       "3  test_3  0.306754\n",
       "4  test_4  0.061679"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light GBM Model\n",
      "Fold idx:1\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924455\tvalid_1's auc: 0.900195\n",
      "[10000]\ttraining's auc: 0.939911\tvalid_1's auc: 0.902635\n",
      "[15000]\ttraining's auc: 0.952602\tvalid_1's auc: 0.902676\n",
      "Early stopping, best iteration is:\n",
      "[13080]\ttraining's auc: 0.947953\tvalid_1's auc: 0.902929\n",
      "Fold idx:2\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924874\tvalid_1's auc: 0.898333\n",
      "[10000]\ttraining's auc: 0.940298\tvalid_1's auc: 0.899948\n",
      "Early stopping, best iteration is:\n",
      "[9203]\ttraining's auc: 0.938113\tvalid_1's auc: 0.900072\n",
      "Fold idx:3\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924764\tvalid_1's auc: 0.895206\n",
      "[10000]\ttraining's auc: 0.94041\tvalid_1's auc: 0.897412\n",
      "Early stopping, best iteration is:\n",
      "[10422]\ttraining's auc: 0.941541\tvalid_1's auc: 0.897481\n",
      "Fold idx:4\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924601\tvalid_1's auc: 0.899623\n",
      "[10000]\ttraining's auc: 0.940187\tvalid_1's auc: 0.900643\n",
      "Early stopping, best iteration is:\n",
      "[9785]\ttraining's auc: 0.939609\tvalid_1's auc: 0.900659\n",
      "Fold idx:5\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.92458\tvalid_1's auc: 0.898887\n",
      "[10000]\ttraining's auc: 0.940157\tvalid_1's auc: 0.900413\n",
      "[15000]\ttraining's auc: 0.952775\tvalid_1's auc: 0.900111\n",
      "Early stopping, best iteration is:\n",
      "[11448]\ttraining's auc: 0.943984\tvalid_1's auc: 0.900617\n",
      "Fold idx:6\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924587\tvalid_1's auc: 0.900406\n",
      "[10000]\ttraining's auc: 0.940212\tvalid_1's auc: 0.902716\n",
      "[15000]\ttraining's auc: 0.952662\tvalid_1's auc: 0.902738\n",
      "Early stopping, best iteration is:\n",
      "[12277]\ttraining's auc: 0.946178\tvalid_1's auc: 0.902996\n",
      "Fold idx:7\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.923967\tvalid_1's auc: 0.912116\n",
      "[10000]\ttraining's auc: 0.939367\tvalid_1's auc: 0.914616\n",
      "[15000]\ttraining's auc: 0.952103\tvalid_1's auc: 0.914895\n",
      "Early stopping, best iteration is:\n",
      "[14597]\ttraining's auc: 0.951162\tvalid_1's auc: 0.915073\n",
      "Fold idx:8\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924759\tvalid_1's auc: 0.896652\n",
      "[10000]\ttraining's auc: 0.940163\tvalid_1's auc: 0.898591\n",
      "Early stopping, best iteration is:\n",
      "[8729]\ttraining's auc: 0.936603\tvalid_1's auc: 0.898804\n",
      "Fold idx:9\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924016\tvalid_1's auc: 0.90463\n",
      "[10000]\ttraining's auc: 0.939666\tvalid_1's auc: 0.906396\n",
      "Early stopping, best iteration is:\n",
      "[10908]\ttraining's auc: 0.94211\tvalid_1's auc: 0.906653\n",
      "Fold idx:10\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924163\tvalid_1's auc: 0.903523\n",
      "[10000]\ttraining's auc: 0.939778\tvalid_1's auc: 0.905471\n",
      "[15000]\ttraining's auc: 0.952433\tvalid_1's auc: 0.905186\n",
      "Early stopping, best iteration is:\n",
      "[11105]\ttraining's auc: 0.94273\tvalid_1's auc: 0.905645\n",
      "Fold idx:11\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924678\tvalid_1's auc: 0.896984\n",
      "[10000]\ttraining's auc: 0.940176\tvalid_1's auc: 0.898958\n",
      "[15000]\ttraining's auc: 0.952627\tvalid_1's auc: 0.89915\n",
      "Early stopping, best iteration is:\n",
      "[12675]\ttraining's auc: 0.947106\tvalid_1's auc: 0.8994\n",
      "Fold idx:12\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924797\tvalid_1's auc: 0.89246\n",
      "[10000]\ttraining's auc: 0.940356\tvalid_1's auc: 0.895895\n",
      "[15000]\ttraining's auc: 0.952914\tvalid_1's auc: 0.895708\n",
      "Early stopping, best iteration is:\n",
      "[12280]\ttraining's auc: 0.946354\tvalid_1's auc: 0.896145\n",
      "Fold idx:13\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924846\tvalid_1's auc: 0.890737\n",
      "[10000]\ttraining's auc: 0.940369\tvalid_1's auc: 0.894102\n",
      "[15000]\ttraining's auc: 0.952866\tvalid_1's auc: 0.893915\n",
      "Early stopping, best iteration is:\n",
      "[12858]\ttraining's auc: 0.947742\tvalid_1's auc: 0.894359\n",
      "Fold idx:14\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924583\tvalid_1's auc: 0.897605\n",
      "[10000]\ttraining's auc: 0.940139\tvalid_1's auc: 0.899593\n",
      "Early stopping, best iteration is:\n",
      "[9523]\ttraining's auc: 0.938839\tvalid_1's auc: 0.899777\n",
      "Fold idx:15\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924823\tvalid_1's auc: 0.89587\n",
      "[10000]\ttraining's auc: 0.940341\tvalid_1's auc: 0.897347\n",
      "Early stopping, best iteration is:\n",
      "[10554]\ttraining's auc: 0.941852\tvalid_1's auc: 0.897477\n",
      "CV score: 0.90108 \n"
     ]
    }
   ],
   "source": [
    "train = X1.copy()\n",
    "test = X2.copy()\n",
    "target = y.copy()\n",
    "\n",
    "# num_folds = 15\n",
    "# features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "\n",
    "# folds = KFold(n_splits=num_folds, random_state=2319)\n",
    "oof = np.zeros(len(train))\n",
    "getVal = np.zeros(len(train))\n",
    "predictions = np.zeros(len(target))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "n_fold = 15\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)\n",
    "\n",
    "f_cats = list(X1.columns)\n",
    "\n",
    "for f in f_cats:\n",
    "    train[f + \"_avg\"], test[f + \"_avg\"] = target_encode(trn_series=train[f],\n",
    "                                     tst_series=test[f],\n",
    "                                     target=y,\n",
    "                                     min_samples_leaf=1000,\n",
    "                                     smoothing=10,\n",
    "                                     noise_level=0)\n",
    "\n",
    "features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "print('Light GBM Model')\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    \n",
    "    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]\n",
    "    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]\n",
    "    \n",
    "#     X_tr, y_tr = augment(X_train.values, y_train.values)\n",
    "    X_tr, y_tr = X_train.values, y_train.values\n",
    "    X_tr = pd.DataFrame(X_tr)\n",
    "    \n",
    "    print(\"Fold idx:{}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "    \n",
    "    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    getVal[val_idx]+= clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    \n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"feature\"] = features\n",
    "#     fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#     fold_importance_df[\"fold\"] = fold_ + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_lgb_me_1_15_2', oof)\n",
    "np.save('../cache/preds_lgb_me_1_15_2', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12l51.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light GBM Model\n",
      "Fold idx:1\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912174\tvalid_1's auc: 0.90005\n",
      "[10000]\ttraining's auc: 0.921985\tvalid_1's auc: 0.903377\n",
      "[15000]\ttraining's auc: 0.929399\tvalid_1's auc: 0.903731\n",
      "Early stopping, best iteration is:\n",
      "[14677]\ttraining's auc: 0.928937\tvalid_1's auc: 0.903788\n",
      "Fold idx:2\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911572\tvalid_1's auc: 0.898171\n",
      "[10000]\ttraining's auc: 0.921435\tvalid_1's auc: 0.900399\n",
      "[15000]\ttraining's auc: 0.928847\tvalid_1's auc: 0.900708\n",
      "Early stopping, best iteration is:\n",
      "[13483]\ttraining's auc: 0.926684\tvalid_1's auc: 0.900861\n",
      "Fold idx:3\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912244\tvalid_1's auc: 0.894716\n",
      "[10000]\ttraining's auc: 0.922089\tvalid_1's auc: 0.898166\n",
      "[15000]\ttraining's auc: 0.92949\tvalid_1's auc: 0.898518\n",
      "Early stopping, best iteration is:\n",
      "[13788]\ttraining's auc: 0.927747\tvalid_1's auc: 0.898681\n",
      "Fold idx:4\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911417\tvalid_1's auc: 0.898338\n",
      "[10000]\ttraining's auc: 0.921364\tvalid_1's auc: 0.900565\n",
      "Early stopping, best iteration is:\n",
      "[9685]\ttraining's auc: 0.920858\tvalid_1's auc: 0.900681\n",
      "Fold idx:5\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.91186\tvalid_1's auc: 0.897694\n",
      "[10000]\ttraining's auc: 0.921679\tvalid_1's auc: 0.900287\n",
      "[15000]\ttraining's auc: 0.929115\tvalid_1's auc: 0.900621\n",
      "[20000]\ttraining's auc: 0.935864\tvalid_1's auc: 0.90041\n",
      "Early stopping, best iteration is:\n",
      "[16833]\ttraining's auc: 0.931633\tvalid_1's auc: 0.900725\n",
      "Fold idx:6\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.9121\tvalid_1's auc: 0.89989\n",
      "[10000]\ttraining's auc: 0.922026\tvalid_1's auc: 0.902822\n",
      "[15000]\ttraining's auc: 0.929443\tvalid_1's auc: 0.903019\n",
      "Early stopping, best iteration is:\n",
      "[12930]\ttraining's auc: 0.926469\tvalid_1's auc: 0.903166\n",
      "Fold idx:7\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911565\tvalid_1's auc: 0.911636\n",
      "[10000]\ttraining's auc: 0.921366\tvalid_1's auc: 0.915271\n",
      "[15000]\ttraining's auc: 0.928804\tvalid_1's auc: 0.915419\n",
      "Early stopping, best iteration is:\n",
      "[12699]\ttraining's auc: 0.925483\tvalid_1's auc: 0.915499\n",
      "Fold idx:8\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912092\tvalid_1's auc: 0.89673\n",
      "[10000]\ttraining's auc: 0.921919\tvalid_1's auc: 0.899623\n",
      "[15000]\ttraining's auc: 0.929298\tvalid_1's auc: 0.899695\n",
      "Early stopping, best iteration is:\n",
      "[14145]\ttraining's auc: 0.9281\tvalid_1's auc: 0.899806\n",
      "Fold idx:9\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911735\tvalid_1's auc: 0.902806\n",
      "[10000]\ttraining's auc: 0.92158\tvalid_1's auc: 0.906571\n",
      "[15000]\ttraining's auc: 0.928944\tvalid_1's auc: 0.906873\n",
      "Early stopping, best iteration is:\n",
      "[13213]\ttraining's auc: 0.926412\tvalid_1's auc: 0.906937\n",
      "Fold idx:10\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911198\tvalid_1's auc: 0.902067\n",
      "[10000]\ttraining's auc: 0.921056\tvalid_1's auc: 0.90605\n",
      "[15000]\ttraining's auc: 0.928481\tvalid_1's auc: 0.906447\n",
      "Early stopping, best iteration is:\n",
      "[14820]\ttraining's auc: 0.928234\tvalid_1's auc: 0.906468\n",
      "Fold idx:11\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911894\tvalid_1's auc: 0.895715\n",
      "[10000]\ttraining's auc: 0.921717\tvalid_1's auc: 0.899397\n",
      "[15000]\ttraining's auc: 0.929135\tvalid_1's auc: 0.899822\n",
      "Early stopping, best iteration is:\n",
      "[15679]\ttraining's auc: 0.930083\tvalid_1's auc: 0.899936\n",
      "Fold idx:12\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911798\tvalid_1's auc: 0.891721\n",
      "[10000]\ttraining's auc: 0.921754\tvalid_1's auc: 0.895521\n",
      "[15000]\ttraining's auc: 0.92915\tvalid_1's auc: 0.896206\n",
      "[20000]\ttraining's auc: 0.935936\tvalid_1's auc: 0.896314\n",
      "Early stopping, best iteration is:\n",
      "[18027]\ttraining's auc: 0.933317\tvalid_1's auc: 0.896496\n",
      "Fold idx:13\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912211\tvalid_1's auc: 0.890469\n",
      "[10000]\ttraining's auc: 0.922032\tvalid_1's auc: 0.894979\n",
      "[15000]\ttraining's auc: 0.929475\tvalid_1's auc: 0.895831\n",
      "[20000]\ttraining's auc: 0.936239\tvalid_1's auc: 0.896016\n",
      "Early stopping, best iteration is:\n",
      "[20497]\ttraining's auc: 0.936877\tvalid_1's auc: 0.896093\n",
      "Fold idx:14\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911906\tvalid_1's auc: 0.897038\n",
      "[10000]\ttraining's auc: 0.921819\tvalid_1's auc: 0.899969\n",
      "Early stopping, best iteration is:\n",
      "[10874]\ttraining's auc: 0.923178\tvalid_1's auc: 0.900237\n",
      "Fold idx:15\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.913036\tvalid_1's auc: 0.896251\n",
      "[10000]\ttraining's auc: 0.922935\tvalid_1's auc: 0.899001\n",
      "Early stopping, best iteration is:\n",
      "[10992]\ttraining's auc: 0.924435\tvalid_1's auc: 0.899155\n",
      "CV score: 0.90168 \n"
     ]
    }
   ],
   "source": [
    "train = X1.copy()\n",
    "test = X2.copy()\n",
    "target = y.copy()\n",
    "\n",
    "# num_folds = 15\n",
    "# features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "\n",
    "# folds = KFold(n_splits=num_folds, random_state=2319)\n",
    "oof = np.zeros(len(train))\n",
    "getVal = np.zeros(len(train))\n",
    "predictions = np.zeros(len(target))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "n_fold = 15\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)\n",
    "\n",
    "f_cats = list(X1.columns)\n",
    "\n",
    "for f in f_cats:\n",
    "    train[f + \"_avg\"], test[f + \"_avg\"] = target_encode(trn_series=train[f],\n",
    "                                     tst_series=test[f],\n",
    "                                     target=y,\n",
    "                                     min_samples_leaf=1000,\n",
    "                                     smoothing=10,\n",
    "                                     noise_level=0)\n",
    "\n",
    "features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "print('Light GBM Model')\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    \n",
    "    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]\n",
    "    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]\n",
    "    \n",
    "    X_tr, y_tr = augment(X_train.values, y_train.values)\n",
    "#     X_tr, y_tr = X_train.values, y_train.values\n",
    "    X_tr = pd.DataFrame(X_tr)\n",
    "    \n",
    "    print(\"Fold idx:{}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "    \n",
    "    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    getVal[val_idx]+= clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    \n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"feature\"] = features\n",
    "#     fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#     fold_importance_df[\"fold\"] = fold_ + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_lgb_me_aug_1_15_3', oof)\n",
    "np.save('../cache/preds_lgb_me_aug_1_15_3', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12l52.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light GBM Model\n",
      "Fold idx:1\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912099\tvalid_1's auc: 0.898286\n",
      "[10000]\ttraining's auc: 0.922307\tvalid_1's auc: 0.90187\n",
      "[15000]\ttraining's auc: 0.929969\tvalid_1's auc: 0.902198\n",
      "Early stopping, best iteration is:\n",
      "[13602]\ttraining's auc: 0.927915\tvalid_1's auc: 0.902255\n",
      "Fold idx:2\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.913141\tvalid_1's auc: 0.895863\n",
      "[10000]\ttraining's auc: 0.923112\tvalid_1's auc: 0.899455\n",
      "[15000]\ttraining's auc: 0.930636\tvalid_1's auc: 0.899518\n",
      "Early stopping, best iteration is:\n",
      "[15337]\ttraining's auc: 0.931113\tvalid_1's auc: 0.899526\n",
      "Fold idx:3\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912315\tvalid_1's auc: 0.897885\n",
      "[10000]\ttraining's auc: 0.92247\tvalid_1's auc: 0.900008\n",
      "[15000]\ttraining's auc: 0.930076\tvalid_1's auc: 0.900008\n",
      "Early stopping, best iteration is:\n",
      "[11331]\ttraining's auc: 0.924606\tvalid_1's auc: 0.900114\n",
      "Fold idx:4\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912428\tvalid_1's auc: 0.899659\n",
      "[10000]\ttraining's auc: 0.922493\tvalid_1's auc: 0.902316\n",
      "[15000]\ttraining's auc: 0.930184\tvalid_1's auc: 0.902424\n",
      "Early stopping, best iteration is:\n",
      "[11956]\ttraining's auc: 0.925612\tvalid_1's auc: 0.902573\n",
      "Fold idx:5\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911581\tvalid_1's auc: 0.906401\n",
      "[10000]\ttraining's auc: 0.921642\tvalid_1's auc: 0.909509\n",
      "[15000]\ttraining's auc: 0.929256\tvalid_1's auc: 0.910134\n",
      "[20000]\ttraining's auc: 0.936209\tvalid_1's auc: 0.910065\n",
      "Early stopping, best iteration is:\n",
      "[19140]\ttraining's auc: 0.935041\tvalid_1's auc: 0.91018\n",
      "Fold idx:6\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912309\tvalid_1's auc: 0.901081\n",
      "[10000]\ttraining's auc: 0.922331\tvalid_1's auc: 0.904193\n",
      "[15000]\ttraining's auc: 0.929936\tvalid_1's auc: 0.904279\n",
      "Early stopping, best iteration is:\n",
      "[13133]\ttraining's auc: 0.92721\tvalid_1's auc: 0.904464\n",
      "Fold idx:7\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.91204\tvalid_1's auc: 0.901133\n",
      "[10000]\ttraining's auc: 0.922114\tvalid_1's auc: 0.904634\n",
      "[15000]\ttraining's auc: 0.929807\tvalid_1's auc: 0.905016\n",
      "Early stopping, best iteration is:\n",
      "[14686]\ttraining's auc: 0.929346\tvalid_1's auc: 0.905059\n",
      "Fold idx:8\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912622\tvalid_1's auc: 0.891816\n",
      "[10000]\ttraining's auc: 0.922586\tvalid_1's auc: 0.89562\n",
      "[15000]\ttraining's auc: 0.930111\tvalid_1's auc: 0.896235\n",
      "[20000]\ttraining's auc: 0.937045\tvalid_1's auc: 0.896226\n",
      "Early stopping, best iteration is:\n",
      "[16571]\ttraining's auc: 0.932329\tvalid_1's auc: 0.896343\n",
      "Fold idx:9\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912672\tvalid_1's auc: 0.893305\n",
      "[10000]\ttraining's auc: 0.922633\tvalid_1's auc: 0.897056\n",
      "[15000]\ttraining's auc: 0.930219\tvalid_1's auc: 0.897217\n",
      "Early stopping, best iteration is:\n",
      "[13744]\ttraining's auc: 0.928391\tvalid_1's auc: 0.897313\n",
      "Fold idx:10\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911908\tvalid_1's auc: 0.895317\n",
      "[10000]\ttraining's auc: 0.922112\tvalid_1's auc: 0.89798\n",
      "[15000]\ttraining's auc: 0.929808\tvalid_1's auc: 0.89778\n",
      "Early stopping, best iteration is:\n",
      "[11728]\ttraining's auc: 0.924893\tvalid_1's auc: 0.898129\n",
      "CV score: 0.90153 \n"
     ]
    }
   ],
   "source": [
    "train = X1.copy()\n",
    "test = X2.copy()\n",
    "target = y.copy()\n",
    "\n",
    "# num_folds = 15\n",
    "# features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "\n",
    "# folds = KFold(n_splits=num_folds, random_state=2319)\n",
    "oof = np.zeros(len(train))\n",
    "getVal = np.zeros(len(train))\n",
    "predictions = np.zeros(len(target))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "n_fold = 10\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)\n",
    "\n",
    "f_cats = list(X1.columns)\n",
    "\n",
    "for f in f_cats:\n",
    "    train[f + \"_avg\"], test[f + \"_avg\"] = target_encode(trn_series=train[f],\n",
    "                                     tst_series=test[f],\n",
    "                                     target=y,\n",
    "                                     min_samples_leaf=1000,\n",
    "                                     smoothing=10,\n",
    "                                     noise_level=0)\n",
    "\n",
    "features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "print('Light GBM Model')\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    \n",
    "    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]\n",
    "    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]\n",
    "    \n",
    "    X_tr, y_tr = augment(X_train.values, y_train.values)\n",
    "#     X_tr, y_tr = X_train.values, y_train.values\n",
    "    X_tr = pd.DataFrame(X_tr)\n",
    "    \n",
    "    print(\"Fold idx:{}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "    \n",
    "    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    getVal[val_idx]+= clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    \n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"feature\"] = features\n",
    "#     fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#     fold_importance_df[\"fold\"] = fold_ + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_lgb_me_aug_1_10_4', oof)\n",
    "np.save('../cache/preds_lgb_me_aug_1_10_4', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12l53.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light GBM Model\n",
      "Fold idx:1\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912787\tvalid_1's auc: 0.898586\n",
      "[10000]\ttraining's auc: 0.92265\tvalid_1's auc: 0.901503\n",
      "[15000]\ttraining's auc: 0.930131\tvalid_1's auc: 0.901766\n",
      "Early stopping, best iteration is:\n",
      "[11887]\ttraining's auc: 0.925584\tvalid_1's auc: 0.901898\n",
      "Fold idx:2\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.913026\tvalid_1's auc: 0.896466\n",
      "[10000]\ttraining's auc: 0.922933\tvalid_1's auc: 0.899787\n",
      "[15000]\ttraining's auc: 0.93048\tvalid_1's auc: 0.90006\n",
      "Early stopping, best iteration is:\n",
      "[13289]\ttraining's auc: 0.928005\tvalid_1's auc: 0.900131\n",
      "Fold idx:3\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912496\tvalid_1's auc: 0.89726\n",
      "[10000]\ttraining's auc: 0.922507\tvalid_1's auc: 0.899529\n",
      "Early stopping, best iteration is:\n",
      "[9800]\ttraining's auc: 0.92218\tvalid_1's auc: 0.899606\n",
      "Fold idx:4\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912515\tvalid_1's auc: 0.90028\n",
      "[10000]\ttraining's auc: 0.922482\tvalid_1's auc: 0.902763\n",
      "[15000]\ttraining's auc: 0.929999\tvalid_1's auc: 0.902965\n",
      "Early stopping, best iteration is:\n",
      "[11931]\ttraining's auc: 0.92551\tvalid_1's auc: 0.903038\n",
      "Fold idx:5\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912394\tvalid_1's auc: 0.906781\n",
      "[10000]\ttraining's auc: 0.922338\tvalid_1's auc: 0.910286\n",
      "[15000]\ttraining's auc: 0.929898\tvalid_1's auc: 0.910738\n",
      "Early stopping, best iteration is:\n",
      "[14699]\ttraining's auc: 0.929467\tvalid_1's auc: 0.910825\n",
      "Fold idx:6\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912394\tvalid_1's auc: 0.900501\n",
      "[10000]\ttraining's auc: 0.922352\tvalid_1's auc: 0.903638\n",
      "[15000]\ttraining's auc: 0.929972\tvalid_1's auc: 0.90374\n",
      "Early stopping, best iteration is:\n",
      "[13259]\ttraining's auc: 0.927412\tvalid_1's auc: 0.904035\n",
      "Fold idx:7\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912602\tvalid_1's auc: 0.901109\n",
      "[10000]\ttraining's auc: 0.922613\tvalid_1's auc: 0.904745\n",
      "[15000]\ttraining's auc: 0.930232\tvalid_1's auc: 0.905237\n",
      "Early stopping, best iteration is:\n",
      "[14518]\ttraining's auc: 0.929536\tvalid_1's auc: 0.905288\n",
      "Fold idx:8\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.91275\tvalid_1's auc: 0.892392\n",
      "[10000]\ttraining's auc: 0.922694\tvalid_1's auc: 0.896431\n",
      "[15000]\ttraining's auc: 0.930233\tvalid_1's auc: 0.897021\n",
      "[20000]\ttraining's auc: 0.937091\tvalid_1's auc: 0.897219\n",
      "Early stopping, best iteration is:\n",
      "[19328]\ttraining's auc: 0.936183\tvalid_1's auc: 0.897289\n",
      "Fold idx:9\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.913307\tvalid_1's auc: 0.893426\n",
      "[10000]\ttraining's auc: 0.923168\tvalid_1's auc: 0.896999\n",
      "[15000]\ttraining's auc: 0.930695\tvalid_1's auc: 0.897197\n",
      "[20000]\ttraining's auc: 0.937535\tvalid_1's auc: 0.896932\n",
      "Early stopping, best iteration is:\n",
      "[16507]\ttraining's auc: 0.932833\tvalid_1's auc: 0.897257\n",
      "Fold idx:10\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912523\tvalid_1's auc: 0.895486\n",
      "[10000]\ttraining's auc: 0.922493\tvalid_1's auc: 0.898497\n",
      "Early stopping, best iteration is:\n",
      "[10591]\ttraining's auc: 0.923462\tvalid_1's auc: 0.89865\n",
      "CV score: 0.90162 \n"
     ]
    }
   ],
   "source": [
    "train = X1.copy()\n",
    "test = X2.copy()\n",
    "target = y.copy()\n",
    "\n",
    "# num_folds = 15\n",
    "# features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "\n",
    "# folds = KFold(n_splits=num_folds, random_state=2319)\n",
    "oof = np.zeros(len(train))\n",
    "getVal = np.zeros(len(train))\n",
    "predictions = np.zeros(len(target))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "n_fold = 10\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)\n",
    "\n",
    "f_cats = list(X1.columns)\n",
    "\n",
    "for f in f_cats:\n",
    "    train[f + \"_avg\"], test[f + \"_avg\"] = target_encode(trn_series=train[f],\n",
    "                                     tst_series=test[f],\n",
    "                                     target=y,\n",
    "                                     min_samples_leaf=500,\n",
    "                                     smoothing=10,\n",
    "                                     noise_level=0)\n",
    "\n",
    "features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "print('Light GBM Model')\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    \n",
    "    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]\n",
    "    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]\n",
    "    \n",
    "    X_tr, y_tr = augment(X_train.values, y_train.values)\n",
    "#     X_tr, y_tr = X_train.values, y_train.values\n",
    "    X_tr = pd.DataFrame(X_tr)\n",
    "    \n",
    "    print(\"Fold idx:{}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "    \n",
    "    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    getVal[val_idx]+= clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    \n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"feature\"] = features\n",
    "#     fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#     fold_importance_df[\"fold\"] = fold_ + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_lgb_me_aug_1_10_5', oof)\n",
    "np.save('../cache/preds_lgb_me_aug_1_10_5', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12l54.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv.zip')\n",
    "y = train['target']\n",
    "test = pd.read_csv('../data/test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light GBM Model\n",
      "Fold idx:1\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912806\tvalid_1's auc: 0.898221\n",
      "[10000]\ttraining's auc: 0.922683\tvalid_1's auc: 0.901586\n",
      "[15000]\ttraining's auc: 0.930232\tvalid_1's auc: 0.902143\n",
      "Early stopping, best iteration is:\n",
      "[13347]\ttraining's auc: 0.927832\tvalid_1's auc: 0.902257\n",
      "Fold idx:2\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.913005\tvalid_1's auc: 0.896095\n",
      "[10000]\ttraining's auc: 0.922871\tvalid_1's auc: 0.89953\n",
      "[15000]\ttraining's auc: 0.930398\tvalid_1's auc: 0.899646\n",
      "Early stopping, best iteration is:\n",
      "[14082]\ttraining's auc: 0.929085\tvalid_1's auc: 0.899785\n",
      "Fold idx:3\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.913029\tvalid_1's auc: 0.898304\n",
      "[10000]\ttraining's auc: 0.922923\tvalid_1's auc: 0.900538\n",
      "Early stopping, best iteration is:\n",
      "[10164]\ttraining's auc: 0.923188\tvalid_1's auc: 0.900588\n",
      "Fold idx:4\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912756\tvalid_1's auc: 0.900002\n",
      "[10000]\ttraining's auc: 0.922542\tvalid_1's auc: 0.902703\n",
      "[15000]\ttraining's auc: 0.930087\tvalid_1's auc: 0.902748\n",
      "Early stopping, best iteration is:\n",
      "[13332]\ttraining's auc: 0.927657\tvalid_1's auc: 0.90292\n",
      "Fold idx:5\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.911516\tvalid_1's auc: 0.906631\n",
      "[10000]\ttraining's auc: 0.921659\tvalid_1's auc: 0.910132\n",
      "[15000]\ttraining's auc: 0.92932\tvalid_1's auc: 0.910604\n",
      "Early stopping, best iteration is:\n",
      "[14514]\ttraining's auc: 0.928608\tvalid_1's auc: 0.910636\n",
      "Fold idx:6\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.91222\tvalid_1's auc: 0.901435\n",
      "[10000]\ttraining's auc: 0.922157\tvalid_1's auc: 0.904539\n",
      "[15000]\ttraining's auc: 0.929788\tvalid_1's auc: 0.904493\n",
      "Early stopping, best iteration is:\n",
      "[11668]\ttraining's auc: 0.924832\tvalid_1's auc: 0.904693\n",
      "Fold idx:7\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912364\tvalid_1's auc: 0.900586\n",
      "[10000]\ttraining's auc: 0.922373\tvalid_1's auc: 0.904145\n",
      "[15000]\ttraining's auc: 0.929978\tvalid_1's auc: 0.904431\n",
      "Early stopping, best iteration is:\n",
      "[14429]\ttraining's auc: 0.929149\tvalid_1's auc: 0.904485\n",
      "Fold idx:8\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912656\tvalid_1's auc: 0.892182\n",
      "[10000]\ttraining's auc: 0.92253\tvalid_1's auc: 0.895879\n",
      "[15000]\ttraining's auc: 0.930107\tvalid_1's auc: 0.896418\n",
      "[20000]\ttraining's auc: 0.936994\tvalid_1's auc: 0.896372\n",
      "Early stopping, best iteration is:\n",
      "[16009]\ttraining's auc: 0.931521\tvalid_1's auc: 0.896557\n",
      "Fold idx:9\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912874\tvalid_1's auc: 0.893654\n",
      "[10000]\ttraining's auc: 0.922806\tvalid_1's auc: 0.897784\n",
      "[15000]\ttraining's auc: 0.930405\tvalid_1's auc: 0.89838\n",
      "[20000]\ttraining's auc: 0.937248\tvalid_1's auc: 0.8984\n",
      "Early stopping, best iteration is:\n",
      "[17834]\ttraining's auc: 0.934345\tvalid_1's auc: 0.8985\n",
      "Fold idx:10\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.912542\tvalid_1's auc: 0.895208\n",
      "[10000]\ttraining's auc: 0.922509\tvalid_1's auc: 0.898177\n",
      "[15000]\ttraining's auc: 0.930119\tvalid_1's auc: 0.898127\n",
      "Early stopping, best iteration is:\n",
      "[11659]\ttraining's auc: 0.925124\tvalid_1's auc: 0.898435\n",
      "CV score: 0.90173 \n"
     ]
    }
   ],
   "source": [
    "# train = X1.copy()\n",
    "# test = X2.copy()\n",
    "target = y.copy()\n",
    "\n",
    "# num_folds = 15\n",
    "# features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "\n",
    "# folds = KFold(n_splits=num_folds, random_state=2319)\n",
    "oof = np.zeros(len(train))\n",
    "getVal = np.zeros(len(train))\n",
    "predictions = np.zeros(len(target))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "n_fold = 10\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)\n",
    "\n",
    "f_cats = list(X1.columns)\n",
    "\n",
    "for f in f_cats:\n",
    "    train[f + \"_avg\"], test[f + \"_avg\"] = target_encode(trn_series=train[f],\n",
    "                                     tst_series=test[f],\n",
    "                                     target=y,\n",
    "                                     min_samples_leaf=500,\n",
    "                                     smoothing=10,\n",
    "                                     noise_level=0)\n",
    "\n",
    "features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "print('Light GBM Model')\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    \n",
    "    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]\n",
    "    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]\n",
    "    \n",
    "    X_tr, y_tr = augment(X_train.values, y_train.values)\n",
    "#     X_tr, y_tr = X_train.values, y_train.values\n",
    "    X_tr = pd.DataFrame(X_tr)\n",
    "    \n",
    "    print(\"Fold idx:{}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "    \n",
    "    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    getVal[val_idx]+= clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    \n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"feature\"] = features\n",
    "#     fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#     fold_importance_df[\"fold\"] = fold_ + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_lgb_me_aug_1_10_6', oof)\n",
    "np.save('../cache/preds_lgb_me_aug_1_10_6', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "preds = np.load('../cache/preds_lgb_me_aug_1_10_6.npy')\n",
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = preds\n",
    "sub.to_csv('../submissions/sub12l57.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0.136217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>0.287360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0.221627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.359317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0.064728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    target\n",
       "0  test_0  0.136217\n",
       "1  test_1  0.287360\n",
       "2  test_2  0.221627\n",
       "3  test_3  0.359317\n",
       "4  test_4  0.064728"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light GBM Model\n",
      "Fold idx:1\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.918874\tvalid_1's auc: 0.905975\n",
      "[10000]\ttraining's auc: 0.928391\tvalid_1's auc: 0.909114\n",
      "[15000]\ttraining's auc: 0.935513\tvalid_1's auc: 0.909498\n",
      "[20000]\ttraining's auc: 0.941936\tvalid_1's auc: 0.909598\n",
      "[25000]\ttraining's auc: 0.947888\tvalid_1's auc: 0.909459\n",
      "Early stopping, best iteration is:\n",
      "[22424]\ttraining's auc: 0.944889\tvalid_1's auc: 0.909675\n",
      "Fold idx:2\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.919522\tvalid_1's auc: 0.905858\n",
      "[10000]\ttraining's auc: 0.928974\tvalid_1's auc: 0.909177\n",
      "[15000]\ttraining's auc: 0.936004\tvalid_1's auc: 0.909631\n",
      "Early stopping, best iteration is:\n",
      "[14335]\ttraining's auc: 0.935121\tvalid_1's auc: 0.909722\n",
      "Fold idx:3\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.9197\tvalid_1's auc: 0.903712\n",
      "[10000]\ttraining's auc: 0.929037\tvalid_1's auc: 0.906228\n",
      "[15000]\ttraining's auc: 0.936094\tvalid_1's auc: 0.906546\n",
      "Early stopping, best iteration is:\n",
      "[12240]\ttraining's auc: 0.932324\tvalid_1's auc: 0.906637\n",
      "Fold idx:4\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.919136\tvalid_1's auc: 0.90755\n",
      "[10000]\ttraining's auc: 0.928544\tvalid_1's auc: 0.910107\n",
      "[15000]\ttraining's auc: 0.935624\tvalid_1's auc: 0.910363\n",
      "Early stopping, best iteration is:\n",
      "[12858]\ttraining's auc: 0.932713\tvalid_1's auc: 0.910558\n",
      "Fold idx:5\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.916891\tvalid_1's auc: 0.909598\n",
      "[10000]\ttraining's auc: 0.926474\tvalid_1's auc: 0.913316\n",
      "[15000]\ttraining's auc: 0.933766\tvalid_1's auc: 0.914021\n",
      "[20000]\ttraining's auc: 0.940344\tvalid_1's auc: 0.913816\n",
      "Early stopping, best iteration is:\n",
      "[16687]\ttraining's auc: 0.936046\tvalid_1's auc: 0.914069\n",
      "Fold idx:6\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.918947\tvalid_1's auc: 0.906965\n",
      "[10000]\ttraining's auc: 0.928448\tvalid_1's auc: 0.910257\n",
      "[15000]\ttraining's auc: 0.935614\tvalid_1's auc: 0.910596\n",
      "Early stopping, best iteration is:\n",
      "[15954]\ttraining's auc: 0.936879\tvalid_1's auc: 0.91068\n",
      "Fold idx:7\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.91851\tvalid_1's auc: 0.90795\n",
      "[10000]\ttraining's auc: 0.927968\tvalid_1's auc: 0.91152\n",
      "[15000]\ttraining's auc: 0.935131\tvalid_1's auc: 0.911984\n",
      "Early stopping, best iteration is:\n",
      "[13212]\ttraining's auc: 0.93267\tvalid_1's auc: 0.912055\n",
      "Fold idx:8\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.919584\tvalid_1's auc: 0.899394\n",
      "[10000]\ttraining's auc: 0.929014\tvalid_1's auc: 0.902536\n",
      "[15000]\ttraining's auc: 0.936114\tvalid_1's auc: 0.903172\n",
      "[20000]\ttraining's auc: 0.942546\tvalid_1's auc: 0.903439\n",
      "[25000]\ttraining's auc: 0.948512\tvalid_1's auc: 0.90329\n",
      "Early stopping, best iteration is:\n",
      "[21582]\ttraining's auc: 0.944491\tvalid_1's auc: 0.903593\n",
      "Fold idx:9\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.917734\tvalid_1's auc: 0.899508\n",
      "[10000]\ttraining's auc: 0.927304\tvalid_1's auc: 0.903584\n",
      "[15000]\ttraining's auc: 0.934517\tvalid_1's auc: 0.90384\n",
      "[20000]\ttraining's auc: 0.941045\tvalid_1's auc: 0.903654\n",
      "Early stopping, best iteration is:\n",
      "[16592]\ttraining's auc: 0.936619\tvalid_1's auc: 0.903898\n",
      "Fold idx:10\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.917823\tvalid_1's auc: 0.901788\n",
      "[10000]\ttraining's auc: 0.927378\tvalid_1's auc: 0.904915\n",
      "[15000]\ttraining's auc: 0.934569\tvalid_1's auc: 0.905142\n",
      "Early stopping, best iteration is:\n",
      "[13686]\ttraining's auc: 0.932771\tvalid_1's auc: 0.905255\n",
      "CV score: 0.90847 \n"
     ]
    }
   ],
   "source": [
    "# train = X1.copy()\n",
    "# test = X2.copy()\n",
    "target = y.copy()\n",
    "\n",
    "# num_folds = 15\n",
    "# features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "\n",
    "# folds = KFold(n_splits=num_folds, random_state=2319)\n",
    "oof = np.zeros(len(train))\n",
    "getVal = np.zeros(len(train))\n",
    "predictions = np.zeros(len(target))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "n_fold = 10\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)\n",
    "\n",
    "f_cats = list(X1.columns)\n",
    "\n",
    "for f in f_cats:\n",
    "    train[f + \"_avg\"], test[f + \"_avg\"] = target_encode(trn_series=train[f],\n",
    "                                     tst_series=test[f],\n",
    "                                     target=y,\n",
    "                                     min_samples_leaf=400,\n",
    "                                     smoothing=10,\n",
    "                                     noise_level=0)\n",
    "\n",
    "features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "print('Light GBM Model')\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    \n",
    "    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]\n",
    "    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]\n",
    "    \n",
    "    X_tr, y_tr = augment(X_train.values, y_train.values)\n",
    "#     X_tr, y_tr = X_train.values, y_train.values\n",
    "    X_tr = pd.DataFrame(X_tr)\n",
    "    \n",
    "    print(\"Fold idx:{}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "    \n",
    "    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000,\n",
    "                    early_stopping_rounds = 4000)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    getVal[val_idx]+= clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    \n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"feature\"] = features\n",
    "#     fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#     fold_importance_df[\"fold\"] = fold_ + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_lgb_me_aug_1_10_7', oof)\n",
    "np.save('../cache/preds_lgb_me_aug_1_10_7', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = predictions\n",
    "sub.to_csv('../submissions/sub12l56.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908746\tvalid_1's auc: 0.893987\n",
      "[2000]\ttraining's auc: 0.917076\tvalid_1's auc: 0.898643\n",
      "[3000]\ttraining's auc: 0.922699\tvalid_1's auc: 0.901938\n",
      "[4000]\ttraining's auc: 0.927513\tvalid_1's auc: 0.904047\n",
      "[5000]\ttraining's auc: 0.931611\tvalid_1's auc: 0.905439\n",
      "[6000]\ttraining's auc: 0.935334\tvalid_1's auc: 0.906245\n",
      "[7000]\ttraining's auc: 0.938679\tvalid_1's auc: 0.906783\n",
      "[8000]\ttraining's auc: 0.941808\tvalid_1's auc: 0.906958\n",
      "[9000]\ttraining's auc: 0.944859\tvalid_1's auc: 0.907207\n",
      "[10000]\ttraining's auc: 0.947685\tvalid_1's auc: 0.907221\n",
      "Early stopping, best iteration is:\n",
      "[9124]\ttraining's auc: 0.945196\tvalid_1's auc: 0.907228\n",
      "var_12 10 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.907275\tvalid_1's auc: 0.893154\n",
      "[2000]\ttraining's auc: 0.916613\tvalid_1's auc: 0.899089\n",
      "[3000]\ttraining's auc: 0.922859\tvalid_1's auc: 0.902577\n",
      "[4000]\ttraining's auc: 0.927673\tvalid_1's auc: 0.904701\n",
      "[5000]\ttraining's auc: 0.931684\tvalid_1's auc: 0.905922\n",
      "[6000]\ttraining's auc: 0.935353\tvalid_1's auc: 0.906623\n",
      "[7000]\ttraining's auc: 0.938756\tvalid_1's auc: 0.907085\n",
      "[8000]\ttraining's auc: 0.941944\tvalid_1's auc: 0.907241\n",
      "Early stopping, best iteration is:\n",
      "[7915]\ttraining's auc: 0.941673\tvalid_1's auc: 0.907298\n",
      "var_12 10 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908129\tvalid_1's auc: 0.89409\n",
      "[2000]\ttraining's auc: 0.916906\tvalid_1's auc: 0.899919\n",
      "[3000]\ttraining's auc: 0.922853\tvalid_1's auc: 0.90289\n",
      "[4000]\ttraining's auc: 0.927781\tvalid_1's auc: 0.904864\n",
      "[5000]\ttraining's auc: 0.931788\tvalid_1's auc: 0.906037\n",
      "[6000]\ttraining's auc: 0.935499\tvalid_1's auc: 0.906626\n",
      "[7000]\ttraining's auc: 0.93891\tvalid_1's auc: 0.906979\n",
      "[8000]\ttraining's auc: 0.942124\tvalid_1's auc: 0.907276\n",
      "[9000]\ttraining's auc: 0.945187\tvalid_1's auc: 0.907469\n",
      "[10000]\ttraining's auc: 0.948007\tvalid_1's auc: 0.907469\n",
      "Early stopping, best iteration is:\n",
      "[9590]\ttraining's auc: 0.946861\tvalid_1's auc: 0.907543\n",
      "var_12 10 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.906767\tvalid_1's auc: 0.891524\n",
      "[2000]\ttraining's auc: 0.916036\tvalid_1's auc: 0.898592\n",
      "[3000]\ttraining's auc: 0.922371\tvalid_1's auc: 0.902254\n",
      "[4000]\ttraining's auc: 0.927308\tvalid_1's auc: 0.904523\n",
      "[5000]\ttraining's auc: 0.931694\tvalid_1's auc: 0.90568\n",
      "[6000]\ttraining's auc: 0.935467\tvalid_1's auc: 0.90643\n",
      "[7000]\ttraining's auc: 0.938933\tvalid_1's auc: 0.907024\n",
      "[8000]\ttraining's auc: 0.942143\tvalid_1's auc: 0.907237\n",
      "[9000]\ttraining's auc: 0.945137\tvalid_1's auc: 0.907398\n",
      "[10000]\ttraining's auc: 0.947969\tvalid_1's auc: 0.907487\n",
      "[11000]\ttraining's auc: 0.950667\tvalid_1's auc: 0.907476\n",
      "Early stopping, best iteration is:\n",
      "[10221]\ttraining's auc: 0.948575\tvalid_1's auc: 0.907555\n",
      "var_12 10 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908961\tvalid_1's auc: 0.893768\n",
      "[2000]\ttraining's auc: 0.916782\tvalid_1's auc: 0.899244\n",
      "[3000]\ttraining's auc: 0.922665\tvalid_1's auc: 0.902614\n",
      "[4000]\ttraining's auc: 0.927534\tvalid_1's auc: 0.904498\n",
      "[5000]\ttraining's auc: 0.931907\tvalid_1's auc: 0.905671\n",
      "[6000]\ttraining's auc: 0.935734\tvalid_1's auc: 0.9063\n",
      "[7000]\ttraining's auc: 0.93909\tvalid_1's auc: 0.90666\n",
      "[8000]\ttraining's auc: 0.942317\tvalid_1's auc: 0.906949\n",
      "[9000]\ttraining's auc: 0.945361\tvalid_1's auc: 0.907055\n",
      "[10000]\ttraining's auc: 0.948277\tvalid_1's auc: 0.907307\n",
      "[11000]\ttraining's auc: 0.951058\tvalid_1's auc: 0.907248\n",
      "Early stopping, best iteration is:\n",
      "[10702]\ttraining's auc: 0.95022\tvalid_1's auc: 0.907353\n",
      "var_12 20 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.907\tvalid_1's auc: 0.89246\n",
      "[2000]\ttraining's auc: 0.916545\tvalid_1's auc: 0.899127\n",
      "[3000]\ttraining's auc: 0.922768\tvalid_1's auc: 0.902349\n",
      "[4000]\ttraining's auc: 0.927695\tvalid_1's auc: 0.904428\n",
      "[5000]\ttraining's auc: 0.932111\tvalid_1's auc: 0.905825\n",
      "[6000]\ttraining's auc: 0.935836\tvalid_1's auc: 0.906593\n",
      "[7000]\ttraining's auc: 0.93933\tvalid_1's auc: 0.907213\n",
      "[8000]\ttraining's auc: 0.942503\tvalid_1's auc: 0.907591\n",
      "[9000]\ttraining's auc: 0.945507\tvalid_1's auc: 0.907662\n",
      "[10000]\ttraining's auc: 0.94837\tvalid_1's auc: 0.90777\n",
      "Early stopping, best iteration is:\n",
      "[9856]\ttraining's auc: 0.947966\tvalid_1's auc: 0.90782\n",
      "var_12 20 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908213\tvalid_1's auc: 0.894172\n",
      "[2000]\ttraining's auc: 0.917005\tvalid_1's auc: 0.899868\n",
      "[3000]\ttraining's auc: 0.922924\tvalid_1's auc: 0.90276\n",
      "[4000]\ttraining's auc: 0.92779\tvalid_1's auc: 0.904746\n",
      "[5000]\ttraining's auc: 0.931819\tvalid_1's auc: 0.905959\n",
      "[6000]\ttraining's auc: 0.935497\tvalid_1's auc: 0.906547\n",
      "[7000]\ttraining's auc: 0.938878\tvalid_1's auc: 0.906919\n",
      "[8000]\ttraining's auc: 0.942096\tvalid_1's auc: 0.907169\n",
      "[9000]\ttraining's auc: 0.94518\tvalid_1's auc: 0.907321\n",
      "[10000]\ttraining's auc: 0.94802\tvalid_1's auc: 0.907364\n",
      "Early stopping, best iteration is:\n",
      "[9591]\ttraining's auc: 0.946871\tvalid_1's auc: 0.907432\n",
      "var_12 20 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.906695\tvalid_1's auc: 0.891493\n",
      "[2000]\ttraining's auc: 0.916021\tvalid_1's auc: 0.898516\n",
      "[3000]\ttraining's auc: 0.922337\tvalid_1's auc: 0.902112\n",
      "[4000]\ttraining's auc: 0.927288\tvalid_1's auc: 0.904418\n",
      "[5000]\ttraining's auc: 0.93167\tvalid_1's auc: 0.905611\n",
      "[6000]\ttraining's auc: 0.935448\tvalid_1's auc: 0.906363\n",
      "[7000]\ttraining's auc: 0.93892\tvalid_1's auc: 0.906952\n",
      "[8000]\ttraining's auc: 0.942122\tvalid_1's auc: 0.907149\n",
      "[9000]\ttraining's auc: 0.945155\tvalid_1's auc: 0.907316\n",
      "[10000]\ttraining's auc: 0.948033\tvalid_1's auc: 0.907368\n",
      "[11000]\ttraining's auc: 0.950762\tvalid_1's auc: 0.907387\n",
      "Early stopping, best iteration is:\n",
      "[10221]\ttraining's auc: 0.948637\tvalid_1's auc: 0.907465\n",
      "var_12 20 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908174\tvalid_1's auc: 0.893878\n",
      "[2000]\ttraining's auc: 0.917451\tvalid_1's auc: 0.8997\n",
      "[3000]\ttraining's auc: 0.922784\tvalid_1's auc: 0.902505\n",
      "[4000]\ttraining's auc: 0.927471\tvalid_1's auc: 0.904296\n",
      "[5000]\ttraining's auc: 0.931629\tvalid_1's auc: 0.905532\n",
      "[6000]\ttraining's auc: 0.935331\tvalid_1's auc: 0.906165\n",
      "[7000]\ttraining's auc: 0.938645\tvalid_1's auc: 0.906668\n",
      "[8000]\ttraining's auc: 0.941877\tvalid_1's auc: 0.906879\n",
      "[9000]\ttraining's auc: 0.94494\tvalid_1's auc: 0.907195\n",
      "[10000]\ttraining's auc: 0.947762\tvalid_1's auc: 0.907229\n",
      "Early stopping, best iteration is:\n",
      "[9434]\ttraining's auc: 0.946195\tvalid_1's auc: 0.907289\n",
      "var_12 30 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908013\tvalid_1's auc: 0.892339\n",
      "[2000]\ttraining's auc: 0.917163\tvalid_1's auc: 0.899175\n",
      "[3000]\ttraining's auc: 0.922792\tvalid_1's auc: 0.902204\n",
      "[4000]\ttraining's auc: 0.92765\tvalid_1's auc: 0.904164\n",
      "[5000]\ttraining's auc: 0.93183\tvalid_1's auc: 0.905567\n",
      "[6000]\ttraining's auc: 0.935567\tvalid_1's auc: 0.906325\n",
      "[7000]\ttraining's auc: 0.938994\tvalid_1's auc: 0.906868\n",
      "[8000]\ttraining's auc: 0.942136\tvalid_1's auc: 0.907292\n",
      "[9000]\ttraining's auc: 0.945191\tvalid_1's auc: 0.907568\n",
      "[10000]\ttraining's auc: 0.948073\tvalid_1's auc: 0.907462\n",
      "Early stopping, best iteration is:\n",
      "[9225]\ttraining's auc: 0.945854\tvalid_1's auc: 0.907597\n",
      "var_12 30 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908212\tvalid_1's auc: 0.893979\n",
      "[2000]\ttraining's auc: 0.917096\tvalid_1's auc: 0.899761\n",
      "[3000]\ttraining's auc: 0.923018\tvalid_1's auc: 0.902675\n",
      "[4000]\ttraining's auc: 0.927923\tvalid_1's auc: 0.90469\n",
      "[5000]\ttraining's auc: 0.931946\tvalid_1's auc: 0.905956\n",
      "[6000]\ttraining's auc: 0.935619\tvalid_1's auc: 0.906571\n",
      "[7000]\ttraining's auc: 0.939007\tvalid_1's auc: 0.906935\n",
      "[8000]\ttraining's auc: 0.94218\tvalid_1's auc: 0.907213\n",
      "[9000]\ttraining's auc: 0.945226\tvalid_1's auc: 0.907306\n",
      "[10000]\ttraining's auc: 0.948052\tvalid_1's auc: 0.907399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11000]\ttraining's auc: 0.950786\tvalid_1's auc: 0.907328\n",
      "Early stopping, best iteration is:\n",
      "[10256]\ttraining's auc: 0.948769\tvalid_1's auc: 0.907449\n",
      "var_12 30 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.906719\tvalid_1's auc: 0.891345\n",
      "[2000]\ttraining's auc: 0.916059\tvalid_1's auc: 0.898515\n",
      "[3000]\ttraining's auc: 0.922387\tvalid_1's auc: 0.902198\n",
      "[4000]\ttraining's auc: 0.927328\tvalid_1's auc: 0.904492\n",
      "[5000]\ttraining's auc: 0.93169\tvalid_1's auc: 0.905711\n",
      "[6000]\ttraining's auc: 0.935452\tvalid_1's auc: 0.906485\n",
      "[7000]\ttraining's auc: 0.938929\tvalid_1's auc: 0.907019\n",
      "[8000]\ttraining's auc: 0.942126\tvalid_1's auc: 0.907262\n",
      "[9000]\ttraining's auc: 0.945168\tvalid_1's auc: 0.907443\n",
      "[10000]\ttraining's auc: 0.948039\tvalid_1's auc: 0.907538\n",
      "[11000]\ttraining's auc: 0.950756\tvalid_1's auc: 0.907449\n",
      "Early stopping, best iteration is:\n",
      "[10221]\ttraining's auc: 0.948644\tvalid_1's auc: 0.9076\n",
      "var_12 30 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.91023\tvalid_1's auc: 0.89537\n",
      "[2000]\ttraining's auc: 0.918166\tvalid_1's auc: 0.900852\n",
      "[3000]\ttraining's auc: 0.923524\tvalid_1's auc: 0.903586\n",
      "[4000]\ttraining's auc: 0.927798\tvalid_1's auc: 0.905139\n",
      "[5000]\ttraining's auc: 0.931832\tvalid_1's auc: 0.906314\n",
      "[6000]\ttraining's auc: 0.935444\tvalid_1's auc: 0.907019\n",
      "[7000]\ttraining's auc: 0.938668\tvalid_1's auc: 0.907386\n",
      "[8000]\ttraining's auc: 0.941857\tvalid_1's auc: 0.907675\n",
      "[9000]\ttraining's auc: 0.944804\tvalid_1's auc: 0.907762\n",
      "[10000]\ttraining's auc: 0.947594\tvalid_1's auc: 0.907855\n",
      "[11000]\ttraining's auc: 0.950348\tvalid_1's auc: 0.907969\n",
      "[12000]\ttraining's auc: 0.953028\tvalid_1's auc: 0.907945\n",
      "Early stopping, best iteration is:\n",
      "[11097]\ttraining's auc: 0.9506\tvalid_1's auc: 0.908008\n",
      "var_12 40 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908439\tvalid_1's auc: 0.892546\n",
      "[2000]\ttraining's auc: 0.917079\tvalid_1's auc: 0.899428\n",
      "[3000]\ttraining's auc: 0.923044\tvalid_1's auc: 0.902288\n",
      "[4000]\ttraining's auc: 0.927651\tvalid_1's auc: 0.904091\n",
      "[5000]\ttraining's auc: 0.931844\tvalid_1's auc: 0.905332\n",
      "[6000]\ttraining's auc: 0.935595\tvalid_1's auc: 0.906417\n",
      "[7000]\ttraining's auc: 0.938909\tvalid_1's auc: 0.906793\n",
      "[8000]\ttraining's auc: 0.942061\tvalid_1's auc: 0.907136\n",
      "[9000]\ttraining's auc: 0.945048\tvalid_1's auc: 0.907358\n",
      "[10000]\ttraining's auc: 0.947881\tvalid_1's auc: 0.907419\n",
      "Early stopping, best iteration is:\n",
      "[9560]\ttraining's auc: 0.94666\tvalid_1's auc: 0.907481\n",
      "var_12 40 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908221\tvalid_1's auc: 0.894007\n",
      "[2000]\ttraining's auc: 0.917044\tvalid_1's auc: 0.899801\n",
      "[3000]\ttraining's auc: 0.922987\tvalid_1's auc: 0.902806\n",
      "[4000]\ttraining's auc: 0.927882\tvalid_1's auc: 0.904749\n",
      "[5000]\ttraining's auc: 0.931908\tvalid_1's auc: 0.905959\n",
      "[6000]\ttraining's auc: 0.935601\tvalid_1's auc: 0.906594\n",
      "[7000]\ttraining's auc: 0.938994\tvalid_1's auc: 0.906943\n",
      "[8000]\ttraining's auc: 0.942181\tvalid_1's auc: 0.907162\n",
      "[9000]\ttraining's auc: 0.945229\tvalid_1's auc: 0.907278\n",
      "[10000]\ttraining's auc: 0.948072\tvalid_1's auc: 0.907336\n",
      "Early stopping, best iteration is:\n",
      "[9590]\ttraining's auc: 0.946921\tvalid_1's auc: 0.907407\n",
      "var_12 40 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.906577\tvalid_1's auc: 0.891352\n",
      "[2000]\ttraining's auc: 0.916007\tvalid_1's auc: 0.898543\n",
      "[3000]\ttraining's auc: 0.922361\tvalid_1's auc: 0.902157\n",
      "[4000]\ttraining's auc: 0.927346\tvalid_1's auc: 0.904446\n",
      "[5000]\ttraining's auc: 0.931742\tvalid_1's auc: 0.905659\n",
      "[6000]\ttraining's auc: 0.935501\tvalid_1's auc: 0.90641\n",
      "[7000]\ttraining's auc: 0.938987\tvalid_1's auc: 0.906954\n",
      "[8000]\ttraining's auc: 0.942163\tvalid_1's auc: 0.907223\n",
      "[9000]\ttraining's auc: 0.94519\tvalid_1's auc: 0.907356\n",
      "[10000]\ttraining's auc: 0.948043\tvalid_1's auc: 0.907476\n",
      "[11000]\ttraining's auc: 0.950744\tvalid_1's auc: 0.907463\n",
      "Early stopping, best iteration is:\n",
      "[10221]\ttraining's auc: 0.948647\tvalid_1's auc: 0.907552\n",
      "var_12 40 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.907559\tvalid_1's auc: 0.892073\n",
      "[2000]\ttraining's auc: 0.916591\tvalid_1's auc: 0.899201\n",
      "[3000]\ttraining's auc: 0.922512\tvalid_1's auc: 0.902587\n",
      "[4000]\ttraining's auc: 0.927434\tvalid_1's auc: 0.904742\n",
      "[5000]\ttraining's auc: 0.931498\tvalid_1's auc: 0.905781\n",
      "[6000]\ttraining's auc: 0.935215\tvalid_1's auc: 0.906475\n",
      "[7000]\ttraining's auc: 0.938693\tvalid_1's auc: 0.90699\n",
      "[8000]\ttraining's auc: 0.941954\tvalid_1's auc: 0.90724\n",
      "[9000]\ttraining's auc: 0.945016\tvalid_1's auc: 0.907473\n",
      "Early stopping, best iteration is:\n",
      "[8956]\ttraining's auc: 0.944886\tvalid_1's auc: 0.907497\n",
      "var_12 50 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.907144\tvalid_1's auc: 0.89238\n",
      "[2000]\ttraining's auc: 0.916298\tvalid_1's auc: 0.898868\n",
      "[3000]\ttraining's auc: 0.922437\tvalid_1's auc: 0.902538\n",
      "[4000]\ttraining's auc: 0.927393\tvalid_1's auc: 0.904415\n",
      "[5000]\ttraining's auc: 0.931723\tvalid_1's auc: 0.905728\n",
      "[6000]\ttraining's auc: 0.935571\tvalid_1's auc: 0.906639\n",
      "[7000]\ttraining's auc: 0.939111\tvalid_1's auc: 0.907194\n",
      "[8000]\ttraining's auc: 0.94234\tvalid_1's auc: 0.907566\n",
      "[9000]\ttraining's auc: 0.945395\tvalid_1's auc: 0.907649\n",
      "[10000]\ttraining's auc: 0.94823\tvalid_1's auc: 0.907781\n",
      "Early stopping, best iteration is:\n",
      "[9880]\ttraining's auc: 0.947899\tvalid_1's auc: 0.907807\n",
      "var_12 50 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908226\tvalid_1's auc: 0.894049\n",
      "[2000]\ttraining's auc: 0.917089\tvalid_1's auc: 0.899808\n",
      "[3000]\ttraining's auc: 0.923013\tvalid_1's auc: 0.902758\n",
      "[4000]\ttraining's auc: 0.927899\tvalid_1's auc: 0.904696\n",
      "[5000]\ttraining's auc: 0.931938\tvalid_1's auc: 0.90598\n",
      "[6000]\ttraining's auc: 0.935634\tvalid_1's auc: 0.906573\n",
      "[7000]\ttraining's auc: 0.939024\tvalid_1's auc: 0.906932\n",
      "[8000]\ttraining's auc: 0.942236\tvalid_1's auc: 0.907176\n",
      "[9000]\ttraining's auc: 0.945281\tvalid_1's auc: 0.907241\n",
      "[10000]\ttraining's auc: 0.948094\tvalid_1's auc: 0.907342\n",
      "[11000]\ttraining's auc: 0.950814\tvalid_1's auc: 0.907282\n",
      "Early stopping, best iteration is:\n",
      "[10279]\ttraining's auc: 0.948871\tvalid_1's auc: 0.907387\n",
      "var_12 50 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.906664\tvalid_1's auc: 0.891306\n",
      "[2000]\ttraining's auc: 0.916014\tvalid_1's auc: 0.898513\n",
      "[3000]\ttraining's auc: 0.922374\tvalid_1's auc: 0.90218\n",
      "[4000]\ttraining's auc: 0.927307\tvalid_1's auc: 0.904426\n",
      "[5000]\ttraining's auc: 0.931682\tvalid_1's auc: 0.905628\n",
      "[6000]\ttraining's auc: 0.935462\tvalid_1's auc: 0.906356\n",
      "[7000]\ttraining's auc: 0.938952\tvalid_1's auc: 0.906951\n",
      "[8000]\ttraining's auc: 0.942117\tvalid_1's auc: 0.907214\n",
      "[9000]\ttraining's auc: 0.945154\tvalid_1's auc: 0.90736\n",
      "[10000]\ttraining's auc: 0.948012\tvalid_1's auc: 0.907463\n",
      "[11000]\ttraining's auc: 0.950728\tvalid_1's auc: 0.907433\n",
      "Early stopping, best iteration is:\n",
      "[10221]\ttraining's auc: 0.948618\tvalid_1's auc: 0.907522\n",
      "var_12 50 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908351\tvalid_1's auc: 0.893818\n",
      "[2000]\ttraining's auc: 0.916732\tvalid_1's auc: 0.899464\n",
      "[3000]\ttraining's auc: 0.922702\tvalid_1's auc: 0.902982\n",
      "[4000]\ttraining's auc: 0.927501\tvalid_1's auc: 0.9047\n",
      "[5000]\ttraining's auc: 0.931666\tvalid_1's auc: 0.905828\n",
      "[6000]\ttraining's auc: 0.935392\tvalid_1's auc: 0.906719\n",
      "[7000]\ttraining's auc: 0.9388\tvalid_1's auc: 0.907155\n",
      "[8000]\ttraining's auc: 0.94204\tvalid_1's auc: 0.907441\n",
      "[9000]\ttraining's auc: 0.945035\tvalid_1's auc: 0.907613\n",
      "[10000]\ttraining's auc: 0.947905\tvalid_1's auc: 0.907801\n",
      "[11000]\ttraining's auc: 0.950648\tvalid_1's auc: 0.907857\n",
      "Early stopping, best iteration is:\n",
      "[10302]\ttraining's auc: 0.948738\tvalid_1's auc: 0.907903\n",
      "var_12 60 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.907388\tvalid_1's auc: 0.892297\n",
      "[2000]\ttraining's auc: 0.916877\tvalid_1's auc: 0.899436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000]\ttraining's auc: 0.922783\tvalid_1's auc: 0.902224\n",
      "[4000]\ttraining's auc: 0.927689\tvalid_1's auc: 0.904182\n",
      "[5000]\ttraining's auc: 0.931927\tvalid_1's auc: 0.905505\n",
      "[6000]\ttraining's auc: 0.935638\tvalid_1's auc: 0.906312\n",
      "[7000]\ttraining's auc: 0.939081\tvalid_1's auc: 0.906938\n",
      "[8000]\ttraining's auc: 0.942305\tvalid_1's auc: 0.907144\n",
      "[9000]\ttraining's auc: 0.94538\tvalid_1's auc: 0.907304\n",
      "[10000]\ttraining's auc: 0.948272\tvalid_1's auc: 0.907399\n",
      "[11000]\ttraining's auc: 0.951024\tvalid_1's auc: 0.907233\n",
      "Early stopping, best iteration is:\n",
      "[10271]\ttraining's auc: 0.949068\tvalid_1's auc: 0.907433\n",
      "var_12 60 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.90823\tvalid_1's auc: 0.894056\n",
      "[2000]\ttraining's auc: 0.91706\tvalid_1's auc: 0.89981\n",
      "[3000]\ttraining's auc: 0.922998\tvalid_1's auc: 0.90275\n",
      "[4000]\ttraining's auc: 0.927909\tvalid_1's auc: 0.904686\n",
      "[5000]\ttraining's auc: 0.931939\tvalid_1's auc: 0.90589\n",
      "[6000]\ttraining's auc: 0.935612\tvalid_1's auc: 0.906508\n",
      "[7000]\ttraining's auc: 0.939009\tvalid_1's auc: 0.906849\n",
      "[8000]\ttraining's auc: 0.942193\tvalid_1's auc: 0.907127\n",
      "[9000]\ttraining's auc: 0.945237\tvalid_1's auc: 0.907195\n",
      "[10000]\ttraining's auc: 0.948083\tvalid_1's auc: 0.907294\n",
      "[11000]\ttraining's auc: 0.950783\tvalid_1's auc: 0.907252\n",
      "Early stopping, best iteration is:\n",
      "[10266]\ttraining's auc: 0.948817\tvalid_1's auc: 0.907347\n",
      "var_12 60 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.906683\tvalid_1's auc: 0.891322\n",
      "[2000]\ttraining's auc: 0.916028\tvalid_1's auc: 0.898508\n",
      "[3000]\ttraining's auc: 0.922354\tvalid_1's auc: 0.902229\n",
      "[4000]\ttraining's auc: 0.927313\tvalid_1's auc: 0.904496\n",
      "[5000]\ttraining's auc: 0.931703\tvalid_1's auc: 0.905693\n",
      "[6000]\ttraining's auc: 0.935473\tvalid_1's auc: 0.90651\n",
      "[7000]\ttraining's auc: 0.93894\tvalid_1's auc: 0.907\n",
      "[8000]\ttraining's auc: 0.942134\tvalid_1's auc: 0.907309\n",
      "[9000]\ttraining's auc: 0.94516\tvalid_1's auc: 0.907423\n",
      "[10000]\ttraining's auc: 0.948026\tvalid_1's auc: 0.907469\n",
      "[11000]\ttraining's auc: 0.950738\tvalid_1's auc: 0.907439\n",
      "Early stopping, best iteration is:\n",
      "[10221]\ttraining's auc: 0.948626\tvalid_1's auc: 0.907543\n",
      "var_12 60 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.910185\tvalid_1's auc: 0.89523\n",
      "[2000]\ttraining's auc: 0.917327\tvalid_1's auc: 0.899563\n",
      "[3000]\ttraining's auc: 0.923033\tvalid_1's auc: 0.902311\n",
      "[4000]\ttraining's auc: 0.927663\tvalid_1's auc: 0.903871\n",
      "[5000]\ttraining's auc: 0.931764\tvalid_1's auc: 0.905296\n",
      "[6000]\ttraining's auc: 0.93539\tvalid_1's auc: 0.906145\n",
      "[7000]\ttraining's auc: 0.938819\tvalid_1's auc: 0.90659\n",
      "[8000]\ttraining's auc: 0.941929\tvalid_1's auc: 0.906907\n",
      "[9000]\ttraining's auc: 0.944947\tvalid_1's auc: 0.907002\n",
      "[10000]\ttraining's auc: 0.947816\tvalid_1's auc: 0.907161\n",
      "[11000]\ttraining's auc: 0.950521\tvalid_1's auc: 0.907047\n",
      "Early stopping, best iteration is:\n",
      "[10415]\ttraining's auc: 0.948949\tvalid_1's auc: 0.907187\n",
      "var_12 70 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908971\tvalid_1's auc: 0.8928\n",
      "[2000]\ttraining's auc: 0.916884\tvalid_1's auc: 0.898868\n",
      "[3000]\ttraining's auc: 0.922819\tvalid_1's auc: 0.902257\n",
      "[4000]\ttraining's auc: 0.927779\tvalid_1's auc: 0.904447\n",
      "[5000]\ttraining's auc: 0.931901\tvalid_1's auc: 0.90566\n",
      "[6000]\ttraining's auc: 0.935627\tvalid_1's auc: 0.906515\n",
      "[7000]\ttraining's auc: 0.938937\tvalid_1's auc: 0.906727\n",
      "[8000]\ttraining's auc: 0.942146\tvalid_1's auc: 0.907098\n",
      "[9000]\ttraining's auc: 0.945177\tvalid_1's auc: 0.9073\n",
      "[10000]\ttraining's auc: 0.947981\tvalid_1's auc: 0.907396\n",
      "[11000]\ttraining's auc: 0.950723\tvalid_1's auc: 0.907373\n",
      "Early stopping, best iteration is:\n",
      "[10634]\ttraining's auc: 0.94973\tvalid_1's auc: 0.907478\n",
      "var_12 70 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908216\tvalid_1's auc: 0.894049\n",
      "[2000]\ttraining's auc: 0.91705\tvalid_1's auc: 0.899776\n",
      "[3000]\ttraining's auc: 0.923035\tvalid_1's auc: 0.902757\n",
      "[4000]\ttraining's auc: 0.927952\tvalid_1's auc: 0.904707\n",
      "[5000]\ttraining's auc: 0.932012\tvalid_1's auc: 0.905911\n",
      "[6000]\ttraining's auc: 0.93566\tvalid_1's auc: 0.90655\n",
      "[7000]\ttraining's auc: 0.939046\tvalid_1's auc: 0.906831\n",
      "[8000]\ttraining's auc: 0.942235\tvalid_1's auc: 0.907077\n",
      "[9000]\ttraining's auc: 0.945296\tvalid_1's auc: 0.9072\n",
      "[10000]\ttraining's auc: 0.94813\tvalid_1's auc: 0.907251\n",
      "Early stopping, best iteration is:\n",
      "[9582]\ttraining's auc: 0.946953\tvalid_1's auc: 0.907274\n",
      "var_12 70 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.906665\tvalid_1's auc: 0.891322\n",
      "[2000]\ttraining's auc: 0.916016\tvalid_1's auc: 0.898504\n",
      "[3000]\ttraining's auc: 0.922386\tvalid_1's auc: 0.902146\n",
      "[4000]\ttraining's auc: 0.927337\tvalid_1's auc: 0.90447\n",
      "[5000]\ttraining's auc: 0.931706\tvalid_1's auc: 0.905652\n",
      "[6000]\ttraining's auc: 0.935475\tvalid_1's auc: 0.906432\n",
      "[7000]\ttraining's auc: 0.938959\tvalid_1's auc: 0.90702\n",
      "[8000]\ttraining's auc: 0.94216\tvalid_1's auc: 0.907263\n",
      "[9000]\ttraining's auc: 0.945193\tvalid_1's auc: 0.907378\n",
      "[10000]\ttraining's auc: 0.948059\tvalid_1's auc: 0.907481\n",
      "[11000]\ttraining's auc: 0.950782\tvalid_1's auc: 0.907426\n",
      "Early stopping, best iteration is:\n",
      "[10221]\ttraining's auc: 0.948666\tvalid_1's auc: 0.907546\n",
      "var_12 70 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.90958\tvalid_1's auc: 0.894929\n",
      "[2000]\ttraining's auc: 0.917745\tvalid_1's auc: 0.900499\n",
      "[3000]\ttraining's auc: 0.923452\tvalid_1's auc: 0.903288\n",
      "[4000]\ttraining's auc: 0.927815\tvalid_1's auc: 0.904622\n",
      "[5000]\ttraining's auc: 0.93183\tvalid_1's auc: 0.905448\n",
      "[6000]\ttraining's auc: 0.935444\tvalid_1's auc: 0.906181\n",
      "[7000]\ttraining's auc: 0.938764\tvalid_1's auc: 0.906659\n",
      "[8000]\ttraining's auc: 0.941899\tvalid_1's auc: 0.906781\n",
      "[9000]\ttraining's auc: 0.944936\tvalid_1's auc: 0.907051\n",
      "[10000]\ttraining's auc: 0.947769\tvalid_1's auc: 0.907271\n",
      "Early stopping, best iteration is:\n",
      "[9812]\ttraining's auc: 0.947227\tvalid_1's auc: 0.907281\n",
      "var_12 80 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908571\tvalid_1's auc: 0.892756\n",
      "[2000]\ttraining's auc: 0.917569\tvalid_1's auc: 0.899581\n",
      "[3000]\ttraining's auc: 0.92307\tvalid_1's auc: 0.902812\n",
      "[4000]\ttraining's auc: 0.92783\tvalid_1's auc: 0.904955\n",
      "[5000]\ttraining's auc: 0.931928\tvalid_1's auc: 0.906225\n",
      "[6000]\ttraining's auc: 0.935606\tvalid_1's auc: 0.906844\n",
      "[7000]\ttraining's auc: 0.938919\tvalid_1's auc: 0.90731\n",
      "[8000]\ttraining's auc: 0.942043\tvalid_1's auc: 0.907448\n",
      "[9000]\ttraining's auc: 0.944985\tvalid_1's auc: 0.907724\n",
      "[10000]\ttraining's auc: 0.94778\tvalid_1's auc: 0.907682\n",
      "Early stopping, best iteration is:\n",
      "[9614]\ttraining's auc: 0.946727\tvalid_1's auc: 0.907731\n",
      "var_12 80 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908238\tvalid_1's auc: 0.894051\n",
      "[2000]\ttraining's auc: 0.917037\tvalid_1's auc: 0.89987\n",
      "[3000]\ttraining's auc: 0.922979\tvalid_1's auc: 0.902779\n",
      "[4000]\ttraining's auc: 0.927899\tvalid_1's auc: 0.904737\n",
      "[5000]\ttraining's auc: 0.931954\tvalid_1's auc: 0.90599\n",
      "[6000]\ttraining's auc: 0.935626\tvalid_1's auc: 0.906628\n",
      "[7000]\ttraining's auc: 0.938995\tvalid_1's auc: 0.90696\n",
      "[8000]\ttraining's auc: 0.94218\tvalid_1's auc: 0.907253\n",
      "[9000]\ttraining's auc: 0.94524\tvalid_1's auc: 0.907364\n",
      "[10000]\ttraining's auc: 0.948063\tvalid_1's auc: 0.907462\n",
      "[11000]\ttraining's auc: 0.950774\tvalid_1's auc: 0.907378\n",
      "Early stopping, best iteration is:\n",
      "[10279]\ttraining's auc: 0.948838\tvalid_1's auc: 0.907502\n",
      "var_12 80 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.906555\tvalid_1's auc: 0.891316\n",
      "[2000]\ttraining's auc: 0.916041\tvalid_1's auc: 0.898478\n",
      "[3000]\ttraining's auc: 0.922399\tvalid_1's auc: 0.902113\n",
      "[4000]\ttraining's auc: 0.927351\tvalid_1's auc: 0.904423\n",
      "[5000]\ttraining's auc: 0.931725\tvalid_1's auc: 0.905614\n",
      "[6000]\ttraining's auc: 0.935489\tvalid_1's auc: 0.906408\n",
      "[7000]\ttraining's auc: 0.938959\tvalid_1's auc: 0.906991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8000]\ttraining's auc: 0.942138\tvalid_1's auc: 0.907256\n",
      "[9000]\ttraining's auc: 0.945146\tvalid_1's auc: 0.907377\n",
      "[10000]\ttraining's auc: 0.947988\tvalid_1's auc: 0.907459\n",
      "[11000]\ttraining's auc: 0.9507\tvalid_1's auc: 0.907403\n",
      "Early stopping, best iteration is:\n",
      "[10221]\ttraining's auc: 0.948593\tvalid_1's auc: 0.907519\n",
      "var_12 80 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.906925\tvalid_1's auc: 0.892303\n",
      "[2000]\ttraining's auc: 0.916682\tvalid_1's auc: 0.899569\n",
      "[3000]\ttraining's auc: 0.922469\tvalid_1's auc: 0.90226\n",
      "[4000]\ttraining's auc: 0.927298\tvalid_1's auc: 0.904246\n",
      "[5000]\ttraining's auc: 0.931431\tvalid_1's auc: 0.905368\n",
      "[6000]\ttraining's auc: 0.935227\tvalid_1's auc: 0.906243\n",
      "[7000]\ttraining's auc: 0.938723\tvalid_1's auc: 0.906717\n",
      "[8000]\ttraining's auc: 0.941891\tvalid_1's auc: 0.907098\n",
      "[9000]\ttraining's auc: 0.944985\tvalid_1's auc: 0.907254\n",
      "[10000]\ttraining's auc: 0.947819\tvalid_1's auc: 0.907262\n",
      "[11000]\ttraining's auc: 0.95062\tvalid_1's auc: 0.907278\n",
      "Early stopping, best iteration is:\n",
      "[10441]\ttraining's auc: 0.949051\tvalid_1's auc: 0.907326\n",
      "var_12 90 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.907898\tvalid_1's auc: 0.892674\n",
      "[2000]\ttraining's auc: 0.916098\tvalid_1's auc: 0.898729\n",
      "[3000]\ttraining's auc: 0.922661\tvalid_1's auc: 0.902362\n",
      "[4000]\ttraining's auc: 0.927487\tvalid_1's auc: 0.904207\n",
      "[5000]\ttraining's auc: 0.931758\tvalid_1's auc: 0.905427\n",
      "[6000]\ttraining's auc: 0.935567\tvalid_1's auc: 0.906502\n",
      "[7000]\ttraining's auc: 0.939053\tvalid_1's auc: 0.907087\n",
      "[8000]\ttraining's auc: 0.942278\tvalid_1's auc: 0.907344\n",
      "[9000]\ttraining's auc: 0.945329\tvalid_1's auc: 0.907477\n",
      "[10000]\ttraining's auc: 0.94821\tvalid_1's auc: 0.907495\n",
      "[11000]\ttraining's auc: 0.950988\tvalid_1's auc: 0.907576\n",
      "[12000]\ttraining's auc: 0.953649\tvalid_1's auc: 0.907523\n",
      "Early stopping, best iteration is:\n",
      "[11215]\ttraining's auc: 0.951552\tvalid_1's auc: 0.907626\n",
      "var_12 90 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908205\tvalid_1's auc: 0.893975\n",
      "[2000]\ttraining's auc: 0.917077\tvalid_1's auc: 0.899712\n",
      "[3000]\ttraining's auc: 0.923005\tvalid_1's auc: 0.902728\n",
      "[4000]\ttraining's auc: 0.9279\tvalid_1's auc: 0.904692\n",
      "[5000]\ttraining's auc: 0.931968\tvalid_1's auc: 0.905947\n",
      "[6000]\ttraining's auc: 0.935632\tvalid_1's auc: 0.906564\n",
      "[7000]\ttraining's auc: 0.939011\tvalid_1's auc: 0.906867\n",
      "[8000]\ttraining's auc: 0.942209\tvalid_1's auc: 0.907132\n",
      "[9000]\ttraining's auc: 0.945265\tvalid_1's auc: 0.907271\n",
      "[10000]\ttraining's auc: 0.948105\tvalid_1's auc: 0.9073\n",
      "Early stopping, best iteration is:\n",
      "[9582]\ttraining's auc: 0.946911\tvalid_1's auc: 0.907371\n",
      "var_12 90 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.906673\tvalid_1's auc: 0.891315\n",
      "[2000]\ttraining's auc: 0.916081\tvalid_1's auc: 0.898489\n",
      "[3000]\ttraining's auc: 0.922424\tvalid_1's auc: 0.902163\n",
      "[4000]\ttraining's auc: 0.927355\tvalid_1's auc: 0.904445\n",
      "[5000]\ttraining's auc: 0.931714\tvalid_1's auc: 0.905662\n",
      "[6000]\ttraining's auc: 0.935487\tvalid_1's auc: 0.906468\n",
      "[7000]\ttraining's auc: 0.938967\tvalid_1's auc: 0.90704\n",
      "[8000]\ttraining's auc: 0.942176\tvalid_1's auc: 0.907327\n",
      "[9000]\ttraining's auc: 0.945181\tvalid_1's auc: 0.907469\n",
      "[10000]\ttraining's auc: 0.948033\tvalid_1's auc: 0.90759\n",
      "[11000]\ttraining's auc: 0.95076\tvalid_1's auc: 0.907561\n",
      "Early stopping, best iteration is:\n",
      "[10291]\ttraining's auc: 0.948842\tvalid_1's auc: 0.907649\n",
      "var_12 90 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908476\tvalid_1's auc: 0.894131\n",
      "[2000]\ttraining's auc: 0.916793\tvalid_1's auc: 0.899233\n",
      "[3000]\ttraining's auc: 0.922727\tvalid_1's auc: 0.902325\n",
      "[4000]\ttraining's auc: 0.927401\tvalid_1's auc: 0.903967\n",
      "[5000]\ttraining's auc: 0.931469\tvalid_1's auc: 0.904985\n",
      "[6000]\ttraining's auc: 0.935153\tvalid_1's auc: 0.905836\n",
      "[7000]\ttraining's auc: 0.938433\tvalid_1's auc: 0.906345\n",
      "[8000]\ttraining's auc: 0.941702\tvalid_1's auc: 0.906642\n",
      "[9000]\ttraining's auc: 0.944722\tvalid_1's auc: 0.906746\n",
      "[10000]\ttraining's auc: 0.947653\tvalid_1's auc: 0.906797\n",
      "[11000]\ttraining's auc: 0.950374\tvalid_1's auc: 0.906728\n",
      "Early stopping, best iteration is:\n",
      "[10275]\ttraining's auc: 0.948418\tvalid_1's auc: 0.906839\n",
      "var_12 100 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.907978\tvalid_1's auc: 0.892578\n",
      "[2000]\ttraining's auc: 0.916466\tvalid_1's auc: 0.899197\n",
      "[3000]\ttraining's auc: 0.922746\tvalid_1's auc: 0.903062\n",
      "[4000]\ttraining's auc: 0.927542\tvalid_1's auc: 0.904644\n",
      "[5000]\ttraining's auc: 0.931838\tvalid_1's auc: 0.905899\n",
      "[6000]\ttraining's auc: 0.935486\tvalid_1's auc: 0.906724\n",
      "[7000]\ttraining's auc: 0.938912\tvalid_1's auc: 0.907166\n",
      "[8000]\ttraining's auc: 0.94208\tvalid_1's auc: 0.907416\n",
      "Early stopping, best iteration is:\n",
      "[7935]\ttraining's auc: 0.941872\tvalid_1's auc: 0.907455\n",
      "var_12 100 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908191\tvalid_1's auc: 0.893966\n",
      "[2000]\ttraining's auc: 0.917047\tvalid_1's auc: 0.899712\n",
      "[3000]\ttraining's auc: 0.922979\tvalid_1's auc: 0.902746\n",
      "[4000]\ttraining's auc: 0.927913\tvalid_1's auc: 0.904704\n",
      "[5000]\ttraining's auc: 0.931951\tvalid_1's auc: 0.905928\n",
      "[6000]\ttraining's auc: 0.935641\tvalid_1's auc: 0.906567\n",
      "[7000]\ttraining's auc: 0.939003\tvalid_1's auc: 0.906876\n",
      "[8000]\ttraining's auc: 0.942181\tvalid_1's auc: 0.907137\n",
      "[9000]\ttraining's auc: 0.945261\tvalid_1's auc: 0.907251\n",
      "[10000]\ttraining's auc: 0.948077\tvalid_1's auc: 0.907291\n",
      "Early stopping, best iteration is:\n",
      "[9582]\ttraining's auc: 0.946899\tvalid_1's auc: 0.907337\n",
      "var_12 100 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.906662\tvalid_1's auc: 0.891316\n",
      "[2000]\ttraining's auc: 0.916014\tvalid_1's auc: 0.898494\n",
      "[3000]\ttraining's auc: 0.922383\tvalid_1's auc: 0.902147\n",
      "[4000]\ttraining's auc: 0.927316\tvalid_1's auc: 0.904426\n",
      "[5000]\ttraining's auc: 0.931718\tvalid_1's auc: 0.905637\n",
      "[6000]\ttraining's auc: 0.935509\tvalid_1's auc: 0.906432\n",
      "[7000]\ttraining's auc: 0.938964\tvalid_1's auc: 0.906973\n",
      "[8000]\ttraining's auc: 0.942144\tvalid_1's auc: 0.907235\n",
      "[9000]\ttraining's auc: 0.945169\tvalid_1's auc: 0.907416\n",
      "[10000]\ttraining's auc: 0.948036\tvalid_1's auc: 0.907472\n",
      "[11000]\ttraining's auc: 0.950767\tvalid_1's auc: 0.907424\n",
      "Early stopping, best iteration is:\n",
      "[10325]\ttraining's auc: 0.948936\tvalid_1's auc: 0.907526\n",
      "var_12 100 le no drop\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "num_bins = [10,20,30,40,50,60,70,80,90,100]\n",
    "col = 'var_12'\n",
    "for bins in num_bins:\n",
    "    X3 = train.copy()\n",
    "    X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "    X = pd.get_dummies(pd.cut(train[col].values, bins))\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    X3 = pd.concat([X3, X], axis=1)\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X3, y, test_size=0.2, stratify=y, random_state=123)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "   \n",
    "    print(col, bins, 'drop')\n",
    "    print('--------------------')\n",
    "    \n",
    "    X3 = train.copy()\n",
    "    X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "    X = pd.get_dummies(pd.cut(train[col].values, bins))\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    X3 = pd.concat([X3, X], axis=1)\n",
    "#     X3 = X3.drop(col, axis=1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X3, y, test_size=0.2, stratify=y, random_state=123)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "   \n",
    "    print(col, bins, 'no drop')\n",
    "    print('--------------------')\n",
    "    \n",
    "    X3 = train.copy()\n",
    "    X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "    X = pd.cut(train[col].values, bins)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    X = le.fit_transform(X)\n",
    "    X3['new_col'] = X\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X3, y, test_size=0.2, stratify=y, random_state=123)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "   \n",
    "    print(col, bins, 'le drop')\n",
    "    print('--------------------')\n",
    "    \n",
    "    X3 = train.copy()\n",
    "    X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "    X = pd.cut(train[col].values, bins)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    X = le.fit_transform(X)\n",
    "    X3['new_col'] = X\n",
    "#     X3 = X3.drop(col, axis=1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X3, y, test_size=0.2, stratify=y, random_state=123)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "   \n",
    "    print(col, bins, 'le no drop')\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908746\tvalid_1's auc: 0.893987\n",
      "[2000]\ttraining's auc: 0.917076\tvalid_1's auc: 0.898643\n",
      "[3000]\ttraining's auc: 0.922699\tvalid_1's auc: 0.901938\n",
      "[4000]\ttraining's auc: 0.927513\tvalid_1's auc: 0.904047\n",
      "[5000]\ttraining's auc: 0.931611\tvalid_1's auc: 0.905439\n",
      "[6000]\ttraining's auc: 0.935334\tvalid_1's auc: 0.906245\n",
      "[7000]\ttraining's auc: 0.938679\tvalid_1's auc: 0.906783\n",
      "[8000]\ttraining's auc: 0.941808\tvalid_1's auc: 0.906958\n",
      "[9000]\ttraining's auc: 0.944859\tvalid_1's auc: 0.907207\n",
      "[10000]\ttraining's auc: 0.947685\tvalid_1's auc: 0.907221\n",
      "Early stopping, best iteration is:\n",
      "[9124]\ttraining's auc: 0.945196\tvalid_1's auc: 0.907228\n",
      "var_12 10 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908961\tvalid_1's auc: 0.893768\n",
      "[2000]\ttraining's auc: 0.916782\tvalid_1's auc: 0.899244\n",
      "[3000]\ttraining's auc: 0.922665\tvalid_1's auc: 0.902614\n",
      "[4000]\ttraining's auc: 0.927534\tvalid_1's auc: 0.904498\n",
      "[5000]\ttraining's auc: 0.931907\tvalid_1's auc: 0.905671\n",
      "[6000]\ttraining's auc: 0.935734\tvalid_1's auc: 0.9063\n",
      "[7000]\ttraining's auc: 0.93909\tvalid_1's auc: 0.90666\n",
      "[8000]\ttraining's auc: 0.942317\tvalid_1's auc: 0.906949\n",
      "[9000]\ttraining's auc: 0.945361\tvalid_1's auc: 0.907055\n",
      "[10000]\ttraining's auc: 0.948277\tvalid_1's auc: 0.907307\n",
      "[11000]\ttraining's auc: 0.951058\tvalid_1's auc: 0.907248\n",
      "Early stopping, best iteration is:\n",
      "[10702]\ttraining's auc: 0.95022\tvalid_1's auc: 0.907353\n",
      "var_12 20 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908174\tvalid_1's auc: 0.893878\n",
      "[2000]\ttraining's auc: 0.917451\tvalid_1's auc: 0.8997\n",
      "[3000]\ttraining's auc: 0.922784\tvalid_1's auc: 0.902505\n",
      "[4000]\ttraining's auc: 0.927471\tvalid_1's auc: 0.904296\n",
      "[5000]\ttraining's auc: 0.931629\tvalid_1's auc: 0.905532\n",
      "[6000]\ttraining's auc: 0.935331\tvalid_1's auc: 0.906165\n",
      "[7000]\ttraining's auc: 0.938645\tvalid_1's auc: 0.906668\n",
      "[8000]\ttraining's auc: 0.941877\tvalid_1's auc: 0.906879\n",
      "[9000]\ttraining's auc: 0.94494\tvalid_1's auc: 0.907195\n",
      "[10000]\ttraining's auc: 0.947762\tvalid_1's auc: 0.907229\n",
      "Early stopping, best iteration is:\n",
      "[9434]\ttraining's auc: 0.946195\tvalid_1's auc: 0.907289\n",
      "var_12 30 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.91023\tvalid_1's auc: 0.89537\n",
      "[2000]\ttraining's auc: 0.918166\tvalid_1's auc: 0.900852\n",
      "[3000]\ttraining's auc: 0.923524\tvalid_1's auc: 0.903586\n",
      "[4000]\ttraining's auc: 0.927798\tvalid_1's auc: 0.905139\n",
      "[5000]\ttraining's auc: 0.931832\tvalid_1's auc: 0.906314\n",
      "[6000]\ttraining's auc: 0.935444\tvalid_1's auc: 0.907019\n",
      "[7000]\ttraining's auc: 0.938668\tvalid_1's auc: 0.907386\n",
      "[8000]\ttraining's auc: 0.941857\tvalid_1's auc: 0.907675\n",
      "[9000]\ttraining's auc: 0.944804\tvalid_1's auc: 0.907762\n",
      "[10000]\ttraining's auc: 0.947594\tvalid_1's auc: 0.907855\n",
      "[11000]\ttraining's auc: 0.950348\tvalid_1's auc: 0.907969\n",
      "[12000]\ttraining's auc: 0.953028\tvalid_1's auc: 0.907945\n",
      "Early stopping, best iteration is:\n",
      "[11097]\ttraining's auc: 0.9506\tvalid_1's auc: 0.908008\n",
      "var_12 40 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.907559\tvalid_1's auc: 0.892073\n",
      "[2000]\ttraining's auc: 0.916591\tvalid_1's auc: 0.899201\n",
      "[3000]\ttraining's auc: 0.922512\tvalid_1's auc: 0.902587\n",
      "[4000]\ttraining's auc: 0.927434\tvalid_1's auc: 0.904742\n",
      "[5000]\ttraining's auc: 0.931498\tvalid_1's auc: 0.905781\n",
      "[6000]\ttraining's auc: 0.935215\tvalid_1's auc: 0.906475\n",
      "[7000]\ttraining's auc: 0.938693\tvalid_1's auc: 0.90699\n",
      "[8000]\ttraining's auc: 0.941954\tvalid_1's auc: 0.90724\n",
      "[9000]\ttraining's auc: 0.945016\tvalid_1's auc: 0.907473\n",
      "Early stopping, best iteration is:\n",
      "[8956]\ttraining's auc: 0.944886\tvalid_1's auc: 0.907497\n",
      "var_12 50 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908351\tvalid_1's auc: 0.893818\n",
      "[2000]\ttraining's auc: 0.916732\tvalid_1's auc: 0.899464\n",
      "[3000]\ttraining's auc: 0.922702\tvalid_1's auc: 0.902982\n",
      "[4000]\ttraining's auc: 0.927501\tvalid_1's auc: 0.9047\n",
      "[5000]\ttraining's auc: 0.931666\tvalid_1's auc: 0.905828\n",
      "[6000]\ttraining's auc: 0.935392\tvalid_1's auc: 0.906719\n",
      "[7000]\ttraining's auc: 0.9388\tvalid_1's auc: 0.907155\n",
      "[8000]\ttraining's auc: 0.94204\tvalid_1's auc: 0.907441\n",
      "[9000]\ttraining's auc: 0.945035\tvalid_1's auc: 0.907613\n",
      "[10000]\ttraining's auc: 0.947905\tvalid_1's auc: 0.907801\n",
      "[11000]\ttraining's auc: 0.950648\tvalid_1's auc: 0.907857\n",
      "Early stopping, best iteration is:\n",
      "[10302]\ttraining's auc: 0.948738\tvalid_1's auc: 0.907903\n",
      "var_12 60 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.910185\tvalid_1's auc: 0.89523\n",
      "[2000]\ttraining's auc: 0.917327\tvalid_1's auc: 0.899563\n",
      "[3000]\ttraining's auc: 0.923033\tvalid_1's auc: 0.902311\n",
      "[4000]\ttraining's auc: 0.927663\tvalid_1's auc: 0.903871\n",
      "[5000]\ttraining's auc: 0.931764\tvalid_1's auc: 0.905296\n",
      "[6000]\ttraining's auc: 0.93539\tvalid_1's auc: 0.906145\n",
      "[7000]\ttraining's auc: 0.938819\tvalid_1's auc: 0.90659\n",
      "[8000]\ttraining's auc: 0.941929\tvalid_1's auc: 0.906907\n",
      "[9000]\ttraining's auc: 0.944947\tvalid_1's auc: 0.907002\n",
      "[10000]\ttraining's auc: 0.947816\tvalid_1's auc: 0.907161\n",
      "[11000]\ttraining's auc: 0.950521\tvalid_1's auc: 0.907047\n",
      "Early stopping, best iteration is:\n",
      "[10415]\ttraining's auc: 0.948949\tvalid_1's auc: 0.907187\n",
      "var_12 70 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.90958\tvalid_1's auc: 0.894929\n",
      "[2000]\ttraining's auc: 0.917745\tvalid_1's auc: 0.900499\n",
      "[3000]\ttraining's auc: 0.923452\tvalid_1's auc: 0.903288\n",
      "[4000]\ttraining's auc: 0.927815\tvalid_1's auc: 0.904622\n",
      "[5000]\ttraining's auc: 0.93183\tvalid_1's auc: 0.905448\n",
      "[6000]\ttraining's auc: 0.935444\tvalid_1's auc: 0.906181\n",
      "[7000]\ttraining's auc: 0.938764\tvalid_1's auc: 0.906659\n",
      "[8000]\ttraining's auc: 0.941899\tvalid_1's auc: 0.906781\n",
      "[9000]\ttraining's auc: 0.944936\tvalid_1's auc: 0.907051\n",
      "[10000]\ttraining's auc: 0.947769\tvalid_1's auc: 0.907271\n",
      "Early stopping, best iteration is:\n",
      "[9812]\ttraining's auc: 0.947227\tvalid_1's auc: 0.907281\n",
      "var_12 80 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.906925\tvalid_1's auc: 0.892303\n",
      "[2000]\ttraining's auc: 0.916682\tvalid_1's auc: 0.899569\n",
      "[3000]\ttraining's auc: 0.922469\tvalid_1's auc: 0.90226\n",
      "[4000]\ttraining's auc: 0.927298\tvalid_1's auc: 0.904246\n",
      "[5000]\ttraining's auc: 0.931431\tvalid_1's auc: 0.905368\n",
      "[6000]\ttraining's auc: 0.935227\tvalid_1's auc: 0.906243\n",
      "[7000]\ttraining's auc: 0.938723\tvalid_1's auc: 0.906717\n",
      "[8000]\ttraining's auc: 0.941891\tvalid_1's auc: 0.907098\n",
      "[9000]\ttraining's auc: 0.944985\tvalid_1's auc: 0.907254\n",
      "[10000]\ttraining's auc: 0.947819\tvalid_1's auc: 0.907262\n",
      "[11000]\ttraining's auc: 0.95062\tvalid_1's auc: 0.907278\n",
      "Early stopping, best iteration is:\n",
      "[10441]\ttraining's auc: 0.949051\tvalid_1's auc: 0.907326\n",
      "var_12 90 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.908476\tvalid_1's auc: 0.894131\n",
      "[2000]\ttraining's auc: 0.916793\tvalid_1's auc: 0.899233\n",
      "[3000]\ttraining's auc: 0.922727\tvalid_1's auc: 0.902325\n",
      "[4000]\ttraining's auc: 0.927401\tvalid_1's auc: 0.903967\n",
      "[5000]\ttraining's auc: 0.931469\tvalid_1's auc: 0.904985\n",
      "[6000]\ttraining's auc: 0.935153\tvalid_1's auc: 0.905836\n",
      "[7000]\ttraining's auc: 0.938433\tvalid_1's auc: 0.906345\n",
      "[8000]\ttraining's auc: 0.941702\tvalid_1's auc: 0.906642\n",
      "[9000]\ttraining's auc: 0.944722\tvalid_1's auc: 0.906746\n",
      "[10000]\ttraining's auc: 0.947653\tvalid_1's auc: 0.906797\n",
      "[11000]\ttraining's auc: 0.950374\tvalid_1's auc: 0.906728\n",
      "Early stopping, best iteration is:\n",
      "[10275]\ttraining's auc: 0.948418\tvalid_1's auc: 0.906839\n",
      "var_12 100 drop\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "num_bins = [10,20,30,40,50,60,70,80,90,100]\n",
    "col = 'var_12'\n",
    "for bins in num_bins:\n",
    "    X3 = train.copy()\n",
    "    X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "    X = pd.get_dummies(pd.cut(train[col].values, bins))\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    X3 = pd.concat([X3, X], axis=1)\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X3, y, test_size=0.2, stratify=y, random_state=123)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "   \n",
    "    print(col, bins, 'drop')\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv.zip')\n",
    "y = train['target']\n",
    "test = pd.read_csv('../data/test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    200000.000000\n",
       "mean         14.023978\n",
       "std           0.190059\n",
       "min          13.434600\n",
       "25%          13.894000\n",
       "50%          14.025500\n",
       "75%          14.164200\n",
       "max          14.654500\n",
       "Name: var_12, dtype: float64"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['var_12'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    200000.000000\n",
       "mean         14.022662\n",
       "std           0.190071\n",
       "min          13.424500\n",
       "25%          13.891000\n",
       "50%          14.024600\n",
       "75%          14.162900\n",
       "max          14.682000\n",
       "Name: var_12, dtype: float64"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['var_12'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(13.42, 14.69, 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.42   , 13.45175, 13.4835 , 13.51525, 13.547  , 13.57875,\n",
       "       13.6105 , 13.64225, 13.674  , 13.70575, 13.7375 , 13.76925,\n",
       "       13.801  , 13.83275, 13.8645 , 13.89625, 13.928  , 13.95975,\n",
       "       13.9915 , 14.02325, 14.055  , 14.08675, 14.1185 , 14.15025,\n",
       "       14.182  , 14.21375, 14.2455 , 14.27725, 14.309  , 14.34075,\n",
       "       14.3725 , 14.40425, 14.436  , 14.46775, 14.4995 , 14.53125,\n",
       "       14.563  , 14.59475, 14.6265 , 14.65825, 14.69   ])"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = [13.42   , 13.4835 , 13.51525, 13.547  , 13.57875,\n",
    "       13.6105 , 13.64225, 13.674  , 13.70575, 13.7375 , 13.76925,\n",
    "       13.801  , 13.83275, 13.8645 , 13.89625, 13.928  , 13.95975,\n",
    "       13.9915 , 14.02325, 14.055  , 14.08675, 14.1185 , 14.15025,\n",
    "       14.182  , 14.21375, 14.2455 , 14.27725, 14.309  , 14.34075,\n",
    "       14.3725 , 14.40425, 14.436  , 14.46775, 14.4995 , 14.53125,\n",
    "       14.563  , 14.59475, 14.6265 , 14.69   ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_12\n",
      "-------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200000, 200), (200000, 200))"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = train.copy()\n",
    "X4 = test.copy()\n",
    "# bins = np.linspace(-6.0,6.0,40)\n",
    "X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "X4 = X4.drop('ID_code', axis=1)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "for col in ['var_12']:\n",
    "    print(col)\n",
    "    X3_col = pd.cut(X3[col], bins)\n",
    "    X4_col = pd.cut(X4[col], bins)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(X3_col)\n",
    "    X3_col = le.transform(X3_col)\n",
    "    X4_col = le.transform(X4_col)\n",
    "    \n",
    "#     X3_col_bins = pd.DataFrame(X3_col_bins)\n",
    "#     cols = [col + '_' + str(i) for i in range(X3_col_bins.shape[1])]\n",
    "#     X3_col_bins.columns = cols\n",
    "\n",
    "#     X4_col_bins = pd.DataFrame(X4_col_bins)\n",
    "#     X4_col_bins.columns = cols\n",
    " \n",
    "#     X3 = pd.concat([X3, X3_col_bins], axis=1)\n",
    "#     X4 = pd.concat([X4, X4_col_bins], axis=1)\n",
    "    X3['var_12_new'] = X3_col\n",
    "    X4['var_12_new'] = X4_col\n",
    "    \n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X4 = X4.drop(col, axis=1)\n",
    "    \n",
    "#     del X3_col_bins, X4_col_bins\n",
    "    gc.collect()\n",
    "    print('-------------')\n",
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Fri Mar 22 19:33:11 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885688\n",
      "[2000]\tvalid_0's auc: 0.892383\n",
      "[3000]\tvalid_0's auc: 0.89596\n",
      "[4000]\tvalid_0's auc: 0.897816\n",
      "[5000]\tvalid_0's auc: 0.899061\n",
      "[6000]\tvalid_0's auc: 0.899805\n",
      "[7000]\tvalid_0's auc: 0.900372\n",
      "[8000]\tvalid_0's auc: 0.900486\n",
      "[9000]\tvalid_0's auc: 0.900799\n",
      "[10000]\tvalid_0's auc: 0.900968\n",
      "[11000]\tvalid_0's auc: 0.901067\n",
      "[12000]\tvalid_0's auc: 0.900934\n",
      "[13000]\tvalid_0's auc: 0.900793\n",
      "[14000]\tvalid_0's auc: 0.90072\n",
      "Early stopping, best iteration is:\n",
      "[11304]\tvalid_0's auc: 0.901137\n",
      "Fold 1 started at Fri Mar 22 19:34:47 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883724\n",
      "[2000]\tvalid_0's auc: 0.890109\n",
      "[3000]\tvalid_0's auc: 0.89323\n",
      "[4000]\tvalid_0's auc: 0.894841\n",
      "[5000]\tvalid_0's auc: 0.895962\n",
      "[6000]\tvalid_0's auc: 0.896906\n",
      "[7000]\tvalid_0's auc: 0.897548\n",
      "[8000]\tvalid_0's auc: 0.897775\n",
      "[9000]\tvalid_0's auc: 0.898159\n",
      "[10000]\tvalid_0's auc: 0.898058\n",
      "[11000]\tvalid_0's auc: 0.898165\n",
      "[12000]\tvalid_0's auc: 0.898155\n",
      "[13000]\tvalid_0's auc: 0.898235\n",
      "[14000]\tvalid_0's auc: 0.898061\n",
      "Early stopping, best iteration is:\n",
      "[11720]\tvalid_0's auc: 0.898278\n",
      "Fold 2 started at Fri Mar 22 19:36:30 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.887984\n",
      "[2000]\tvalid_0's auc: 0.892712\n",
      "[3000]\tvalid_0's auc: 0.895597\n",
      "[4000]\tvalid_0's auc: 0.897332\n",
      "[5000]\tvalid_0's auc: 0.89839\n",
      "[6000]\tvalid_0's auc: 0.898963\n",
      "[7000]\tvalid_0's auc: 0.899384\n",
      "[8000]\tvalid_0's auc: 0.899388\n",
      "[9000]\tvalid_0's auc: 0.899412\n",
      "[10000]\tvalid_0's auc: 0.899296\n",
      "[11000]\tvalid_0's auc: 0.899273\n",
      "Early stopping, best iteration is:\n",
      "[8408]\tvalid_0's auc: 0.899512\n",
      "Fold 3 started at Fri Mar 22 19:37:56 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.888761\n",
      "[2000]\tvalid_0's auc: 0.894288\n",
      "[3000]\tvalid_0's auc: 0.8975\n",
      "[4000]\tvalid_0's auc: 0.899478\n",
      "[5000]\tvalid_0's auc: 0.900767\n",
      "[6000]\tvalid_0's auc: 0.901539\n",
      "[7000]\tvalid_0's auc: 0.902054\n",
      "[8000]\tvalid_0's auc: 0.902448\n",
      "[9000]\tvalid_0's auc: 0.902604\n",
      "[10000]\tvalid_0's auc: 0.902541\n",
      "[11000]\tvalid_0's auc: 0.90255\n",
      "[12000]\tvalid_0's auc: 0.902648\n",
      "[13000]\tvalid_0's auc: 0.902594\n",
      "[14000]\tvalid_0's auc: 0.90263\n",
      "[15000]\tvalid_0's auc: 0.902592\n",
      "[16000]\tvalid_0's auc: 0.902502\n",
      "[17000]\tvalid_0's auc: 0.902535\n",
      "Early stopping, best iteration is:\n",
      "[14323]\tvalid_0's auc: 0.902724\n",
      "Fold 4 started at Fri Mar 22 19:40:37 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.892887\n",
      "[2000]\tvalid_0's auc: 0.899252\n",
      "[3000]\tvalid_0's auc: 0.903504\n",
      "[4000]\tvalid_0's auc: 0.906084\n",
      "[5000]\tvalid_0's auc: 0.907808\n",
      "[6000]\tvalid_0's auc: 0.908646\n",
      "[7000]\tvalid_0's auc: 0.908944\n",
      "[8000]\tvalid_0's auc: 0.909343\n",
      "[9000]\tvalid_0's auc: 0.90972\n",
      "[10000]\tvalid_0's auc: 0.909871\n",
      "[11000]\tvalid_0's auc: 0.909962\n",
      "[12000]\tvalid_0's auc: 0.910058\n",
      "[13000]\tvalid_0's auc: 0.910019\n",
      "[14000]\tvalid_0's auc: 0.909803\n",
      "[15000]\tvalid_0's auc: 0.909809\n",
      "Early stopping, best iteration is:\n",
      "[12727]\tvalid_0's auc: 0.910082\n",
      "Fold 5 started at Fri Mar 22 19:43:16 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.887939\n",
      "[2000]\tvalid_0's auc: 0.894899\n",
      "[3000]\tvalid_0's auc: 0.898556\n",
      "[4000]\tvalid_0's auc: 0.900522\n",
      "[5000]\tvalid_0's auc: 0.901878\n",
      "[6000]\tvalid_0's auc: 0.902928\n",
      "[7000]\tvalid_0's auc: 0.903308\n",
      "[8000]\tvalid_0's auc: 0.903562\n",
      "[9000]\tvalid_0's auc: 0.903602\n",
      "[10000]\tvalid_0's auc: 0.90373\n",
      "[11000]\tvalid_0's auc: 0.903756\n",
      "[12000]\tvalid_0's auc: 0.903486\n",
      "[13000]\tvalid_0's auc: 0.90354\n",
      "Early stopping, best iteration is:\n",
      "[10459]\tvalid_0's auc: 0.903801\n",
      "Fold 6 started at Fri Mar 22 19:45:36 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.889299\n",
      "[2000]\tvalid_0's auc: 0.89478\n",
      "[3000]\tvalid_0's auc: 0.89808\n",
      "[4000]\tvalid_0's auc: 0.900645\n",
      "[5000]\tvalid_0's auc: 0.902074\n",
      "[6000]\tvalid_0's auc: 0.903177\n",
      "[7000]\tvalid_0's auc: 0.903786\n",
      "[8000]\tvalid_0's auc: 0.904185\n",
      "[9000]\tvalid_0's auc: 0.904347\n",
      "[10000]\tvalid_0's auc: 0.904362\n",
      "[11000]\tvalid_0's auc: 0.904505\n",
      "[12000]\tvalid_0's auc: 0.904542\n",
      "[13000]\tvalid_0's auc: 0.904533\n",
      "[14000]\tvalid_0's auc: 0.904597\n",
      "[15000]\tvalid_0's auc: 0.904527\n",
      "[16000]\tvalid_0's auc: 0.904421\n",
      "[17000]\tvalid_0's auc: 0.904202\n",
      "Early stopping, best iteration is:\n",
      "[14355]\tvalid_0's auc: 0.90466\n",
      "Fold 7 started at Fri Mar 22 19:48:58 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.879395\n",
      "[2000]\tvalid_0's auc: 0.885881\n",
      "[3000]\tvalid_0's auc: 0.8893\n",
      "[4000]\tvalid_0's auc: 0.891166\n",
      "[5000]\tvalid_0's auc: 0.892748\n",
      "[6000]\tvalid_0's auc: 0.893604\n",
      "[7000]\tvalid_0's auc: 0.894253\n",
      "[8000]\tvalid_0's auc: 0.894615\n",
      "[9000]\tvalid_0's auc: 0.894957\n",
      "[10000]\tvalid_0's auc: 0.895118\n",
      "[11000]\tvalid_0's auc: 0.895059\n",
      "[12000]\tvalid_0's auc: 0.895075\n",
      "Early stopping, best iteration is:\n",
      "[9990]\tvalid_0's auc: 0.895142\n",
      "Fold 8 started at Fri Mar 22 19:51:13 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.881655\n",
      "[2000]\tvalid_0's auc: 0.887395\n",
      "[3000]\tvalid_0's auc: 0.890537\n",
      "[4000]\tvalid_0's auc: 0.893001\n",
      "[5000]\tvalid_0's auc: 0.894218\n",
      "[6000]\tvalid_0's auc: 0.895227\n",
      "[7000]\tvalid_0's auc: 0.895826\n",
      "[8000]\tvalid_0's auc: 0.896194\n",
      "[9000]\tvalid_0's auc: 0.896106\n",
      "[10000]\tvalid_0's auc: 0.896118\n",
      "[11000]\tvalid_0's auc: 0.896056\n",
      "Early stopping, best iteration is:\n",
      "[8301]\tvalid_0's auc: 0.896238\n",
      "Fold 9 started at Fri Mar 22 19:53:17 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.882541\n",
      "[2000]\tvalid_0's auc: 0.888078\n",
      "[3000]\tvalid_0's auc: 0.892038\n",
      "[4000]\tvalid_0's auc: 0.894403\n",
      "[5000]\tvalid_0's auc: 0.895866\n",
      "[6000]\tvalid_0's auc: 0.896898\n",
      "[7000]\tvalid_0's auc: 0.897359\n",
      "[8000]\tvalid_0's auc: 0.897723\n",
      "[9000]\tvalid_0's auc: 0.897925\n",
      "[10000]\tvalid_0's auc: 0.898007\n",
      "[11000]\tvalid_0's auc: 0.898041\n",
      "[12000]\tvalid_0's auc: 0.897935\n",
      "[13000]\tvalid_0's auc: 0.897836\n",
      "Early stopping, best iteration is:\n",
      "[10080]\tvalid_0's auc: 0.898074\n",
      "CV mean score: 0.9010, std: 0.0043.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_lgb_var12_1_10_3', oof)\n",
    "np.save('../cache/preds_lgb_var12_1_10_3', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = prediction_lgb\n",
    "sub.to_csv('../submissions/sub12l55.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    200000.000000\n",
       "mean          0.002393\n",
       "std           0.998112\n",
       "min          -5.199338\n",
       "25%          -0.676328\n",
       "50%           0.003262\n",
       "75%           0.678959\n",
       "max           5.199338\n",
       "Name: var_12, dtype: float64"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1['var_12'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    200000.000000\n",
       "mean          0.005028\n",
       "std           0.999882\n",
       "min          -5.199338\n",
       "25%          -0.670034\n",
       "50%           0.004391\n",
       "75%           0.682650\n",
       "max           5.199338\n",
       "Name: var_12, dtype: float64"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2['var_12'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.        , -5.69230769, -5.38461538, -5.07692308, -4.76923077,\n",
       "       -4.46153846, -4.15384615, -3.84615385, -3.53846154, -3.23076923,\n",
       "       -2.92307692, -2.61538462, -2.30769231, -2.        , -1.69230769,\n",
       "       -1.38461538, -1.07692308, -0.76923077, -0.46153846, -0.15384615,\n",
       "        0.15384615,  0.46153846,  0.76923077,  1.07692308,  1.38461538,\n",
       "        1.69230769,  2.        ,  2.30769231,  2.61538462,  2.92307692,\n",
       "        3.23076923,  3.53846154,  3.84615385,  4.15384615,  4.46153846,\n",
       "        4.76923077,  5.07692308,  5.38461538,  5.69230769,  6.        ])"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = np.linspace(-6.0,6.0,40)\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = [-6.        , -5.69230769, -5.38461538, -5.07692308, -4.76923077,\n",
    "       -4.46153846, -4.15384615, -3.84615385, -3.53846154, -3.23076923,\n",
    "       -2.92307692, -2.61538462, -2.30769231, -2.        , -1.69230769,\n",
    "       -1.38461538, -1.07692308, -0.76923077, -0.46153846, -0.15384615,\n",
    "        0.15384615,  0.46153846,  0.76923077,  1.07692308,  1.38461538,\n",
    "        1.69230769,  2.        ,  2.30769231,  2.61538462,  2.92307692,\n",
    "        3.23076923,  3.53846154,  3.84615385,  4.15384615,  4.46153846,\n",
    "        4.76923077,  5.07692308,  5.38461538,  5.69230769,  6.        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_12\n",
      "-------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200000, 200), (200000, 200))"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = X1.copy()\n",
    "X4 = X2.copy()\n",
    "\n",
    "# X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "# X4 = X4.drop('ID_code', axis=1)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "for col in ['var_12']:\n",
    "    print(col)\n",
    "    X3_col = pd.cut(X3[col], bins)\n",
    "    X4_col = pd.cut(X4[col], bins)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(X3_col)\n",
    "    X3_col = le.transform(X3_col)\n",
    "    X4_col = le.transform(X4_col)\n",
    "    \n",
    "#     X3_col_bins = pd.DataFrame(X3_col_bins)\n",
    "#     cols = [col + '_' + str(i) for i in range(X3_col_bins.shape[1])]\n",
    "#     X3_col_bins.columns = cols\n",
    "\n",
    "#     X4_col_bins = pd.DataFrame(X4_col_bins)\n",
    "#     X4_col_bins.columns = cols\n",
    " \n",
    "#     X3 = pd.concat([X3, X3_col_bins], axis=1)\n",
    "#     X4 = pd.concat([X4, X4_col_bins], axis=1)\n",
    "    X3['var_12_new'] = X3_col\n",
    "    X4['var_12_new'] = X4_col\n",
    "    \n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X4 = X4.drop(col, axis=1)\n",
    "    \n",
    "#     del X3_col_bins, X4_col_bins\n",
    "    gc.collect()\n",
    "    print('-------------')\n",
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14    24377\n",
       "15    23315\n",
       "13    23201\n",
       "16    20319\n",
       "12    20129\n",
       "17    16155\n",
       "11    16013\n",
       "18    11685\n",
       "10    11631\n",
       "9      7597\n",
       "19     7558\n",
       "8      4519\n",
       "20     4511\n",
       "21     2431\n",
       "7      2426\n",
       "6      1248\n",
       "22     1204\n",
       "23      565\n",
       "5       499\n",
       "24      283\n",
       "4       273\n",
       "25       33\n",
       "3        11\n",
       "26        7\n",
       "2         4\n",
       "28        2\n",
       "0         2\n",
       "27        1\n",
       "1         1\n",
       "Name: var_12_new, dtype: int64"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3['var_12_new'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Fri Mar 22 21:30:47 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886154\n",
      "[2000]\tvalid_0's auc: 0.892802\n",
      "[3000]\tvalid_0's auc: 0.895867\n",
      "[4000]\tvalid_0's auc: 0.89773\n",
      "[5000]\tvalid_0's auc: 0.899014\n",
      "[6000]\tvalid_0's auc: 0.899845\n",
      "[7000]\tvalid_0's auc: 0.900332\n",
      "[8000]\tvalid_0's auc: 0.900382\n",
      "[9000]\tvalid_0's auc: 0.900706\n",
      "[10000]\tvalid_0's auc: 0.90084\n",
      "[11000]\tvalid_0's auc: 0.901014\n",
      "[12000]\tvalid_0's auc: 0.900946\n",
      "[13000]\tvalid_0's auc: 0.900953\n",
      "[14000]\tvalid_0's auc: 0.900779\n",
      "Early stopping, best iteration is:\n",
      "[11155]\tvalid_0's auc: 0.901099\n",
      "Fold 1 started at Fri Mar 22 21:32:23 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883791\n",
      "[2000]\tvalid_0's auc: 0.889943\n",
      "[3000]\tvalid_0's auc: 0.893304\n",
      "[4000]\tvalid_0's auc: 0.894968\n",
      "[5000]\tvalid_0's auc: 0.89594\n",
      "[6000]\tvalid_0's auc: 0.896912\n",
      "[7000]\tvalid_0's auc: 0.897658\n",
      "[8000]\tvalid_0's auc: 0.897807\n",
      "[9000]\tvalid_0's auc: 0.898082\n",
      "[10000]\tvalid_0's auc: 0.898015\n",
      "[11000]\tvalid_0's auc: 0.898108\n",
      "[12000]\tvalid_0's auc: 0.898109\n",
      "[13000]\tvalid_0's auc: 0.897955\n",
      "[14000]\tvalid_0's auc: 0.89787\n",
      "Early stopping, best iteration is:\n",
      "[11714]\tvalid_0's auc: 0.89824\n",
      "Fold 2 started at Fri Mar 22 21:34:11 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88863\n",
      "[2000]\tvalid_0's auc: 0.89328\n",
      "[3000]\tvalid_0's auc: 0.896003\n",
      "[4000]\tvalid_0's auc: 0.897683\n",
      "[5000]\tvalid_0's auc: 0.898785\n",
      "[6000]\tvalid_0's auc: 0.89934\n",
      "[7000]\tvalid_0's auc: 0.899652\n",
      "[8000]\tvalid_0's auc: 0.899622\n",
      "[9000]\tvalid_0's auc: 0.899717\n",
      "[10000]\tvalid_0's auc: 0.899701\n",
      "[11000]\tvalid_0's auc: 0.899588\n",
      "[12000]\tvalid_0's auc: 0.89948\n",
      "Early stopping, best iteration is:\n",
      "[9781]\tvalid_0's auc: 0.89975\n",
      "Fold 3 started at Fri Mar 22 21:35:48 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.888782\n",
      "[2000]\tvalid_0's auc: 0.894418\n",
      "[3000]\tvalid_0's auc: 0.89754\n",
      "[4000]\tvalid_0's auc: 0.899482\n",
      "[5000]\tvalid_0's auc: 0.900838\n",
      "[6000]\tvalid_0's auc: 0.901397\n",
      "[7000]\tvalid_0's auc: 0.901871\n",
      "[8000]\tvalid_0's auc: 0.902272\n",
      "[9000]\tvalid_0's auc: 0.902334\n",
      "[10000]\tvalid_0's auc: 0.902385\n",
      "[11000]\tvalid_0's auc: 0.902459\n",
      "[12000]\tvalid_0's auc: 0.90258\n",
      "[13000]\tvalid_0's auc: 0.902447\n",
      "[14000]\tvalid_0's auc: 0.902536\n",
      "Early stopping, best iteration is:\n",
      "[11951]\tvalid_0's auc: 0.902606\n",
      "Fold 4 started at Fri Mar 22 21:38:05 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.892822\n",
      "[2000]\tvalid_0's auc: 0.899242\n",
      "[3000]\tvalid_0's auc: 0.903376\n",
      "[4000]\tvalid_0's auc: 0.905814\n",
      "[5000]\tvalid_0's auc: 0.907588\n",
      "[6000]\tvalid_0's auc: 0.908436\n",
      "[7000]\tvalid_0's auc: 0.908648\n",
      "[8000]\tvalid_0's auc: 0.909091\n",
      "[9000]\tvalid_0's auc: 0.909487\n",
      "[10000]\tvalid_0's auc: 0.909724\n",
      "[11000]\tvalid_0's auc: 0.909945\n",
      "[12000]\tvalid_0's auc: 0.910015\n",
      "[13000]\tvalid_0's auc: 0.910026\n",
      "[14000]\tvalid_0's auc: 0.909851\n",
      "[15000]\tvalid_0's auc: 0.90995\n",
      "Early stopping, best iteration is:\n",
      "[12755]\tvalid_0's auc: 0.910067\n",
      "Fold 5 started at Fri Mar 22 21:40:50 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.888236\n",
      "[2000]\tvalid_0's auc: 0.895281\n",
      "[3000]\tvalid_0's auc: 0.898429\n",
      "[4000]\tvalid_0's auc: 0.900243\n",
      "[5000]\tvalid_0's auc: 0.901554\n",
      "[6000]\tvalid_0's auc: 0.902671\n",
      "[7000]\tvalid_0's auc: 0.90314\n",
      "[8000]\tvalid_0's auc: 0.903496\n",
      "[9000]\tvalid_0's auc: 0.903602\n",
      "[10000]\tvalid_0's auc: 0.903733\n",
      "[11000]\tvalid_0's auc: 0.903799\n",
      "[12000]\tvalid_0's auc: 0.903523\n",
      "[13000]\tvalid_0's auc: 0.903595\n",
      "Early stopping, best iteration is:\n",
      "[10518]\tvalid_0's auc: 0.903813\n",
      "Fold 6 started at Fri Mar 22 21:43:03 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.888866\n",
      "[2000]\tvalid_0's auc: 0.89445\n",
      "[3000]\tvalid_0's auc: 0.897992\n",
      "[4000]\tvalid_0's auc: 0.900684\n",
      "[5000]\tvalid_0's auc: 0.902023\n",
      "[6000]\tvalid_0's auc: 0.903091\n",
      "[7000]\tvalid_0's auc: 0.903739\n",
      "[8000]\tvalid_0's auc: 0.904178\n",
      "[9000]\tvalid_0's auc: 0.904387\n",
      "[10000]\tvalid_0's auc: 0.904531\n",
      "[11000]\tvalid_0's auc: 0.904604\n",
      "[12000]\tvalid_0's auc: 0.904687\n",
      "[13000]\tvalid_0's auc: 0.904797\n",
      "[14000]\tvalid_0's auc: 0.904743\n",
      "[15000]\tvalid_0's auc: 0.904751\n",
      "[16000]\tvalid_0's auc: 0.904587\n",
      "[17000]\tvalid_0's auc: 0.904337\n",
      "Early stopping, best iteration is:\n",
      "[14152]\tvalid_0's auc: 0.904811\n",
      "Fold 7 started at Fri Mar 22 21:46:13 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.879938\n",
      "[2000]\tvalid_0's auc: 0.886293\n",
      "[3000]\tvalid_0's auc: 0.889561\n",
      "[4000]\tvalid_0's auc: 0.891446\n",
      "[5000]\tvalid_0's auc: 0.892896\n",
      "[6000]\tvalid_0's auc: 0.893875\n",
      "[7000]\tvalid_0's auc: 0.894527\n",
      "[8000]\tvalid_0's auc: 0.894717\n",
      "[9000]\tvalid_0's auc: 0.894954\n",
      "[10000]\tvalid_0's auc: 0.895167\n",
      "[11000]\tvalid_0's auc: 0.895281\n",
      "[12000]\tvalid_0's auc: 0.895238\n",
      "[13000]\tvalid_0's auc: 0.895194\n",
      "[14000]\tvalid_0's auc: 0.895124\n",
      "[15000]\tvalid_0's auc: 0.895087\n",
      "Early stopping, best iteration is:\n",
      "[12360]\tvalid_0's auc: 0.895314\n",
      "Fold 8 started at Fri Mar 22 21:49:16 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.881011\n",
      "[2000]\tvalid_0's auc: 0.886962\n",
      "[3000]\tvalid_0's auc: 0.890096\n",
      "[4000]\tvalid_0's auc: 0.892655\n",
      "[5000]\tvalid_0's auc: 0.894095\n",
      "[6000]\tvalid_0's auc: 0.89509\n",
      "[7000]\tvalid_0's auc: 0.895706\n",
      "[8000]\tvalid_0's auc: 0.89612\n",
      "[9000]\tvalid_0's auc: 0.896151\n",
      "[10000]\tvalid_0's auc: 0.896144\n",
      "[11000]\tvalid_0's auc: 0.896134\n",
      "[12000]\tvalid_0's auc: 0.896201\n",
      "[13000]\tvalid_0's auc: 0.896169\n",
      "[14000]\tvalid_0's auc: 0.89611\n",
      "Early stopping, best iteration is:\n",
      "[11931]\tvalid_0's auc: 0.896233\n",
      "Fold 9 started at Fri Mar 22 21:52:11 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.882591\n",
      "[2000]\tvalid_0's auc: 0.887925\n",
      "[3000]\tvalid_0's auc: 0.891814\n",
      "[4000]\tvalid_0's auc: 0.894182\n",
      "[5000]\tvalid_0's auc: 0.895458\n",
      "[6000]\tvalid_0's auc: 0.896502\n",
      "[7000]\tvalid_0's auc: 0.896985\n",
      "[8000]\tvalid_0's auc: 0.897352\n",
      "[9000]\tvalid_0's auc: 0.897515\n",
      "[10000]\tvalid_0's auc: 0.89761\n",
      "[11000]\tvalid_0's auc: 0.897594\n",
      "[12000]\tvalid_0's auc: 0.89743\n",
      "[13000]\tvalid_0's auc: 0.897319\n",
      "Early stopping, best iteration is:\n",
      "[10081]\tvalid_0's auc: 0.897659\n",
      "CV mean score: 0.9010, std: 0.0043.\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('../cache/oof_lgb_new_quant_var12_1_10_3', oof)\n",
    "np.save('../cache/preds_lgb_new_quant_var12_1_10_3', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "sub['target'] = prediction_lgb\n",
    "sub.to_csv('../submissions/sub12l56.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903606\tvalid_1's auc: 0.887936\n",
      "[2000]\ttraining's auc: 0.912298\tvalid_1's auc: 0.89315\n",
      "[3000]\ttraining's auc: 0.918339\tvalid_1's auc: 0.896172\n",
      "[4000]\ttraining's auc: 0.923166\tvalid_1's auc: 0.897752\n",
      "[5000]\ttraining's auc: 0.92731\tvalid_1's auc: 0.899167\n",
      "[6000]\ttraining's auc: 0.931247\tvalid_1's auc: 0.900106\n",
      "[7000]\ttraining's auc: 0.934749\tvalid_1's auc: 0.900533\n",
      "[8000]\ttraining's auc: 0.937992\tvalid_1's auc: 0.900668\n",
      "[9000]\ttraining's auc: 0.941076\tvalid_1's auc: 0.900835\n",
      "Early stopping, best iteration is:\n",
      "[8566]\ttraining's auc: 0.939783\tvalid_1's auc: 0.900898\n",
      "var_81 10 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901795\tvalid_1's auc: 0.886686\n",
      "[2000]\ttraining's auc: 0.911817\tvalid_1's auc: 0.893592\n",
      "[3000]\ttraining's auc: 0.917953\tvalid_1's auc: 0.897504\n",
      "[4000]\ttraining's auc: 0.923098\tvalid_1's auc: 0.899359\n",
      "[5000]\ttraining's auc: 0.927427\tvalid_1's auc: 0.900542\n",
      "[6000]\ttraining's auc: 0.931239\tvalid_1's auc: 0.901185\n",
      "[7000]\ttraining's auc: 0.934815\tvalid_1's auc: 0.901554\n",
      "[8000]\ttraining's auc: 0.938184\tvalid_1's auc: 0.901781\n",
      "[9000]\ttraining's auc: 0.941252\tvalid_1's auc: 0.901779\n",
      "[10000]\ttraining's auc: 0.94422\tvalid_1's auc: 0.90191\n",
      "Early stopping, best iteration is:\n",
      "[9907]\ttraining's auc: 0.943955\tvalid_1's auc: 0.901919\n",
      "var_81 10 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903265\tvalid_1's auc: 0.887474\n",
      "[2000]\ttraining's auc: 0.911985\tvalid_1's auc: 0.893021\n",
      "[3000]\ttraining's auc: 0.918055\tvalid_1's auc: 0.896425\n",
      "[4000]\ttraining's auc: 0.923266\tvalid_1's auc: 0.898637\n",
      "[5000]\ttraining's auc: 0.927561\tvalid_1's auc: 0.899768\n",
      "[6000]\ttraining's auc: 0.931488\tvalid_1's auc: 0.900611\n",
      "[7000]\ttraining's auc: 0.934917\tvalid_1's auc: 0.90102\n",
      "[8000]\ttraining's auc: 0.938237\tvalid_1's auc: 0.901199\n",
      "[9000]\ttraining's auc: 0.941464\tvalid_1's auc: 0.901397\n",
      "[10000]\ttraining's auc: 0.944491\tvalid_1's auc: 0.901506\n",
      "[11000]\ttraining's auc: 0.947377\tvalid_1's auc: 0.901546\n",
      "Early stopping, best iteration is:\n",
      "[10631]\ttraining's auc: 0.946354\tvalid_1's auc: 0.901626\n",
      "var_81 10 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.899497\tvalid_1's auc: 0.882252\n",
      "[2000]\ttraining's auc: 0.910093\tvalid_1's auc: 0.891441\n",
      "[3000]\ttraining's auc: 0.917441\tvalid_1's auc: 0.896155\n",
      "[4000]\ttraining's auc: 0.923083\tvalid_1's auc: 0.898224\n",
      "[5000]\ttraining's auc: 0.927524\tvalid_1's auc: 0.899692\n",
      "[6000]\ttraining's auc: 0.931576\tvalid_1's auc: 0.900709\n",
      "[7000]\ttraining's auc: 0.935116\tvalid_1's auc: 0.90098\n",
      "[8000]\ttraining's auc: 0.938457\tvalid_1's auc: 0.901261\n",
      "[9000]\ttraining's auc: 0.941674\tvalid_1's auc: 0.90143\n",
      "[10000]\ttraining's auc: 0.94476\tvalid_1's auc: 0.901556\n",
      "[11000]\ttraining's auc: 0.947665\tvalid_1's auc: 0.901647\n",
      "Early stopping, best iteration is:\n",
      "[10489]\ttraining's auc: 0.946191\tvalid_1's auc: 0.901666\n",
      "var_81 10 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.904217\tvalid_1's auc: 0.887328\n",
      "[2000]\ttraining's auc: 0.912902\tvalid_1's auc: 0.89352\n",
      "[3000]\ttraining's auc: 0.918528\tvalid_1's auc: 0.896879\n",
      "[4000]\ttraining's auc: 0.923313\tvalid_1's auc: 0.898741\n",
      "[5000]\ttraining's auc: 0.927408\tvalid_1's auc: 0.899939\n",
      "[6000]\ttraining's auc: 0.931258\tvalid_1's auc: 0.900741\n",
      "[7000]\ttraining's auc: 0.934787\tvalid_1's auc: 0.901217\n",
      "[8000]\ttraining's auc: 0.937976\tvalid_1's auc: 0.901391\n",
      "[9000]\ttraining's auc: 0.941035\tvalid_1's auc: 0.901512\n",
      "[10000]\ttraining's auc: 0.944004\tvalid_1's auc: 0.901491\n",
      "Early stopping, best iteration is:\n",
      "[9063]\ttraining's auc: 0.94123\tvalid_1's auc: 0.901538\n",
      "var_81 20 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902226\tvalid_1's auc: 0.886879\n",
      "[2000]\ttraining's auc: 0.912302\tvalid_1's auc: 0.893971\n",
      "[3000]\ttraining's auc: 0.918628\tvalid_1's auc: 0.897483\n",
      "[4000]\ttraining's auc: 0.923547\tvalid_1's auc: 0.899498\n",
      "[5000]\ttraining's auc: 0.927618\tvalid_1's auc: 0.900504\n",
      "[6000]\ttraining's auc: 0.931428\tvalid_1's auc: 0.901304\n",
      "[7000]\ttraining's auc: 0.934864\tvalid_1's auc: 0.901793\n",
      "[8000]\ttraining's auc: 0.93818\tvalid_1's auc: 0.901969\n",
      "[9000]\ttraining's auc: 0.941299\tvalid_1's auc: 0.902147\n",
      "[10000]\ttraining's auc: 0.944263\tvalid_1's auc: 0.902156\n",
      "[11000]\ttraining's auc: 0.947072\tvalid_1's auc: 0.902261\n",
      "Early stopping, best iteration is:\n",
      "[10986]\ttraining's auc: 0.947034\tvalid_1's auc: 0.902271\n",
      "var_81 20 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903597\tvalid_1's auc: 0.887847\n",
      "[2000]\ttraining's auc: 0.912276\tvalid_1's auc: 0.893406\n",
      "[3000]\ttraining's auc: 0.918369\tvalid_1's auc: 0.896812\n",
      "[4000]\ttraining's auc: 0.923511\tvalid_1's auc: 0.898904\n",
      "[5000]\ttraining's auc: 0.927793\tvalid_1's auc: 0.90005\n",
      "[6000]\ttraining's auc: 0.931682\tvalid_1's auc: 0.900849\n",
      "[7000]\ttraining's auc: 0.935123\tvalid_1's auc: 0.901221\n",
      "[8000]\ttraining's auc: 0.938401\tvalid_1's auc: 0.901442\n",
      "[9000]\ttraining's auc: 0.941603\tvalid_1's auc: 0.901599\n",
      "[10000]\ttraining's auc: 0.944617\tvalid_1's auc: 0.901682\n",
      "[11000]\ttraining's auc: 0.947519\tvalid_1's auc: 0.901779\n",
      "Early stopping, best iteration is:\n",
      "[10544]\ttraining's auc: 0.94622\tvalid_1's auc: 0.901825\n",
      "var_81 20 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.899633\tvalid_1's auc: 0.882656\n",
      "[2000]\ttraining's auc: 0.910126\tvalid_1's auc: 0.89169\n",
      "[3000]\ttraining's auc: 0.917468\tvalid_1's auc: 0.896424\n",
      "[4000]\ttraining's auc: 0.923088\tvalid_1's auc: 0.898435\n",
      "[5000]\ttraining's auc: 0.927529\tvalid_1's auc: 0.899956\n",
      "[6000]\ttraining's auc: 0.931566\tvalid_1's auc: 0.900964\n",
      "[7000]\ttraining's auc: 0.935112\tvalid_1's auc: 0.901231\n",
      "[8000]\ttraining's auc: 0.938448\tvalid_1's auc: 0.901528\n",
      "[9000]\ttraining's auc: 0.941671\tvalid_1's auc: 0.901707\n",
      "[10000]\ttraining's auc: 0.944725\tvalid_1's auc: 0.901718\n",
      "[11000]\ttraining's auc: 0.947639\tvalid_1's auc: 0.901816\n",
      "Early stopping, best iteration is:\n",
      "[10942]\ttraining's auc: 0.947475\tvalid_1's auc: 0.901824\n",
      "var_81 20 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902454\tvalid_1's auc: 0.886266\n",
      "[2000]\ttraining's auc: 0.91183\tvalid_1's auc: 0.892703\n",
      "[3000]\ttraining's auc: 0.918098\tvalid_1's auc: 0.896045\n",
      "[4000]\ttraining's auc: 0.923107\tvalid_1's auc: 0.898108\n",
      "[5000]\ttraining's auc: 0.927617\tvalid_1's auc: 0.899869\n",
      "[6000]\ttraining's auc: 0.93145\tvalid_1's auc: 0.900613\n",
      "[7000]\ttraining's auc: 0.934985\tvalid_1's auc: 0.901094\n",
      "[8000]\ttraining's auc: 0.938281\tvalid_1's auc: 0.901444\n",
      "[9000]\ttraining's auc: 0.94146\tvalid_1's auc: 0.901467\n",
      "[10000]\ttraining's auc: 0.944498\tvalid_1's auc: 0.901657\n",
      "[11000]\ttraining's auc: 0.947387\tvalid_1's auc: 0.901655\n",
      "[12000]\ttraining's auc: 0.950224\tvalid_1's auc: 0.901589\n",
      "Early stopping, best iteration is:\n",
      "[11264]\ttraining's auc: 0.948137\tvalid_1's auc: 0.901702\n",
      "var_81 30 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901679\tvalid_1's auc: 0.886012\n",
      "[2000]\ttraining's auc: 0.911404\tvalid_1's auc: 0.893424\n",
      "[3000]\ttraining's auc: 0.917684\tvalid_1's auc: 0.896735\n",
      "[4000]\ttraining's auc: 0.922812\tvalid_1's auc: 0.898745\n",
      "[5000]\ttraining's auc: 0.927207\tvalid_1's auc: 0.899994\n",
      "[6000]\ttraining's auc: 0.931176\tvalid_1's auc: 0.900813\n",
      "[7000]\ttraining's auc: 0.934733\tvalid_1's auc: 0.901306\n",
      "[8000]\ttraining's auc: 0.938067\tvalid_1's auc: 0.901522\n",
      "Early stopping, best iteration is:\n",
      "[7903]\ttraining's auc: 0.937768\tvalid_1's auc: 0.901556\n",
      "var_81 30 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903474\tvalid_1's auc: 0.887856\n",
      "[2000]\ttraining's auc: 0.912217\tvalid_1's auc: 0.893392\n",
      "[3000]\ttraining's auc: 0.918307\tvalid_1's auc: 0.896766\n",
      "[4000]\ttraining's auc: 0.923425\tvalid_1's auc: 0.898889\n",
      "[5000]\ttraining's auc: 0.927713\tvalid_1's auc: 0.900078\n",
      "[6000]\ttraining's auc: 0.93164\tvalid_1's auc: 0.900979\n",
      "[7000]\ttraining's auc: 0.935067\tvalid_1's auc: 0.901314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8000]\ttraining's auc: 0.938361\tvalid_1's auc: 0.901505\n",
      "[9000]\ttraining's auc: 0.941571\tvalid_1's auc: 0.901702\n",
      "[10000]\ttraining's auc: 0.944607\tvalid_1's auc: 0.901829\n",
      "[11000]\ttraining's auc: 0.947519\tvalid_1's auc: 0.901824\n",
      "Early stopping, best iteration is:\n",
      "[10544]\ttraining's auc: 0.9462\tvalid_1's auc: 0.901899\n",
      "var_81 30 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.899555\tvalid_1's auc: 0.882613\n",
      "[2000]\ttraining's auc: 0.910122\tvalid_1's auc: 0.891748\n",
      "[3000]\ttraining's auc: 0.91751\tvalid_1's auc: 0.896432\n",
      "[4000]\ttraining's auc: 0.923113\tvalid_1's auc: 0.898409\n",
      "[5000]\ttraining's auc: 0.927556\tvalid_1's auc: 0.899925\n",
      "[6000]\ttraining's auc: 0.931613\tvalid_1's auc: 0.900828\n",
      "[7000]\ttraining's auc: 0.935141\tvalid_1's auc: 0.901176\n",
      "[8000]\ttraining's auc: 0.938473\tvalid_1's auc: 0.901499\n",
      "[9000]\ttraining's auc: 0.941688\tvalid_1's auc: 0.901656\n",
      "[10000]\ttraining's auc: 0.944766\tvalid_1's auc: 0.901722\n",
      "[11000]\ttraining's auc: 0.947659\tvalid_1's auc: 0.90181\n",
      "Early stopping, best iteration is:\n",
      "[10937]\ttraining's auc: 0.947478\tvalid_1's auc: 0.901829\n",
      "var_81 30 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903348\tvalid_1's auc: 0.886242\n",
      "[2000]\ttraining's auc: 0.912012\tvalid_1's auc: 0.892571\n",
      "[3000]\ttraining's auc: 0.918284\tvalid_1's auc: 0.896106\n",
      "[4000]\ttraining's auc: 0.923176\tvalid_1's auc: 0.898167\n",
      "[5000]\ttraining's auc: 0.92753\tvalid_1's auc: 0.899418\n",
      "[6000]\ttraining's auc: 0.931251\tvalid_1's auc: 0.900289\n",
      "[7000]\ttraining's auc: 0.934763\tvalid_1's auc: 0.900744\n",
      "[8000]\ttraining's auc: 0.938054\tvalid_1's auc: 0.901002\n",
      "[9000]\ttraining's auc: 0.941124\tvalid_1's auc: 0.901282\n",
      "[10000]\ttraining's auc: 0.94407\tvalid_1's auc: 0.901451\n",
      "[11000]\ttraining's auc: 0.947001\tvalid_1's auc: 0.901367\n",
      "Early stopping, best iteration is:\n",
      "[10000]\ttraining's auc: 0.94407\tvalid_1's auc: 0.901451\n",
      "var_81 40 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902835\tvalid_1's auc: 0.887389\n",
      "[2000]\ttraining's auc: 0.911987\tvalid_1's auc: 0.893484\n",
      "[3000]\ttraining's auc: 0.918271\tvalid_1's auc: 0.896985\n",
      "[4000]\ttraining's auc: 0.923225\tvalid_1's auc: 0.89905\n",
      "[5000]\ttraining's auc: 0.927547\tvalid_1's auc: 0.900095\n",
      "[6000]\ttraining's auc: 0.931323\tvalid_1's auc: 0.900858\n",
      "[7000]\ttraining's auc: 0.934779\tvalid_1's auc: 0.90125\n",
      "[8000]\ttraining's auc: 0.938066\tvalid_1's auc: 0.901398\n",
      "[9000]\ttraining's auc: 0.941197\tvalid_1's auc: 0.901523\n",
      "[10000]\ttraining's auc: 0.944182\tvalid_1's auc: 0.901645\n",
      "Early stopping, best iteration is:\n",
      "[9876]\ttraining's auc: 0.943828\tvalid_1's auc: 0.901687\n",
      "var_81 40 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903588\tvalid_1's auc: 0.887899\n",
      "[2000]\ttraining's auc: 0.912269\tvalid_1's auc: 0.893381\n",
      "[3000]\ttraining's auc: 0.918352\tvalid_1's auc: 0.896776\n",
      "[4000]\ttraining's auc: 0.923525\tvalid_1's auc: 0.898915\n",
      "[5000]\ttraining's auc: 0.927807\tvalid_1's auc: 0.900053\n",
      "[6000]\ttraining's auc: 0.931696\tvalid_1's auc: 0.90086\n",
      "[7000]\ttraining's auc: 0.935144\tvalid_1's auc: 0.901239\n",
      "[8000]\ttraining's auc: 0.938429\tvalid_1's auc: 0.90151\n",
      "[9000]\ttraining's auc: 0.941663\tvalid_1's auc: 0.901617\n",
      "[10000]\ttraining's auc: 0.944698\tvalid_1's auc: 0.901734\n",
      "[11000]\ttraining's auc: 0.947585\tvalid_1's auc: 0.901786\n",
      "Early stopping, best iteration is:\n",
      "[10544]\ttraining's auc: 0.946286\tvalid_1's auc: 0.901854\n",
      "var_81 40 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.899609\tvalid_1's auc: 0.88266\n",
      "[2000]\ttraining's auc: 0.910087\tvalid_1's auc: 0.891695\n",
      "[3000]\ttraining's auc: 0.917441\tvalid_1's auc: 0.896375\n",
      "[4000]\ttraining's auc: 0.923069\tvalid_1's auc: 0.898451\n",
      "[5000]\ttraining's auc: 0.927539\tvalid_1's auc: 0.899877\n",
      "[6000]\ttraining's auc: 0.931622\tvalid_1's auc: 0.900882\n",
      "[7000]\ttraining's auc: 0.935157\tvalid_1's auc: 0.901154\n",
      "[8000]\ttraining's auc: 0.938493\tvalid_1's auc: 0.901445\n",
      "[9000]\ttraining's auc: 0.941708\tvalid_1's auc: 0.901582\n",
      "[10000]\ttraining's auc: 0.944764\tvalid_1's auc: 0.901631\n",
      "[11000]\ttraining's auc: 0.94769\tvalid_1's auc: 0.901699\n",
      "Early stopping, best iteration is:\n",
      "[10919]\ttraining's auc: 0.947451\tvalid_1's auc: 0.901707\n",
      "var_81 40 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903193\tvalid_1's auc: 0.886624\n",
      "[2000]\ttraining's auc: 0.91233\tvalid_1's auc: 0.892987\n",
      "[3000]\ttraining's auc: 0.918218\tvalid_1's auc: 0.895913\n",
      "[4000]\ttraining's auc: 0.923064\tvalid_1's auc: 0.897905\n",
      "[5000]\ttraining's auc: 0.927302\tvalid_1's auc: 0.89931\n",
      "[6000]\ttraining's auc: 0.931133\tvalid_1's auc: 0.900354\n",
      "[7000]\ttraining's auc: 0.934617\tvalid_1's auc: 0.900814\n",
      "[8000]\ttraining's auc: 0.93792\tvalid_1's auc: 0.901182\n",
      "[9000]\ttraining's auc: 0.941011\tvalid_1's auc: 0.901369\n",
      "[10000]\ttraining's auc: 0.943995\tvalid_1's auc: 0.90147\n",
      "[11000]\ttraining's auc: 0.946914\tvalid_1's auc: 0.901473\n",
      "Early stopping, best iteration is:\n",
      "[10165]\ttraining's auc: 0.944498\tvalid_1's auc: 0.901553\n",
      "var_81 50 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901788\tvalid_1's auc: 0.886729\n",
      "[2000]\ttraining's auc: 0.912318\tvalid_1's auc: 0.89357\n",
      "[3000]\ttraining's auc: 0.918337\tvalid_1's auc: 0.896563\n",
      "[4000]\ttraining's auc: 0.923191\tvalid_1's auc: 0.898882\n",
      "[5000]\ttraining's auc: 0.927475\tvalid_1's auc: 0.900035\n",
      "[6000]\ttraining's auc: 0.931289\tvalid_1's auc: 0.900618\n",
      "[7000]\ttraining's auc: 0.934804\tvalid_1's auc: 0.900938\n",
      "[8000]\ttraining's auc: 0.938087\tvalid_1's auc: 0.901085\n",
      "[9000]\ttraining's auc: 0.941232\tvalid_1's auc: 0.901321\n",
      "[10000]\ttraining's auc: 0.944244\tvalid_1's auc: 0.901382\n",
      "[11000]\ttraining's auc: 0.947135\tvalid_1's auc: 0.901354\n",
      "Early stopping, best iteration is:\n",
      "[10635]\ttraining's auc: 0.946097\tvalid_1's auc: 0.901397\n",
      "var_81 50 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903468\tvalid_1's auc: 0.887887\n",
      "[2000]\ttraining's auc: 0.912228\tvalid_1's auc: 0.893409\n",
      "[3000]\ttraining's auc: 0.918314\tvalid_1's auc: 0.896851\n",
      "[4000]\ttraining's auc: 0.92343\tvalid_1's auc: 0.898976\n",
      "[5000]\ttraining's auc: 0.927705\tvalid_1's auc: 0.900161\n",
      "[6000]\ttraining's auc: 0.93161\tvalid_1's auc: 0.900968\n",
      "[7000]\ttraining's auc: 0.935037\tvalid_1's auc: 0.901344\n",
      "[8000]\ttraining's auc: 0.938347\tvalid_1's auc: 0.901503\n",
      "[9000]\ttraining's auc: 0.941575\tvalid_1's auc: 0.901676\n",
      "[10000]\ttraining's auc: 0.944616\tvalid_1's auc: 0.901794\n",
      "[11000]\ttraining's auc: 0.947513\tvalid_1's auc: 0.901845\n",
      "Early stopping, best iteration is:\n",
      "[10544]\ttraining's auc: 0.946211\tvalid_1's auc: 0.901888\n",
      "var_81 50 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.899551\tvalid_1's auc: 0.882563\n",
      "[2000]\ttraining's auc: 0.910089\tvalid_1's auc: 0.891712\n",
      "[3000]\ttraining's auc: 0.917424\tvalid_1's auc: 0.896438\n",
      "[4000]\ttraining's auc: 0.923037\tvalid_1's auc: 0.898504\n",
      "[5000]\ttraining's auc: 0.927524\tvalid_1's auc: 0.900016\n",
      "[6000]\ttraining's auc: 0.931582\tvalid_1's auc: 0.901031\n",
      "[7000]\ttraining's auc: 0.935113\tvalid_1's auc: 0.901278\n",
      "[8000]\ttraining's auc: 0.938455\tvalid_1's auc: 0.901535\n",
      "[9000]\ttraining's auc: 0.941699\tvalid_1's auc: 0.901706\n",
      "[10000]\ttraining's auc: 0.944763\tvalid_1's auc: 0.901793\n",
      "[11000]\ttraining's auc: 0.947684\tvalid_1's auc: 0.901883\n",
      "Early stopping, best iteration is:\n",
      "[10996]\ttraining's auc: 0.947671\tvalid_1's auc: 0.901892\n",
      "var_81 50 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901354\tvalid_1's auc: 0.885194\n",
      "[2000]\ttraining's auc: 0.911158\tvalid_1's auc: 0.892601\n",
      "[3000]\ttraining's auc: 0.917518\tvalid_1's auc: 0.896195\n",
      "[4000]\ttraining's auc: 0.922773\tvalid_1's auc: 0.8985\n",
      "[5000]\ttraining's auc: 0.927207\tvalid_1's auc: 0.899816\n",
      "[6000]\ttraining's auc: 0.931074\tvalid_1's auc: 0.900587\n",
      "[7000]\ttraining's auc: 0.934582\tvalid_1's auc: 0.901116\n",
      "[8000]\ttraining's auc: 0.937909\tvalid_1's auc: 0.90128\n",
      "[9000]\ttraining's auc: 0.941184\tvalid_1's auc: 0.901517\n",
      "[10000]\ttraining's auc: 0.944186\tvalid_1's auc: 0.901583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9642]\ttraining's auc: 0.943085\tvalid_1's auc: 0.901627\n",
      "var_81 60 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902172\tvalid_1's auc: 0.886921\n",
      "[2000]\ttraining's auc: 0.911669\tvalid_1's auc: 0.893325\n",
      "[3000]\ttraining's auc: 0.918123\tvalid_1's auc: 0.897007\n",
      "[4000]\ttraining's auc: 0.923194\tvalid_1's auc: 0.898647\n",
      "[5000]\ttraining's auc: 0.927562\tvalid_1's auc: 0.899831\n",
      "[6000]\ttraining's auc: 0.931459\tvalid_1's auc: 0.900661\n",
      "[7000]\ttraining's auc: 0.935104\tvalid_1's auc: 0.901234\n",
      "[8000]\ttraining's auc: 0.938412\tvalid_1's auc: 0.901652\n",
      "[9000]\ttraining's auc: 0.941563\tvalid_1's auc: 0.90179\n",
      "[10000]\ttraining's auc: 0.944576\tvalid_1's auc: 0.901897\n",
      "Early stopping, best iteration is:\n",
      "[9878]\ttraining's auc: 0.9442\tvalid_1's auc: 0.901934\n",
      "var_81 60 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903512\tvalid_1's auc: 0.887939\n",
      "[2000]\ttraining's auc: 0.912264\tvalid_1's auc: 0.89342\n",
      "[3000]\ttraining's auc: 0.918353\tvalid_1's auc: 0.896752\n",
      "[4000]\ttraining's auc: 0.923525\tvalid_1's auc: 0.898972\n",
      "[5000]\ttraining's auc: 0.9278\tvalid_1's auc: 0.900111\n",
      "[6000]\ttraining's auc: 0.931725\tvalid_1's auc: 0.900915\n",
      "[7000]\ttraining's auc: 0.935151\tvalid_1's auc: 0.901265\n",
      "[8000]\ttraining's auc: 0.938441\tvalid_1's auc: 0.901469\n",
      "[9000]\ttraining's auc: 0.941664\tvalid_1's auc: 0.90163\n",
      "[10000]\ttraining's auc: 0.944692\tvalid_1's auc: 0.901722\n",
      "[11000]\ttraining's auc: 0.947587\tvalid_1's auc: 0.901811\n",
      "Early stopping, best iteration is:\n",
      "[10554]\ttraining's auc: 0.946317\tvalid_1's auc: 0.901901\n",
      "var_81 60 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.899526\tvalid_1's auc: 0.882722\n",
      "[2000]\ttraining's auc: 0.910047\tvalid_1's auc: 0.89175\n",
      "[3000]\ttraining's auc: 0.917418\tvalid_1's auc: 0.896409\n",
      "[4000]\ttraining's auc: 0.923043\tvalid_1's auc: 0.898441\n",
      "[5000]\ttraining's auc: 0.927481\tvalid_1's auc: 0.89988\n",
      "[6000]\ttraining's auc: 0.931565\tvalid_1's auc: 0.900839\n",
      "[7000]\ttraining's auc: 0.935128\tvalid_1's auc: 0.901114\n",
      "[8000]\ttraining's auc: 0.938445\tvalid_1's auc: 0.901419\n",
      "[9000]\ttraining's auc: 0.94166\tvalid_1's auc: 0.901602\n",
      "[10000]\ttraining's auc: 0.944724\tvalid_1's auc: 0.90169\n",
      "[11000]\ttraining's auc: 0.94765\tvalid_1's auc: 0.901771\n",
      "Early stopping, best iteration is:\n",
      "[10938]\ttraining's auc: 0.947469\tvalid_1's auc: 0.901779\n",
      "var_81 60 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902666\tvalid_1's auc: 0.885935\n",
      "[2000]\ttraining's auc: 0.911877\tvalid_1's auc: 0.892329\n",
      "[3000]\ttraining's auc: 0.917944\tvalid_1's auc: 0.895979\n",
      "[4000]\ttraining's auc: 0.923043\tvalid_1's auc: 0.898159\n",
      "[5000]\ttraining's auc: 0.92734\tvalid_1's auc: 0.899401\n",
      "[6000]\ttraining's auc: 0.931158\tvalid_1's auc: 0.900533\n",
      "[7000]\ttraining's auc: 0.934674\tvalid_1's auc: 0.900857\n",
      "[8000]\ttraining's auc: 0.937993\tvalid_1's auc: 0.901285\n",
      "[9000]\ttraining's auc: 0.941106\tvalid_1's auc: 0.901501\n",
      "[10000]\ttraining's auc: 0.944077\tvalid_1's auc: 0.901639\n",
      "Early stopping, best iteration is:\n",
      "[9701]\ttraining's auc: 0.943182\tvalid_1's auc: 0.901693\n",
      "var_81 70 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902633\tvalid_1's auc: 0.88691\n",
      "[2000]\ttraining's auc: 0.912067\tvalid_1's auc: 0.893902\n",
      "[3000]\ttraining's auc: 0.918086\tvalid_1's auc: 0.896762\n",
      "[4000]\ttraining's auc: 0.923236\tvalid_1's auc: 0.898647\n",
      "[5000]\ttraining's auc: 0.927618\tvalid_1's auc: 0.899781\n",
      "[6000]\ttraining's auc: 0.931493\tvalid_1's auc: 0.900621\n",
      "[7000]\ttraining's auc: 0.935008\tvalid_1's auc: 0.901149\n",
      "[8000]\ttraining's auc: 0.938396\tvalid_1's auc: 0.901465\n",
      "[9000]\ttraining's auc: 0.941604\tvalid_1's auc: 0.901614\n",
      "[10000]\ttraining's auc: 0.944625\tvalid_1's auc: 0.901652\n",
      "Early stopping, best iteration is:\n",
      "[9704]\ttraining's auc: 0.943744\tvalid_1's auc: 0.901711\n",
      "var_81 70 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903442\tvalid_1's auc: 0.887938\n",
      "[2000]\ttraining's auc: 0.91223\tvalid_1's auc: 0.893402\n",
      "[3000]\ttraining's auc: 0.918322\tvalid_1's auc: 0.896747\n",
      "[4000]\ttraining's auc: 0.923482\tvalid_1's auc: 0.89897\n",
      "[5000]\ttraining's auc: 0.92772\tvalid_1's auc: 0.900138\n",
      "[6000]\ttraining's auc: 0.931653\tvalid_1's auc: 0.900983\n",
      "[7000]\ttraining's auc: 0.935092\tvalid_1's auc: 0.901326\n",
      "[8000]\ttraining's auc: 0.938369\tvalid_1's auc: 0.901528\n",
      "[9000]\ttraining's auc: 0.941586\tvalid_1's auc: 0.901674\n",
      "[10000]\ttraining's auc: 0.944617\tvalid_1's auc: 0.9018\n",
      "[11000]\ttraining's auc: 0.947517\tvalid_1's auc: 0.901829\n",
      "Early stopping, best iteration is:\n",
      "[10631]\ttraining's auc: 0.946469\tvalid_1's auc: 0.901883\n",
      "var_81 70 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.899506\tvalid_1's auc: 0.88256\n",
      "[2000]\ttraining's auc: 0.910037\tvalid_1's auc: 0.891667\n",
      "[3000]\ttraining's auc: 0.917407\tvalid_1's auc: 0.896378\n",
      "[4000]\ttraining's auc: 0.923006\tvalid_1's auc: 0.89835\n",
      "[5000]\ttraining's auc: 0.927464\tvalid_1's auc: 0.899806\n",
      "[6000]\ttraining's auc: 0.931524\tvalid_1's auc: 0.900784\n",
      "[7000]\ttraining's auc: 0.935066\tvalid_1's auc: 0.90111\n",
      "[8000]\ttraining's auc: 0.938421\tvalid_1's auc: 0.901449\n",
      "[9000]\ttraining's auc: 0.941648\tvalid_1's auc: 0.90159\n",
      "[10000]\ttraining's auc: 0.944719\tvalid_1's auc: 0.901627\n",
      "[11000]\ttraining's auc: 0.947651\tvalid_1's auc: 0.901727\n",
      "Early stopping, best iteration is:\n",
      "[10982]\ttraining's auc: 0.947596\tvalid_1's auc: 0.901739\n",
      "var_81 70 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902591\tvalid_1's auc: 0.88559\n",
      "[2000]\ttraining's auc: 0.911403\tvalid_1's auc: 0.8926\n",
      "[3000]\ttraining's auc: 0.917716\tvalid_1's auc: 0.89595\n",
      "[4000]\ttraining's auc: 0.92284\tvalid_1's auc: 0.897791\n",
      "[5000]\ttraining's auc: 0.927239\tvalid_1's auc: 0.899051\n",
      "[6000]\ttraining's auc: 0.931178\tvalid_1's auc: 0.899992\n",
      "[7000]\ttraining's auc: 0.934615\tvalid_1's auc: 0.90044\n",
      "[8000]\ttraining's auc: 0.937964\tvalid_1's auc: 0.900863\n",
      "[9000]\ttraining's auc: 0.941064\tvalid_1's auc: 0.900997\n",
      "[10000]\ttraining's auc: 0.944017\tvalid_1's auc: 0.901016\n",
      "[11000]\ttraining's auc: 0.946915\tvalid_1's auc: 0.901035\n",
      "Early stopping, best iteration is:\n",
      "[10821]\ttraining's auc: 0.946397\tvalid_1's auc: 0.901082\n",
      "var_81 80 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903617\tvalid_1's auc: 0.887717\n",
      "[2000]\ttraining's auc: 0.913072\tvalid_1's auc: 0.894533\n",
      "[3000]\ttraining's auc: 0.91895\tvalid_1's auc: 0.897736\n",
      "[4000]\ttraining's auc: 0.92357\tvalid_1's auc: 0.899238\n",
      "[5000]\ttraining's auc: 0.927762\tvalid_1's auc: 0.900406\n",
      "[6000]\ttraining's auc: 0.931535\tvalid_1's auc: 0.901144\n",
      "[7000]\ttraining's auc: 0.935007\tvalid_1's auc: 0.901725\n",
      "[8000]\ttraining's auc: 0.938279\tvalid_1's auc: 0.901862\n",
      "[9000]\ttraining's auc: 0.941535\tvalid_1's auc: 0.901963\n",
      "[10000]\ttraining's auc: 0.9445\tvalid_1's auc: 0.902139\n",
      "[11000]\ttraining's auc: 0.94733\tvalid_1's auc: 0.902104\n",
      "Early stopping, best iteration is:\n",
      "[10545]\ttraining's auc: 0.946084\tvalid_1's auc: 0.902195\n",
      "var_81 80 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903514\tvalid_1's auc: 0.88796\n",
      "[2000]\ttraining's auc: 0.91224\tvalid_1's auc: 0.89352\n",
      "[3000]\ttraining's auc: 0.918387\tvalid_1's auc: 0.896785\n",
      "[4000]\ttraining's auc: 0.923544\tvalid_1's auc: 0.898854\n",
      "[5000]\ttraining's auc: 0.927831\tvalid_1's auc: 0.900043\n",
      "[6000]\ttraining's auc: 0.931738\tvalid_1's auc: 0.900834\n",
      "[7000]\ttraining's auc: 0.935168\tvalid_1's auc: 0.901127\n",
      "[8000]\ttraining's auc: 0.938444\tvalid_1's auc: 0.901326\n",
      "[9000]\ttraining's auc: 0.941638\tvalid_1's auc: 0.901472\n",
      "[10000]\ttraining's auc: 0.944681\tvalid_1's auc: 0.901594\n",
      "[11000]\ttraining's auc: 0.947584\tvalid_1's auc: 0.901537\n",
      "Early stopping, best iteration is:\n",
      "[10361]\ttraining's auc: 0.945748\tvalid_1's auc: 0.901646\n",
      "var_81 80 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.89955\tvalid_1's auc: 0.882592\n",
      "[2000]\ttraining's auc: 0.910036\tvalid_1's auc: 0.891672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000]\ttraining's auc: 0.917417\tvalid_1's auc: 0.896429\n",
      "[4000]\ttraining's auc: 0.923047\tvalid_1's auc: 0.898455\n",
      "[5000]\ttraining's auc: 0.927511\tvalid_1's auc: 0.899917\n",
      "[6000]\ttraining's auc: 0.931565\tvalid_1's auc: 0.900827\n",
      "[7000]\ttraining's auc: 0.935108\tvalid_1's auc: 0.901068\n",
      "[8000]\ttraining's auc: 0.938435\tvalid_1's auc: 0.901371\n",
      "[9000]\ttraining's auc: 0.941653\tvalid_1's auc: 0.901556\n",
      "[10000]\ttraining's auc: 0.944745\tvalid_1's auc: 0.901642\n",
      "[11000]\ttraining's auc: 0.947662\tvalid_1's auc: 0.90175\n",
      "[12000]\ttraining's auc: 0.950417\tvalid_1's auc: 0.90164\n",
      "Early stopping, best iteration is:\n",
      "[11010]\ttraining's auc: 0.947686\tvalid_1's auc: 0.901761\n",
      "var_81 80 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903226\tvalid_1's auc: 0.887623\n",
      "[2000]\ttraining's auc: 0.911694\tvalid_1's auc: 0.89303\n",
      "[3000]\ttraining's auc: 0.917864\tvalid_1's auc: 0.896184\n",
      "[4000]\ttraining's auc: 0.922901\tvalid_1's auc: 0.897902\n",
      "[5000]\ttraining's auc: 0.927266\tvalid_1's auc: 0.899317\n",
      "[6000]\ttraining's auc: 0.931163\tvalid_1's auc: 0.900273\n",
      "[7000]\ttraining's auc: 0.934665\tvalid_1's auc: 0.900724\n",
      "[8000]\ttraining's auc: 0.937918\tvalid_1's auc: 0.900754\n",
      "[9000]\ttraining's auc: 0.940969\tvalid_1's auc: 0.900984\n",
      "[10000]\ttraining's auc: 0.943882\tvalid_1's auc: 0.90131\n",
      "[11000]\ttraining's auc: 0.946725\tvalid_1's auc: 0.901308\n",
      "Early stopping, best iteration is:\n",
      "[10635]\ttraining's auc: 0.945711\tvalid_1's auc: 0.901428\n",
      "var_81 90 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903473\tvalid_1's auc: 0.886983\n",
      "[2000]\ttraining's auc: 0.912546\tvalid_1's auc: 0.893598\n",
      "[3000]\ttraining's auc: 0.918323\tvalid_1's auc: 0.896974\n",
      "[4000]\ttraining's auc: 0.92331\tvalid_1's auc: 0.899119\n",
      "[5000]\ttraining's auc: 0.927566\tvalid_1's auc: 0.900364\n",
      "[6000]\ttraining's auc: 0.931367\tvalid_1's auc: 0.901124\n",
      "[7000]\ttraining's auc: 0.934744\tvalid_1's auc: 0.901638\n",
      "[8000]\ttraining's auc: 0.937988\tvalid_1's auc: 0.901809\n",
      "[9000]\ttraining's auc: 0.941153\tvalid_1's auc: 0.902052\n",
      "[10000]\ttraining's auc: 0.944123\tvalid_1's auc: 0.902099\n",
      "[11000]\ttraining's auc: 0.947031\tvalid_1's auc: 0.902053\n",
      "Early stopping, best iteration is:\n",
      "[10348]\ttraining's auc: 0.94515\tvalid_1's auc: 0.902169\n",
      "var_81 90 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903513\tvalid_1's auc: 0.888009\n",
      "[2000]\ttraining's auc: 0.912221\tvalid_1's auc: 0.89349\n",
      "[3000]\ttraining's auc: 0.918326\tvalid_1's auc: 0.896908\n",
      "[4000]\ttraining's auc: 0.923482\tvalid_1's auc: 0.898999\n",
      "[5000]\ttraining's auc: 0.92778\tvalid_1's auc: 0.900185\n",
      "[6000]\ttraining's auc: 0.931677\tvalid_1's auc: 0.900986\n",
      "[7000]\ttraining's auc: 0.935094\tvalid_1's auc: 0.901298\n",
      "[8000]\ttraining's auc: 0.93841\tvalid_1's auc: 0.90149\n",
      "[9000]\ttraining's auc: 0.941616\tvalid_1's auc: 0.901627\n",
      "[10000]\ttraining's auc: 0.944662\tvalid_1's auc: 0.901759\n",
      "[11000]\ttraining's auc: 0.947526\tvalid_1's auc: 0.901806\n",
      "Early stopping, best iteration is:\n",
      "[10544]\ttraining's auc: 0.946252\tvalid_1's auc: 0.901875\n",
      "var_81 90 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.899483\tvalid_1's auc: 0.882616\n",
      "[2000]\ttraining's auc: 0.909986\tvalid_1's auc: 0.891614\n",
      "[3000]\ttraining's auc: 0.917402\tvalid_1's auc: 0.896418\n",
      "[4000]\ttraining's auc: 0.923029\tvalid_1's auc: 0.898426\n",
      "[5000]\ttraining's auc: 0.927466\tvalid_1's auc: 0.899842\n",
      "[6000]\ttraining's auc: 0.931541\tvalid_1's auc: 0.900852\n",
      "[7000]\ttraining's auc: 0.935084\tvalid_1's auc: 0.901087\n",
      "[8000]\ttraining's auc: 0.938397\tvalid_1's auc: 0.901371\n",
      "[9000]\ttraining's auc: 0.94162\tvalid_1's auc: 0.901585\n",
      "[10000]\ttraining's auc: 0.944706\tvalid_1's auc: 0.901703\n",
      "[11000]\ttraining's auc: 0.947627\tvalid_1's auc: 0.901812\n",
      "Early stopping, best iteration is:\n",
      "[10919]\ttraining's auc: 0.947392\tvalid_1's auc: 0.901828\n",
      "var_81 90 le no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.901427\tvalid_1's auc: 0.884371\n",
      "[2000]\ttraining's auc: 0.911341\tvalid_1's auc: 0.891605\n",
      "[3000]\ttraining's auc: 0.917496\tvalid_1's auc: 0.895488\n",
      "[4000]\ttraining's auc: 0.92279\tvalid_1's auc: 0.897872\n",
      "[5000]\ttraining's auc: 0.927277\tvalid_1's auc: 0.899422\n",
      "[6000]\ttraining's auc: 0.931202\tvalid_1's auc: 0.900315\n",
      "[7000]\ttraining's auc: 0.934876\tvalid_1's auc: 0.900653\n",
      "[8000]\ttraining's auc: 0.938145\tvalid_1's auc: 0.900994\n",
      "[9000]\ttraining's auc: 0.941298\tvalid_1's auc: 0.901327\n",
      "[10000]\ttraining's auc: 0.944232\tvalid_1's auc: 0.901427\n",
      "Early stopping, best iteration is:\n",
      "[9618]\ttraining's auc: 0.943127\tvalid_1's auc: 0.901513\n",
      "var_81 100 drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.902677\tvalid_1's auc: 0.886481\n",
      "[2000]\ttraining's auc: 0.911706\tvalid_1's auc: 0.892946\n",
      "[3000]\ttraining's auc: 0.918377\tvalid_1's auc: 0.896422\n",
      "[4000]\ttraining's auc: 0.923283\tvalid_1's auc: 0.898368\n",
      "[5000]\ttraining's auc: 0.927754\tvalid_1's auc: 0.899573\n",
      "[6000]\ttraining's auc: 0.931668\tvalid_1's auc: 0.90056\n",
      "[7000]\ttraining's auc: 0.935172\tvalid_1's auc: 0.901079\n",
      "[8000]\ttraining's auc: 0.938518\tvalid_1's auc: 0.901379\n",
      "[9000]\ttraining's auc: 0.941689\tvalid_1's auc: 0.901603\n",
      "[10000]\ttraining's auc: 0.944638\tvalid_1's auc: 0.901702\n",
      "[11000]\ttraining's auc: 0.947575\tvalid_1's auc: 0.901798\n",
      "[12000]\ttraining's auc: 0.950316\tvalid_1's auc: 0.901757\n",
      "Early stopping, best iteration is:\n",
      "[11283]\ttraining's auc: 0.94837\tvalid_1's auc: 0.901851\n",
      "var_81 100 no drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.903488\tvalid_1's auc: 0.887945\n",
      "[2000]\ttraining's auc: 0.912238\tvalid_1's auc: 0.893483\n",
      "[3000]\ttraining's auc: 0.918324\tvalid_1's auc: 0.89677\n",
      "[4000]\ttraining's auc: 0.923483\tvalid_1's auc: 0.898918\n",
      "[5000]\ttraining's auc: 0.927795\tvalid_1's auc: 0.900102\n",
      "[6000]\ttraining's auc: 0.931679\tvalid_1's auc: 0.900908\n",
      "[7000]\ttraining's auc: 0.935101\tvalid_1's auc: 0.90125\n",
      "[8000]\ttraining's auc: 0.938406\tvalid_1's auc: 0.901462\n",
      "[9000]\ttraining's auc: 0.941634\tvalid_1's auc: 0.901593\n",
      "[10000]\ttraining's auc: 0.94468\tvalid_1's auc: 0.90169\n",
      "[11000]\ttraining's auc: 0.947552\tvalid_1's auc: 0.901746\n",
      "Early stopping, best iteration is:\n",
      "[10544]\ttraining's auc: 0.946265\tvalid_1's auc: 0.901803\n",
      "var_81 100 le drop\n",
      "--------------------\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.899513\tvalid_1's auc: 0.882681\n",
      "[2000]\ttraining's auc: 0.91007\tvalid_1's auc: 0.891728\n",
      "[3000]\ttraining's auc: 0.917437\tvalid_1's auc: 0.896432\n",
      "[4000]\ttraining's auc: 0.923048\tvalid_1's auc: 0.898459\n",
      "[5000]\ttraining's auc: 0.927505\tvalid_1's auc: 0.899934\n",
      "[6000]\ttraining's auc: 0.931562\tvalid_1's auc: 0.900922\n",
      "[7000]\ttraining's auc: 0.935124\tvalid_1's auc: 0.901195\n",
      "[8000]\ttraining's auc: 0.938443\tvalid_1's auc: 0.901556\n",
      "[9000]\ttraining's auc: 0.941657\tvalid_1's auc: 0.901722\n",
      "[10000]\ttraining's auc: 0.944727\tvalid_1's auc: 0.90181\n",
      "[11000]\ttraining's auc: 0.947657\tvalid_1's auc: 0.901929\n",
      "Early stopping, best iteration is:\n",
      "[10917]\ttraining's auc: 0.947408\tvalid_1's auc: 0.901943\n",
      "var_81 100 le no drop\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "num_bins = [10,20,30,40,50,60,70,80,90,100]\n",
    "col = 'var_81'\n",
    "for bins in num_bins:\n",
    "    X3 = train.copy()\n",
    "    X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "    X = pd.get_dummies(pd.cut(train[col].values, bins))\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    X3 = pd.concat([X3, X], axis=1)\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X3, y, test_size=0.2, stratify=y, random_state=123)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "   \n",
    "    print(col, bins, 'drop')\n",
    "    print('--------------------')\n",
    "    \n",
    "    X3 = train.copy()\n",
    "    X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "    X = pd.get_dummies(pd.cut(train[col].values, bins))\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    X3 = pd.concat([X3, X], axis=1)\n",
    "#     X3 = X3.drop(col, axis=1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X3, y, test_size=0.2, stratify=y, random_state=123)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "   \n",
    "    print(col, bins, 'no drop')\n",
    "    print('--------------------')\n",
    "    \n",
    "    X3 = train.copy()\n",
    "    X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "    X = pd.cut(train[col].values, bins)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    X = le.fit_transform(X)\n",
    "    X3['new_col'] = X\n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X3, y, test_size=0.2, stratify=y, random_state=123)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "   \n",
    "    print(col, bins, 'le drop')\n",
    "    print('--------------------')\n",
    "    \n",
    "    X3 = train.copy()\n",
    "    X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "    X = pd.cut(train[col].values, bins)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "                 boost=\"gbdt\",\n",
    "                 metric=\"auc\",\n",
    "                 boost_from_average=\"false\",\n",
    "                 n_estimators=999999,\n",
    "                 learning_rate = 0.0083,\n",
    "                 num_leaves = 13,\n",
    "                 max_depth=-1,\n",
    "                 tree_learner = \"serial\",\n",
    "                 feature_fraction = 0.041,\n",
    "                 bagging_freq = 5,\n",
    "                 bagging_fraction = 0.335,\n",
    "                 min_data_in_leaf = 80,\n",
    "                 min_sum_hessian_in_leaf = 10.0,\n",
    "#                  random_seed = 42 + params,\n",
    "                 objective='binary', \n",
    "                 n_jobs=-1)\n",
    "#     col = 'var_68'\n",
    "    np.random.seed(123)\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    X = le.fit_transform(X)\n",
    "    X3['new_col'] = X\n",
    "#     X3 = X3.drop(col, axis=1)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X3, y, test_size=0.2, stratify=y, random_state=123)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=1000)\n",
    "\n",
    "   \n",
    "    print(col, bins, 'le no drop')\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    200000.000000\n",
       "mean         14.023978\n",
       "std           0.190059\n",
       "min          13.434600\n",
       "25%          13.894000\n",
       "50%          14.025500\n",
       "75%          14.164200\n",
       "max          14.654500\n",
       "Name: var_12, dtype: float64"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['var_12'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    200000.000000\n",
       "mean         14.022662\n",
       "std           0.190071\n",
       "min          13.424500\n",
       "25%          13.891000\n",
       "50%          14.024600\n",
       "75%          14.162900\n",
       "max          14.682000\n",
       "Name: var_12, dtype: float64"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['var_12'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(13.4, 14.7, 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.4       , 13.42131148, 13.44262295, 13.46393443, 13.4852459 ,\n",
       "       13.50655738, 13.52786885, 13.54918033, 13.5704918 , 13.59180328,\n",
       "       13.61311475, 13.63442623, 13.6557377 , 13.67704918, 13.69836066,\n",
       "       13.71967213, 13.74098361, 13.76229508, 13.78360656, 13.80491803,\n",
       "       13.82622951, 13.84754098, 13.86885246, 13.89016393, 13.91147541,\n",
       "       13.93278689, 13.95409836, 13.97540984, 13.99672131, 14.01803279,\n",
       "       14.03934426, 14.06065574, 14.08196721, 14.10327869, 14.12459016,\n",
       "       14.14590164, 14.16721311, 14.18852459, 14.20983607, 14.23114754,\n",
       "       14.25245902, 14.27377049, 14.29508197, 14.31639344, 14.33770492,\n",
       "       14.35901639, 14.38032787, 14.40163934, 14.42295082, 14.4442623 ,\n",
       "       14.46557377, 14.48688525, 14.50819672, 14.5295082 , 14.55081967,\n",
       "       14.57213115, 14.59344262, 14.6147541 , 14.63606557, 14.65737705,\n",
       "       14.67868852, 14.7       ])"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = [13.4       , 13.42131148,13.46393443, 13.4852459 ,\n",
    "       13.50655738, 13.52786885, 13.54918033, 13.5704918 , 13.59180328,\n",
    "       13.61311475, 13.63442623, 13.6557377 , 13.67704918, 13.69836066,\n",
    "       13.71967213, 13.74098361, 13.76229508, 13.78360656, 13.80491803,\n",
    "       13.82622951, 13.84754098, 13.86885246, 13.89016393, 13.91147541,\n",
    "       13.93278689, 13.95409836, 13.97540984, 13.99672131, 14.01803279,\n",
    "       14.03934426, 14.06065574, 14.08196721, 14.10327869, 14.12459016,\n",
    "       14.14590164, 14.16721311, 14.18852459, 14.20983607, 14.23114754,\n",
    "       14.25245902, 14.27377049, 14.29508197, 14.31639344, 14.33770492,\n",
    "       14.35901639, 14.38032787, 14.40163934, 14.42295082, 14.4442623 ,\n",
    "       14.46557377, 14.48688525, 14.50819672, 14.5295082 , 14.55081967,\n",
    "       14.57213115, 14.59344262, 14.6147541 , 14.63606557, 14.7       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_12\n",
      "-------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200000, 200), (200000, 200))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = train.copy()\n",
    "X4 = test.copy()\n",
    "# bins = np.linspace(-6.0,6.0,40)\n",
    "X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "X4 = X4.drop('ID_code', axis=1)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "for col in ['var_12']:\n",
    "    print(col)\n",
    "    X3_col = pd.cut(X3[col], bins)\n",
    "    X4_col = pd.cut(X4[col], bins)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(X3_col)\n",
    "    X3_col = le.transform(X3_col)\n",
    "    X4_col = le.transform(X4_col)\n",
    "    \n",
    "#     X3_col_bins = pd.DataFrame(X3_col_bins)\n",
    "#     cols = [col + '_' + str(i) for i in range(X3_col_bins.shape[1])]\n",
    "#     X3_col_bins.columns = cols\n",
    "\n",
    "#     X4_col_bins = pd.DataFrame(X4_col_bins)\n",
    "#     X4_col_bins.columns = cols\n",
    " \n",
    "#     X3 = pd.concat([X3, X3_col_bins], axis=1)\n",
    "#     X4 = pd.concat([X4, X4_col_bins], axis=1)\n",
    "    X3['var_12_new'] = X3_col\n",
    "    X4['var_12_new'] = X4_col\n",
    "    \n",
    "    X3 = X3.drop(col, axis=1)\n",
    "    X4 = X4.drop(col, axis=1)\n",
    "    \n",
    "#     del X3_col_bins, X4_col_bins\n",
    "    gc.collect()\n",
    "    print('-------------')\n",
    "X3.shape, X4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Sat Mar 23 14:30:21 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88455\n",
      "[2000]\tvalid_0's auc: 0.891194\n",
      "[3000]\tvalid_0's auc: 0.894311\n",
      "[4000]\tvalid_0's auc: 0.896361\n",
      "[5000]\tvalid_0's auc: 0.897611\n",
      "[6000]\tvalid_0's auc: 0.898408\n",
      "[7000]\tvalid_0's auc: 0.898869\n",
      "[8000]\tvalid_0's auc: 0.899258\n",
      "[9000]\tvalid_0's auc: 0.899627\n",
      "[10000]\tvalid_0's auc: 0.899859\n",
      "[11000]\tvalid_0's auc: 0.899814\n",
      "[12000]\tvalid_0's auc: 0.899779\n",
      "[13000]\tvalid_0's auc: 0.899732\n",
      "Early stopping, best iteration is:\n",
      "[10642]\tvalid_0's auc: 0.900003\n",
      "Fold 1 started at Sat Mar 23 14:31:54 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885794\n",
      "[2000]\tvalid_0's auc: 0.890794\n",
      "[3000]\tvalid_0's auc: 0.894521\n",
      "[4000]\tvalid_0's auc: 0.896708\n",
      "[5000]\tvalid_0's auc: 0.897814\n",
      "[6000]\tvalid_0's auc: 0.898435\n",
      "[7000]\tvalid_0's auc: 0.899085\n",
      "[8000]\tvalid_0's auc: 0.899465\n",
      "[9000]\tvalid_0's auc: 0.89952\n",
      "[10000]\tvalid_0's auc: 0.89969\n",
      "[11000]\tvalid_0's auc: 0.899759\n",
      "[12000]\tvalid_0's auc: 0.899917\n",
      "[13000]\tvalid_0's auc: 0.899964\n",
      "[14000]\tvalid_0's auc: 0.899936\n",
      "[15000]\tvalid_0's auc: 0.899909\n",
      "Early stopping, best iteration is:\n",
      "[12263]\tvalid_0's auc: 0.900021\n",
      "Fold 2 started at Sat Mar 23 14:33:51 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.888536\n",
      "[2000]\tvalid_0's auc: 0.894367\n",
      "[3000]\tvalid_0's auc: 0.897934\n",
      "[4000]\tvalid_0's auc: 0.89992\n",
      "[5000]\tvalid_0's auc: 0.901274\n",
      "[6000]\tvalid_0's auc: 0.902082\n",
      "[7000]\tvalid_0's auc: 0.902484\n",
      "[8000]\tvalid_0's auc: 0.902639\n",
      "[9000]\tvalid_0's auc: 0.902897\n",
      "[10000]\tvalid_0's auc: 0.902962\n",
      "[11000]\tvalid_0's auc: 0.902936\n",
      "[12000]\tvalid_0's auc: 0.902766\n",
      "Early stopping, best iteration is:\n",
      "[9296]\tvalid_0's auc: 0.903036\n",
      "Fold 3 started at Sat Mar 23 14:35:41 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.887127\n",
      "[2000]\tvalid_0's auc: 0.893252\n",
      "[3000]\tvalid_0's auc: 0.896142\n",
      "[4000]\tvalid_0's auc: 0.898105\n",
      "[5000]\tvalid_0's auc: 0.899482\n",
      "[6000]\tvalid_0's auc: 0.900307\n",
      "[7000]\tvalid_0's auc: 0.900702\n",
      "[8000]\tvalid_0's auc: 0.900951\n",
      "[9000]\tvalid_0's auc: 0.901038\n",
      "[10000]\tvalid_0's auc: 0.901219\n",
      "[11000]\tvalid_0's auc: 0.901288\n",
      "[12000]\tvalid_0's auc: 0.901064\n",
      "[13000]\tvalid_0's auc: 0.900918\n",
      "Early stopping, best iteration is:\n",
      "[10843]\tvalid_0's auc: 0.901368\n",
      "Fold 4 started at Sat Mar 23 14:38:15 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.887165\n",
      "[2000]\tvalid_0's auc: 0.8928\n",
      "[3000]\tvalid_0's auc: 0.895916\n",
      "[4000]\tvalid_0's auc: 0.897699\n",
      "[5000]\tvalid_0's auc: 0.898973\n",
      "[6000]\tvalid_0's auc: 0.899605\n",
      "[7000]\tvalid_0's auc: 0.899902\n",
      "[8000]\tvalid_0's auc: 0.900412\n",
      "[9000]\tvalid_0's auc: 0.900404\n",
      "[10000]\tvalid_0's auc: 0.900118\n",
      "[11000]\tvalid_0's auc: 0.900264\n",
      "Early stopping, best iteration is:\n",
      "[8379]\tvalid_0's auc: 0.900538\n",
      "Fold 5 started at Sat Mar 23 14:40:20 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.888193\n",
      "[2000]\tvalid_0's auc: 0.894168\n",
      "[3000]\tvalid_0's auc: 0.897617\n",
      "[4000]\tvalid_0's auc: 0.900111\n",
      "[5000]\tvalid_0's auc: 0.901517\n",
      "[6000]\tvalid_0's auc: 0.902122\n",
      "[7000]\tvalid_0's auc: 0.902752\n",
      "[8000]\tvalid_0's auc: 0.903109\n",
      "[9000]\tvalid_0's auc: 0.903075\n",
      "[10000]\tvalid_0's auc: 0.903049\n",
      "[11000]\tvalid_0's auc: 0.903004\n",
      "Early stopping, best iteration is:\n",
      "[8646]\tvalid_0's auc: 0.903224\n",
      "Fold 6 started at Sat Mar 23 14:42:35 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88055\n",
      "[2000]\tvalid_0's auc: 0.88717\n",
      "[3000]\tvalid_0's auc: 0.890579\n",
      "[4000]\tvalid_0's auc: 0.89275\n",
      "[5000]\tvalid_0's auc: 0.89433\n",
      "[6000]\tvalid_0's auc: 0.895194\n",
      "[7000]\tvalid_0's auc: 0.89579\n",
      "[8000]\tvalid_0's auc: 0.895986\n",
      "[9000]\tvalid_0's auc: 0.896136\n",
      "[10000]\tvalid_0's auc: 0.896252\n",
      "[11000]\tvalid_0's auc: 0.896244\n",
      "[12000]\tvalid_0's auc: 0.896252\n",
      "[13000]\tvalid_0's auc: 0.896111\n",
      "Early stopping, best iteration is:\n",
      "[10695]\tvalid_0's auc: 0.89631\n",
      "Fold 7 started at Sat Mar 23 14:52:45 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88366\n",
      "[2000]\tvalid_0's auc: 0.889848\n",
      "[3000]\tvalid_0's auc: 0.892984\n"
     ]
    }
   ],
   "source": [
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light GBM Model\n",
      "Fold idx:1\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924556\tvalid_1's auc: 0.899198\n",
      "[10000]\ttraining's auc: 0.939844\tvalid_1's auc: 0.901029\n",
      "[15000]\ttraining's auc: 0.952252\tvalid_1's auc: 0.901101\n",
      "Early stopping, best iteration is:\n",
      "[14310]\ttraining's auc: 0.950616\tvalid_1's auc: 0.901219\n",
      "Fold idx:2\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924593\tvalid_1's auc: 0.896174\n",
      "[10000]\ttraining's auc: 0.939853\tvalid_1's auc: 0.898048\n",
      "Early stopping, best iteration is:\n",
      "[10974]\ttraining's auc: 0.942453\tvalid_1's auc: 0.898217\n",
      "Fold idx:3\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924515\tvalid_1's auc: 0.898457\n",
      "[10000]\ttraining's auc: 0.939777\tvalid_1's auc: 0.899203\n",
      "Early stopping, best iteration is:\n",
      "[8424]\ttraining's auc: 0.93546\tvalid_1's auc: 0.899518\n",
      "Fold idx:4\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924258\tvalid_1's auc: 0.899905\n",
      "[10000]\ttraining's auc: 0.939626\tvalid_1's auc: 0.901624\n",
      "Early stopping, best iteration is:\n",
      "[10592]\ttraining's auc: 0.941152\tvalid_1's auc: 0.90179\n",
      "Fold idx:5\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.923549\tvalid_1's auc: 0.907254\n",
      "[10000]\ttraining's auc: 0.938949\tvalid_1's auc: 0.909136\n",
      "[15000]\ttraining's auc: 0.95156\tvalid_1's auc: 0.909381\n",
      "Early stopping, best iteration is:\n",
      "[13704]\ttraining's auc: 0.948452\tvalid_1's auc: 0.909564\n",
      "Fold idx:6\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924166\tvalid_1's auc: 0.901551\n",
      "[10000]\ttraining's auc: 0.939494\tvalid_1's auc: 0.903072\n",
      "[15000]\ttraining's auc: 0.95211\tvalid_1's auc: 0.903059\n",
      "Early stopping, best iteration is:\n",
      "[12760]\ttraining's auc: 0.946732\tvalid_1's auc: 0.903311\n",
      "Fold idx:7\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924131\tvalid_1's auc: 0.901874\n",
      "[10000]\ttraining's auc: 0.939457\tvalid_1's auc: 0.904208\n",
      "[15000]\ttraining's auc: 0.95204\tvalid_1's auc: 0.904237\n",
      "Early stopping, best iteration is:\n",
      "[12971]\ttraining's auc: 0.947197\tvalid_1's auc: 0.904368\n",
      "Fold idx:8\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924884\tvalid_1's auc: 0.892575\n",
      "[10000]\ttraining's auc: 0.940038\tvalid_1's auc: 0.894152\n",
      "[15000]\ttraining's auc: 0.952416\tvalid_1's auc: 0.894241\n",
      "Early stopping, best iteration is:\n",
      "[14697]\ttraining's auc: 0.951729\tvalid_1's auc: 0.894279\n",
      "Fold idx:9\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924962\tvalid_1's auc: 0.893686\n",
      "[10000]\ttraining's auc: 0.940087\tvalid_1's auc: 0.89586\n",
      "[15000]\ttraining's auc: 0.95253\tvalid_1's auc: 0.895634\n",
      "Early stopping, best iteration is:\n",
      "[11079]\ttraining's auc: 0.942933\tvalid_1's auc: 0.896113\n",
      "Fold idx:10\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[5000]\ttraining's auc: 0.924715\tvalid_1's auc: 0.895578\n",
      "[10000]\ttraining's auc: 0.939996\tvalid_1's auc: 0.897351\n",
      "Early stopping, best iteration is:\n",
      "[10116]\ttraining's auc: 0.940291\tvalid_1's auc: 0.897368\n",
      "CV score: 0.90043 \n"
     ]
    }
   ],
   "source": [
    "train = X1.copy()\n",
    "test = X2.copy()\n",
    "target = y.copy()\n",
    "\n",
    "# num_folds = 15\n",
    "# features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "\n",
    "# folds = KFold(n_splits=num_folds, random_state=2319)\n",
    "oof = np.zeros(len(train))\n",
    "getVal = np.zeros(len(train))\n",
    "predictions = np.zeros(len(target))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "n_fold = 10\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "repeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)\n",
    "\n",
    "# f_cats = list(X1.columns)\n",
    "\n",
    "# for f in f_cats:\n",
    "#     train[f + \"_avg\"], test[f + \"_avg\"] = target_encode(trn_series=train[f],\n",
    "#                                      tst_series=test[f],\n",
    "#                                      target=y,\n",
    "#                                      min_samples_leaf=1000,\n",
    "#                                      smoothing=10,\n",
    "#                                      noise_level=0)\n",
    "\n",
    "features = [c for c in train.columns if c not in ['ID_code', 'target']]\n",
    "done = ['var_4','var_10', 'var_14', 'var_17', 'var_27', 'var_29', 'var_37', 'var_39', 'var_42', 'var_46', \n",
    "       'var_47', 'var_69', 'var_79', 'var_96', 'var_124', 'var_126', 'var_140']\n",
    "features = [c for c in features if c not in done]\n",
    "print('Light GBM Model')\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    \n",
    "    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]\n",
    "    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]\n",
    "    \n",
    "#     X_tr, y_tr = augment(X_train.values, y_train.values)\n",
    "    X_tr, y_tr = X_train.values, y_train.values\n",
    "    X_tr = pd.DataFrame(X_tr)\n",
    "    \n",
    "    print(\"Fold idx:{}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "    \n",
    "    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    getVal[val_idx]+= clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    \n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"feature\"] = features\n",
    "#     fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#     fold_importance_df[\"fold\"] = fold_ + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Mar 26 06:59:57 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883656\n",
      "[2000]\tvalid_0's auc: 0.890518\n",
      "[3000]\tvalid_0's auc: 0.893795\n",
      "[4000]\tvalid_0's auc: 0.896115\n",
      "[5000]\tvalid_0's auc: 0.897815\n",
      "[6000]\tvalid_0's auc: 0.898639\n",
      "[7000]\tvalid_0's auc: 0.89913\n",
      "[8000]\tvalid_0's auc: 0.89946\n",
      "[9000]\tvalid_0's auc: 0.899437\n",
      "[10000]\tvalid_0's auc: 0.899558\n",
      "[11000]\tvalid_0's auc: 0.899774\n",
      "[12000]\tvalid_0's auc: 0.899756\n",
      "[13000]\tvalid_0's auc: 0.899566\n",
      "[14000]\tvalid_0's auc: 0.899599\n",
      "Early stopping, best iteration is:\n",
      "[11880]\tvalid_0's auc: 0.899837\n",
      "Fold 1 started at Tue Mar 26 07:02:54 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883539\n",
      "[2000]\tvalid_0's auc: 0.890338\n",
      "[3000]\tvalid_0's auc: 0.893526\n",
      "[4000]\tvalid_0's auc: 0.896134\n",
      "[5000]\tvalid_0's auc: 0.897382\n",
      "[6000]\tvalid_0's auc: 0.898068\n",
      "[7000]\tvalid_0's auc: 0.898635\n",
      "[8000]\tvalid_0's auc: 0.898999\n",
      "[9000]\tvalid_0's auc: 0.899101\n",
      "[10000]\tvalid_0's auc: 0.899301\n",
      "[11000]\tvalid_0's auc: 0.899285\n",
      "[12000]\tvalid_0's auc: 0.899302\n",
      "[13000]\tvalid_0's auc: 0.899284\n",
      "Early stopping, best iteration is:\n",
      "[10474]\tvalid_0's auc: 0.899413\n",
      "Fold 2 started at Tue Mar 26 07:22:43 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.889131\n",
      "[2000]\tvalid_0's auc: 0.895317\n",
      "[3000]\tvalid_0's auc: 0.89833\n",
      "[4000]\tvalid_0's auc: 0.900263\n",
      "[5000]\tvalid_0's auc: 0.901139\n",
      "[6000]\tvalid_0's auc: 0.901874\n",
      "[7000]\tvalid_0's auc: 0.90227\n",
      "[8000]\tvalid_0's auc: 0.902688\n",
      "[9000]\tvalid_0's auc: 0.902849\n",
      "[10000]\tvalid_0's auc: 0.902907\n",
      "[11000]\tvalid_0's auc: 0.903035\n",
      "[12000]\tvalid_0's auc: 0.90312\n",
      "[13000]\tvalid_0's auc: 0.903012\n",
      "[14000]\tvalid_0's auc: 0.902814\n",
      "[15000]\tvalid_0's auc: 0.902735\n",
      "Early stopping, best iteration is:\n",
      "[12090]\tvalid_0's auc: 0.903176\n",
      "Fold 3 started at Tue Mar 26 07:55:11 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886658\n",
      "[2000]\tvalid_0's auc: 0.893063\n",
      "[3000]\tvalid_0's auc: 0.895746\n",
      "[4000]\tvalid_0's auc: 0.897456\n",
      "[5000]\tvalid_0's auc: 0.898761\n",
      "[6000]\tvalid_0's auc: 0.89964\n",
      "[7000]\tvalid_0's auc: 0.9001\n",
      "[8000]\tvalid_0's auc: 0.900511\n",
      "[9000]\tvalid_0's auc: 0.900639\n",
      "[10000]\tvalid_0's auc: 0.900946\n",
      "[11000]\tvalid_0's auc: 0.901076\n",
      "[12000]\tvalid_0's auc: 0.90105\n",
      "[13000]\tvalid_0's auc: 0.900935\n",
      "[14000]\tvalid_0's auc: 0.900771\n",
      "[15000]\tvalid_0's auc: 0.900606\n",
      "Early stopping, best iteration is:\n",
      "[12221]\tvalid_0's auc: 0.901131\n",
      "Fold 4 started at Tue Mar 26 08:23:20 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.887076\n",
      "[2000]\tvalid_0's auc: 0.892745\n",
      "[3000]\tvalid_0's auc: 0.895705\n",
      "[4000]\tvalid_0's auc: 0.897629\n",
      "[5000]\tvalid_0's auc: 0.89871\n",
      "[6000]\tvalid_0's auc: 0.899618\n",
      "[7000]\tvalid_0's auc: 0.899977\n",
      "[8000]\tvalid_0's auc: 0.90003\n",
      "[9000]\tvalid_0's auc: 0.900148\n",
      "[10000]\tvalid_0's auc: 0.900231\n",
      "[11000]\tvalid_0's auc: 0.900162\n",
      "[12000]\tvalid_0's auc: 0.900078\n",
      "[13000]\tvalid_0's auc: 0.899883\n",
      "Early stopping, best iteration is:\n",
      "[10514]\tvalid_0's auc: 0.900328\n",
      "Fold 5 started at Tue Mar 26 08:47:51 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.888499\n",
      "[2000]\tvalid_0's auc: 0.894596\n",
      "[3000]\tvalid_0's auc: 0.89777\n",
      "[4000]\tvalid_0's auc: 0.899617\n",
      "[5000]\tvalid_0's auc: 0.900684\n",
      "[6000]\tvalid_0's auc: 0.901426\n",
      "[7000]\tvalid_0's auc: 0.901929\n",
      "[8000]\tvalid_0's auc: 0.902279\n",
      "[9000]\tvalid_0's auc: 0.902317\n",
      "[10000]\tvalid_0's auc: 0.90234\n",
      "[11000]\tvalid_0's auc: 0.902304\n",
      "[12000]\tvalid_0's auc: 0.90231\n",
      "[13000]\tvalid_0's auc: 0.902128\n",
      "Early stopping, best iteration is:\n",
      "[10434]\tvalid_0's auc: 0.902414\n",
      "Fold 6 started at Tue Mar 26 09:10:03 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.881922\n",
      "[2000]\tvalid_0's auc: 0.887967\n",
      "[3000]\tvalid_0's auc: 0.89111\n",
      "[4000]\tvalid_0's auc: 0.893158\n",
      "[5000]\tvalid_0's auc: 0.894368\n",
      "[6000]\tvalid_0's auc: 0.895302\n",
      "[7000]\tvalid_0's auc: 0.895778\n",
      "[8000]\tvalid_0's auc: 0.896167\n",
      "[9000]\tvalid_0's auc: 0.896333\n",
      "[10000]\tvalid_0's auc: 0.896322\n",
      "[11000]\tvalid_0's auc: 0.896437\n",
      "[12000]\tvalid_0's auc: 0.896466\n",
      "[13000]\tvalid_0's auc: 0.896479\n",
      "[14000]\tvalid_0's auc: 0.896323\n",
      "[15000]\tvalid_0's auc: 0.896176\n",
      "Early stopping, best iteration is:\n",
      "[12547]\tvalid_0's auc: 0.89655\n",
      "Fold 7 started at Tue Mar 26 09:37:29 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883857\n",
      "[2000]\tvalid_0's auc: 0.889996\n",
      "[3000]\tvalid_0's auc: 0.893247\n",
      "[4000]\tvalid_0's auc: 0.895579\n",
      "[5000]\tvalid_0's auc: 0.896907\n",
      "[6000]\tvalid_0's auc: 0.89802\n",
      "[7000]\tvalid_0's auc: 0.898728\n",
      "[8000]\tvalid_0's auc: 0.899027\n",
      "[9000]\tvalid_0's auc: 0.899368\n",
      "[10000]\tvalid_0's auc: 0.899702\n",
      "[11000]\tvalid_0's auc: 0.89988\n",
      "[12000]\tvalid_0's auc: 0.899987\n",
      "[13000]\tvalid_0's auc: 0.899987\n",
      "[14000]\tvalid_0's auc: 0.900044\n",
      "[15000]\tvalid_0's auc: 0.900044\n",
      "[16000]\tvalid_0's auc: 0.899902\n",
      "[17000]\tvalid_0's auc: 0.899808\n",
      "Early stopping, best iteration is:\n",
      "[14661]\tvalid_0's auc: 0.900153\n",
      "Fold 8 started at Tue Mar 26 10:20:42 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.880114\n",
      "[2000]\tvalid_0's auc: 0.88634\n",
      "[3000]\tvalid_0's auc: 0.889419\n",
      "[4000]\tvalid_0's auc: 0.891754\n",
      "[5000]\tvalid_0's auc: 0.893325\n",
      "[6000]\tvalid_0's auc: 0.894367\n",
      "[7000]\tvalid_0's auc: 0.895026\n",
      "[8000]\tvalid_0's auc: 0.895231\n",
      "[9000]\tvalid_0's auc: 0.895364\n",
      "[10000]\tvalid_0's auc: 0.895328\n",
      "[11000]\tvalid_0's auc: 0.89537\n",
      "Early stopping, best iteration is:\n",
      "[8633]\tvalid_0's auc: 0.895513\n",
      "Fold 9 started at Tue Mar 26 11:04:33 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.89258\n",
      "[2000]\tvalid_0's auc: 0.898251\n",
      "[3000]\tvalid_0's auc: 0.90093\n",
      "[4000]\tvalid_0's auc: 0.902893\n",
      "[5000]\tvalid_0's auc: 0.903921\n",
      "[6000]\tvalid_0's auc: 0.90476\n",
      "[7000]\tvalid_0's auc: 0.905174\n",
      "[8000]\tvalid_0's auc: 0.905593\n",
      "[9000]\tvalid_0's auc: 0.905645\n",
      "[10000]\tvalid_0's auc: 0.905749\n",
      "[11000]\tvalid_0's auc: 0.905912\n",
      "[12000]\tvalid_0's auc: 0.905836\n",
      "[13000]\tvalid_0's auc: 0.905868\n",
      "[14000]\tvalid_0's auc: 0.905738\n",
      "[15000]\tvalid_0's auc: 0.905817\n",
      "Early stopping, best iteration is:\n",
      "[12351]\tvalid_0's auc: 0.905983\n",
      "CV mean score: 0.9004, std: 0.0029.\n"
     ]
    }
   ],
   "source": [
    "X3 = X3.drop('var_8', axis=1)\n",
    "X4 = X4.drop('var_8', axis=1)\n",
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, _ = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Mar 26 14:46:57 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884219\n",
      "[2000]\tvalid_0's auc: 0.890184\n",
      "[3000]\tvalid_0's auc: 0.893869\n",
      "[4000]\tvalid_0's auc: 0.896569\n",
      "[5000]\tvalid_0's auc: 0.897962\n",
      "[6000]\tvalid_0's auc: 0.89912\n",
      "[7000]\tvalid_0's auc: 0.899567\n",
      "[8000]\tvalid_0's auc: 0.899888\n",
      "[9000]\tvalid_0's auc: 0.899833\n",
      "[10000]\tvalid_0's auc: 0.899873\n",
      "[11000]\tvalid_0's auc: 0.900018\n",
      "[12000]\tvalid_0's auc: 0.899909\n",
      "[13000]\tvalid_0's auc: 0.899936\n",
      "[14000]\tvalid_0's auc: 0.900176\n",
      "[15000]\tvalid_0's auc: 0.899991\n",
      "[16000]\tvalid_0's auc: 0.899997\n",
      "[17000]\tvalid_0's auc: 0.899991\n",
      "Early stopping, best iteration is:\n",
      "[14087]\tvalid_0's auc: 0.900212\n",
      "Fold 1 started at Tue Mar 26 14:49:14 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88481\n",
      "[2000]\tvalid_0's auc: 0.890727\n",
      "[3000]\tvalid_0's auc: 0.894485\n",
      "[4000]\tvalid_0's auc: 0.896329\n",
      "[5000]\tvalid_0's auc: 0.897396\n",
      "[6000]\tvalid_0's auc: 0.898066\n",
      "[7000]\tvalid_0's auc: 0.898749\n",
      "[8000]\tvalid_0's auc: 0.899091\n",
      "[9000]\tvalid_0's auc: 0.899252\n",
      "[10000]\tvalid_0's auc: 0.89936\n",
      "[11000]\tvalid_0's auc: 0.899421\n",
      "[12000]\tvalid_0's auc: 0.899405\n",
      "[13000]\tvalid_0's auc: 0.899313\n",
      "Early stopping, best iteration is:\n",
      "[10761]\tvalid_0's auc: 0.89954\n",
      "Fold 2 started at Tue Mar 26 14:52:11 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.888622\n",
      "[2000]\tvalid_0's auc: 0.894462\n",
      "[3000]\tvalid_0's auc: 0.897531\n",
      "[4000]\tvalid_0's auc: 0.899512\n",
      "[5000]\tvalid_0's auc: 0.900732\n",
      "[6000]\tvalid_0's auc: 0.901425\n",
      "[7000]\tvalid_0's auc: 0.901853\n",
      "[8000]\tvalid_0's auc: 0.902272\n",
      "[9000]\tvalid_0's auc: 0.902454\n",
      "[10000]\tvalid_0's auc: 0.902522\n",
      "[11000]\tvalid_0's auc: 0.90245\n",
      "[12000]\tvalid_0's auc: 0.90244\n",
      "[13000]\tvalid_0's auc: 0.902382\n",
      "Early stopping, best iteration is:\n",
      "[10225]\tvalid_0's auc: 0.902542\n",
      "Fold 3 started at Tue Mar 26 15:38:21 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886079\n",
      "[2000]\tvalid_0's auc: 0.892691\n",
      "[3000]\tvalid_0's auc: 0.895849\n",
      "[4000]\tvalid_0's auc: 0.898018\n",
      "[5000]\tvalid_0's auc: 0.899057\n",
      "[6000]\tvalid_0's auc: 0.899864\n",
      "[7000]\tvalid_0's auc: 0.90039\n",
      "[8000]\tvalid_0's auc: 0.900708\n",
      "[9000]\tvalid_0's auc: 0.901046\n",
      "[10000]\tvalid_0's auc: 0.900945\n",
      "[11000]\tvalid_0's auc: 0.900865\n",
      "[12000]\tvalid_0's auc: 0.900915\n",
      "Early stopping, best iteration is:\n",
      "[9089]\tvalid_0's auc: 0.901125\n",
      "Fold 4 started at Tue Mar 26 16:15:52 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885314\n",
      "[2000]\tvalid_0's auc: 0.891864\n",
      "[3000]\tvalid_0's auc: 0.895721\n",
      "[4000]\tvalid_0's auc: 0.897436\n",
      "[5000]\tvalid_0's auc: 0.899088\n",
      "[6000]\tvalid_0's auc: 0.89975\n",
      "[7000]\tvalid_0's auc: 0.900337\n",
      "[8000]\tvalid_0's auc: 0.900525\n",
      "[9000]\tvalid_0's auc: 0.90042\n",
      "[10000]\tvalid_0's auc: 0.90046\n",
      "[11000]\tvalid_0's auc: 0.900533\n",
      "[12000]\tvalid_0's auc: 0.900317\n",
      "[13000]\tvalid_0's auc: 0.900095\n",
      "Early stopping, best iteration is:\n",
      "[10532]\tvalid_0's auc: 0.900626\n",
      "Fold 5 started at Tue Mar 26 17:11:49 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885619\n",
      "[2000]\tvalid_0's auc: 0.893085\n",
      "[3000]\tvalid_0's auc: 0.897018\n",
      "[4000]\tvalid_0's auc: 0.899197\n",
      "[5000]\tvalid_0's auc: 0.900375\n",
      "[6000]\tvalid_0's auc: 0.901151\n",
      "[7000]\tvalid_0's auc: 0.901571\n",
      "[8000]\tvalid_0's auc: 0.901728\n",
      "[9000]\tvalid_0's auc: 0.901865\n",
      "[10000]\tvalid_0's auc: 0.90194\n",
      "[11000]\tvalid_0's auc: 0.901898\n",
      "[12000]\tvalid_0's auc: 0.901922\n",
      "[13000]\tvalid_0's auc: 0.901792\n",
      "Early stopping, best iteration is:\n",
      "[10064]\tvalid_0's auc: 0.902016\n",
      "Fold 6 started at Tue Mar 26 18:06:08 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.87977\n",
      "[2000]\tvalid_0's auc: 0.887242\n",
      "[3000]\tvalid_0's auc: 0.890849\n",
      "[4000]\tvalid_0's auc: 0.89293\n",
      "[5000]\tvalid_0's auc: 0.894444\n",
      "[6000]\tvalid_0's auc: 0.894979\n",
      "[7000]\tvalid_0's auc: 0.895286\n",
      "[8000]\tvalid_0's auc: 0.895663\n",
      "[9000]\tvalid_0's auc: 0.895785\n",
      "[10000]\tvalid_0's auc: 0.895791\n",
      "[11000]\tvalid_0's auc: 0.896032\n",
      "[12000]\tvalid_0's auc: 0.896047\n",
      "[13000]\tvalid_0's auc: 0.895987\n",
      "[14000]\tvalid_0's auc: 0.895884\n",
      "Early stopping, best iteration is:\n",
      "[11649]\tvalid_0's auc: 0.896102\n",
      "Fold 7 started at Tue Mar 26 19:08:57 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884632\n",
      "[2000]\tvalid_0's auc: 0.889955\n",
      "[3000]\tvalid_0's auc: 0.89325\n",
      "[4000]\tvalid_0's auc: 0.895337\n",
      "[5000]\tvalid_0's auc: 0.896861\n",
      "[6000]\tvalid_0's auc: 0.897856\n",
      "[7000]\tvalid_0's auc: 0.898372\n",
      "[8000]\tvalid_0's auc: 0.898954\n",
      "[9000]\tvalid_0's auc: 0.899224\n",
      "[10000]\tvalid_0's auc: 0.899317\n",
      "[11000]\tvalid_0's auc: 0.899323\n",
      "[12000]\tvalid_0's auc: 0.899239\n",
      "[13000]\tvalid_0's auc: 0.899186\n",
      "Early stopping, best iteration is:\n",
      "[10640]\tvalid_0's auc: 0.899423\n",
      "Fold 8 started at Tue Mar 26 20:10:41 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.879905\n",
      "[2000]\tvalid_0's auc: 0.886146\n",
      "[3000]\tvalid_0's auc: 0.889827\n",
      "[4000]\tvalid_0's auc: 0.892453\n",
      "[5000]\tvalid_0's auc: 0.893741\n",
      "[6000]\tvalid_0's auc: 0.894424\n",
      "[7000]\tvalid_0's auc: 0.895063\n",
      "[8000]\tvalid_0's auc: 0.895527\n",
      "[9000]\tvalid_0's auc: 0.895632\n",
      "[10000]\tvalid_0's auc: 0.895786\n",
      "[11000]\tvalid_0's auc: 0.895834\n",
      "[12000]\tvalid_0's auc: 0.895678\n",
      "[13000]\tvalid_0's auc: 0.89575\n",
      "[14000]\tvalid_0's auc: 0.895572\n",
      "Early stopping, best iteration is:\n",
      "[11430]\tvalid_0's auc: 0.895901\n",
      "Fold 9 started at Tue Mar 26 21:10:57 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.892228\n",
      "[2000]\tvalid_0's auc: 0.897655\n",
      "[3000]\tvalid_0's auc: 0.900701\n",
      "[4000]\tvalid_0's auc: 0.903093\n",
      "[5000]\tvalid_0's auc: 0.904263\n",
      "[6000]\tvalid_0's auc: 0.904984\n",
      "[7000]\tvalid_0's auc: 0.905546\n",
      "[8000]\tvalid_0's auc: 0.90603\n",
      "[9000]\tvalid_0's auc: 0.906384\n",
      "[10000]\tvalid_0's auc: 0.906566\n",
      "[11000]\tvalid_0's auc: 0.906636\n",
      "[12000]\tvalid_0's auc: 0.906665\n",
      "[13000]\tvalid_0's auc: 0.906704\n",
      "[14000]\tvalid_0's auc: 0.906669\n",
      "[15000]\tvalid_0's auc: 0.906654\n",
      "Early stopping, best iteration is:\n",
      "[12851]\tvalid_0's auc: 0.906763\n",
      "CV mean score: 0.9004, std: 0.0030.\n"
     ]
    }
   ],
   "source": [
    "X3 = train.copy()\n",
    "X4 = test.copy()\n",
    "X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "X4 = X4.drop('ID_code', axis=1)\n",
    "X3 = X3.drop('var_8', axis=1)\n",
    "X4 = X4.drop('var_8', axis=1)\n",
    "\n",
    "oof = []\n",
    "preds = []\n",
    "\n",
    "oof_lgb, prediction_lgb, scores = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "oof.append(oof_lgb)\n",
    "preds.append(prediction_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9002124611227671, 0.8995398749678873, 0.9025417658787774, 0.9011245329771376, 0.9006264950953957, 0.9020164878774554, 0.896101759130971, 0.8994228966341168, 0.8959005210294642, 0.9067631456112862]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.900424994032526"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Fri Mar 29 07:57:06 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88265\n",
      "[2000]\tvalid_0's auc: 0.890775\n",
      "[3000]\tvalid_0's auc: 0.894639\n",
      "[4000]\tvalid_0's auc: 0.896751\n",
      "[5000]\tvalid_0's auc: 0.898114\n",
      "[6000]\tvalid_0's auc: 0.899049\n",
      "[7000]\tvalid_0's auc: 0.899488\n",
      "[8000]\tvalid_0's auc: 0.899699\n",
      "[9000]\tvalid_0's auc: 0.899881\n",
      "[10000]\tvalid_0's auc: 0.900057\n",
      "[11000]\tvalid_0's auc: 0.900197\n",
      "[12000]\tvalid_0's auc: 0.900138\n",
      "[13000]\tvalid_0's auc: 0.900264\n",
      "[14000]\tvalid_0's auc: 0.90032\n",
      "[15000]\tvalid_0's auc: 0.900256\n",
      "[16000]\tvalid_0's auc: 0.900212\n",
      "[17000]\tvalid_0's auc: 0.900246\n",
      "Early stopping, best iteration is:\n",
      "[14556]\tvalid_0's auc: 0.900393\n",
      "Fold 1 started at Fri Mar 29 07:58:49 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.882687\n",
      "[2000]\tvalid_0's auc: 0.891451\n",
      "[3000]\tvalid_0's auc: 0.894499\n",
      "[4000]\tvalid_0's auc: 0.896226\n",
      "[5000]\tvalid_0's auc: 0.89768\n",
      "[6000]\tvalid_0's auc: 0.898137\n",
      "[7000]\tvalid_0's auc: 0.898552\n",
      "[8000]\tvalid_0's auc: 0.899012\n",
      "[9000]\tvalid_0's auc: 0.899205\n",
      "[10000]\tvalid_0's auc: 0.899575\n",
      "[11000]\tvalid_0's auc: 0.899738\n",
      "[12000]\tvalid_0's auc: 0.899842\n",
      "[13000]\tvalid_0's auc: 0.89994\n",
      "[14000]\tvalid_0's auc: 0.899918\n",
      "[15000]\tvalid_0's auc: 0.899956\n",
      "[16000]\tvalid_0's auc: 0.899853\n",
      "[17000]\tvalid_0's auc: 0.899908\n",
      "Early stopping, best iteration is:\n",
      "[14953]\tvalid_0's auc: 0.899974\n",
      "Fold 2 started at Fri Mar 29 08:00:34 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.887064\n",
      "[2000]\tvalid_0's auc: 0.894574\n",
      "[3000]\tvalid_0's auc: 0.898073\n",
      "[4000]\tvalid_0's auc: 0.900171\n",
      "[5000]\tvalid_0's auc: 0.901222\n",
      "[6000]\tvalid_0's auc: 0.90187\n",
      "[7000]\tvalid_0's auc: 0.902394\n",
      "[8000]\tvalid_0's auc: 0.902347\n",
      "[9000]\tvalid_0's auc: 0.902497\n",
      "[10000]\tvalid_0's auc: 0.902432\n",
      "[11000]\tvalid_0's auc: 0.902361\n",
      "[12000]\tvalid_0's auc: 0.902369\n",
      "Early stopping, best iteration is:\n",
      "[9449]\tvalid_0's auc: 0.902598\n",
      "Fold 3 started at Fri Mar 29 08:01:40 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88394\n",
      "[2000]\tvalid_0's auc: 0.891535\n",
      "[3000]\tvalid_0's auc: 0.895002\n",
      "[4000]\tvalid_0's auc: 0.897369\n",
      "[5000]\tvalid_0's auc: 0.898631\n",
      "[6000]\tvalid_0's auc: 0.899478\n",
      "[7000]\tvalid_0's auc: 0.899989\n",
      "[8000]\tvalid_0's auc: 0.900347\n",
      "[9000]\tvalid_0's auc: 0.900548\n",
      "[10000]\tvalid_0's auc: 0.900657\n",
      "[11000]\tvalid_0's auc: 0.90059\n",
      "[12000]\tvalid_0's auc: 0.900532\n",
      "Early stopping, best iteration is:\n",
      "[9702]\tvalid_0's auc: 0.900717\n",
      "Fold 4 started at Fri Mar 29 08:02:45 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886223\n",
      "[2000]\tvalid_0's auc: 0.892784\n",
      "[3000]\tvalid_0's auc: 0.896095\n",
      "[4000]\tvalid_0's auc: 0.898076\n",
      "[5000]\tvalid_0's auc: 0.898921\n",
      "[6000]\tvalid_0's auc: 0.899481\n",
      "[7000]\tvalid_0's auc: 0.899998\n",
      "[8000]\tvalid_0's auc: 0.900331\n",
      "[9000]\tvalid_0's auc: 0.900372\n",
      "[10000]\tvalid_0's auc: 0.900405\n",
      "[11000]\tvalid_0's auc: 0.900243\n",
      "[12000]\tvalid_0's auc: 0.900242\n",
      "Early stopping, best iteration is:\n",
      "[9720]\tvalid_0's auc: 0.90046\n",
      "Fold 5 started at Fri Mar 29 08:03:53 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885283\n",
      "[2000]\tvalid_0's auc: 0.892269\n",
      "[3000]\tvalid_0's auc: 0.896726\n",
      "[4000]\tvalid_0's auc: 0.899062\n",
      "[5000]\tvalid_0's auc: 0.900379\n",
      "[6000]\tvalid_0's auc: 0.901324\n",
      "[7000]\tvalid_0's auc: 0.901798\n",
      "[8000]\tvalid_0's auc: 0.902125\n",
      "[9000]\tvalid_0's auc: 0.90252\n",
      "[10000]\tvalid_0's auc: 0.902786\n",
      "[11000]\tvalid_0's auc: 0.902885\n",
      "[12000]\tvalid_0's auc: 0.902782\n",
      "[13000]\tvalid_0's auc: 0.902764\n",
      "[14000]\tvalid_0's auc: 0.902566\n",
      "Early stopping, best iteration is:\n",
      "[11215]\tvalid_0's auc: 0.902955\n",
      "Fold 6 started at Fri Mar 29 08:05:08 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.879554\n",
      "[2000]\tvalid_0's auc: 0.887232\n",
      "[3000]\tvalid_0's auc: 0.890846\n",
      "[4000]\tvalid_0's auc: 0.893188\n",
      "[5000]\tvalid_0's auc: 0.894461\n",
      "[6000]\tvalid_0's auc: 0.894997\n",
      "[7000]\tvalid_0's auc: 0.895386\n",
      "[8000]\tvalid_0's auc: 0.895618\n",
      "[9000]\tvalid_0's auc: 0.895582\n",
      "[10000]\tvalid_0's auc: 0.895738\n",
      "[11000]\tvalid_0's auc: 0.895858\n",
      "[12000]\tvalid_0's auc: 0.895843\n",
      "[13000]\tvalid_0's auc: 0.895813\n",
      "[14000]\tvalid_0's auc: 0.895765\n",
      "Early stopping, best iteration is:\n",
      "[11477]\tvalid_0's auc: 0.895959\n",
      "Fold 7 started at Fri Mar 29 08:06:25 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883334\n",
      "[2000]\tvalid_0's auc: 0.890998\n",
      "[3000]\tvalid_0's auc: 0.894121\n",
      "[4000]\tvalid_0's auc: 0.896402\n",
      "[5000]\tvalid_0's auc: 0.897732\n",
      "[6000]\tvalid_0's auc: 0.89873\n",
      "[7000]\tvalid_0's auc: 0.899335\n",
      "[8000]\tvalid_0's auc: 0.899833\n",
      "[9000]\tvalid_0's auc: 0.900152\n",
      "[10000]\tvalid_0's auc: 0.900307\n",
      "[11000]\tvalid_0's auc: 0.900518\n",
      "[12000]\tvalid_0's auc: 0.90042\n",
      "[13000]\tvalid_0's auc: 0.900468\n",
      "[14000]\tvalid_0's auc: 0.90044\n",
      "Early stopping, best iteration is:\n",
      "[11046]\tvalid_0's auc: 0.900528\n",
      "Fold 8 started at Fri Mar 29 08:07:40 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.876772\n",
      "[2000]\tvalid_0's auc: 0.885146\n",
      "[3000]\tvalid_0's auc: 0.889385\n",
      "[4000]\tvalid_0's auc: 0.891922\n",
      "[5000]\tvalid_0's auc: 0.893211\n",
      "[6000]\tvalid_0's auc: 0.894434\n",
      "[7000]\tvalid_0's auc: 0.895162\n",
      "[8000]\tvalid_0's auc: 0.8957\n",
      "[9000]\tvalid_0's auc: 0.895987\n",
      "[10000]\tvalid_0's auc: 0.896309\n",
      "[11000]\tvalid_0's auc: 0.896351\n",
      "[12000]\tvalid_0's auc: 0.896256\n",
      "[13000]\tvalid_0's auc: 0.896323\n",
      "Early stopping, best iteration is:\n",
      "[10632]\tvalid_0's auc: 0.896446\n",
      "Fold 9 started at Fri Mar 29 08:08:51 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.889979\n",
      "[2000]\tvalid_0's auc: 0.896965\n",
      "[3000]\tvalid_0's auc: 0.900634\n",
      "[4000]\tvalid_0's auc: 0.902923\n",
      "[5000]\tvalid_0's auc: 0.90424\n",
      "[6000]\tvalid_0's auc: 0.904993\n",
      "[7000]\tvalid_0's auc: 0.905581\n",
      "[8000]\tvalid_0's auc: 0.905915\n",
      "[9000]\tvalid_0's auc: 0.906074\n",
      "[10000]\tvalid_0's auc: 0.906204\n",
      "[11000]\tvalid_0's auc: 0.906044\n",
      "[12000]\tvalid_0's auc: 0.906119\n",
      "[13000]\tvalid_0's auc: 0.906304\n",
      "[14000]\tvalid_0's auc: 0.906453\n",
      "[15000]\tvalid_0's auc: 0.906487\n",
      "[16000]\tvalid_0's auc: 0.906308\n",
      "[17000]\tvalid_0's auc: 0.906067\n",
      "Early stopping, best iteration is:\n",
      "[14932]\tvalid_0's auc: 0.906518\n",
      "CV mean score: 0.9007, std: 0.0029.\n"
     ]
    }
   ],
   "source": [
    "X3 = train.copy()\n",
    "X4 = test.copy()\n",
    "# bins = np.linspace(-6.0,6.0,40)\n",
    "X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "X4 = X4.drop('ID_code', axis=1)\n",
    "cols = ['var_'+str(i) for i in range(200)]\n",
    "\n",
    "oof_lgb, prediction_lgb, scores = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                     model_type='lgb', plot_feature_importance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9006549370593551"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Fri Mar 29 08:14:45 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.882635\n",
      "[2000]\tvalid_0's auc: 0.890785\n",
      "[3000]\tvalid_0's auc: 0.894591\n",
      "[4000]\tvalid_0's auc: 0.896683\n",
      "[5000]\tvalid_0's auc: 0.898116\n",
      "[6000]\tvalid_0's auc: 0.899121\n",
      "[7000]\tvalid_0's auc: 0.899614\n",
      "[8000]\tvalid_0's auc: 0.899882\n",
      "[9000]\tvalid_0's auc: 0.900061\n",
      "[10000]\tvalid_0's auc: 0.900187\n",
      "[11000]\tvalid_0's auc: 0.900364\n",
      "[12000]\tvalid_0's auc: 0.900344\n",
      "[13000]\tvalid_0's auc: 0.900368\n",
      "[14000]\tvalid_0's auc: 0.900371\n",
      "[15000]\tvalid_0's auc: 0.900312\n",
      "Early stopping, best iteration is:\n",
      "[12594]\tvalid_0's auc: 0.900486\n",
      "Fold 1 started at Fri Mar 29 08:16:08 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88261\n",
      "[2000]\tvalid_0's auc: 0.891371\n",
      "[3000]\tvalid_0's auc: 0.894544\n",
      "[4000]\tvalid_0's auc: 0.896323\n",
      "[5000]\tvalid_0's auc: 0.897761\n",
      "[6000]\tvalid_0's auc: 0.898206\n",
      "[7000]\tvalid_0's auc: 0.898655\n",
      "[8000]\tvalid_0's auc: 0.899168\n",
      "[9000]\tvalid_0's auc: 0.899296\n",
      "[10000]\tvalid_0's auc: 0.899617\n",
      "[11000]\tvalid_0's auc: 0.899784\n",
      "[12000]\tvalid_0's auc: 0.899873\n",
      "[13000]\tvalid_0's auc: 0.899897\n",
      "[14000]\tvalid_0's auc: 0.899944\n",
      "[15000]\tvalid_0's auc: 0.900024\n",
      "[16000]\tvalid_0's auc: 0.8999\n",
      "[17000]\tvalid_0's auc: 0.89991\n",
      "Early stopping, best iteration is:\n",
      "[14953]\tvalid_0's auc: 0.900039\n",
      "Fold 2 started at Fri Mar 29 08:17:49 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.887033\n",
      "[2000]\tvalid_0's auc: 0.894432\n",
      "[3000]\tvalid_0's auc: 0.897913\n",
      "[4000]\tvalid_0's auc: 0.900067\n",
      "[5000]\tvalid_0's auc: 0.901148\n",
      "[6000]\tvalid_0's auc: 0.901861\n",
      "[7000]\tvalid_0's auc: 0.902319\n",
      "[8000]\tvalid_0's auc: 0.90228\n",
      "[9000]\tvalid_0's auc: 0.902418\n",
      "[10000]\tvalid_0's auc: 0.902375\n",
      "[11000]\tvalid_0's auc: 0.902314\n",
      "[12000]\tvalid_0's auc: 0.902325\n",
      "Early stopping, best iteration is:\n",
      "[9452]\tvalid_0's auc: 0.902531\n",
      "Fold 3 started at Fri Mar 29 08:18:50 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883895\n",
      "[2000]\tvalid_0's auc: 0.891546\n",
      "[3000]\tvalid_0's auc: 0.895079\n",
      "[4000]\tvalid_0's auc: 0.897483\n",
      "[5000]\tvalid_0's auc: 0.898841\n",
      "[6000]\tvalid_0's auc: 0.899637\n",
      "[7000]\tvalid_0's auc: 0.90011\n",
      "[8000]\tvalid_0's auc: 0.900442\n",
      "[9000]\tvalid_0's auc: 0.900667\n",
      "[10000]\tvalid_0's auc: 0.900831\n",
      "[11000]\tvalid_0's auc: 0.900715\n",
      "[12000]\tvalid_0's auc: 0.900623\n",
      "Early stopping, best iteration is:\n",
      "[9853]\tvalid_0's auc: 0.900882\n",
      "Fold 4 started at Fri Mar 29 08:19:55 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886242\n",
      "[2000]\tvalid_0's auc: 0.893004\n",
      "[3000]\tvalid_0's auc: 0.896285\n",
      "[4000]\tvalid_0's auc: 0.898167\n",
      "[5000]\tvalid_0's auc: 0.899021\n",
      "[6000]\tvalid_0's auc: 0.899623\n",
      "[7000]\tvalid_0's auc: 0.900156\n",
      "[8000]\tvalid_0's auc: 0.900457\n",
      "[9000]\tvalid_0's auc: 0.900479\n",
      "[10000]\tvalid_0's auc: 0.900445\n",
      "[11000]\tvalid_0's auc: 0.900179\n",
      "Early stopping, best iteration is:\n",
      "[8892]\tvalid_0's auc: 0.900525\n",
      "Fold 5 started at Fri Mar 29 08:20:53 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885373\n",
      "[2000]\tvalid_0's auc: 0.892344\n",
      "[3000]\tvalid_0's auc: 0.896744\n",
      "[4000]\tvalid_0's auc: 0.898958\n",
      "[5000]\tvalid_0's auc: 0.90023\n",
      "[6000]\tvalid_0's auc: 0.901118\n",
      "[7000]\tvalid_0's auc: 0.901665\n",
      "[8000]\tvalid_0's auc: 0.901947\n",
      "[9000]\tvalid_0's auc: 0.902274\n",
      "[10000]\tvalid_0's auc: 0.902416\n",
      "[11000]\tvalid_0's auc: 0.902595\n",
      "[12000]\tvalid_0's auc: 0.902554\n",
      "[13000]\tvalid_0's auc: 0.90257\n",
      "[14000]\tvalid_0's auc: 0.902402\n",
      "Early stopping, best iteration is:\n",
      "[11203]\tvalid_0's auc: 0.902671\n",
      "Fold 6 started at Fri Mar 29 08:22:06 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.879631\n",
      "[2000]\tvalid_0's auc: 0.887307\n",
      "[3000]\tvalid_0's auc: 0.890988\n",
      "[4000]\tvalid_0's auc: 0.893296\n",
      "[5000]\tvalid_0's auc: 0.89462\n",
      "[6000]\tvalid_0's auc: 0.895149\n",
      "[7000]\tvalid_0's auc: 0.895593\n",
      "[8000]\tvalid_0's auc: 0.895815\n",
      "[9000]\tvalid_0's auc: 0.895733\n",
      "[10000]\tvalid_0's auc: 0.895863\n",
      "[11000]\tvalid_0's auc: 0.896013\n",
      "[12000]\tvalid_0's auc: 0.895995\n",
      "[13000]\tvalid_0's auc: 0.895918\n",
      "[14000]\tvalid_0's auc: 0.895888\n",
      "Early stopping, best iteration is:\n",
      "[11477]\tvalid_0's auc: 0.896135\n",
      "Fold 7 started at Fri Mar 29 08:23:20 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883527\n",
      "[2000]\tvalid_0's auc: 0.891031\n",
      "[3000]\tvalid_0's auc: 0.894161\n",
      "[4000]\tvalid_0's auc: 0.896521\n",
      "[5000]\tvalid_0's auc: 0.897738\n",
      "[6000]\tvalid_0's auc: 0.898689\n",
      "[7000]\tvalid_0's auc: 0.899357\n",
      "[8000]\tvalid_0's auc: 0.899878\n",
      "[9000]\tvalid_0's auc: 0.900196\n",
      "[10000]\tvalid_0's auc: 0.900398\n",
      "[11000]\tvalid_0's auc: 0.900562\n",
      "[12000]\tvalid_0's auc: 0.900543\n",
      "[13000]\tvalid_0's auc: 0.900613\n",
      "[14000]\tvalid_0's auc: 0.900557\n",
      "[15000]\tvalid_0's auc: 0.900463\n",
      "Early stopping, best iteration is:\n",
      "[12932]\tvalid_0's auc: 0.900652\n",
      "Fold 8 started at Fri Mar 29 08:24:45 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.876869\n",
      "[2000]\tvalid_0's auc: 0.885286\n",
      "[3000]\tvalid_0's auc: 0.889462\n",
      "[4000]\tvalid_0's auc: 0.891978\n",
      "[5000]\tvalid_0's auc: 0.893281\n",
      "[6000]\tvalid_0's auc: 0.894416\n",
      "[7000]\tvalid_0's auc: 0.895056\n",
      "[8000]\tvalid_0's auc: 0.895625\n",
      "[9000]\tvalid_0's auc: 0.895904\n",
      "[10000]\tvalid_0's auc: 0.89616\n",
      "[11000]\tvalid_0's auc: 0.89624\n",
      "[12000]\tvalid_0's auc: 0.896114\n",
      "[13000]\tvalid_0's auc: 0.896034\n",
      "Early stopping, best iteration is:\n",
      "[10635]\tvalid_0's auc: 0.896373\n",
      "Fold 9 started at Fri Mar 29 08:25:52 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.889906\n",
      "[2000]\tvalid_0's auc: 0.897044\n",
      "[3000]\tvalid_0's auc: 0.900681\n",
      "[4000]\tvalid_0's auc: 0.902945\n",
      "[5000]\tvalid_0's auc: 0.904367\n",
      "[6000]\tvalid_0's auc: 0.905037\n",
      "[7000]\tvalid_0's auc: 0.90564\n",
      "[8000]\tvalid_0's auc: 0.905898\n",
      "[9000]\tvalid_0's auc: 0.906058\n",
      "[10000]\tvalid_0's auc: 0.906275\n",
      "[11000]\tvalid_0's auc: 0.906084\n",
      "[12000]\tvalid_0's auc: 0.906197\n",
      "[13000]\tvalid_0's auc: 0.906347\n",
      "[14000]\tvalid_0's auc: 0.906438\n",
      "[15000]\tvalid_0's auc: 0.906396\n",
      "[16000]\tvalid_0's auc: 0.906258\n",
      "[17000]\tvalid_0's auc: 0.905992\n",
      "Early stopping, best iteration is:\n",
      "[14052]\tvalid_0's auc: 0.906472\n",
      "CV mean score: 0.9007, std: 0.0028.\n",
      "var_0 0.9006765840539067 0.9006765840539067\n",
      "['var_0']\n",
      "-------------------------------------\n",
      "Fold 0 started at Fri Mar 29 08:27:25 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.882609\n",
      "[2000]\tvalid_0's auc: 0.890727\n",
      "[3000]\tvalid_0's auc: 0.89466\n",
      "[4000]\tvalid_0's auc: 0.896737\n",
      "[5000]\tvalid_0's auc: 0.898116\n",
      "[6000]\tvalid_0's auc: 0.899054\n",
      "[7000]\tvalid_0's auc: 0.89947\n",
      "[8000]\tvalid_0's auc: 0.899705\n",
      "[9000]\tvalid_0's auc: 0.899888\n",
      "[10000]\tvalid_0's auc: 0.900047\n",
      "[11000]\tvalid_0's auc: 0.900223\n",
      "[12000]\tvalid_0's auc: 0.900234\n",
      "[13000]\tvalid_0's auc: 0.900341\n",
      "[14000]\tvalid_0's auc: 0.900335\n",
      "[15000]\tvalid_0's auc: 0.900273\n",
      "Early stopping, best iteration is:\n",
      "[12824]\tvalid_0's auc: 0.900412\n",
      "Fold 1 started at Fri Mar 29 08:28:48 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.882641\n",
      "[2000]\tvalid_0's auc: 0.891298\n",
      "[3000]\tvalid_0's auc: 0.894479\n",
      "[4000]\tvalid_0's auc: 0.896294\n",
      "[5000]\tvalid_0's auc: 0.897687\n",
      "[6000]\tvalid_0's auc: 0.898165\n",
      "[7000]\tvalid_0's auc: 0.898646\n",
      "[8000]\tvalid_0's auc: 0.899173\n",
      "[9000]\tvalid_0's auc: 0.899291\n",
      "[10000]\tvalid_0's auc: 0.899629\n",
      "[11000]\tvalid_0's auc: 0.89976\n",
      "[12000]\tvalid_0's auc: 0.89977\n",
      "[13000]\tvalid_0's auc: 0.899795\n",
      "[14000]\tvalid_0's auc: 0.899782\n",
      "Early stopping, best iteration is:\n",
      "[11837]\tvalid_0's auc: 0.899849\n",
      "Fold 2 started at Fri Mar 29 08:30:04 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.887092\n",
      "[2000]\tvalid_0's auc: 0.894466\n",
      "[3000]\tvalid_0's auc: 0.897905\n",
      "[4000]\tvalid_0's auc: 0.900029\n",
      "[5000]\tvalid_0's auc: 0.901091\n",
      "[6000]\tvalid_0's auc: 0.901753\n",
      "[7000]\tvalid_0's auc: 0.902267\n",
      "[8000]\tvalid_0's auc: 0.902289\n",
      "[9000]\tvalid_0's auc: 0.902524\n",
      "[10000]\tvalid_0's auc: 0.902499\n",
      "[11000]\tvalid_0's auc: 0.902486\n",
      "[12000]\tvalid_0's auc: 0.902433\n",
      "Early stopping, best iteration is:\n",
      "[9631]\tvalid_0's auc: 0.902645\n",
      "Fold 3 started at Fri Mar 29 08:31:11 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884024\n",
      "[2000]\tvalid_0's auc: 0.89155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000]\tvalid_0's auc: 0.895037\n",
      "[4000]\tvalid_0's auc: 0.897461\n",
      "[5000]\tvalid_0's auc: 0.898794\n",
      "[6000]\tvalid_0's auc: 0.899651\n",
      "[7000]\tvalid_0's auc: 0.900218\n",
      "[8000]\tvalid_0's auc: 0.900477\n",
      "[9000]\tvalid_0's auc: 0.90062\n",
      "[10000]\tvalid_0's auc: 0.900781\n",
      "[11000]\tvalid_0's auc: 0.900667\n",
      "[12000]\tvalid_0's auc: 0.900594\n",
      "Early stopping, best iteration is:\n",
      "[9866]\tvalid_0's auc: 0.900839\n",
      "Fold 4 started at Fri Mar 29 08:32:27 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886168\n",
      "[2000]\tvalid_0's auc: 0.892854\n",
      "[3000]\tvalid_0's auc: 0.896133\n",
      "[4000]\tvalid_0's auc: 0.898036\n",
      "[5000]\tvalid_0's auc: 0.898884\n",
      "[6000]\tvalid_0's auc: 0.899421\n",
      "[7000]\tvalid_0's auc: 0.899996\n",
      "[8000]\tvalid_0's auc: 0.900357\n",
      "[9000]\tvalid_0's auc: 0.900361\n",
      "[10000]\tvalid_0's auc: 0.900344\n",
      "[11000]\tvalid_0's auc: 0.900121\n",
      "[12000]\tvalid_0's auc: 0.900231\n",
      "Early stopping, best iteration is:\n",
      "[9676]\tvalid_0's auc: 0.900403\n",
      "Fold 5 started at Fri Mar 29 08:33:51 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.885395\n",
      "[2000]\tvalid_0's auc: 0.892329\n",
      "[3000]\tvalid_0's auc: 0.896715\n",
      "[4000]\tvalid_0's auc: 0.898889\n",
      "[5000]\tvalid_0's auc: 0.900187\n",
      "[6000]\tvalid_0's auc: 0.90114\n",
      "[7000]\tvalid_0's auc: 0.901616\n",
      "[8000]\tvalid_0's auc: 0.901854\n",
      "[9000]\tvalid_0's auc: 0.902189\n",
      "[10000]\tvalid_0's auc: 0.902478\n",
      "[11000]\tvalid_0's auc: 0.90252\n",
      "[12000]\tvalid_0's auc: 0.902443\n",
      "[13000]\tvalid_0's auc: 0.902539\n",
      "[14000]\tvalid_0's auc: 0.902408\n",
      "[15000]\tvalid_0's auc: 0.902358\n",
      "[16000]\tvalid_0's auc: 0.902378\n",
      "Early stopping, best iteration is:\n",
      "[13181]\tvalid_0's auc: 0.90263\n",
      "Fold 6 started at Fri Mar 29 08:35:29 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.879671\n",
      "[2000]\tvalid_0's auc: 0.887407\n",
      "[3000]\tvalid_0's auc: 0.891016\n",
      "[4000]\tvalid_0's auc: 0.893371\n",
      "[5000]\tvalid_0's auc: 0.894713\n",
      "[6000]\tvalid_0's auc: 0.895329\n",
      "[7000]\tvalid_0's auc: 0.895644\n",
      "[8000]\tvalid_0's auc: 0.895923\n",
      "[9000]\tvalid_0's auc: 0.895905\n",
      "[10000]\tvalid_0's auc: 0.896001\n",
      "[11000]\tvalid_0's auc: 0.896095\n",
      "[12000]\tvalid_0's auc: 0.896094\n",
      "[13000]\tvalid_0's auc: 0.896044\n",
      "[14000]\tvalid_0's auc: 0.895966\n",
      "Early stopping, best iteration is:\n",
      "[11477]\tvalid_0's auc: 0.896202\n",
      "Fold 7 started at Fri Mar 29 08:36:45 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.883503\n",
      "[2000]\tvalid_0's auc: 0.891081\n",
      "[3000]\tvalid_0's auc: 0.89422\n",
      "[4000]\tvalid_0's auc: 0.896595\n",
      "[5000]\tvalid_0's auc: 0.897789\n",
      "[6000]\tvalid_0's auc: 0.898877\n",
      "[7000]\tvalid_0's auc: 0.899398\n",
      "[8000]\tvalid_0's auc: 0.899943\n",
      "[9000]\tvalid_0's auc: 0.900257\n",
      "[10000]\tvalid_0's auc: 0.900394\n",
      "[11000]\tvalid_0's auc: 0.900594\n",
      "[12000]\tvalid_0's auc: 0.90055\n",
      "[13000]\tvalid_0's auc: 0.900596\n",
      "[14000]\tvalid_0's auc: 0.90052\n",
      "[15000]\tvalid_0's auc: 0.900451\n",
      "Early stopping, best iteration is:\n",
      "[12870]\tvalid_0's auc: 0.900653\n",
      "Fold 8 started at Fri Mar 29 08:39:04 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.876805\n",
      "[2000]\tvalid_0's auc: 0.885219\n",
      "[3000]\tvalid_0's auc: 0.889451\n",
      "[4000]\tvalid_0's auc: 0.89206\n",
      "[5000]\tvalid_0's auc: 0.893367\n",
      "[6000]\tvalid_0's auc: 0.894545\n",
      "[7000]\tvalid_0's auc: 0.895292\n",
      "[8000]\tvalid_0's auc: 0.895825\n",
      "[9000]\tvalid_0's auc: 0.8961\n",
      "[10000]\tvalid_0's auc: 0.896365\n",
      "[11000]\tvalid_0's auc: 0.896456\n",
      "[12000]\tvalid_0's auc: 0.896293\n",
      "[13000]\tvalid_0's auc: 0.896253\n",
      "Early stopping, best iteration is:\n",
      "[10628]\tvalid_0's auc: 0.896593\n",
      "Fold 9 started at Fri Mar 29 08:40:31 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.889864\n",
      "[2000]\tvalid_0's auc: 0.897037\n",
      "[3000]\tvalid_0's auc: 0.900701\n",
      "[4000]\tvalid_0's auc: 0.902932\n",
      "[5000]\tvalid_0's auc: 0.90419\n",
      "[6000]\tvalid_0's auc: 0.904965\n",
      "[7000]\tvalid_0's auc: 0.905608\n",
      "[8000]\tvalid_0's auc: 0.906002\n",
      "[9000]\tvalid_0's auc: 0.906165\n",
      "[10000]\tvalid_0's auc: 0.906396\n",
      "[11000]\tvalid_0's auc: 0.906295\n",
      "[12000]\tvalid_0's auc: 0.906409\n",
      "[13000]\tvalid_0's auc: 0.906559\n",
      "[14000]\tvalid_0's auc: 0.9067\n",
      "[15000]\tvalid_0's auc: 0.906741\n",
      "[16000]\tvalid_0's auc: 0.906589\n",
      "[17000]\tvalid_0's auc: 0.906349\n",
      "[18000]\tvalid_0's auc: 0.906233\n",
      "Early stopping, best iteration is:\n",
      "[15113]\tvalid_0's auc: 0.906794\n",
      "CV mean score: 0.9007, std: 0.0029.\n",
      "var_0 0.9007020329769286 0.9007020329769286\n",
      "['var_0', 'var_0']\n",
      "-------------------------------------\n",
      "Fold 0 started at Fri Mar 29 08:42:31 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.882646\n",
      "[2000]\tvalid_0's auc: 0.890764\n",
      "[3000]\tvalid_0's auc: 0.894631\n",
      "[4000]\tvalid_0's auc: 0.896753\n",
      "[5000]\tvalid_0's auc: 0.898106\n",
      "[6000]\tvalid_0's auc: 0.899014\n",
      "[7000]\tvalid_0's auc: 0.899508\n",
      "[8000]\tvalid_0's auc: 0.899801\n",
      "[9000]\tvalid_0's auc: 0.89994\n",
      "[10000]\tvalid_0's auc: 0.900084\n",
      "[11000]\tvalid_0's auc: 0.900204\n",
      "[12000]\tvalid_0's auc: 0.900226\n",
      "[13000]\tvalid_0's auc: 0.900286\n",
      "[14000]\tvalid_0's auc: 0.900277\n",
      "[15000]\tvalid_0's auc: 0.900124\n",
      "Early stopping, best iteration is:\n",
      "[12597]\tvalid_0's auc: 0.900347\n",
      "Fold 1 started at Fri Mar 29 08:44:10 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.882586\n",
      "[2000]\tvalid_0's auc: 0.891342\n",
      "[3000]\tvalid_0's auc: 0.894547\n",
      "[4000]\tvalid_0's auc: 0.896331\n",
      "[5000]\tvalid_0's auc: 0.897641\n",
      "[6000]\tvalid_0's auc: 0.89817\n",
      "[7000]\tvalid_0's auc: 0.898675\n",
      "[8000]\tvalid_0's auc: 0.899136\n",
      "[9000]\tvalid_0's auc: 0.89932\n",
      "[10000]\tvalid_0's auc: 0.899633\n",
      "[11000]\tvalid_0's auc: 0.899788\n",
      "[12000]\tvalid_0's auc: 0.899819\n",
      "[13000]\tvalid_0's auc: 0.89982\n",
      "[14000]\tvalid_0's auc: 0.899818\n",
      "[15000]\tvalid_0's auc: 0.899858\n",
      "Early stopping, best iteration is:\n",
      "[12474]\tvalid_0's auc: 0.899902\n",
      "Fold 2 started at Fri Mar 29 08:45:51 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88705\n",
      "[2000]\tvalid_0's auc: 0.894539\n",
      "[3000]\tvalid_0's auc: 0.897984\n",
      "[4000]\tvalid_0's auc: 0.900071\n",
      "[5000]\tvalid_0's auc: 0.901094\n",
      "[6000]\tvalid_0's auc: 0.901883\n",
      "[7000]\tvalid_0's auc: 0.902388\n",
      "[8000]\tvalid_0's auc: 0.902359\n",
      "[9000]\tvalid_0's auc: 0.90251\n",
      "[10000]\tvalid_0's auc: 0.902502\n",
      "[11000]\tvalid_0's auc: 0.902507\n",
      "[12000]\tvalid_0's auc: 0.902483\n",
      "Early stopping, best iteration is:\n",
      "[9631]\tvalid_0's auc: 0.902621\n",
      "Fold 3 started at Fri Mar 29 08:46:58 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.884026\n",
      "[2000]\tvalid_0's auc: 0.89154\n",
      "[3000]\tvalid_0's auc: 0.894992\n",
      "[4000]\tvalid_0's auc: 0.897419\n",
      "[5000]\tvalid_0's auc: 0.898681\n",
      "[6000]\tvalid_0's auc: 0.899522\n",
      "[7000]\tvalid_0's auc: 0.90008\n",
      "[8000]\tvalid_0's auc: 0.900437\n",
      "[9000]\tvalid_0's auc: 0.900633\n",
      "[10000]\tvalid_0's auc: 0.900787\n",
      "[11000]\tvalid_0's auc: 0.900627\n",
      "[12000]\tvalid_0's auc: 0.900536\n",
      "Early stopping, best iteration is:\n",
      "[9864]\tvalid_0's auc: 0.900827\n",
      "Fold 4 started at Fri Mar 29 08:48:09 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.886031\n",
      "[2000]\tvalid_0's auc: 0.892768\n",
      "[3000]\tvalid_0's auc: 0.896078\n",
      "[4000]\tvalid_0's auc: 0.89804\n",
      "[5000]\tvalid_0's auc: 0.898936\n",
      "[6000]\tvalid_0's auc: 0.899498\n",
      "[7000]\tvalid_0's auc: 0.900079\n",
      "[8000]\tvalid_0's auc: 0.900385\n",
      "[9000]\tvalid_0's auc: 0.900502\n",
      "[10000]\tvalid_0's auc: 0.90044\n",
      "[11000]\tvalid_0's auc: 0.900197\n",
      "[12000]\tvalid_0's auc: 0.900234\n",
      "Early stopping, best iteration is:\n",
      "[9676]\tvalid_0's auc: 0.900536\n",
      "Fold 5 started at Fri Mar 29 08:49:16 2019\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\tvalid_0's auc: 0.88542\n",
      "[2000]\tvalid_0's auc: 0.892362\n",
      "[3000]\tvalid_0's auc: 0.896739\n",
      "[4000]\tvalid_0's auc: 0.898987\n",
      "[5000]\tvalid_0's auc: 0.900351\n",
      "[6000]\tvalid_0's auc: 0.90126\n",
      "[7000]\tvalid_0's auc: 0.901852\n",
      "[8000]\tvalid_0's auc: 0.902097\n",
      "[9000]\tvalid_0's auc: 0.902474\n",
      "[10000]\tvalid_0's auc: 0.902684\n",
      "[11000]\tvalid_0's auc: 0.902776\n",
      "[12000]\tvalid_0's auc: 0.902768\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-625630aecc03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mX4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     oof_lgb, prediction_lgb, scores = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n\u001b[0;32m---> 22\u001b[0;31m                                          model_type='lgb', plot_feature_importance=False)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcurr_valid_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-901c66bfdb94>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, averaging, model)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 early_stopping_rounds=3000)\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;31m#             y_pred_valid = model.predict(X_valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m#             y_pred = model.predict(X_test, num_iteration=model.best_iteration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sa/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X3 = train.copy()\n",
    "X4 = test.copy()\n",
    "# bins = np.linspace(-6.0,6.0,40)\n",
    "X3 = X3.drop(['ID_code', 'target'], axis=1)\n",
    "X4 = X4.drop('ID_code', axis=1)\n",
    "cols = ['var_'+str(i) for i in range(200)]\n",
    "best_valid_auc = 0.9006765840539067\n",
    "cols_to_transform = ['var_0']\n",
    "done = ['var_0']\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "for col in cols:\n",
    "    if col in done:\n",
    "        continue\n",
    "    qt = QuantileTransformer(n_quantiles=1000, output_distribution='normal', random_state=0)\n",
    "    X3[col] = qt.fit_transform(X3[col].reshape(-1,1)) \n",
    "    qt = QuantileTransformer(n_quantiles=1000, output_distribution='normal', random_state=0)\n",
    "    X4[col] = qt.fit_transform(X4[col].reshape(-1,1)) \n",
    "    \n",
    "    for c in cols_to_transform:\n",
    "        qt = QuantileTransformer(n_quantiles=1000, output_distribution='normal', random_state=0)\n",
    "        X3[c] = qt.fit_transform(X3[c].reshape(-1,1)) \n",
    "        qt = QuantileTransformer(n_quantiles=1000, output_distribution='normal', random_state=0)\n",
    "        X4[c] = qt.fit_transform(X4[c].reshape(-1,1))\n",
    "        \n",
    "    oof_lgb, prediction_lgb, scores = train_model(X3, X4, y, params=np.random.randint(1,101), folds=folds, \n",
    "                                         model_type='lgb', plot_feature_importance=False)\n",
    "    \n",
    "    curr_valid_auc = np.mean(scores)\n",
    "    if curr_valid_auc > best_valid_auc:\n",
    "        best_valid_auc = curr_valid_auc\n",
    "        cols_to_transform.append(col)\n",
    "    print(col, curr_valid_auc, best_valid_auc)\n",
    "    print(cols_to_transform)\n",
    "    print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa",
   "language": "python",
   "name": "sa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
